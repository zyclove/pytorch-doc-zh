{"./":{"url":"./","title":"Introduction","keywords":"","body":"PyTorch 1.2 中文文档 & 教程 PyTorch 是一个针对深度学习, 并且使用 GPU 和 CPU 来优化的 tensor library (张量库) 正在校验: 1.2 中文版本 1.0 中文版本 最新 英文教程 最新 英文文档 0.4 中文版本 0.3 中文版本 0.2 中文版本 欢迎任何人参与和完善：一个人可以走的很快，但是一群人却可以走的更远。 在线阅读 ApacheCN 学习资源 PyTorch 中文翻译组 | ApacheCN 713436582 目录结构 Introduction 中文教程 入门 PyTorch 深度学习: 60 分钟极速入门 什么是PyTorch？ Autograd：自动求导 神经网络 训练分类器 可选: 数据并行处理 数据加载和处理教程 用例子学习 PyTorch 迁移学习教程 利用 TorchScript 部署 Seq2Seq 模型 使用 TensorBoard 可视化模型，数据和训练 保存和加载模型 torch.nn 到底是什么？ 图片 TorchVision 对象检测微调教程 微调Torchvision模型 空间变压器网络教程 使用PyTorch进行神经网络传递 对抗性示例生成 DCGAN教程 音频 torchaudio教程 文本 NLP From Scratch: 使用char-RNN对姓氏进行分类 NLP From Scratch: 生成名称与字符级RNN NLP From Scratch: 基于注意力机制的 seq2seq 神经网络翻译 文本分类与TorchText 语言翻译与TorchText 序列到序列与nn.Transformer和TorchText建模 强化学习 强化学习（DQN）教程 在生产部署PyTorch模型 部署PyTorch在Python经由REST API从Flask 介绍TorchScript 在C ++中加载TorchScript模型 （可选）将模型从PyTorch导出到ONNX并使用ONNX Runtime运行 并行和分布式训练 模型并行化最佳实践 入门分布式数据并行 PyTorch编写分布式应用 （高级）PyTorch 1.0分布式训练与Amazon AWS 扩展PyTorch 使用自定义 C++ 扩展算TorchScript 用 numpy 和 scipy 创建扩展 自定义 C++ 和CUDA扩展 PyTorch在其他语言 使用PyTorch C++ 前端 中文文档 注解 自动求导机制 广播语义 CPU线程和TorchScript推理 CUDA语义 扩展PyTorch 常见问题 对于大规模部署的特点 多处理最佳实践 重复性 序列化语义 Windows 常见问题 社区 PyTorch贡献说明书 PyTorch治理 PyTorch治理感兴趣的人 封装参考文献 torch torch.Tensor Tensor Attributes Type Info torch.sparse torch.cuda torch.Storage torch.nn torch.nn.functional torch.nn.init torch.optim torch.autograd torch.distributed torch.distributions torch.hub torch.jit torch.multiprocessing torch.random torch.utils.bottleneck torch.utils.checkpoint torch.utils.cpp_extension torch.utils.data torch.utils.dlpack torch.utils.model_zoo torch.utils.tensorboard torch.onnx torch.__ config__ torchvision Reference torchvision torchaudio Reference torchaudio torchtext Reference torchtext 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/deep_learning_60min_blitz.html":{"url":"beginner/deep_learning_60min_blitz.html","title":"PyTorch 深度学习: 60 分钟极速入门","keywords":"","body":"PyTorch 深度学习: 60 分钟极速入门 作者：Soumith Chintala 译者：bat67、Foxerlee 校验：Foxerlee 此教程的目标： 在高层上理解 PyTorch 的 Tensor 库以及神经网络。 训练一个可用于分类图像的简单神经网络。 本教程假设你对 numpy 有基本的了解 注意 确保你安装了 torch 和 torchvision 包。 PyTorch 是什么？ Autograd：自动求导 神经网络 训练分类器 可选：数据并行 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/blitz/tensor_tutorial.html":{"url":"beginner/blitz/tensor_tutorial.html","title":"什么是PyTorch？","keywords":"","body":"什么是PyTorch？ 译者：bat67 校对者：FontTian 作者： Soumith Chintala PyTorch是一个基于python的科学计算包，主要针对两类人群： 作为NumPy的替代品，可以利用GPU的性能进行计算 作为一个高灵活性、速度快的深度学习平台 入门 张量 Tensor（张量）类似于NumPy的ndarray，但还可以在GPU上使用来加速计算。 from __future__ import print_function import torch 创建一个没有初始化的5*3矩阵： x = torch.empty(5, 3) print(x) 输出： tensor([[2.2391e-19, 4.5869e-41, 1.4191e-17], [4.5869e-41, 0.0000e+00, 0.0000e+00], [0.0000e+00, 0.0000e+00, 0.0000e+00], [0.0000e+00, 0.0000e+00, 0.0000e+00], [0.0000e+00, 0.0000e+00, 0.0000e+00]]) 创建一个随机初始化矩阵： x = torch.rand(5, 3) print(x) 输出： tensor([[0.5307, 0.9752, 0.5376], [0.2789, 0.7219, 0.1254], [0.6700, 0.6100, 0.3484], [0.0922, 0.0779, 0.2446], [0.2967, 0.9481, 0.1311]]) 构造一个填满0且数据类型为long的矩阵: x = torch.zeros(5, 3, dtype=torch.long) print(x) 输出： tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]) 直接从数据构造张量： x = torch.tensor([5.5, 3]) print(x) 输出： tensor([5.5000, 3.0000]) 或者根据已有的tensor建立新的tensor。除非用户提供新的值，否则这些方法将重用输入张量的属性，例如dtype等： x = x.new_ones(5, 3, dtype=torch.double) # new_* methods take in sizes print(x) x = torch.randn_like(x, dtype=torch.float) # 重载 dtype! print(x) # 结果size一致 输出： tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], dtype=torch.float64) tensor([[ 1.6040, -0.6769, 0.0555], [ 0.6273, 0.7683, -0.2838], [-0.7159, -0.5566, -0.2020], [ 0.6266, 0.3566, 1.4497], [-0.8092, -0.6741, 0.0406]]) 获取张量的形状： print(x.size()) 输出： torch.Size([5, 3]) 注意： torch.Size本质上还是tuple，所以支持tuple的一切操作。 运算 一种运算有多种语法。在下面的示例中，我们将研究加法运算。 加法：形式一 y = torch.rand(5, 3) print(x + y) 输出： tensor([[ 2.5541, 0.0943, 0.9835], [ 1.4911, 1.3117, 0.5220], [-0.0078, -0.1161, 0.6687], [ 0.8176, 1.1179, 1.9194], [-0.3251, -0.2236, 0.7653]]) 加法：形式二 print(torch.add(x, y)) 输出： tensor([[ 2.5541, 0.0943, 0.9835], [ 1.4911, 1.3117, 0.5220], [-0.0078, -0.1161, 0.6687], [ 0.8176, 1.1179, 1.9194], [-0.3251, -0.2236, 0.7653]]) 加法：给定一个输出张量作为参数 result = torch.empty(5, 3) torch.add(x, y, out=result) print(result) 输出： tensor([[ 2.5541, 0.0943, 0.9835], [ 1.4911, 1.3117, 0.5220], [-0.0078, -0.1161, 0.6687], [ 0.8176, 1.1179, 1.9194], [-0.3251, -0.2236, 0.7653]]) 加法：原位/原地操作（in-place） # adds x to y y.add_(x) print(y) 输出： tensor([[ 2.5541, 0.0943, 0.9835], [ 1.4911, 1.3117, 0.5220], [-0.0078, -0.1161, 0.6687], [ 0.8176, 1.1179, 1.9194], [-0.3251, -0.2236, 0.7653]]) 注意： 任何一个in-place改变张量的操作后面都固定一个_。例如x.copy_(y)、x.t_()将更改x 也可以使用像标准的NumPy一样的各种索引操作： print(x[:, 1]) 输出： tensor([-0.6769, 0.7683, -0.5566, 0.3566, -0.6741]) 改变形状：如果想改变形状，可以使用torch.view x = torch.randn(4, 4) y = x.view(16) z = x.view(-1, 8) # the size -1 is inferred from other dimensions print(x.size(), y.size(), z.size()) 输出： torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) 如果是仅包含一个元素的tensor，可以使用.item()来得到对应的python数值 x = torch.randn(1) print(x) print(x.item()) 输出： tensor([0.0445]) 0.0445479191839695 后续阅读： 超过100种tensor的运算操作，包括转置，索引，切片，数学运算， 线性代数，随机数等，具体访问这里 桥接 NumPy 将一个Torch张量转换为一个NumPy数组是轻而易举的事情，反之亦然。 Torch张量和NumPy数组将共享它们的底层内存位置，因此当一个改变时,另外也会改变。 将torch的Tensor转化为NumPy数组 输入： a = torch.ones(5) print(a) 输出： tensor([1., 1., 1., 1., 1.]) 输入： b = a.numpy() print(b) 输出： [1. 1. 1. 1. 1.] 看NumPy数组是如何改变里面的值的： a.add_(1) print(a) print(b) 输出： tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.] 将NumPy数组转化为Torch张量 看改变NumPy数组是如何自动改变Torch张量的： import numpy as np a = np.ones(5) b = torch.from_numpy(a) np.add(a, 1, out=a) print(a) print(b) 输出： [2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64) CPU上的所有张量(CharTensor除外)都支持与Numpy的相互转换。 CUDA上的张量 张量可以使用.to方法移动到任何设备（device）上： # 当GPU可用时,我们可以运行以下代码 # 我们将使用`torch.device`来将tensor移入和移出GPU if torch.cuda.is_available(): device = torch.device(\"cuda\") # a CUDA device object y = torch.ones_like(x, device=device) # 直接在GPU上创建tensor x = x.to(device) # 或者使用`.to(\"cuda\")`方法 z = x + y print(z) print(z.to(\"cpu\", torch.double)) # `.to`也能在移动时改变dtype 输出： tensor([1.0445], device='cuda:0') tensor([1.0445], dtype=torch.float64) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/blitz/autograd_tutorial.html":{"url":"beginner/blitz/autograd_tutorial.html","title":"Autograd：自动求导","keywords":"","body":"Autograd：自动求导 译者：bat67 校对者：FontTian PyTorch中，所有神经网络的核心是 autograd 包。先简单介绍一下这个包，然后训练我们的第一个的神经网络。 autograd 包为张量上的所有操作提供了自动求导机制。它是一个在运行时定义（define-by-run）的框架，这意味着反向传播是根据代码如何运行来决定的，并且每次迭代可以是不同的. 让我们用一些简单的例子来看看吧。 张量 torch.Tensor 是这个包的核心类。如果设置它的属性 .requires_grad 为 True，那么它将会追踪对于该张量的所有操作。当完成计算后可以通过调用 .backward()，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到.grad属性. 要阻止一个张量被跟踪历史，可以调用 .detach() 方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。 为了防止跟踪历史记录（和使用内存），可以将代码块包装在 with torch.no_grad(): 中。在评估模型时特别有用，因为模型可能具有 requires_grad = True 的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。 还有一个类对于autograd的实现非常重要：Function。 Tensor 和 Function 互相连接生成了一个无圈图(acyclic graph)，它编码了完整的计算历史。每个张量都有一个 .grad_fn 属性，该属性引用了创建 Tensor 自身的Function（除非这个张量是用户手动创建的，即这个张量的 grad_fn 是 None ）。 如果需要计算导数，可以在 Tensor 上调用 .backward()。如果 Tensor 是一个标量（即它包含一个元素的数据），则不需要为 backward() 指定任何参数，但是如果它有更多的元素，则需要指定一个 gradient 参数，该参数是形状匹配的张量。 import torch 创建一个张量并设置requires_grad=True用来追踪其计算历史 x = torch.ones(2, 2, requires_grad=True) print(x) 输出： tensor([[1., 1.], [1., 1.]], requires_grad=True) 对这个张量做一次运算： y = x + 2 print(y) 输出： tensor([[3., 3.], [3., 3.]], grad_fn=) y是计算的结果，所以它有grad_fn属性。 print(y.grad_fn) 输出： 对y进行更多操作 z = y * y * 3 out = z.mean() print(z, out) 输出： tensor([[27., 27.], [27., 27.]], grad_fn=) tensor(27., grad_fn=) .requires_grad_(...) 原地改变了现有张量的 requires_grad 标志。如果没有指定的话，默认输入的这个标志是 False。 a = torch.randn(2, 2) a = ((a * 3) / (a - 1)) print(a.requires_grad) a.requires_grad_(True) print(a.requires_grad) b = (a * a).sum() print(b.grad_fn) 输出： False True 梯度 现在开始进行反向传播，因为 out 是一个标量，因此 out.backward() 和 out.backward(torch.tensor(1.)) 等价。 out.backward() 输出导数 d(out)/dx print(x.grad) 输出： tensor([[4.5000, 4.5000], [4.5000, 4.5000]]) 我们的得到的是一个数取值全部为4.5的矩阵。 让我们来调用 out 张量 “o”。 就可以得到 o = \\frac{1}{4}\\sum_i z_i，z_i = 3(x_i+2)^2 和 z_i\\bigr\\rvert_{x_i=1} = 27 因此, \\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)，因而 \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5。 数学上，若有向量值函数 \\vec{y}=f(\\vec{x})，那么 \\vec{y} 相对于 \\vec{x} 的梯度是一个雅可比矩阵： J=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right) 通常来说，torch.autograd 是计算雅可比向量积的一个“引擎”。也就是说，给定任意向量 v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T}，计算乘积 v^{T}\\cdot J。如果 v 恰好是一个标量函数 l=g\\left(\\vec{y}\\right) 的导数，即 v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}，那么根据链式法则，雅可比向量积应该是 l 对 \\vec{x} 的导数： J^{T}\\cdot v=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial y_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial y_{m}} \\end{array}\\right)=\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial x_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial x_{n}} \\end{array}\\right) （注意：行向量的 v^{T}\\cdot J也可以被视作列向量的J^{T}\\cdot v) 雅可比向量积的这一特性使得将外部梯度输入到具有非标量输出的模型中变得非常方便。 现在我们来看一个雅可比向量积的例子: x = torch.randn(3, requires_grad=True) y = x * 2 while y.data.norm() 输出： tensor([-278.6740, 935.4016, 439.6572], grad_fn=) 在这种情况下，y 不再是标量。torch.autograd 不能直接计算完整的雅可比矩阵，但是如果我们只想要雅可比向量积，只需将这个向量作为参数传给 backward： v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float) y.backward(v) print(x.grad) 输出： tensor([4.0960e+02, 4.0960e+03, 4.0960e-01]) 也可以通过将代码块包装在 with torch.no_grad(): 中，来阻止autograd跟踪设置了 .requires_grad=True 的张量的历史记录。 print(x.requires_grad) print((x ** 2).requires_grad) with torch.no_grad(): print((x ** 2).requires_grad) 输出： True True False 后续阅读： autograd 和 Function 的文档见：https://pytorch.org/docs/autograd 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/blitz/neural_networks_tutorial.html":{"url":"beginner/blitz/neural_networks_tutorial.html","title":"神经网络","keywords":"","body":"神经网络 译者：bat67 校对者：FontTian 可以使用torch.nn包来构建神经网络. 我们已经介绍了autograd，nn包则依赖于autograd包来定义模型并对它们求导。一个nn.Module包含各个层和一个forward(input)方法，该方法返回output。 例如，下面这个神经网络可以对数字进行分类： 这是一个简单的前馈神经网络（feed-forward network）。它接受一个输入，然后将它送入下一层，一层接一层的传递，最后给出输出。 一个神经网络的典型训练过程如下： 定义包含一些可学习参数（或者叫权重）的神经网络 在输入数据集上迭代 通过网络处理输入 计算损失（输出和正确答案的距离） 将梯度反向传播给网络的参数 更新网络的权重，一般使用一个简单的规则：weight = weight - learning_rate * gradient 定义网络 让我们定义这样一个网络： import torch import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net, self).__init__() # 输入图像channel：1；输出channel：6；5x5卷积核 self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # an affine operation: y = Wx + b self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # 2x2 Max pooling x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # 如果是方阵,则可以只使用一个数字进行定义 x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # 除去批处理维度的其他所有维度 num_features = 1 for s in size: num_features *= s return num_features net = Net() print(net) 输出： Net( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) 我们只需要定义 forward 函数，backward函数会在使用autograd时自动定义，backward函数用来计算导数。可以在 forward 函数中使用任何针对张量的操作和计算。 一个模型的可学习参数可以通过net.parameters()返回 params = list(net.parameters()) print(len(params)) print(params[0].size()) # conv1's .weight 输出： 10 torch.Size([6, 1, 5, 5]) 让我们尝试一个随机的32x32的输入。注意:这个网络（LeNet）的期待输入是32x32。如果使用MNIST数据集来训练这个网络，要把图片大小重新调整到32x32。 input = torch.randn(1, 1, 32, 32) out = net(input) print(out) 输出： tensor([[ 0.0399, -0.0856, 0.0668, 0.0915, 0.0453, -0.0680, -0.1024, 0.0493, -0.1043, -0.1267]], grad_fn=) 清零所有参数的梯度缓存，然后进行随机梯度的反向传播： net.zero_grad() out.backward(torch.randn(1, 10)) 注意： torch.nn只支持小批量处理（mini-batches）。整个torch.nn包只支持小批量样本的输入，不支持单个样本。 比如，nn.Conv2d 接受一个4维的张量，即nSamples x nChannels x Height x Width 如果是一个单独的样本，只需要使用input.unsqueeze(0)来添加一个“假的”批大小维度。 在继续之前，让我们回顾一下到目前为止看到的所有类。 复习： torch.Tensor - 一个多维数组，支持诸如backward()等的自动求导操作，同时也保存了张量的梯度。 nn.Module - 神经网络模块。是一种方便封装参数的方式，具有将参数移动到GPU、导出、加载等功能。 nn.Parameter - 张量的一种，当它作为一个属性分配给一个Module时，它会被自动注册为一个参数。 autograd.Function - 实现了自动求导前向和反向传播的定义，每个Tensor至少创建一个Function节点，该节点连接到创建Tensor的函数并对其历史进行编码。 目前为止，我们讨论了： 定义一个神经网络 处理输入调用backward 还剩下： 计算损失 更新网络权重 损失函数 一个损失函数接受一对(output, target)作为输入，计算一个值来估计网络的输出和目标值相差多少。 nn包中有很多不同的损失函数。nn.MSELoss是比较简单的一种，它计算输出和目标的均方误差（mean-squared error）。 例如： output = net(input) target = torch.randn(10) # 本例子中使用模拟数据 target = target.view(1, -1) # 使目标值与数据值形状一致 criterion = nn.MSELoss() loss = criterion(output, target) print(loss) 输出： tensor(1.0263, grad_fn=) 现在，如果使用loss的.grad_fn属性跟踪反向传播过程，会看到计算图如下： input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> view -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss 所以，当我们调用loss.backward()，整张图开始关于loss微分，图中所有设置了requires_grad=True的张量的.grad属性累积着梯度张量。 为了说明这一点，让我们向后跟踪几步： print(loss.grad_fn) # MSELoss print(loss.grad_fn.next_functions[0][0]) # Linear print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # ReLU 输出： 反向传播 我们只需要调用loss.backward()来反向传播权重。我们需要清零现有的梯度，否则梯度将会与已有的梯度累加。 现在，我们将调用loss.backward()，并查看conv1层的偏置（bias）在反向传播前后的梯度。 net.zero_grad() # 清零所有参数（parameter）的梯度缓存 print('conv1.bias.grad before backward') print(net.conv1.bias.grad) loss.backward() print('conv1.bias.grad after backward') print(net.conv1.bias.grad) 输出： conv1.bias.grad before backward tensor([0., 0., 0., 0., 0., 0.]) conv1.bias.grad after backward tensor([ 0.0084, 0.0019, -0.0179, -0.0212, 0.0067, -0.0096]) 现在，我们已经见到了如何使用损失函数。 稍后阅读 神经网络包包含了各种模块和损失函数，这些模块和损失函数构成了深度神经网络的构建模块。完整的文档列表见这里。 现在唯一要学习的是： 更新网络的权重 更新权重 最简单的更新规则是随机梯度下降法（SGD）: weight = weight - learning_rate * gradient 我们可以使用简单的python代码来实现: learning_rate = 0.01 for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) 然而，在使用神经网络时，可能希望使用各种不同的更新规则，如SGD、Nesterov-SGD、Adam、RMSProp等。为此，我们构建了一个较小的包torch.optim，它实现了所有的这些方法。使用它很简单： import torch.optim as optim # 创建优化器（optimizer） optimizer = optim.SGD(net.parameters(), lr=0.01) # 在训练的迭代中： optimizer.zero_grad() # 清零梯度缓存 output = net(input) loss = criterion(output, target) loss.backward() optimizer.step() # 更新参数 注意： 观察梯度缓存区是如何使用optimizer.zero_grad()手动清零的。这是因为梯度是累加的，正如前面反向传播章节叙述的那样。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/blitz/cifar10_tutorial.html":{"url":"beginner/blitz/cifar10_tutorial.html","title":"训练分类器","keywords":"","body":"训练分类器 译者：bat67 校对者：FontTian 目前为止，我们以及看到了如何定义网络，计算损失，并更新网络的权重。所以你现在可能会想, 数据应该怎么办呢？ 通常来说，当必须处理图像、文本、音频或视频数据时，可以使用python标准库将数据加载到numpy数组里。然后将这个数组转化成torch.*Tensor。 对于图片，有Pillow，OpenCV等包可以使用 对于音频，有scipy和librosa等包可以使用 对于文本，不管是原生python的或者是基于Cython的文本，可以使用NLTK和SpaCy 特别对于视觉方面，我们创建了一个包，名字叫torchvision，其中包含了针对Imagenet、CIFAR10、MNIST等常用数据集的数据加载器（data loaders），还有对图片数据变形的操作，即torchvision.datasets和torch.utils.data.DataLoader。 这提供了极大的便利，可以避免编写样板代码。 在这个教程中，我们将使用CIFAR10数据集，它有如下的分类：“飞机”，“汽车”，“鸟”，“猫”，“鹿”，“狗”，“青蛙”，“马”，“船”，“卡车”等。在CIFAR-10里面的图片数据大小是3x32x32，即三通道彩色图，图片大小是32x32像素。 训练一个图片分类器 我们将按顺序做以下步骤： 通过torchvision加载CIFAR10里面的训练和测试数据集，并对数据进行标准化 定义卷积神经网络 定义损失函数 利用训练数据训练网络 利用测试数据测试网络 1.加载并标准化CIFAR10 使用torchvision加载CIFAR10超级简单。 import torch import torchvision import torchvision.transforms as transforms torchvision数据集加载完后的输出是范围在[0, 1]之间的PILImage。我们将其标准化为范围在[-1, 1]之间的张量。 transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2) classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') 输出： Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz Files already downloaded and verified 乐趣所致，现在让我们可视化部分训练数据。 import matplotlib.pyplot as plt import numpy as np # 输出图像的函数 def imshow(img): img = img / 2 + 0.5 # unnormalize npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) plt.show() # 随机获取训练图片 dataiter = iter(trainloader) images, labels = dataiter.next() # 显示图片 imshow(torchvision.utils.make_grid(images)) # 打印图片标签 print(' '.join('%5s' % classes[labels[j]] for j in range(4))) 输出： horse horse horse car 2.定义卷积神经网络 将之前神经网络章节定义的神经网络拿过来，并将其修改成输入为3通道图像（替代原来定义的单通道图像）。 import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() 3.定义损失函数和优化器 我们使用分类的交叉熵损失和随机梯度下降（使用momentum）。 import torch.optim as optim criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) 4.训练网络 事情开始变得有趣了。我们只需要遍历我们的数据迭代器，并将输入“喂”给网络和优化函数。 for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print('Finished Training') 输出： [1, 2000] loss: 2.182 [1, 4000] loss: 1.819 [1, 6000] loss: 1.648 [1, 8000] loss: 1.569 [1, 10000] loss: 1.511 [1, 12000] loss: 1.473 [2, 2000] loss: 1.414 [2, 4000] loss: 1.365 [2, 6000] loss: 1.358 [2, 8000] loss: 1.322 [2, 10000] loss: 1.298 [2, 12000] loss: 1.282 Finished Training 5.使用测试数据测试网络 我们已经在训练集上训练了2遍网络。但是我们需要检查网络是否学到了一些东西。 我们将通过预测神经网络输出的标签来检查这个问题，并和正确样本进行（ground-truth）对比。如果预测是正确的，我们将样本添加到正确预测的列表中。 ok，第一步。让我们显示测试集中的图像来熟悉一下。 dataiter = iter(testloader) images, labels = dataiter.next() # 输出图片 imshow(torchvision.utils.make_grid(images)) print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4))) GroundTruth: cat ship ship plane ok，现在让我们看看神经网络认为上面的例子是: outputs = net(images) 输出是10个类别的量值。一个类的值越高，网络就越认为这个图像属于这个特定的类。让我们得到最高量值的下标/索引； _, predicted = torch.max(outputs, 1) print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(4))) 输出： Predicted: dog ship ship plane 结果还不错。 让我们看看网络在整个数据集上表现的怎么样。 correct = 0 total = 0 with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print('Accuracy of the network on the 10000 test images: %d %%' % ( 100 * correct / total)) 输出： Accuracy of the network on the 10000 test images: 55 % 这比随机选取（即从10个类中随机选择一个类，正确率是10%）要好很多。看来网络确实学到了一些东西。 那么哪些是表现好的类呢？哪些是表现的差的类呢？ class_correct = list(0. for i in range(10)) class_total = list(0. for i in range(10)) with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs, 1) c = (predicted == labels).squeeze() for i in range(4): label = labels[i] class_correct[label] += c[i].item() class_total[label] += 1 for i in range(10): print('Accuracy of %5s : %2d %%' % ( classes[i], 100 * class_correct[i] / class_total[i])) 输出： Accuracy of plane : 70 % Accuracy of car : 70 % Accuracy of bird : 28 % Accuracy of cat : 25 % Accuracy of deer : 37 % Accuracy of dog : 60 % Accuracy of frog : 66 % Accuracy of horse : 62 % Accuracy of ship : 69 % Accuracy of truck : 61 % ok，接下来呢？ 怎么在GPU上运行神经网络呢？ 在GPU上训练 与将一个张量传递给GPU一样，可以这样将神经网络转移到GPU上。 如果我们有cuda可用的话，让我们首先定义第一个设备为可见cuda设备： device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Assuming that we are on a CUDA machine, this should print a CUDA device: print(device) 输出： cuda:0 本节的其余部分假设device是CUDA。 然后这些方法将递归遍历所有模块，并将它们的参数和缓冲区转换为CUDA张量： net.to(device) 请记住，我们不得不将输入和目标在每一步都送入GPU： inputs, labels = inputs.to(device), labels.to(device) 为什么我们感受不到与CPU相比的巨大加速？因为我们的网络实在是太小了。 尝试一下：加宽你的网络（注意第一个nn.Conv2d的第二个参数和第二个nn.Conv2d的第一个参数要相同），看看能获得多少加速。 已实现的目标： 在更高层次上理解PyTorch的Tensor库和神经网络 训练一个小的神经网络做图片分类 在多GPU上训练 如果希望使用您所有GPU获得更大的加速，请查看Optional: Data Parallelism。 接下来要做什么？ Train neural nets to play video games Train a state-of-the-art ResNet network on imagenet Train a face generator using Generative Adversarial Networks Train a word-level language model using Recurrent LSTM networks More examples More tutorials Discuss PyTorch on the Forums Chat with other users on Slack 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/blitz/data_parallel_tutorial.html":{"url":"beginner/blitz/data_parallel_tutorial.html","title":"可选: 数据并行处理","keywords":"","body":"可选: 数据并行处理 作者: Sung Kim Jenny Kang 译者: bat67 校对者: FontTian 片刻 在这个教程里，我们将学习如何使用数据并行（DataParallel）来使用多GPU。 PyTorch非常容易的就可以使用GPU，可以用如下方式把一个模型放到GPU上: device = torch.device(\"cuda: 0\") model.to(device) 然后可以复制所有的张量到GPU上: mytensor = my_tensor.to(device) 请注意，调用my_tensor.to(device)返回一个GPU上的my_tensor副本，而不是重写my_tensor。我们需要把它赋值给一个新的张量并在GPU上使用这个张量。 在多GPU上执行前向和反向传播是自然而然的事。然而，PyTorch默认将只是用一个GPU。你可以使用DataParallel让模型并行运行来轻易的让你的操作在多个GPU上运行。 model = nn.DataParallel(model) 这是这篇教程背后的核心，我们接下来将更详细的介绍它。 导入和参数 导入PyTorch模块和定义参数。 import torch import torch.nn as nn from torch.utils.data import Dataset, DataLoader # Parameters 和 DataLoaders input_size = 5 output_size = 2 batch_size = 30 data_size = 100 设备（Device）: device = torch.device(\"cuda: 0\" if torch.cuda.is_available() else \"cpu\") 虚拟数据集 要制作一个虚拟（随机）数据集，只需实现__getitem__。 class RandomDataset(Dataset): def __init__(self, size, length): self.len = length self.data = torch.randn(length, size) def __getitem__(self, index): return self.data[index] def __len__(self): return self.len rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size), batch_size=batch_size, shuffle=True) 简单模型 作为演示，我们的模型只接受一个输入，执行一个线性操作，然后得到结果。然而，你能在任何模型（CNN，RNN，Capsule Net等）上使用DataParallel。 我们在模型内部放置了一条打印语句来检测输入和输出向量的大小。请注意批等级为0时打印的内容。 class Model(nn.Module): # Our model def __init__(self, input_size, output_size): super(Model, self).__init__() self.fc = nn.Linear(input_size, output_size) def forward(self, input): output = self.fc(input) print(\"\\tIn Model: input size\", input.size(), \"output size\", output.size()) return output 创建一个模型和数据并行 这是本教程的核心部分。首先，我们需要创建一个模型实例和检测我们是否有多个GPU。如果我们有多个GPU，我们使用nn.DataParallel来包装我们的模型。然后通过model.to(device)把模型放到GPU上。 model = Model(input_size, output_size) if torch.cuda.device_count() > 1: print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\") # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs model = nn.DataParallel(model) model.to(device) 输出: Let's use 2 GPUs! 运行模型 现在我们可以看输入和输出张量的大小。 for data in rand_loader: input = data.to(device) output = model(input) print(\"Outside: input size\", input.size(), \"output_size\", output.size()) 输出: In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 结果 当我们对30个输入和输出进行批处理时，我们和期望的一样得到30个输入和30个输出，但是若有多个GPU，会得到如下的结果。 2个GPU 若有2个GPU，将看到: Let's use 2 GPUs! In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 3个GPU 若有3个GPU，将看到: Let's use 3 GPUs! In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 8个GPU 若有8个GPU，将看到: Let's use 8 GPUs! In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 总结 DataParallel自动的划分数据，并将作业发送到多个GPU上的多个模型。DataParallel会在每个模型完成作业后，收集与合并结果然后返回给你。 更多信息，请参考: https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/data_loading_tutorial.html":{"url":"beginner/data_loading_tutorial.html","title":"数据加载和处理教程","keywords":"","body":"数据加载和处理教程 作者：Sasank Chilamkurthy 校验：宁采晨 解决任何机器学习问题都需要花费大量精力来准备数据。PyTorch提供了许多工具来简化数据加载过程，并有望使代码更具可读性。在本教程中，我们将学习如何从非平凡的数据集中加载和预处理/增强数据。 要能够运行本教程中的代码，请确保已安装以下软件包： scikit-image：用于图像io和变换 pandas：为了更方便地处理csv文件 from __future__ import print_function, division import os import torch import pandas as pd from skimage import io, transform import numpy as np import matplotlib.pyplot as plt from torch.utils.data import Dataset, DataLoader from torchvision import transforms, utils # Ignore warnings import warnings warnings.filterwarnings(\"ignore\") plt.ion() # interactive mode 我们要处理的数据集是面部姿势数据集。这意味着对脸部的标注如下： 总体上，每个面孔都标注了68个不同的界标点。 注意 从此处下载数据集，使图像位于名为data/faces/的目录中。该数据集实际上是通过对imagenet上的一些标记为“人脸”的图像应用良好的的dlib姿态估计生成的。 数据集包含一个带注释的csv文件，如下所示： image_name,part_0_x,part_0_y,part_1_x,part_1_y,part_2_x, ... ,part_67_x,part_67_y 0805personali01.jpg,27,83,27,98, ... 84,134 1084239450_e76e00b7e7.jpg,70,236,71,257, ... ,128,312 让我们快速阅读CSV并获取（N，2）数组中的注释，其中N是特征点的数量。 landmarks_frame = pd.read_csv('data/faces/face_landmarks.csv') n = 65 img_name = landmarks_frame.iloc[n, 0] landmarks = landmarks_frame.iloc[n, 1:].as_matrix() landmarks = landmarks.astype('float').reshape(-1, 2) print('Image name: {}'.format(img_name)) print('Landmarks shape: {}'.format(landmarks.shape)) print('First 4 Landmarks: {}'.format(landmarks[:4])) 输出： Image name: person-7.jpg Landmarks shape: (68, 2) First 4 Landmarks: [[32. 65.] [33. 76.] [34. 86.] [34. 97.]] 让我们编写一个简单的辅助函数来显示图像及其特征点，并使用它来显示样例。 def show_landmarks(image, landmarks): \"\"\"Show image with landmarks\"\"\" plt.imshow(image) plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r') plt.pause(0.001) # pause a bit so that plots are updated plt.figure() show_landmarks(io.imread(os.path.join('data/faces/', img_name)), landmarks) plt.show() 数据集类 torch.utils.data.Dataset是表示数据集的抽象类。您的自定义数据集应继承Dataset，并覆盖下列方法： __len__，使得len(dataset)返回数据集的大小。 __getitem__支持索引，使得dataset[i]可以用来获取第i个样本 让我们为面部轮廓数据集创建一个数据集类。我们将在__init__中读取csv文件，在__getitem__中读取图像。由于所有图像不会立即存储在内存中，而是根据需要读取，因此可以提高内存效率。 我们的数据集中的样品是一个字典{'image': image, 'landmarks': landmarks}。我们的数据集将采取一个可选的参数transform，以便可以对样本进行任何必需的处理。我们将在下一节看到transform的用处。 class FaceLandmarksDataset(Dataset): \"\"\"Face Landmarks dataset.\"\"\" def __init__(self, csv_file, root_dir, transform=None): \"\"\" Args: csv_file (string): Path to the csv file with annotations. root_dir (string): Directory with all the images. transform (callable, optional): Optional transform to be applied on a sample. \"\"\" self.landmarks_frame = pd.read_csv(csv_file) self.root_dir = root_dir self.transform = transform def __len__(self): return len(self.landmarks_frame) def __getitem__(self, idx): if torch.is_tensor(idx): idx = idx.tolist() img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, 0]) image = io.imread(img_name) landmarks = self.landmarks_frame.iloc[idx, 1:] landmarks = np.array([landmarks]) landmarks = landmarks.astype('float').reshape(-1, 2) sample = {'image': image, 'landmarks': landmarks} if self.transform: sample = self.transform(sample) return sample 让我们实例化该类并遍历数据样本。我们将打印前4个样本的大小并显示其特征点。 face_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv', root_dir='data/faces/') fig = plt.figure() for i in range(len(face_dataset)): sample = face_dataset[i] print(i, sample['image'].shape, sample['landmarks'].shape) ax = plt.subplot(1, 4, i + 1) plt.tight_layout() ax.set_title('Sample #{}'.format(i)) ax.axis('off') show_landmarks(**sample) if i == 3: plt.show() break 输出： 0 (324, 215, 3) (68, 2) 1 (500, 333, 3) (68, 2) 2 (250, 258, 3) (68, 2) 3 (434, 290, 3) (68, 2) 转换（Transforms） 从上面可以看到的一个问题是样本的大小不同。大多数神经网络期望图像的大小固定。因此，我们将需要编写一些前置代码。让我们创建三个转换：： Rescale：图像缩放 RandomCrop：从图像中随机裁剪。这是数据扩充。 ToTensor：将numpy格式的图片转换为torch格式的图片（我们需要换轴）。 我们将它们编写为可调用的类，而不是简单的函数，这样就不必每次调用转换时都传递其参数。为此，我们只需要实现__call__方法，如果需要的话，可以实现__init__方法。然后我们可以使用这样的转换： tsfm = Transform(params) transformed_sample = tsfm(sample) 在下面观察如何将这些变换同时应用于图像和特征点。 class Rescale(object): \"\"\"Rescale the image in a sample to a given size. Args: output_size (tuple or int): Desired output size. If tuple, output is matched to output_size. If int, smaller of image edges is matched to output_size keeping aspect ratio the same. \"\"\" def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) self.output_size = output_size def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] h, w = image.shape[:2] if isinstance(self.output_size, int): if h > w: new_h, new_w = self.output_size * h / w, self.output_size else: new_h, new_w = self.output_size, self.output_size * w / h else: new_h, new_w = self.output_size new_h, new_w = int(new_h), int(new_w) img = transform.resize(image, (new_h, new_w)) # h and w are swapped for landmarks because for images, # x and y axes are axis 1 and 0 respectively landmarks = landmarks * [new_w / w, new_h / h] return {'image': img, 'landmarks': landmarks} class RandomCrop(object): \"\"\"Crop randomly the image in a sample. Args: output_size (tuple or int): Desired output size. If int, square crop is made. \"\"\" def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) if isinstance(output_size, int): self.output_size = (output_size, output_size) else: assert len(output_size) == 2 self.output_size = output_size def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] h, w = image.shape[:2] new_h, new_w = self.output_size top = np.random.randint(0, h - new_h) left = np.random.randint(0, w - new_w) image = image[top: top + new_h, left: left + new_w] landmarks = landmarks - [left, top] return {'image': image, 'landmarks': landmarks} class ToTensor(object): \"\"\"Convert ndarrays in sample to Tensors.\"\"\" def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] # swap color axis because # numpy image: H x W x C # torch image: C X H X W image = image.transpose((2, 0, 1)) return {'image': torch.from_numpy(image), 'landmarks': torch.from_numpy(landmarks)} 组合转换（Compose transforms） 现在，我们将转换应用于样本。 假设我们要将图像的较短边重新缩放为256，然后从中随机裁剪一个尺寸为224的正方形。即我们要组成 Rescale和RandomCrop变换。 torchvision.transforms.Compose是一个简单的可调用类，它使我们可以执行此操作。 scale = Rescale(256) crop = RandomCrop(128) composed = transforms.Compose([Rescale(256), RandomCrop(224)]) # Apply each of the above transforms on sample. fig = plt.figure() sample = face_dataset[65] for i, tsfrm in enumerate([scale, crop, composed]): transformed_sample = tsfrm(sample) ax = plt.subplot(1, 3, i + 1) plt.tight_layout() ax.set_title(type(tsfrm).__name__) show_landmarks(**transformed_sample) plt.show() 遍历数据集 让我们将所有这些放在一起以创建具有组合转换的数据集。总而言之，每次采样此数据集时： 从文件中即时读取图像 将变换应用于读取的图像 由于其中一种转换是随机的，因此在采样时会增加数据 我们可以像之前一样通过一个for i in range循环遍历创建的数据集。 transformed_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv', root_dir='data/faces/', transform=transforms.Compose([ Rescale(256), RandomCrop(224), ToTensor() ])) for i in range(len(transformed_dataset)): sample = transformed_dataset[i] print(i, sample['image'].size(), sample['landmarks'].size()) if i == 3: break 输出: 0 torch.Size([3, 224, 224]) torch.Size([68, 2]) 1 torch.Size([3, 224, 224]) torch.Size([68, 2]) 2 torch.Size([3, 224, 224]) torch.Size([68, 2]) 3 torch.Size([3, 224, 224]) torch.Size([68, 2]) 但是，通过使用简单的for循环迭代数据，我们失去了很多功能。特别是，我们错过了： 批量处理数据 整理数据 使用multiprocessing并行加载数据。 torch.utils.data.DataLoader是提供所有这些功能的迭代器。下面使用的参数应该清楚。一个重要参数是collate_fn。您可以使用指定要精确批处理样品的数量collate_fn。但是，默认排序规则在大多数情况下都可以正常工作。 dataloader = DataLoader(transformed_dataset, batch_size=4, shuffle=True, num_workers=4) # Helper function to show a batch def show_landmarks_batch(sample_batched): \"\"\"Show image with landmarks for a batch of samples.\"\"\" images_batch, landmarks_batch = \\ sample_batched['image'], sample_batched['landmarks'] batch_size = len(images_batch) im_size = images_batch.size(2) grid_border_size = 2 grid = utils.make_grid(images_batch) plt.imshow(grid.numpy().transpose((1, 2, 0))) for i in range(batch_size): plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size + (i + 1) * grid_border_size, landmarks_batch[i, :, 1].numpy() + grid_border_size, s=10, marker='.', c='r') plt.title('Batch from dataloader') for i_batch, sample_batched in enumerate(dataloader): print(i_batch, sample_batched['image'].size(), sample_batched['landmarks'].size()) # observe 4th batch and stop. if i_batch == 3: plt.figure() show_landmarks_batch(sample_batched) plt.axis('off') plt.ioff() plt.show() break 输出： 0 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 1 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 2 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 3 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 后记：torchvision 在本教程中，我们已经看到了如何构造和使用数据集，转换数据和的数据加载。torchvision软件包提供了一些常见的数据集和转换。您甚至不必编写自定义类。torchvision中可用的更通用的数据集之一是ImageFolder。假定图像的组织方式如下： root/ants/xxx.png root/ants/xxy.jpeg root/ants/xxz.png . . . root/bees/123.jpg root/bees/nsdf3.png root/bees/asd932_.png 其中“ants”，“bees”等是类标签。同样也可以使用PIL.Image中的操作像 RandomHorizontalFlip和Scale来进行通用转换。您可以使用以下代码创建一个数据加载器： import torch from torchvision import transforms, datasets data_transform = transforms.Compose([ transforms.RandomSizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ]) hymenoptera_dataset = datasets.ImageFolder(root='hymenoptera_data/train', transform=data_transform) dataset_loader = torch.utils.data.DataLoader(hymenoptera_dataset, batch_size=4, shuffle=True, num_workers=4) 对于训练代码示例，请参见迁移学习教程 脚本的总运行时间： （0分钟59.213秒） 下载python文件: data_loading_tutorial.py 下载 Jupyter notebook文件: data_loading_tutorial.ipynb 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/pytorch_with_examples.html":{"url":"beginner/pytorch_with_examples.html","title":"用例子学习 PyTorch","keywords":"","body":"用例子学习 PyTorch 作者 ：Justin Johnson 校正：宁采晨 本教程通过独立的示例介绍了PyTorch的基本概念 。 PyTorch的核心是提供两个主要功能： n维张量，类似于numpy，但可以在GPU上运行 自动区分以构建和训练神经网络 我们将使用完全连接的ReLU网络作为我们的运行示例。该网络将具有单个隐藏层，并且将通过最小化网络输出与真实输出之间的欧几里德距离来进行梯度下降训练，以适应随机数据。 注意 您可以在本页结尾浏览各个示例。 [TOC] 张量 热身：Numpy 在介绍PyTorch之前，我们将首先使用numpy实现网络。 Numpy提供了一个n维数组对象，以及许多用于操纵这些数组的函数。Numpy是用于科学计算的通用框架；它对计算图，深度学习或梯度一无所知。然而，我们可以很容易地使用NumPy，手动实现网络的前向和反向传播，来拟合随机数据： # -*- coding: utf-8 -*- import numpy as np # N是批尺寸参数；D_in是输入维度 # H是隐藏层维度；D_out是输出维度 N, D_in, H, D_out = 64, 1000, 100, 10 # 产生随机输入和输出数据 x = np.random.randn(N, D_in) y = np.random.randn(N, D_out) # 随机初始化权重 w1 = np.random.randn(D_in, H) w2 = np.random.randn(H, D_out) learning_rate = 1e-6 for t in range(500): # 前向传播：计算预测值y h = x.dot(w1) h_relu = np.maximum(h, 0) y_pred = h_relu.dot(w2) # 计算并显示loss（损失） loss = np.square(y_pred - y).sum() print(t, loss) # 反向传播，计算w1、w2对loss的梯度 grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.T.dot(grad_y_pred) grad_h_relu = grad_y_pred.dot(w2.T) grad_h = grad_h_relu.copy() grad_h[h PyTorch：张量 Numpy是一个伟大的框架，但它不能利用GPU来加速其数值计算。对于现代的深层神经网络，GPU通常提供的50倍以上的加速，仅凭numpy不足以实现现代深度学习。 在这里，我们介绍最基本的PyTorch概念：张量（Tensor）。PyTorch张量在概念上与numpy数组相同：张量是n维数组，而PyTorch提供了很多函数操作这些tensor。张量可以跟踪计算图和渐变，它们也可用作科学计算的通用工具。 与numpy不同，PyTorch张量可以利用GPU加速其数字计算。要在GPU上运行PyTorch Tensor，只需将其转换为新的数据类型。 这里我们利用PyTorch的tensor在随机数据上训练一个两层的网络。和前面NumPy的例子类似，我们使用PyTorch的tensor，手动在网络中实现前向传播和反向传播： # -*- coding: utf-8 -*- import torch dtype = torch.float device = torch.device(\"cpu\") # device = torch.device(\"cuda:0\") # Uncomment this to run on GPU # N是批尺寸大小； D_in 是输入维度； # H 是隐藏层维度； D_out 是输出维度 N, D_in, H, D_out = 64, 1000, 100, 10 # 产生随机输入和输出数据 x = torch.randn(N, D_in, device=device, dtype=dtype) y = torch.randn(N, D_out, device=device, dtype=dtype) # 随机初始化权重 w1 = torch.randn(D_in, H, device=device, dtype=dtype) w2 = torch.randn(H, D_out, device=device, dtype=dtype) learning_rate = 1e-6 for t in range(500): # 前向传播：计算预测值y h = x.mm(w1) h_relu = h.clamp(min=0) y_pred = h_relu.mm(w2) # 计算并输出loss loss = (y_pred - y).pow(2).sum().item() if t % 100 == 99: print(t, loss) # 反向传播，计算w1、w2对loss的梯度 grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.t().mm(grad_y_pred) grad_h_relu = grad_y_pred.mm(w2.t()) grad_h = grad_h_relu.clone() grad_h[h 自动求导 PyTorch：张量和自动求导 在以上示例中，我们必须手动实现神经网络的前向和后向传递。对于小型的两层网络而言，手动实现反向传递并不重要，但对于大型的复杂网络而言，这变得非常麻烦。 幸运的是，我们可以使用自动微分 来自动计算神经网络中的反向传播。PyTorch中的 autograd软件包提供了这个功能。使用autograd时，您的网络正向传递将定义一个 计算图；图中的节点为张量，图中的边为从输入张量产生输出张量的函数。通过该图进行反向传播，可以轻松计算梯度。 这听起来很复杂，在实践中非常简单。每个张量代表计算图中的一个节点。如果 x是一个张量，并且有 x.requires_grad=True，那么x.grad就是另一个张量，代表着x相对于某个标量值的梯度。 在这里，我们使用PyTorch张量和autograd来实现我们的两层网络。现在我们不再需要手动实现网络的反向传播： # -*- coding: utf-8 -*- import torch dtype = torch.float device = torch.device(\"cpu\") # device = torch.device(\"cuda:0\") # Uncomment this to run on GPU # N是批尺寸大小；D_in是输入维度； # H是隐藏层维度；D_out是输出维度 N, D_in, H, D_out = 64, 1000, 100, 10 # 产生随机输入和输出数据，将requires_grad置为False，意味着我们不需要在反向传播时候计算这些值的梯度 x = torch.randn(N, D_in, device=device, dtype=dtype) y = torch.randn(N, D_out, device=device, dtype=dtype) # 产生随机权重tensor，将requires_grad设置为True，意味着我们希望在反向传播时候计算这些值的梯度 w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True) w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True) learning_rate = 1e-6 for t in range(500): # 前向传播：使用tensor的操作计算预测值y。 # 由于w1和w2有requires_grad=True,涉及这些张量的操作将让PyTorch构建计算图，从而允许自动计算梯度。 # 由于我们不再手工实现反向传播，所以不需要保留中间值的引用。 y_pred = x.mm(w1).clamp(min=0).mm(w2) # 计算并输出loss # loss是一个形状为(1,)的张量 # loss.item()是这个张量对应的python数值 loss = (y_pred - y).pow(2).sum() if t % 100 == 99: print(t, loss.item()) # 使用autograd计算反向传播,这个调用将计算loss对所有requires_grad=True的tensor的梯度。 # 这次调用后，w1.grad和w2.grad将分别是loss对w1和w2的梯度张量。 loss.backward() # 使用梯度下降更新权重。对于这一步，我们只想对w1和w2的值进行原地改变；不想为更新阶段构建计算图， # 所以我们使用torch.no_grad()上下文管理器防止PyTorch为更新构建计算图 with torch.no_grad(): w1 -= learning_rate * w1.grad w2 -= learning_rate * w2.grad # 反向传播之后手动将梯度置零 w1.grad.zero_() w2.grad.zero_() PyTorch：定义新的自动求导函数 在底层，每一个原始的自动求导运算实际上是两个在Tensor上运行的函数。其中，forward函数计算从输入Tensors获得的输出Tensors。而backward函数接收输出Tensors对于某个标量值的梯度，并且计算输入Tensors相对于该相同标量值的梯度。 在PyTorch中，我们可以很容易地通过定义torch.autograd.Function的子类并实现forward和backward函数，来定义自己的自动求导运算。然后，我们可以通过构造实例并像调用函数一样调用它，并传递包含输入数据的张量。 这个例子中，我们自定义一个自动求导函数来展示ReLU的非线性。并用它实现我们的两层网络： # -*- coding: utf-8 -*- import torch class MyReLU(torch.autograd.Function): \"\"\" 我们可以通过建立torch.autograd的子类来实现我们自定义的autograd函数，并完成张量的正向和反向传播。 \"\"\" @staticmethod def forward(ctx, input): \"\"\" 在前向传播中，我们收到包含输入和返回的张量包含输出的张量。 ctx是可以使用的上下文对象存储信息以进行向后计算。 您可以使用ctx.save_for_backward方法缓存任意对象，以便反向传播使用。 \"\"\" ctx.save_for_backward(input) return input.clamp(min=0) @staticmethod def backward(ctx, grad_output): \"\"\" 在反向传播中，我们接收到上下文对象和一个张量，其包含了相对于正向传播过程中产生的输出的损失的梯度。 我们可以从上下文对象中检索缓存的数据，并且必须计算并返回与正向传播的输入相关的损失的梯度。 \"\"\" input, = ctx.saved_tensors grad_input = grad_output.clone() grad_input[input TensorFlow：静态图 PyTorch自动求导看起来很像TensorFlow：在两个框架中我们都定义了一个计算图，并使用自动微分来计算梯度。两者之间的最大区别是TensorFlow的计算图是静态的，而PyTorch使用 动态计算图。 在TensorFlow中，我们一次定义了计算图，然后一遍又一遍地执行相同的图，可能将不同的输入数据提供给该图。在PyTorch中，每个前向传递都定义一个新的计算图。 静态图的好处在于您可以预先优化图。例如，框架可能决定融合某些图形操作以提高效率，或者想出一种在多个GPU或许多机器之间分布图形的策略。如果您重复用同一张图，那么随着一遍一遍地重复运行同一张图，可以分摊这种潜在的昂贵的前期优化。 静态图和动态图不同的一个方面是控制流。对于某些模型，我们可能希望对每个数据点执行不同的计算。例如，对于每个数据点，循环网络可能会展开不同数量的时间步长；此展开可以实现为循环。对于静态图，循环构造必须是图的一部分；因此，TensorFlow提供了诸如tf.scan将循环嵌入到图中的运算符。使用动态图，情况更简单：由于我们为每个示例动态生成图，因此可以使用常规命令流控制来执行针对每个输入而不同的计算。 与上面的PyTorch autograd示例形成对比，这里我们使用TensorFlow来拟合一个简单的两层网络： # -*- coding: utf-8 -*- import tensorflow as tf import numpy as np # 首先我们建立计算图: # N是批大小；D是输入维度； # H是隐藏层维度；D_out是输出维度。 N, D_in, H, D_out = 64, 1000, 100, 10 # 为输入和目标数据创建placeholder # 当执行计算图时，他们将会被真实的数据填充 x = tf.placeholder(tf.float32, shape=(None, D_in)) y = tf.placeholder(tf.float32, shape=(None, D_out)) # 为权重创建Variable并用随机数据初始化 # TensorFlow的Variable在执行计算图时不会改变 w1 = tf.Variable(tf.random_normal((D_in, H))) w2 = tf.Variable(tf.random_normal((H, D_out))) # 前向传播：使用TensorFlow的张量运算计算预测值y # 注意这段代码实际上不执行任何数值运算 # 它只是建立了我们稍后将执行的计算图 h = tf.matmul(x, w1) h_relu = tf.maximum(h, tf.zeros(1)) y_pred = tf.matmul(h_relu, w2) # 使用TensorFlow的张量运算损失（loss） loss = tf.reduce_sum((y - y_pred) ** 2.0) # 计算loss对于w1和w2的梯度 grad_w1, grad_w2 = tf.gradients(loss, [w1, w2]) # 使用梯度下降更新权重。为了实际更新权重，我们需要在执行计算图时计算new_w1和new_w2 # 注意，在TensorFlow中，更新权重值的行为是计算图的一部分 # 但在PyTorch中，这发生在计算图形之外 learning_rate = 1e-6 new_w1 = w1.assign(w1 - learning_rate * grad_w1) new_w2 = w2.assign(w2 - learning_rate * grad_w2) # 现在我们搭建好了计算图，所以我们开始一个TensorFlow的会话（session）来实际执行计算图 with tf.Session() as sess: # 运行一次计算图来初始化变量w1和w2 sess.run(tf.global_variables_initializer()) # 创建numpy数组来存储输入x和目标y的实际数据 x_value = np.random.randn(N, D_in) y_value = np.random.randn(N, D_out) for t in range(500): # 多次运行计算图。每次执行时，我们都用feed_dict参数 # 将x_value绑定到x，将y_value绑定到y # 每次执行图形时我们都要计算损失、new_w1和new_w2 # 这些张量的值以numpy数组的形式返回 loss_value, _, _ = sess.run([loss, new_w1, new_w2], feed_dict={x: x_value, y: y_value}) if t % 100 == 99: print(t, loss_value) nn 模块 PyTorch：nn 计算图和autograd是定义复杂运算符并自动采用导数的非常强大的范例。但是对于大型神经网络，原始的autograd可能会有点太低了。 在构建神经网络时，我们经常考虑将计算分为几层，其中一些层具有可学习的参数 ，这些参数将在学习过程中进行优化。 在TensorFlow中，诸如Keras， TensorFlow-Slim和TFLearn之类的软件包在原始计算图上提供了更高级别的抽象接口，这些封装对构建神经网络很有用。 在PyTorch中，该nn程序包达到了相同的目的。该nn 包定义了一组Modules，它们大致等效于神经网络层。模块接收输入张量并计算输出张量，但也可以保持内部状态，例如包含可学习参数的张量。该nn软件包还定义了一组有用的损失函数，这些函数通常在训练神经网络时使用。 在此示例中，我们使用该nn包来实现我们的两层网络： # -*- coding: utf-8 -*- import torch # N是批大小；D是输入维度 # H是隐藏层维度；D_out是输出维度 N, D_in, H, D_out = 64, 1000, 100, 10 # 产生输入和输出随机张量 x = torch.randn(N, D_in) y = torch.randn(N, D_out) # 使用nn包将我们的模型定义为一系列的层 # nn.Sequential是包含其他模块的模块，并按顺序应用这些模块来产生其输出 # 每个线性模块使用线性函数从输入计算输出，并保存其内部的权重和偏差张量 # 在构造模型之后，我们使用.to()方法将其移动到所需的设备 model = torch.nn.Sequential( torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out), ) # nn包还包含常用的损失函数的定义 # 在这种情况下，我们将使用平均平方误差(MSE)作为我们的损失函数 loss_fn = torch.nn.MSELoss(reduction='sum') learning_rate = 1e-4 for t in range(500): # 前向传播：通过向模型传入x计算预测的y # 模块对象重载了__call__运算符，所以可以像函数那样调用它们 # 这么做相当于向模块传入了一个张量，然后它返回了一个输出张量 y_pred = model(x) # 计算并打印损失。我们传递包含y的预测值和真实值的张量，损失函数返回包含损失的张量 loss = loss_fn(y_pred, y) if t % 100 == 99: print(t, loss.item()) # 反向传播之前清零梯度 model.zero_grad() # 反向传播：计算模型的损失对所有可学习参数的梯度 # 在内部，每个模块的参数存储在requires_grad=True的张量中 # 因此这个调用将计算模型中所有可学习参数的梯度 loss.backward() # 使用梯度下降更新权重 # 每个参数都是张量，所以我们可以像我们以前那样可以得到它的数值和梯度 with torch.no_grad(): for param in model.parameters(): param -= learning_rate * param.grad PyTorch：optim 到现在为止，我们已经通过手动更改持有可学习参数的张量来更新模型的权重（使用torch.no_grad() 或.data避免在autograd中跟踪历史记录）。对于像随机梯度下降这样的简单优化算法来说，这并不是一个沉重的负担，但是在实践中，我们经常使用更复杂的优化器（例如AdaGrad，RMSProp，Adam等）来训练神经网络。 PyTorch中的软件包optim抽象了优化算法的思想，并提供了常用优化算法的实现。 在此示例中，我们将像之前一样使用nn包来定义模型，但是用optim包提供的Adam算法来优化模型： # -*- coding: utf-8 -*- import torch # N是批大小；D是输入维度 # H是隐藏层维度；D_out是输出维度 N, D_in, H, D_out = 64, 1000, 100, 10 # 产生随机输入和输出张量 x = torch.randn(N, D_in) y = torch.randn(N, D_out) # 使用nn包定义模型和损失函数 model = torch.nn.Sequential( torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out), ) loss_fn = torch.nn.MSELoss(reduction='sum') # 使用optim包定义优化器（Optimizer）。Optimizer将会为我们更新模型的权重 # 这里我们使用Adam优化方法；optim包还包含了许多别的优化算法 # Adam构造函数的第一个参数告诉优化器应该更新哪些张量 learning_rate = 1e-4 optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) for t in range(500): # 前向传播：通过像模型输入x计算预测的y y_pred = model(x) # 计算并输出loss loss = loss_fn(y_pred, y) if t % 100 == 99: print(t, loss.item()) # 在反向传播之前，使用optimizer将它要更新的所有张量的梯度清零(这些张量是模型可学习的权重)。 # 这是因为默认情况下，每当调用.backward（）时，渐变都会累积在缓冲区中（即不会被覆盖） # 有关更多详细信息，请查看torch.autograd.backward的文档。 optimizer.zero_grad() # 反向传播：根据模型的参数计算loss的梯度 loss.backward() # 调用Optimizer的step函数使它所有参数更新 optimizer.step() PyTorch：自定义nn模块 有时，您将需要指定比一系列现有模块更复杂的模型。在这些情况下，您可以通过继承nn.Module和定义一个forward来定义自己的模型，这个forward模块可以使用其他模块或在Tensors上的其他自动求导运算来接收输入Tensors并生成输出Tensors。 在此示例中，我们将使用自定义的Module子类构建两层网络： # -*- coding: utf-8 -*- import torch class TwoLayerNet(torch.nn.Module): def __init__(self, D_in, H, D_out): \"\"\" 在构造函数中，我们实例化了两个nn.Linear模块，并将它们作为成员变量。 \"\"\" super(TwoLayerNet, self).__init__() self.linear1 = torch.nn.Linear(D_in, H) self.linear2 = torch.nn.Linear(H, D_out) def forward(self, x): \"\"\" 在前向传播的函数中，我们接收一个输入的张量，也必须返回一个输出张量。 我们可以使用构造函数中定义的模块以及张量上的任意的（可微分的）操作。 \"\"\" h_relu = self.linear1(x).clamp(min=0) y_pred = self.linear2(h_relu) return y_pred # N是批大小； D_in 是输入维度； # H 是隐藏层维度； D_out 是输出维度 N, D_in, H, D_out = 64, 1000, 100, 10 # 产生输入和输出的随机张量 x = torch.randn(N, D_in) y = torch.randn(N, D_out) # 通过实例化上面定义的类来构建我们的模型 model = TwoLayerNet(D_in, H, D_out) # 构造损失函数和优化器 # SGD构造函数中对model.parameters()的调用 # 将包含模型的一部分，即两个nn.Linear模块的可学习参数 criterion = torch.nn.MSELoss(reduction='sum') optimizer = torch.optim.SGD(model.parameters(), lr=1e-4) for t in range(500): # 前向传播：通过向模型传递x计算预测值y y_pred = model(x) # 计算并输出loss loss = criterion(y_pred, y) if t % 100 == 99: print(t, loss.item()) # 清零梯度，反向传播，更新权重 optimizer.zero_grad() loss.backward() optimizer.step() PyTorch：控制流+权重共享 作为动态图和权重共享的示例，我们实现了一个非常奇怪的模型：一个完全连接的ReLU网络，该网络在每个前向传递中选择1到4之间的随机数作为隐藏层的层数，多次重复使用相同的权重计算最里面的隐藏层。 对于此模型，我们可以使用常规的Python流控制来实现循环，并且可以通过在定义前向传递时简单地多次重复使用同一模块来实现最内层之间的权重共享。 我们利用Mudule的子类很容易实现这个模型： # -*- coding: utf-8 -*- import random import torch class DynamicNet(torch.nn.Module): def __init__(self, D_in, H, D_out): \"\"\" 在构造函数中，我们构造了三个nn.Linear实例，它们将在前向传播时被使用。 \"\"\" super(DynamicNet, self).__init__() self.input_linear = torch.nn.Linear(D_in, H) self.middle_linear = torch.nn.Linear(H, H) self.output_linear = torch.nn.Linear(H, D_out) def forward(self, x): \"\"\" 对于模型的前向传播，我们随机选择0、1、2、3，并重用了多次计算隐藏层的middle_linear模块。 由于每个前向传播构建一个动态计算图， 我们可以在定义模型的前向传播时使用常规Python控制流运算符，如循环或条件语句。 在这里，我们还看到，在定义计算图形时多次重用同一个模块是完全安全的。 这是Lua Torch的一大改进，因为Lua Torch中每个模块只能使用一次。 \"\"\" h_relu = self.input_linear(x).clamp(min=0) for _ in range(random.randint(0, 3)): h_relu = self.middle_linear(h_relu).clamp(min=0) y_pred = self.output_linear(h_relu) return y_pred # N是批大小；D是输入维度 # H是隐藏层维度；D_out是输出维度 N, D_in, H, D_out = 64, 1000, 100, 10 # 产生输入和输出随机张量 x = torch.randn(N, D_in) y = torch.randn(N, D_out) # 实例化上面定义的类来构造我们的模型 model = DynamicNet(D_in, H, D_out) # 构造我们的损失函数（loss function）和优化器（Optimizer） # 用平凡的随机梯度下降训练这个奇怪的模型是困难的，所以我们使用了momentum方法 criterion = torch.nn.MSELoss(reduction='sum') optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9) for t in range(500): # 前向传播：通过向模型传入x计算预测的y y_pred = model(x) # 计算并输出损失loss loss = criterion(y_pred, y) if t % 100 == 99: print(t, loss.item()) # 清零梯度，反向传播，更新权重 optimizer.zero_grad() loss.backward() optimizer.step() 例子 你可以在此处浏览以上示例。 张量 热身：Numpy PyTorch：张量 Autograd PyTorch：张量和自动求导 PyTorch：定义新的自动求导函数 TensorFlow：静态图 nn模块 PyTorch：nn PyTorch：optim PyTorch：自定义nn模块 PyTorch：控制流+权重共享 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/transfer_learning_tutorial.html":{"url":"beginner/transfer_learning_tutorial.html","title":"迁移学习教程","keywords":"","body":"迁移学习教程 作者 ： Sasank Chilamkurthy 在本教程中，您将学习如何使用迁移学习训练网络。你可以在 cs231n笔记中阅读更多关于迁移学习的内容。 引用笔记， > 在实践中，很少有人从头开始训练整个卷积网络（随机初始化），因为足够大的数据集是相对少见的。相反，通常在非常大的数据集（例如 ImageNet，其包含具有1000个类别的120万张图片）上预先训练一个卷积神经网络，然后使用这个卷积神经网络对目标任务进行初始化或用作固定特征提取器。 如下是两个主要的迁移学习场景： 微调卷积神经网络 我们使用预训练网络来初始化网络，而不是随机初始化，比如一个已经在imagenet 1000数据集上训练好的网络一样。其余训练和往常一样。 将卷积神经网络作为固定特征提取器 ：在这里，我们将冻结除最终全连接层之外的整个网络的权重。最后一个全连接层被替换为具有随机权重的新层，并且仅训练该层。 # License: BSD # Author: Sasank Chilamkurthy from __future__ import print_function, division import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler import numpy as np import torchvision from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import time import os import copy plt.ion() # interactive mode 加载数据 我们将使用 torchvision 和 torch.utils.data 包来加载数据。 今天，我们要解决的问题是训练一个模型来对蚂蚁和蜜蜂进行分类。我们蚂蚁和蜜蜂分别准备了大约120个训练图像，并且每类还有75个验证图像。通常，如果从头开始训练，这是一个非常小的数据集。由于我们正在使用迁移学习，我们应该能够合理地进行泛化。 该数据集是imagenet的一个很小的子集。 注意 从此处下载数据，并将其解压到当前目录。 # Data augmentation and normalization for training # Just normalization for validation data_transforms = { 'train': transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), 'val': transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } data_dir = 'data/hymenoptera_data' image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']} dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val']} dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']} class_names = image_datasets['train'].classes device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") 可视化一些图像 让我们通过可视化一些训练图像，来理解什么是数据增强。 def imshow(inp, title=None): \"\"\"Imshow for Tensor.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated # Get a batch of training data inputs, classes = next(iter(dataloaders['train'])) # Make a grid from batch out = torchvision.utils.make_grid(inputs) imshow(out, title=[class_names[x] for x in classes]) 训练模型 现在, 让我们编写一个通用函数来训练一个模型。这里, 我们将会举例说明: 调整学习率 保存最好的模型 下面函数中, scheduler 参数是 torch.optim.lr_scheduler 中的学习率调整（LR scheduler）对象. def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): print('Epoch {}/{}'.format(epoch, num_epochs - 1)) print('-' * 10) # Each epoch has a training and validation phase for phase in ['train', 'val']: if phase == 'train': model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == 'train'): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # backward + optimize only if in training phase if phase == 'train': loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) if phase == 'train': scheduler.step() epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print('{} Loss: {:.4f} Acc: {:.4f}'.format( phase, epoch_loss, epoch_acc)) # deep copy the model if phase == 'val' and epoch_acc > best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) print() time_elapsed = time.time() - since print('Training complete in {:.0f}m {:.0f}s'.format( time_elapsed // 60, time_elapsed % 60)) print('Best val Acc: {:4f}'.format(best_acc)) # load best model weights model.load_state_dict(best_model_wts) return model 模型预测的可视化 用于显示少量预测图像的通用函数 def visualize_model(model, num_images=6): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): for i, (inputs, labels) in enumerate(dataloaders['val']): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis('off') ax.set_title('predicted: {}'.format(class_names[preds[j]])) imshow(inputs.cpu().data[j]) if images_so_far == num_images: model.train(mode=was_training) return model.train(mode=was_training) 微调卷积神经网络 加载预训练模型并重置最后的全连接层。 model_ft = models.resnet18(pretrained=True) num_ftrs = model_ft.fc.in_features # Here the size of each output sample is set to 2. # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)). model_ft.fc = nn.Linear(num_ftrs, 2) model_ft = model_ft.to(device) criterion = nn.CrossEntropyLoss() # Observe that all parameters are being optimized optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) 训练与评价 在CPU上训练需要大约15-25分钟。但是在GPU上，它只需不到一分钟。 model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25) 输出： Epoch 0/24 ---------- train Loss: 0.6751 Acc: 0.7049 val Loss: 0.1834 Acc: 0.9346 Epoch 1/24 ---------- train Loss: 0.5892 Acc: 0.7746 val Loss: 1.0048 Acc: 0.6667 Epoch 2/24 ---------- train Loss: 0.6568 Acc: 0.7459 val Loss: 0.6047 Acc: 0.8366 Epoch 3/24 ---------- train Loss: 0.4196 Acc: 0.8320 val Loss: 0.4388 Acc: 0.8562 Epoch 4/24 ---------- train Loss: 0.5883 Acc: 0.8033 val Loss: 0.4013 Acc: 0.8889 Epoch 5/24 ---------- train Loss: 0.6684 Acc: 0.7705 val Loss: 0.2666 Acc: 0.9412 Epoch 6/24 ---------- train Loss: 0.5308 Acc: 0.7787 val Loss: 0.4803 Acc: 0.8693 Epoch 7/24 ---------- train Loss: 0.3464 Acc: 0.8566 val Loss: 0.2385 Acc: 0.8954 Epoch 8/24 ---------- train Loss: 0.4586 Acc: 0.7910 val Loss: 0.2064 Acc: 0.9020 Epoch 9/24 ---------- train Loss: 0.3438 Acc: 0.8402 val Loss: 0.2336 Acc: 0.9020 Epoch 10/24 ---------- train Loss: 0.2405 Acc: 0.9016 val Loss: 0.1866 Acc: 0.9346 Epoch 11/24 ---------- train Loss: 0.2335 Acc: 0.8852 val Loss: 0.2152 Acc: 0.9216 Epoch 12/24 ---------- train Loss: 0.3441 Acc: 0.8402 val Loss: 0.2298 Acc: 0.9020 Epoch 13/24 ---------- train Loss: 0.2513 Acc: 0.9098 val Loss: 0.2204 Acc: 0.9020 Epoch 14/24 ---------- train Loss: 0.2745 Acc: 0.8934 val Loss: 0.2439 Acc: 0.8889 Epoch 15/24 ---------- train Loss: 0.2978 Acc: 0.8607 val Loss: 0.2817 Acc: 0.8497 Epoch 16/24 ---------- train Loss: 0.2560 Acc: 0.8975 val Loss: 0.1933 Acc: 0.9281 Epoch 17/24 ---------- train Loss: 0.2326 Acc: 0.9098 val Loss: 0.2176 Acc: 0.9085 Epoch 18/24 ---------- train Loss: 0.2274 Acc: 0.9016 val Loss: 0.2084 Acc: 0.9346 Epoch 19/24 ---------- train Loss: 0.3091 Acc: 0.8689 val Loss: 0.2270 Acc: 0.9150 Epoch 20/24 ---------- train Loss: 0.2540 Acc: 0.8975 val Loss: 0.1957 Acc: 0.9216 Epoch 21/24 ---------- train Loss: 0.3203 Acc: 0.8648 val Loss: 0.1969 Acc: 0.9216 Epoch 22/24 ---------- train Loss: 0.3048 Acc: 0.8443 val Loss: 0.1981 Acc: 0.9346 Epoch 23/24 ---------- train Loss: 0.2526 Acc: 0.9016 val Loss: 0.2415 Acc: 0.8889 Epoch 24/24 ---------- train Loss: 0.3041 Acc: 0.8689 val Loss: 0.1894 Acc: 0.9346 Training complete in 1m 7s Best val Acc: 0.941176 visualize_model(model_ft) 将卷积神经网络为固定特征提取器 在这里，我们需要冻结除最后一层之外的所有网络。我们需要设置requires_grad == False来冻结参数，以便在backward()中不会计算梯度。 您可以在此处的文档中阅读更多相关信息。 model_conv = torchvision.models.resnet18(pretrained=True) for param in model_conv.parameters(): param.requires_grad = False # Parameters of newly constructed modules have requires_grad=True by default num_ftrs = model_conv.fc.in_features model_conv.fc = nn.Linear(num_ftrs, 2) model_conv = model_conv.to(device) criterion = nn.CrossEntropyLoss() # Observe that only parameters of final layer are being optimized as # opposed to before. optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) 训练与评价 在CPU上，与前一个场景相比，大概只花费一半的时间。这在预料之中，因为不需要为绝大多数网络计算梯度。当然，我们还是需要计算前向传播。 model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25) Out: Epoch 0/24 ---------- train Loss: 0.6073 Acc: 0.6598 val Loss: 0.2511 Acc: 0.8954 Epoch 1/24 ---------- train Loss: 0.5457 Acc: 0.7459 val Loss: 0.5169 Acc: 0.7647 Epoch 2/24 ---------- train Loss: 0.4023 Acc: 0.8320 val Loss: 0.2361 Acc: 0.9150 Epoch 3/24 ---------- train Loss: 0.5150 Acc: 0.7869 val Loss: 0.5423 Acc: 0.8039 Epoch 4/24 ---------- train Loss: 0.4142 Acc: 0.8115 val Loss: 0.2257 Acc: 0.9216 Epoch 5/24 ---------- train Loss: 0.6364 Acc: 0.7418 val Loss: 0.3133 Acc: 0.8889 Epoch 6/24 ---------- train Loss: 0.5543 Acc: 0.7664 val Loss: 0.1959 Acc: 0.9412 Epoch 7/24 ---------- train Loss: 0.3552 Acc: 0.8443 val Loss: 0.2013 Acc: 0.9477 Epoch 8/24 ---------- train Loss: 0.3538 Acc: 0.8525 val Loss: 0.1825 Acc: 0.9542 Epoch 9/24 ---------- train Loss: 0.3954 Acc: 0.8402 val Loss: 0.1959 Acc: 0.9477 Epoch 10/24 ---------- train Loss: 0.3615 Acc: 0.8443 val Loss: 0.1779 Acc: 0.9542 Epoch 11/24 ---------- train Loss: 0.3951 Acc: 0.8320 val Loss: 0.1730 Acc: 0.9542 Epoch 12/24 ---------- train Loss: 0.4111 Acc: 0.8156 val Loss: 0.2573 Acc: 0.9150 Epoch 13/24 ---------- train Loss: 0.3073 Acc: 0.8525 val Loss: 0.1901 Acc: 0.9477 Epoch 14/24 ---------- train Loss: 0.3288 Acc: 0.8279 val Loss: 0.2114 Acc: 0.9346 Epoch 15/24 ---------- train Loss: 0.3472 Acc: 0.8525 val Loss: 0.1989 Acc: 0.9412 Epoch 16/24 ---------- train Loss: 0.3309 Acc: 0.8689 val Loss: 0.1757 Acc: 0.9412 Epoch 17/24 ---------- train Loss: 0.3963 Acc: 0.8197 val Loss: 0.1881 Acc: 0.9608 Epoch 18/24 ---------- train Loss: 0.3332 Acc: 0.8484 val Loss: 0.2175 Acc: 0.9412 Epoch 19/24 ---------- train Loss: 0.3419 Acc: 0.8320 val Loss: 0.1932 Acc: 0.9412 Epoch 20/24 ---------- train Loss: 0.3471 Acc: 0.8689 val Loss: 0.1851 Acc: 0.9477 Epoch 21/24 ---------- train Loss: 0.2843 Acc: 0.8811 val Loss: 0.1772 Acc: 0.9477 Epoch 22/24 ---------- train Loss: 0.4024 Acc: 0.8402 val Loss: 0.1818 Acc: 0.9542 Epoch 23/24 ---------- train Loss: 0.2409 Acc: 0.8975 val Loss: 0.2211 Acc: 0.9346 Epoch 24/24 ---------- train Loss: 0.3838 Acc: 0.8238 val Loss: 0.1918 Acc: 0.9412 Training complete in 0m 34s Best val Acc: 0.960784 visualize_model(model_conv) plt.ioff() plt.show() 脚本的总运行时间： （1分钟53.655秒） 由Sphinx-Gallery生成的图库 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/deploy_seq2seq_hybrid_frontend_tutorial.html":{"url":"beginner/deploy_seq2seq_hybrid_frontend_tutorial.html","title":"利用 TorchScript 部署 Seq2Seq 模型","keywords":"","body":"利用 TorchScript 部署 Seq2Seq 模型 作者： Matthew Inkawhich 译者：Foxerlee 校验：Foxerlee 本教程已经更新以适配 pyTorch 1.2 版本。 本教程将逐步介绍使用 TorchScript API 将 sequence-to-sequence 模型转换为 TorchScript 的过程。我们将转换的模型是聊天机器人教程的 Chatbot 模型。您可以将本教程视为聊天机器人教程的“第 2 部分”，并部署自己的预训练模型，也可以从本文档开始使用我们提供的预训练模型。如果您选择使用我们提供的预训练模型，您也可以参考原始的聊天机器人教程，以获取有关数据预处理，模型理论和定义，以及模型训练的详细信息。 什么是 TorchScript？ 在基于深度学习的项目的研究和开发阶段，能够与及时、命令行的界面（例如PyTorch的界面）进行交互是非常有利的。这使用户能够使用熟悉、惯用的 Python 编写 Python 的数据结构，控制流操作，print 语句和调试方法。尽管及时的界面对于研究和实验应用程序是一种有益的工具，但是当需要在生产环境中部署模型时，基于图形的模型表现将会更加适用。延迟的图形表示意味着可以进行无序执行等优化，并具有针对高度优化的硬件体系结构的能力。此外，基于图的表示形式还可以导出框架无关的模型。 PyTorch 提供了将及时模式代码增量转换为 TorchScript 的机制。TorchScript 是 Python 的静态可分析和可优化的子集，Torch 使用它以不依赖于 Python 而运行深度学习程序。 在 torch.jit 模块中可以找到将及时模式的 PyTorch 程序转换为 TorchScript 的 API。该模块中两种将及时模式模型转换为 TorchScript 图形表示形式的核心方式分别为：tracing--追踪和 scripting--脚本。torch.jit.trace 函数接受一个模块或函数以及一组示例的输入。然后通过输入的函数或模块运行输入示例，同时跟跟踪遇到的计算步骤，最后输出一个可以展示跟踪流程的基于图的函数。对于不涉及依赖数据的控制流的简单模块和功能（例如标准卷积神经网络），tracing--追踪非常有用。然而，如果一个有数据依赖的if语句和循环的函数被跟踪，则只记录示例输入沿执行路径调用的操作。换句话说，控制流本身并没有被捕获。为了转换包含依赖于数据的控制流的模块和功能，TorchScript 提供了 scripting--脚本机制。 torch.jit.script 函数/修饰器接受一个模块或函数，不需要示例输入。之后 scripting--脚本 显式化地将模型或函数转换为 TorchScript，包括所有控制流。使用脚本化的需要注意的一点是，它只支持 Python 的一个受限子集。因此您可能需要重写代码以使其与 TorchScript 语法兼容。 有关所有支持的功能的详细信息，请参阅 TorchScript 语言参考。 为了提供最大的灵活性，您还可以将 tracing--追踪和 scripting--脚本模式混合在一起使用而表现整个程序，这种方式可以通过增量的形式实现。 致谢 本教程的灵感来自以下内容： Yuan-Kuei Wu’s pytorch-chatbot implementation: https://github.com/ywk991112/pytorch-chatbot Sean Robertson’s practical-pytorch seq2seq-translation example: https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation FloydHub’s Cornell Movie Corpus preprocessing code: https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus 准备环境 首先，我们将导入所需的模块并设置一些常量。 如果您打算使用自己的模型，请确保正确设置了 MAX_LENGTH 常数。注意，此常数定义了训练期间允许的最大句子长度以及模型能够产生的最大长度输出。 from __future__ import absolute_import from __future__ import division from __future__ import print_function from __future__ import unicode_literals import torch import torch.nn as nn import torch.nn.functional as F import re import os import unicodedata import numpy as np device = torch.device(\"cpu\") MAX_LENGTH = 10 # Maximum sentence length # Default word tokens PAD_token = 0 # Used for padding short sentences SOS_token = 1 # Start-of-sentence token EOS_token = 2 # End-of-sentence token 模型概览 如前所述，我们使用的是 sequence-to-sequence (seq2seq) 模型。 当我们的输入是一个可变长度序列，而我们的输出也是一个可变长度序列，并且不要求输入的一对一映射时，就可以使用这种类型的模型。seq2seq 模型由两个协同工作的递归神经网络 (RNN) 组成：编码器 encoder 和解码器 decoder。 图片来源：https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/ 编码器(Encoder) 编码器 RNN 在输入语句中每次迭代一个标记（例如单词），每个步骤输出一个“输出”向量和一个“隐藏状态”向量。之后，隐藏状态向量将传递到下一个单位步骤，同时记录输出向量。编码器将序列中每个点代表的文本转换为高维空间中的一组点，解码器将使用这些点为给定的任务生成有意义的输出。 解码器(Decoder) 解码器 RNN 以逐个令牌的方式生成响应语句。它使用编码器的上下文向量和内部隐藏状态来生成序列中的下一个单词。它将持续生成单词，直到输出代表句子结尾的 EOS_token。 我们在解码器中使用注意机制 attention mechanism 来帮助它在生成输出时“注意”输入的某些部分。对于我们的模型，我们实现了 Luong 等人的“全球关注 Global attention”模块，并将其用作解码模型中的子模块。 数据处理 尽管我们的模型从概念上讲处理标记序列，但实际上，它们像所有机器学习模型一样处理数字。在这种情况下，训练之前建立的模型词汇表中的每个单词都将映射到一个整数索引。我们使用 Voc 对象存储单词到索引的映射以及词汇表中单词的总数。稍后我们将在运行模型之前加载这个对象。 另外，为了使我们能够进行评估，我们必须提供用于处理字符串输入的工具。normalizeString 函数将字符串中的所有字符转换为小写并删除所有非字母字符。indexsFromSentence 函数接受一个句子并返回包含的单词索引序列。 class Voc: def __init__(self, name): self.name = name self.trimmed = False self.word2index = {} self.word2count = {} self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"} self.num_words = 3 # Count SOS, EOS, PAD def addSentence(self, sentence): for word in sentence.split(' '): self.addWord(word) def addWord(self, word): if word not in self.word2index: self.word2index[word] = self.num_words self.word2count[word] = 1 self.index2word[self.num_words] = word self.num_words += 1 else: self.word2count[word] += 1 # Remove words below a certain count threshold def trim(self, min_count): if self.trimmed: return self.trimmed = True keep_words = [] for k, v in self.word2count.items(): if v >= min_count: keep_words.append(k) print('keep_words {} / {} = {:.4f}'.format( len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index) )) # Reinitialize dictionaries self.word2index = {} self.word2count = {} self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"} self.num_words = 3 # Count default tokens for word in keep_words: self.addWord(word) # Lowercase and remove non-letter characters def normalizeString(s): s = s.lower() s = re.sub(r\"([.!?])\", r\" \\1\", s) s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) return s # Takes string sentence, returns sentence of word indexes def indexesFromSentence(voc, sentence): return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token] 定义编码器 我们使用 torch.nn.GRU 模块实现了编码器的 RNN，该模块提供了一批句子 (嵌入单词的向量) 作为输入，并且它在内部一次遍历一个句子，计算出隐藏状态。我们将此模块初始化为双向的 RNN，这意味着我们有两个独立的 GRU：一个按时间顺序遍历序列，另一个以相反的顺序遍历。我们最终返回这两个 GRU 的输出之和。 由于我们的模型是使用批处理进行训练的，因此我们的 EncoderRNN 模型的前向 forward 函数需要添加一个可填充的批处理输入接口。要批处理可变长度的句子，我们在一个句子中最多允许使用 MAX_LENGTH 个标记，并且批处理中所有少于 MAX_LENGTH 标记的句子都将使用我们专用的 PAD_token 标记在尾部填充。为了使得批处理与 PyTorch RNN 模块可以一起使用，我们必须使用 torch.nn.utils.rnn.pack_padded_sequence 和 torch.nn.utils.rnn.pad_packed_sequence 数据转换函数对前向 forward 密令使用打包。请注意，前向功能还采用了 input_lengths 列表，其中包含批处理中每个句子的长度。 填充时，torch.nn.utils.rnn.pack_padded_sequence 函数将使用此输入。 TorchScript 备注： 由于编码器的前向 forward 函数不包含任何与数据相关的控制流，因此我们将使用 tracing 跟踪将其转换为 script 脚本模式。跟踪模块时，我们可以按原样保留模块的定义。 在进行评估之前，我们将在本文档末尾初始化所有模型。 class EncoderRNN(nn.Module): def __init__(self, hidden_size, embedding, n_layers=1, dropout=0): super(EncoderRNN, self).__init__() self.n_layers = n_layers self.hidden_size = hidden_size self.embedding = embedding # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size' # because our input size is a word embedding with number of features == hidden_size self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout), bidirectional=True) def forward(self, input_seq, input_lengths, hidden=None): # type: (Tensor, Tensor, Optional[Tensor]) -> Tuple[Tensor, Tensor] # Convert word indexes to embeddings embedded = self.embedding(input_seq) # Pack padded batch of sequences for RNN module packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths) # Forward pass through GRU outputs, hidden = self.gru(packed, hidden) # Unpack padding outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs) # Sum bidirectional GRU outputs outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Return output and final hidden state return outputs, hidden 定义解码器的注意力机制模块 接下来，我们将定义注意力模块 (Attn)。请注意，此模块将在我们的解码器模型中用作子模块。在 Luong 的论文中，他考虑了各种“得分函数”，这些函数将当前的解码器 RNN 输出和整个编码器输出作为输入，并返回注意力“能量”。此注意力能量张量的大小与编码器输出的大小相同，并且最终将两者相乘，从而生成加权张量，该张量的最大值表示在特定的解码时间步长下，查询语句中最重要的部分。 # Luong attention layer class Attn(torch.nn.Module): def __init__(self, method, hidden_size): super(Attn, self).__init__() self.method = method if self.method not in ['dot', 'general', 'concat']: raise ValueError(self.method, \"is not an appropriate attention method.\") self.hidden_size = hidden_size if self.method == 'general': self.attn = torch.nn.Linear(self.hidden_size, hidden_size) elif self.method == 'concat': self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size) self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size)) def dot_score(self, hidden, encoder_output): return torch.sum(hidden * encoder_output, dim=2) def general_score(self, hidden, encoder_output): energy = self.attn(encoder_output) return torch.sum(hidden * energy, dim=2) def concat_score(self, hidden, encoder_output): energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh() return torch.sum(self.v * energy, dim=2) def forward(self, hidden, encoder_outputs): # Calculate the attention weights (energies) based on the given method if self.method == 'general': attn_energies = self.general_score(hidden, encoder_outputs) elif self.method == 'concat': attn_energies = self.concat_score(hidden, encoder_outputs) elif self.method == 'dot': attn_energies = self.dot_score(hidden, encoder_outputs) # Transpose max_length and batch_size dimensions attn_energies = attn_energies.t() # Return the softmax normalized probability scores (with added dimension) return F.softmax(attn_energies, dim=1).unsqueeze(1) 定义解码器 与 EncoderRNN 类似，在定义解码器的 RNN 时我们使用 torch.nn.GRU 模块。 但是在这里我们使用单向 GRU。十分需要注意的一点是，与编码器不同，每一次我们只向解码器 RNN 提供一个字。我们首先获得当前单词的嵌入并应用 Dropout。接下来，我们将嵌入和上一步的隐藏状态传播到 GRU，并获得当前 GRU 的输出和隐藏状态。然后，我们使用 Attn 模块作为一个层，来获取注意力权重，然后将其乘以编码器的输出，以获取带参与的编码器输出。我们使用这个带参与的编码器输出作为上下文张量，它表示一个加权和，指示编码器输出中要注意的部分。之后，我们使用线性层和 softmax 归一化层来选择输出序列中的下一个单词。 # TorchScript Notes: # ~~~~~~~~~~~~~~~~~~~~~~ # # Similarly to the ``EncoderRNN``, this module does not contain any # data-dependent control flow. Therefore, we can once again use # **tracing** to convert this model to TorchScript after it # is initialized and its parameters are loaded. # class LuongAttnDecoderRNN(nn.Module): def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1): super(LuongAttnDecoderRNN, self).__init__() # Keep for reference self.attn_model = attn_model self.hidden_size = hidden_size self.output_size = output_size self.n_layers = n_layers self.dropout = dropout # Define layers self.embedding = embedding self.embedding_dropout = nn.Dropout(dropout) self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout)) self.concat = nn.Linear(hidden_size * 2, hidden_size) self.out = nn.Linear(hidden_size, output_size) self.attn = Attn(attn_model, hidden_size) def forward(self, input_step, last_hidden, encoder_outputs): # Note: we run this one step (word) at a time # Get embedding of current input word embedded = self.embedding(input_step) embedded = self.embedding_dropout(embedded) # Forward through unidirectional GRU rnn_output, hidden = self.gru(embedded, last_hidden) # Calculate attention weights from the current GRU output attn_weights = self.attn(rnn_output, encoder_outputs) # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # Concatenate weighted context vector and GRU output using Luong eq. 5 rnn_output = rnn_output.squeeze(0) context = context.squeeze(1) concat_input = torch.cat((rnn_output, context), 1) concat_output = torch.tanh(self.concat(concat_input)) # Predict next word using Luong eq. 6 output = self.out(concat_output) output = F.softmax(output, dim=1) # Return output and final hidden state return output, hidden 定义评估部分 贪心搜索解码器(Greedy Search Decoder) 就像在聊天机器人教程中一样，我们使用贪心搜索解码器 GreedySearchDecoder 模块来简化实际的解码过程。该模块将训练好的编码器和解码器模型作为属性，驱动输入句子 (单词索引的向量) 的编码过程，并且以迭代的方式一次对一个单词解码其对应的输出响应队列。 对输入序列进行编码非常简单：只需将整个序列张量及其对应的长度向量传播给编码器。十分重要的一点是，该模块一次只处理一个输入序列，而不是处理一批序列。因此，当常量 1 用于声明张量大小时，它对应于批处理大小 1。要解码给定的解码器输出，我们必须迭代地向前遍历我们的解码器模型，该模型输出与一个与该单词对应的 softmax 分数，该分数表示这个单词在解码序列中是正确的下一个单词的概率。我们将解码器输入 decoder_input 初始化为包含 SOS_token 的张量。每次通过解码器后，我们都会贪心地将 softmax 概率最高的单词附加到解码的单词列表 decoded_words 中。我们还将这个词用作下一次迭代的解码器输入 decoder_input。如果解码的单词列表 decoded_words 列表的长度已达到 MAX_LENGTH 的长度，或者预测的单词是 EOS_token，则解码过程终止。 TorchScript 备注： 此模块的前向 forward 方法在每次对一个输出字解码时，在 [0，max_length) 范围内进行迭代。因此，我们应该使用脚本将该模块转换为 TorchScript。不同于我们可以跟踪的编码器和解码器模型，我们必须对 GreedySearchDecoder 模块进行一些必要的更改，以初始化一个没有错误的对象。换句话说，我们必须确保我们的模块遵守 TorchScript 机制的规则，并且不使用 TorchScript 所包含的 Python 子集之外的任何语言特性。 变化内容： 在构造函数参数中添加 decoder_n_layers 这种变化是源于：我们传递给该模块的编码器和解码器模型将是 TracedModule(而不是 Module) 的子对象。 因此，我们无法使用 coder.n_layers 访问解码器的层数。 取而代之的是，我们计划使用 decoder_n_layers，并在模块构建过程中将此值传递给模块。 将新属性存储为常量 在最初的实现中，我们可以在 GreedySearchDecoder 的前向 forward 函数中自由使用周围 (全局) 范围的变量。但是，既然我们正在使用脚本，我们就没有这种自由，因为脚本假设我们不一定必须坚持使用 Python 对象，尤其是在导出时。一个简单的解决方案是将这些全局值作为属性存储在构造函数中的模块中，并将它们添加到名为 __constants__ 的特殊列表中，以便在使用前向 forward 方法构造图像时将它们用作文本值。 这种用法的一个例子是在 19 行，在这里我们使用常量属性 self._device 和 self._SOS_token 而不是使用全局值 device 和 SOS_token。 强制定义前向 forward 方法中参数的类型 默认情况下，TorchScript 函数的所有参数均假定为张量 Tensor。如果需要传递其他类型的参数，则可以使用 PEP 3107 中引入的函数类型注释。此外，可以使用 MyPy-style 样式类型注释声明不同类型的参数 (参加文档)。 更改解码器输入 decoder_input 的初始化方法 在原始实现中，我们使用 torch.LongTensor([[SOS_token]]) 初始化了解码器输入张量。编写脚本时，不允许以这种文本方式初始化张量。相反，我们可以使用显式的张量函数 (例如 torch.ones) 来初始化张量。在这种情况下，我们可以通过 1 乘以存储在常量 self._SOS_token 中的 SOS_token 值来简单地实现复制解码器输入 decoder_input 的张量。 class GreedySearchDecoder(nn.Module): def __init__(self, encoder, decoder, decoder_n_layers): super(GreedySearchDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self._device = device self._SOS_token = SOS_token self._decoder_n_layers = decoder_n_layers __constants__ = ['_device', '_SOS_token', '_decoder_n_layers'] def forward(self, input_seq : torch.Tensor, input_length : torch.Tensor, max_length : int): # Forward input through encoder model encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length) # Prepare encoder's final hidden layer to be first hidden input to the decoder decoder_hidden = encoder_hidden[:self._decoder_n_layers] # Initialize decoder input with SOS_token decoder_input = torch.ones(1, 1, device=self._device, dtype=torch.long) * self._SOS_token # Initialize tensors to append decoded words to all_tokens = torch.zeros([0], device=self._device, dtype=torch.long) all_scores = torch.zeros([0], device=self._device) # Iteratively decode one word token at a time for _ in range(max_length): # Forward pass through decoder decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs) # Obtain most likely word token and its softmax score decoder_scores, decoder_input = torch.max(decoder_output, dim=1) # Record token and score all_tokens = torch.cat((all_tokens, decoder_input), dim=0) all_scores = torch.cat((all_scores, decoder_scores), dim=0) # Prepare current token to be next decoder input (add a dimension) decoder_input = torch.unsqueeze(decoder_input, 0) # Return collections of word tokens and scores return all_tokens, all_scores 输入评估 接下来，我们定义一些函数来评估输入的值。评估 evaluate 函数接受正则化的字符串语句，将其处理为其对应的单词索引的张量 (批大小为 1)，并将该张量传递给命名为 Searcher 的 GreedySearchDecoder 实例以处理编码/解码过程。searcher 返回输出的单词索引向量和与每个解码的单词对应的 softmax 得分相对应的得分张量。最后一步是使用 voc.index2word 将每个单词索引转换回其字符串表示形式。 我们还定义了两个函数来评估输入句子。validateInput 函数提示用户输入并进行评估。它将一直请求用户输入，直到用户输入“q”或“quit”。 validateExample 函数仅将字符串输入语句作为参数，对其进行规范化，评估并输出响应。 def evaluate(searcher, voc, sentence, max_length=MAX_LENGTH): ### Format input sentence as a batch # words -> indexes indexes_batch = [indexesFromSentence(voc, sentence)] # Create lengths tensor lengths = torch.tensor([len(indexes) for indexes in indexes_batch]) # Transpose dimensions of batch to match models' expectations input_batch = torch.LongTensor(indexes_batch).transpose(0, 1) # Use appropriate device input_batch = input_batch.to(device) lengths = lengths.to(device) # Decode sentence with searcher tokens, scores = searcher(input_batch, lengths, max_length) # indexes -> words decoded_words = [voc.index2word[token.item()] for token in tokens] return decoded_words # Evaluate inputs from user input (stdin) def evaluateInput(searcher, voc): input_sentence = '' while(1): try: # Get input sentence input_sentence = input('> ') # Check if it is quit case if input_sentence == 'q' or input_sentence == 'quit': break # Normalize sentence input_sentence = normalizeString(input_sentence) # Evaluate sentence output_words = evaluate(searcher, voc, input_sentence) # Format and print response sentence output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')] print('Bot:', ' '.join(output_words)) except KeyError: print(\"Error: Encountered unknown word.\") # Normalize input sentence and call evaluate() def evaluateExample(sentence, searcher, voc): print(\"> \" + sentence) # Normalize sentence input_sentence = normalizeString(sentence) # Evaluate sentence output_words = evaluate(searcher, voc, input_sentence) output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')] print('Bot:', ' '.join(output_words)) 加载预训练参数 好了，是时候加载我们的模型了！ 使用本文提供的模型 要加载本文提供的模型： 在这里下载模型。 设置 loadFilename 变量为下载的检查点文件的路径。 取消 checkpoint = torch.load(loadFilename) 的注释，因为该模型在 CPU 上训练。 使用您自己的模型 加载您自己的预先训练的模型： 将 loadFilename 变量设置为要加载的检查点文件的路径。 请注意，如果您遵循了聊天机器人教程中保存模型的约定，则可能涉及更改 model_name，encoder_n_layers，decoder_n_layers，hidden_size 和 checkpoint_iter (因为这些值在模型路径中会被使用)。 如果您在 CPU 上训练模型，请确保在 checkpoint = torch.load(loadFilename) 行读取检查点文件。 如果您在 GPU 上训练模型并且在 CPU 上运行本教程，请取消注释 checkpoint = torch.load(loadFilename，map_location = torch.device('cpu')) 行。 TorchScript 备注 注意，我们像通常一样将参数初始化并加载到我们的编码器和解码器模型中。如果您对模型的某些部分使用跟踪模式 (torch.jit.trace)，则必须先调用 .to(device) 设置模型的设备选项，然后在追踪模型之前调用 .eval() 来将 dropout 层设置为测试模型。TracedModule 对象不继承 to 或 eval 方法。由于在本教程中，我们仅使用脚本而不是跟踪，因此只需要在执行评估之前执行此操作 (这与我们在命令行模式下通常执行的操作相同)。 save_dir = os.path.join(\"data\", \"save\") corpus_name = \"cornell movie-dialogs corpus\" # Configure models model_name = 'cb_model' attn_model = 'dot' #attn_model = 'general' #attn_model = 'concat' hidden_size = 500 encoder_n_layers = 2 decoder_n_layers = 2 dropout = 0.1 batch_size = 64 # If you're loading your own model # Set checkpoint to load from checkpoint_iter = 4000 # loadFilename = os.path.join(save_dir, model_name, corpus_name, # '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size), # '{}_checkpoint.tar'.format(checkpoint_iter)) # If you're loading the hosted model loadFilename = 'data/4000_checkpoint.tar' # Load model # Force CPU device options (to match tensors in this tutorial) checkpoint = torch.load(loadFilename, map_location=torch.device('cpu')) encoder_sd = checkpoint['en'] decoder_sd = checkpoint['de'] encoder_optimizer_sd = checkpoint['en_opt'] decoder_optimizer_sd = checkpoint['de_opt'] embedding_sd = checkpoint['embedding'] voc = Voc(corpus_name) voc.__dict__ = checkpoint['voc_dict'] print('Building encoder and decoder ...') # Initialize word embeddings embedding = nn.Embedding(voc.num_words, hidden_size) embedding.load_state_dict(embedding_sd) # Initialize encoder & decoder models encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout) decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout) # Load trained model params encoder.load_state_dict(encoder_sd) decoder.load_state_dict(decoder_sd) # Use appropriate device encoder = encoder.to(device) decoder = decoder.to(device) # Set dropout layers to eval mode encoder.eval() decoder.eval() print('Models built and ready to go!') 输出： Building encoder and decoder ... Models built and ready to go! 转换模型为 TorchScript 编码器 如前所述，要将编码器模型转换为 TorchScript ，我们使用脚本 scripting。 编码器模型接受一个输入序列和相应的长度张量。因此，我们创建一个示例输入序列张量 test_seq，其大小适当 (MAX_LENGTH, 1)，包含适当范围[0, voc.num_words) 中的数字，并且具有适当的类型 (int64)。 我们还创建了一个 test_seq_length 标量，该标量实际上等于 test_seq 中单词的个数。下一步是使用 torch.jit.trace 函数来跟踪模型。 请注意，我们传递的第一个参数是我们要跟踪的模块，第二个参数是模块的前向 forward 方法的参数元组。 解码器 我们执行与跟踪编码器相同的过程来跟踪解码器。注意，我们将一组随机输入传播给 traced_encoder 以获得解码器所需的输出。这不是必需的，因为我们也可以简单地制造出具有正确形状，类型和值范围的张量。 这种方法之所以可行，是因为在我们的情况下，我们对张量的值没有任何限制，因为我们没有任何可能会超出范围输入的操作。 贪心搜索解码器(GreedySearchDecoder) 回想一下，由于存在依赖于数据的控制流，我们为搜索器模块编写了脚本。在脚本化的情况下，我们通过添加修饰符并确保实现符合脚本规则来预先完成转换工作。我们初始化脚本搜索器的方式与初始化未脚本化变量的方式相同。 ### Compile the whole greedy search model to TorchScript model # Create artificial inputs test_seq = torch.LongTensor(MAX_LENGTH, 1).random_(0, voc.num_words).to(device) test_seq_length = torch.LongTensor([test_seq.size()[0]]).to(device) # Trace the model traced_encoder = torch.jit.trace(encoder, (test_seq, test_seq_length)) ### Convert decoder model # Create and generate artificial inputs test_encoder_outputs, test_encoder_hidden = traced_encoder(test_seq, test_seq_length) test_decoder_hidden = test_encoder_hidden[:decoder.n_layers] test_decoder_input = torch.LongTensor(1, 1).random_(0, voc.num_words) # Trace the model traced_decoder = torch.jit.trace(decoder, (test_decoder_input, test_decoder_hidden, test_encoder_outputs)) ### Initialize searcher module by wrapping ``torch.jit.script`` call scripted_searcher = torch.jit.script(GreedySearchDecoder(traced_encoder, traced_decoder, decoder.n_layers)) 图像输出 现在我们的模型为 TorchScript 形式，我们可以打印每个模型的图，以确保适当地捕获了计算图。由于TorchScript允许我们递归编译整个模型层次结构，并将编码器和解码器图内联到单个图中，因此我们只需要打印 scripted_searcher 图。 print('scripted_searcher graph:\\n', scripted_searcher.graph) 输出: scripted_searcher graph: graph(%self : ClassType, %input_seq.1 : Tensor, %input_length.1 : Tensor, %max_length.1 : int): %20 : None = prim::Constant() %157 : int = prim::Constant[value=9223372036854775807](), scope: EncoderRNN # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:310:0 %152 : float = prim::Constant[value=0](), scope: EncoderRNN # /opt/conda/lib/python3.6/site-packages/torch/nn/utils/rnn.py:329:0 %142 : float = prim::Constant[value=0.1](), scope: EncoderRNN/GRU[gru] # /opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:683:0 %141 : int = prim::Constant[value=2](), scope: EncoderRNN/GRU[gru] # /opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:683:0 %140 : bool = prim::Constant[value=1](), scope: EncoderRNN/GRU[gru] # /opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:683:0 %134 : int = prim::Constant[value=6](), scope: EncoderRNN/GRU[gru] # /opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692:0 %132 : int = prim::Constant[value=500](), scope: EncoderRNN/GRU[gru] # /opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692:0 %123 : int = prim::Constant[value=4](), scope: EncoderRNN # /opt/conda/lib/python3.6/site-packages/torch/nn/utils/rnn.py:272:0 %122 : Device = prim::Constant[value=\"cpu\"](), scope: EncoderRNN # /opt/conda/lib/python3.6/site-packages/torch/nn/utils/rnn.py:272:0 %119 : bool = prim::Constant[value=0](), scope: EncoderRNN/Embedding[embedding] # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1484:0 %118 : int = prim::Constant[value=-1](), scope: EncoderRNN/Embedding[embedding] # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1484:0 %12 : int = prim::Constant[value=0]() # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:560:25 %14 : int = prim::Constant[value=1]() # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:560:25 %4 : ClassType = prim::GetAttr[name=\"encoder\"](%self) %99 : ClassType = prim::GetAttr[name=\"embedding\"](%4) %weight.3 : Tensor = prim::GetAttr[name=\"weight\"](%99) %101 : ClassType = prim::GetAttr[name=\"gru\"](%4) %102 : Tensor = prim::GetAttr[name=\"bias_hh_l0\"](%101) %103 : Tensor = prim::GetAttr[name=\"bias_hh_l0_reverse\"](%101) %104 : Tensor = prim::GetAttr[name=\"bias_hh_l1\"](%101) %105 : Tensor = prim::GetAttr[name=\"bias_hh_l1_reverse\"](%101) %106 : Tensor = prim::GetAttr[name=\"bias_ih_l0\"](%101) %107 : Tensor = prim::GetAttr[name=\"bias_ih_l0_reverse\"](%101) %108 : Tensor = prim::GetAttr[name=\"bias_ih_l1\"](%101) %109 : Tensor = prim::GetAttr[name=\"bias_ih_l1_reverse\"](%101) %110 : Tensor = prim::GetAttr[name=\"weight_hh_l0\"](%101) %111 : Tensor = prim::GetAttr[name=\"weight_hh_l0_reverse\"](%101) %112 : Tensor = prim::GetAttr[name=\"weight_hh_l1\"](%101) %113 : Tensor = prim::GetAttr[name=\"weight_hh_l1_reverse\"](%101) %114 : Tensor = prim::GetAttr[name=\"weight_ih_l0\"](%101) %115 : Tensor = prim::GetAttr[name=\"weight_ih_l0_reverse\"](%101) %116 : Tensor = prim::GetAttr[name=\"weight_ih_l1\"](%101) %117 : Tensor = prim::GetAttr[name=\"weight_ih_l1_reverse\"](%101) %input.7 : Float(10, 1, 500) = aten::embedding(%weight.3, %input_seq.1, %118, %119, %119), scope: EncoderRNN/Embedding[embedding] # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1484:0 %lengths : Long(1) = aten::to(%input_length.1, %122, %123, %119, %119), scope: EncoderRNN # /opt/conda/lib/python3.6/site-packages/torch/nn/utils/rnn.py:272:0 %input.1 : Float(10, 500), %batch_sizes : Long(10) = aten::_pack_padded_sequence(%input.7, %lengths, %119), scope: EncoderRNN # /opt/conda/lib/python3.6/site-packages/torch/nn/utils/rnn.py:282:0 %133 : int[] = prim::ListConstruct(%123, %14, %132), scope: EncoderRNN/GRU[gru] %hx : Float(4, 1, 500) = aten::zeros(%133, %134, %12, %122, %119), scope: EncoderRNN/GRU[gru] # /opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692:0 %139 : Tensor[] = prim::ListConstruct(%114, %110, %106, %102, %115, %111, %107, %103, %116, %112, %108, %104, %117, %113, %109, %105), scope: EncoderRNN/GRU[gru] %145 : Float(10, 1000), %146 : Float(4, 1, 500) = aten::gru(%input.1, %batch_sizes, %hx, %139, %140, %141, %142, %119, %140), scope: EncoderRNN/GRU[gru] # /opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:683:0 %148 : int = aten::size(%batch_sizes, %12), scope: EncoderRNN # /opt/conda/lib/python3.6/site-packages/torch/nn/utils/rnn.py:320:0 %max_seq_length : Long() = prim::NumToTensor(%148), scope: EncoderRNN %150 : int = aten::Int(%max_seq_length), scope: EncoderRNN %outputs : Float(10, 1, 1000), %154 : Long(1) = aten::_pad_packed_sequence(%145, %batch_sizes, %119, %152, %150), scope: EncoderRNN # /opt/conda/lib/python3.6/site-packages/torch/nn/utils/rnn.py:329:0 %159 : Float(10, 1, 1000) = aten::slice(%outputs, %12, %12, %157, %14), scope: EncoderRNN # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:310:0 %164 : Float(10, 1, 1000) = aten::slice(%159, %14, %12, %157, %14), scope: EncoderRNN # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:310:0 %169 : Float(10, 1, 500) = aten::slice(%164, %141, %12, %132, %14), scope: EncoderRNN # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:310:0 %174 : Float(10, 1, 1000) = aten::slice(%outputs, %12, %12, %157, %14), scope: EncoderRNN # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:310:0 %179 : Float(10, 1, 1000) = aten::slice(%174, %14, %12, %157, %14), scope: EncoderRNN # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:310:0 %184 : Float(10, 1, 500) = aten::slice(%179, %141, %132, %157, %14), scope: EncoderRNN # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:310:0 %186 : Float(10, 1, 500) = aten::add(%169, %184, %14), scope: EncoderRNN # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:310:0 %decoder_hidden.1 : Tensor = aten::slice(%146, %12, %12, %141, %14) # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:560:25 %19 : int[] = prim::ListConstruct(%14, %14) %22 : Tensor = aten::ones(%19, %123, %20, %122, %20) # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:562:24 %decoder_input.1 : Tensor = aten::mul(%22, %14) # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:562:24 %25 : int[] = prim::ListConstruct(%12) %all_tokens.1 : Tensor = aten::zeros(%25, %123, %20, %122, %20) # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:564:21 %31 : int[] = prim::ListConstruct(%12) %all_scores.1 : Tensor = aten::zeros(%31, %20, %20, %122, %20) # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:565:21 %all_tokens : Tensor, %all_scores : Tensor, %decoder_hidden : Tensor, %decoder_input : Tensor = prim::Loop(%max_length.1, %140, %all_tokens.1, %all_scores.1, %decoder_hidden.1, %decoder_input.1) # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:567:8 block0(%41 : int, %all_tokens.6 : Tensor, %all_scores.6 : Tensor, %decoder_hidden.5 : Tensor, %decoder_input.9 : Tensor): %42 : ClassType = prim::GetAttr[name=\"decoder\"](%self) %188 : ClassType = prim::GetAttr[name=\"embedding\"](%42) %weight.1 : Tensor = prim::GetAttr[name=\"weight\"](%188) %190 : ClassType = prim::GetAttr[name=\"gru\"](%42) %191 : Tensor = prim::GetAttr[name=\"bias_hh_l0\"](%190) %192 : Tensor = prim::GetAttr[name=\"bias_hh_l1\"](%190) %193 : Tensor = prim::GetAttr[name=\"bias_ih_l0\"](%190) %194 : Tensor = prim::GetAttr[name=\"bias_ih_l1\"](%190) %195 : Tensor = prim::GetAttr[name=\"weight_hh_l0\"](%190) %196 : Tensor = prim::GetAttr[name=\"weight_hh_l1\"](%190) %197 : Tensor = prim::GetAttr[name=\"weight_ih_l0\"](%190) %198 : Tensor = prim::GetAttr[name=\"weight_ih_l1\"](%190) %199 : ClassType = prim::GetAttr[name=\"concat\"](%42) %weight.2 : Tensor = prim::GetAttr[name=\"weight\"](%199) %bias.1 : Tensor = prim::GetAttr[name=\"bias\"](%199) %202 : ClassType = prim::GetAttr[name=\"out\"](%42) %weight : Tensor = prim::GetAttr[name=\"weight\"](%202) %bias : Tensor = prim::GetAttr[name=\"bias\"](%202) %input.2 : Float(1, 1, 500) = aten::embedding(%weight.1, %decoder_input.9, %118, %119, %119), scope: LuongAttnDecoderRNN/Embedding[embedding] # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1484:0 %input.3 : Float(1, 1, 500) = aten::dropout(%input.2, %142, %119), scope: LuongAttnDecoderRNN/Dropout[embedding_dropout] # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:807:0 %212 : Tensor[] = prim::ListConstruct(%197, %195, %193, %191, %198, %196, %194, %192), scope: LuongAttnDecoderRNN/GRU[gru] %hidden : Float(1, 1, 500), %220 : Float(2, 1, 500) = aten::gru(%input.3, %decoder_hidden.5, %212, %140, %141, %142, %119, %119, %119), scope: LuongAttnDecoderRNN/GRU[gru] # /opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:680:0 %221 : Float(10, 1, 500) = aten::mul(%hidden, %186), scope: LuongAttnDecoderRNN/Attn[attn] # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:344:0 %223 : int[] = prim::ListConstruct(%141), scope: LuongAttnDecoderRNN/Attn[attn] %attn_energies : Float(10, 1) = aten::sum(%221, %223, %119, %20), scope: LuongAttnDecoderRNN/Attn[attn] # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:344:0 %input.4 : Float(1, 10) = aten::t(%attn_energies), scope: LuongAttnDecoderRNN/Attn[attn] # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:364:0 %230 : Float(1, 10) = aten::softmax(%input.4, %14, %20), scope: LuongAttnDecoderRNN/Attn[attn] # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1231:0 %attn_weights : Float(1, 1, 10) = aten::unsqueeze(%230, %14), scope: LuongAttnDecoderRNN/Attn[attn] # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:367:0 %235 : Float(1, 10, 500) = aten::transpose(%186, %12, %14), scope: LuongAttnDecoderRNN # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:428:0 %context.1 : Float(1, 1, 500) = aten::bmm(%attn_weights, %235), scope: LuongAttnDecoderRNN # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:428:0 %rnn_output : Float(1, 500) = aten::squeeze(%hidden, %12), scope: LuongAttnDecoderRNN # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:430:0 %context : Float(1, 500) = aten::squeeze(%context.1, %14), scope: LuongAttnDecoderRNN # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:431:0 %241 : Tensor[] = prim::ListConstruct(%rnn_output, %context), scope: LuongAttnDecoderRNN %input.5 : Float(1, 1000) = aten::cat(%241, %14), scope: LuongAttnDecoderRNN # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:432:0 %244 : Float(1000, 500) = aten::t(%weight.2), scope: LuongAttnDecoderRNN/Linear[concat] # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1370:0 %247 : Float(1, 500) = aten::addmm(%bias.1, %input.5, %244, %14, %14), scope: LuongAttnDecoderRNN/Linear[concat] # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1370:0 %input.6 : Float(1, 500) = aten::tanh(%247), scope: LuongAttnDecoderRNN # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:433:0 %249 : Float(500, 7826) = aten::t(%weight), scope: LuongAttnDecoderRNN/Linear[out] # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1370:0 %input : Float(1, 7826) = aten::addmm(%bias, %input.6, %249, %14, %14), scope: LuongAttnDecoderRNN/Linear[out] # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1370:0 %255 : Float(1, 7826) = aten::softmax(%input, %14, %20), scope: LuongAttnDecoderRNN # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1231:0 %decoder_scores.1 : Tensor, %decoder_input.3 : Tensor = aten::max(%255, %14, %119) # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:571:44 %59 : Tensor[] = prim::ListConstruct(%all_tokens.6, %decoder_input.3) %all_tokens.3 : Tensor = aten::cat(%59, %12) # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:573:25 %65 : Tensor[] = prim::ListConstruct(%all_scores.6, %decoder_scores.1) %all_scores.3 : Tensor = aten::cat(%65, %12) # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:574:25 %decoder_input.7 : Tensor = aten::unsqueeze(%decoder_input.3, %12) # /var/lib/jenkins/workspace/beginner_source/deploy_seq2seq_hybrid_frontend_tutorial.py:576:28 -> (%140, %all_tokens.3, %all_scores.3, %220, %decoder_input.7) %73 : (Tensor, Tensor) = prim::TupleConstruct(%all_tokens, %all_scores) return (%73) 运行结果评估 最后，我们将使用 TorchScript 模型对聊天机器人模型进行评估。如果转换正确，模型的行为将与它们在即时模式表示中的行为完全相同。 默认情况下，我们计算一些常见的查询语句。如果您想自己与机器人聊天，取消对 evaluateInput 行的注释。 # Use appropriate device scripted_searcher.to(device) # Set dropout layers to eval mode scripted_searcher.eval() # Evaluate examples sentences = [\"hello\", \"what's up?\", \"who are you?\", \"where am I?\", \"where are you from?\"] for s in sentences: evaluateExample(s, scripted_searcher, voc) # Evaluate your input #evaluateInput(traced_encoder, traced_decoder, scripted_searcher, voc) 输出: > hello Bot: hello . > what's up? Bot: i m going to get my car . > who are you? Bot: i m the owner . > where am I? Bot: in the house . > where are you from? Bot: south america . 保存模型 现在我们已经成功地将模型转换为 TorchScript，接下来将对其进行序列化，以便在非 python 部署环境中使用。为此，我们只需保存 scripted_searcher 模块，因为这是用于对聊天机器人模型运行推理的面向用户的接口。保存脚本模块时，使用 script_module.save(PATH) 代替 torch.save(model, PATH)。 scripted_searcher.save(\"scripted_chatbot.pth\") 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"intermediate/tensorboard_tutorial.html":{"url":"intermediate/tensorboard_tutorial.html","title":"使用 TensorBoard 可视化模型，数据和训练","keywords":"","body":"使用 TensorBoard 可视化模型，数据和训练 在60分钟闪电战中，我们向您展示了如何加载数据，如何通过定义为的子类的nn.Module模型提供数据，如何在训练数据上训练该模型以及如何在测试数据上对其进行测试。为了了解发生了什么，我们在模型训练期间打印一些统计数据，以了解训练是否在进行。但是，我们可以做得更好：PyTorch与TensorBoard集成在一起，TensorBoard是一种工具，用于可视化神经网络训练运行的结果。本教程使用Fashion-MNIST数据集说明了其某些功能，该 数据集 可以使用torchvision.datasets读取到PyTorch中。 在本教程中，我们将学习如何： 读入数据并进行适当的转换（与先前的教程几乎相同）。 设置TensorBoard。 写入TensorBoard。 使用TensorBoard检查模型架构。 使用TensorBoard以更少的代码创建我们在上一个教程中创建的可视化的交互式版本 具体来说，在第5点，我们将看到： 检查我们训练数据的几种方法 在训练过程中如何跟踪模型的性能 训练后如何评估模型的性能。 我们将从与CIFAR-10教程类似的样板代码开始： # imports import matplotlib.pyplot as plt import numpy as np import torch import torchvision import torchvision.transforms as transforms import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # transforms transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) # datasets trainset = torchvision.datasets.FashionMNIST('./data', download=True, train=True, transform=transform) testset = torchvision.datasets.FashionMNIST('./data', download=True, train=False, transform=transform) # dataloaders trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2) # constant for classes classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot') # helper function to show an image # (used in the `plot_classes_preds` function below) def matplotlib_imshow(img, one_channel=False): if one_channel: img = img.mean(dim=0) img = img / 2 + 0.5 # unnormalize npimg = img.numpy() if one_channel: plt.imshow(npimg, cmap=\"Greys\") else: plt.imshow(np.transpose(npimg, (1, 2, 0))) 我们将在该教程中定义一个类似的模型体系结构，仅需进行少量修改即可解决以下事实：图像现在是一个通道而不是三个通道，而图像是28x28而不是32x32： class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 4 * 4, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 4 * 4) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() 我们将optimizer与criterion之前定义相同： criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) 1. TensorBoard设置 现在我们将设置TensorBoard，tensorboard从我们的关键对象导入torch.utils并定义它SummaryWriter，该关键对象用于将信息写入TensorBoard。 from torch.utils.tensorboard import SummaryWriter # default `log_dir`is \"runs\" - we'll be more specific here writer = SummaryWriter('runs/fashion_mnist_experiment_1') 请注意，这条线单独创建一个runs/fashion_mnist_experiment_1文件夹中。 2.写入TensorBoard 现在，让我们写我们的TensorBoard形象-具体而言，一个网格-使用make_grid。 # get some random training images dataiter = iter(trainloader) images, labels = dataiter.next() # create grid of images img_grid = torchvision.utils.make_grid(images) # show images matplotlib_imshow(img_grid, one_channel=True) # write to tensorboard writer.add_image('four_fashion_mnist_images', img_grid) 现在运行 tensorboard --logdir=runs 在命令行，然后导航到https://localhost:6006/应该显示如下。 现在您知道如何使用TensorBoard了！但是，此示例可以在Jupyter Notebook中完成-TensorBoard真正擅长的地方是创建交互式可视化。我们将在接下来的内容中介绍其中之一，并在本教程结束时介绍更多内容。 3. 使用TensorBoard检查模型 TensorBoard的优势之一是其可视化复杂模型结构的能力。让我们可视化我们构建的模型。 writer.add_graph(net, images) writer.close() 现在刷新TensorBoard后，您应该会看到一个“ Graphs”标签，如下所示： 继续并双击 “Net” 以展开它，查看组成模型的各个操作的详细视图。 TensorBoard具有非常方便的功能，用于可视化高维数据，例如在低维空间中的图像数据；接下来我们将介绍。 4. 在TensorBoard中添加一个“投影仪” 我们可以通过 add_embedding 方法可视化高维数据的低维表示 # helper function def select_n_random(data, labels, n=100): ''' Selects n random datapoints and their corresponding labels from a dataset ''' assert len(data) == len(labels) perm = torch.randperm(len(data)) return data[perm][:n], labels[perm][:n] # select random images and their target indices images, labels = select_n_random(trainset.data, trainset.targets) # get the class labels for each image class_labels = [classes[lab] for lab in labels] # log embeddings features = images.view(-1, 28 * 28) writer.add_embedding(features, metadata=class_labels, label_img=images.unsqueeze(1)) writer.close() 现在，在TensorBoard的“投影仪”选项卡中，您可以看到这100张图像-每个图像784维-向下投影到三维空间中。此外，这是交互式的：您可以单击并拖动以旋转三维投影。最后，一些技巧可以使可视化效果更容易看到：在左上方选择“颜色：标签”，并启用“夜间模式”，这将使图像更容易看到，因为它们的背景是白色的： 现在我们已经彻底检查了我们的数据，让我们展示了TensorBoard如何从训练开始就可以使跟踪模型训练和评估更加清晰。 5. 使用TensorBoard跟踪模型训练 在前面的示例中，我们仅每2000次迭代打印一次模型的运行损失。现在，我们将运行损失记录到TensorBoard中，并通过模型查看模型所做的预测plot_classes_preds。 # helper functions def images_to_probs(net, images): ''' Generates predictions and corresponding probabilities from a trained network and a list of images ''' output = net(images) # convert output probabilities to predicted class _, preds_tensor = torch.max(output, 1) preds = np.squeeze(preds_tensor.numpy()) return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)] def plot_classes_preds(net, images, labels): ''' Generates matplotlib Figure using a trained network, along with images and labels from a batch, that shows the network's top prediction along with its probability, alongside the actual label, coloring this information based on whether the prediction was correct or not. Uses the \"images_to_probs\" function. ''' preds, probs = images_to_probs(net, images) # plot the images in the batch, along with predicted and true labels fig = plt.figure(figsize=(12, 48)) for idx in np.arange(4): ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[]) matplotlib_imshow(images[idx], one_channel=True) ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format( classes[preds[idx]], probs[idx] * 100.0, classes[labels[idx]]), color=(\"green\" if preds[idx]==labels[idx].item() else \"red\")) return fig 最后，让我们使用与之前教程中相同的模型训练代码来训练模型，但是每1000批将结果写入TensorBoard，而不是打印到控制台。这是使用 add_scalar 函数完成的 。 另外，在训练过程中，我们将生成一幅图像，显示该批次中包含的四幅图像的模型预测与实际结果。 running_loss = 0.0 for epoch in range(1): # loop over the dataset multiple times for i, data in enumerate(trainloader, 0): # get the inputs; data is a list of [inputs, labels] inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() if i % 1000 == 999: # every 1000 mini-batches... # ...log the running loss writer.add_scalar('training loss', running_loss / 1000, epoch * len(trainloader) + i) # ...log a Matplotlib Figure showing the model's predictions on a # random mini-batch writer.add_figure('predictions vs. actuals', plot_classes_preds(net, inputs, labels), global_step=epoch * len(trainloader) + i) running_loss = 0.0 print('Finished Training') 现在，您可以查看“标量”选项卡，以查看在15,000次训练迭代中绘制的运行损失： 此外，我们可以看看预测在整个学习任意批量制造的模型。查看“图像”选项卡，然后在“预测与实际”可视化条件下向下滚动以查看此内容；这向我们表明，例如，仅经过3000次训练迭代，该模型就已经能够区分出视觉上截然不同的类，例如衬衫，运动鞋和外套，尽管它并没有像后来的训练那样充满信心： 在之前的教程中，我们研究了模型训练后的每班准确性；在这里，我们将使用TensorBoard绘制每个类的精确调用曲线（此处有很好的解释 ）。 6. 使用TensorBoard评估经过训练的模型 # 1. gets the probability predictions in a test_size x num_classes Tensor # 2. gets the preds in a test_size Tensor # takes ~10 seconds to run class_probs = [] class_preds = [] with torch.no_grad(): for data in testloader: images, labels = data output = net(images) class_probs_batch = [F.softmax(el, dim=0) for el in output] _, class_preds_batch = torch.max(output, 1) class_probs.append(class_probs_batch) class_preds.append(class_preds_batch) test_probs = torch.cat([torch.stack(batch) for batch in class_probs]) test_preds = torch.cat(class_preds) # helper function def add_pr_curve_tensorboard(class_index, test_probs, test_preds, global_step=0): ''' Takes in a \"class_index\" from 0 to 9 and plots the corresponding precision-recall curve ''' tensorboard_preds = test_preds == class_index tensorboard_probs = test_probs[:, class_index] writer.add_pr_curve(classes[class_index], tensorboard_preds, tensorboard_probs, global_step=global_step) writer.close() # plot all the pr curves for i in range(len(classes)): add_pr_curve_tensorboard(i, test_probs, test_preds) 现在，您将看到一个“ PR Curves”选项卡，其中包含每个类别的精确调用曲线。继续戳一下；您会看到，在某些类别上，模型的“曲线下面积”接近100％，而在另一些类别上，该面积更低： 这是TensorBoard和PyTorch与之集成的介绍。当然，你可以做一切TensorBoard确实在Jupyter笔记本电脑，但TensorBoard，你得到了默认情况下交互的视觉效果。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/saving_loading_models.html":{"url":"beginner/saving_loading_models.html","title":"保存和加载模型","keywords":"","body":"保存和加载模型 作者： 马修Inkawhich 本文档提供解决方案，以各种关于PyTorch模型的保存和加载使用情况。随时阅读整个文档，或者只是跳到你需要一个期望的使用情况下的代码。 当涉及到保存和加载模型，有三个核心功能熟悉： torch.save ：保存一个序列化的对象到磁盘。此功能使用Python的泡菜实用程序进行序列化。模型，张量，以及各类对象的字典可以使用该功能进行保存。 torch.load ：使用泡菜的在unpickle设施到腌对象文件反序列化到存储器。该功能也有助于该装置加载数据到（见保存&安培;荷载模型跨设备）。 torch.nn.Module.load_state_dict ：使用反序列化 state_dict 加载一个模型的参数字典。有关 更多信息state_dict 参见什么是state_dict？ 。 内容： 什么是state_dict？ 保存&安培;为推理荷载模型 保存&安培;载入通用检查点 在一个文件中保存多个模型 Warmstarting模型从一个不同的模型使用参数 保存&安培;荷载模型跨设备 什么是state_dict？ 在PyTorch中，可学习的参数（即重量和偏见）的torch.nn.Module模型中包含的模型 参数 （带有访问model.parameters（））。 A state_dict 仅仅是每一层映射到其参数张量Python字典对象。注意与可学习参数（卷积层，线性层等）和注册缓冲器（batchnorm的runningmean），只有层具有在条目模型的 _state_dict 。优化器对象（torch.optim）也有一个 state_dict ，它包含有关该优化程序的状态的信息，以及所使用的超参数。 因为 state_dict 对象是Python字典，它们可以方便地保存，更新，修改和恢复，加上模块化的大量工作PyTorch模型和优化。 例如： 让我们来看看从训练分类教程中使用的简单模型 state_dict [HTG1。 # Define model class TheModelClass(nn.Module): def __init__(self): super(TheModelClass, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x # Initialize model model = TheModelClass() # Initialize optimizer optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) # Print model's state_dict print(\"Model's state_dict:\") for param_tensor in model.state_dict(): print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size()) # Print optimizer's state_dict print(\"Optimizer's state_dict:\") for var_name in optimizer.state_dict(): print(var_name, \"\\t\", optimizer.state_dict()[var_name]) 输出： Model's state_dict: conv1.weight torch.Size([6, 3, 5, 5]) conv1.bias torch.Size([6]) conv2.weight torch.Size([16, 6, 5, 5]) conv2.bias torch.Size([16]) fc1.weight torch.Size([120, 400]) fc1.bias torch.Size([120]) fc2.weight torch.Size([84, 120]) fc2.bias torch.Size([84]) fc3.weight torch.Size([10, 84]) fc3.bias torch.Size([10]) Optimizer's state_dict: state {} param_groups [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [4675713712, 4675713784, 4675714000, 4675714072, 4675714216, 4675714288, 4675714432, 4675714504, 4675714648, 4675714720]}] 节省&安培;为荷载模型推断 保存/加载state_dict（推荐） 保存： torch.save(model.state_dict(), PATH) 负载： model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH)) model.eval() 当节省推理模型，只需要保存训练模型的参数得知。保存模型 state_dict 与torch.save（）功能会给你最大的灵活性后恢复模型，这就是为什么它是推荐的方法为保存模型。 一个常见的PyTorch惯例是使用一个.PT或.pth文件扩展名来保存模式。 请记住，你必须调用model.eval（）运行推论之前设置辍学率和批标准化层为评估模式。如果不这样做会产生不一致的推断结果。 Note 注意，load_state_dict（）函数采用一个字典对象，而不是路径保存的对象。这意味着你必须反序列化保存 state_dict 你将它传递给load_state_dict前（）功能。例如，你不能加载使用model.load_state_dict（PATH）。 SAVE / LOAD整个模型 Save: torch.save(model, PATH) Load: # Model class must be defined somewhere model = torch.load(PATH) model.eval() 此保存/加载处理使用最直观的语法和涉及的代码量最少。以这种方式保存的模型将使用Python的泡菜模块保存整个模块。这种方法的缺点是串行数据绑定到特定的类和在保存的模型中使用的精确的目录结构。这样做的原因是因为泡菜不保存模型类本身。相反，它保存到包含类，这是在负载时所使用的文件的路径。正因为如此，你的代码可以在其他项目或refactors后使用时，以各种方式突破。 A common PyTorch convention is to save models using either a .ptor .pth file extension. Remember that you must call model.eval()to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results. 保存&放;加载一般检查点推断和/或恢复训练 保存： torch.save({ 'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'loss': loss, ... }, PATH) 负载： model = TheModelClass(*args, **kwargs) optimizer = TheOptimizerClass(*args, **kwargs) checkpoint = torch.load(PATH) model.load_state_dict(checkpoint['model_state_dict']) optimizer.load_state_dict(checkpoint['optimizer_state_dict']) epoch = checkpoint['epoch'] loss = checkpoint['loss'] model.eval() # - or - model.train() 当保存一般的检查点，以用于任何推理或恢复训练，你必须保存不仅仅是模型的 state_dict [HTG1。它也保存重要的是优化的 _state_dict ，因为这包含了更新的模型火车缓冲区和参数。你可能希望保存其他项目，你离开时，最新的培训记录丢失，外部torch.nn.Embedding层等时代_ 保存多个组件，但在一个字典组织它们，并使用torch.save（）序列化的词典。一个常见的PyTorch约定是为了节省使用的.tar文件扩展名，这些检查站。 要装入的物品，首先初始化模型和优化器，然后装入词典本地使用torch.load（）。从这里，你可以很容易地通过简单的查询你所期望的字典访问保存的项目。 请记住，你必须调用model.eval（）运行推论之前设置辍学率和批标准化层为评估模式。如果不这样做会产生不一致的推断结果。如果你想恢复训练，调用model.train（），以确保这些层在训练模式。 在一个文件中保存多个模型 保存： torch.save({ 'modelA_state_dict': modelA.state_dict(), 'modelB_state_dict': modelB.state_dict(), 'optimizerA_state_dict': optimizerA.state_dict(), 'optimizerB_state_dict': optimizerB.state_dict(), ... }, PATH) 负载： modelA = TheModelAClass(*args, **kwargs) modelB = TheModelBClass(*args, **kwargs) optimizerA = TheOptimizerAClass(*args, **kwargs) optimizerB = TheOptimizerBClass(*args, **kwargs) checkpoint = torch.load(PATH) modelA.load_state_dict(checkpoint['modelA_state_dict']) modelB.load_state_dict(checkpoint['modelB_state_dict']) optimizerA.load_state_dict(checkpoint['optimizerA_state_dict']) optimizerB.load_state_dict(checkpoint['optimizerB_state_dict']) modelA.eval() modelB.eval() # - or - modelA.train() modelB.train() 当保存由多个torch.nn.Modules，例如GAN，序列到序列模型或模型的集合的模型，就按照同样的方法，因为当您要保存一般的检查点。换言之，保存每个模型的 state_dict 和相应的优化的字典。正如前面提到的，你可以保存可以通过简单地追加他们的字典帮助您恢复训练其他任何物品。 一个常见的PyTorch约定是为了节省使用的.tar文件扩展名，这些检查站。 要加载模型中，首先初始化模型和优化器，然后装入词典本地使用torch.load（）。从这里，你可以很容易地通过简单的查询你所期望的字典访问保存的项目。 请记住，你必须调用model.eval（）运行推论之前设置辍学率和批标准化层为评估模式。如果不这样做会产生不一致的推断结果。如果你想恢复训练，调用model.train（）设置这些层的培训模式。 Warmstarting模型中使用的参数从一个不同的模型 保存： torch.save(modelA.state_dict(), PATH) 负载： modelB = TheModelBClass(*args, **kwargs) modelB.load_state_dict(torch.load(PATH), strict=False) 部分加载模型或加载局部模型常见的场景时，转移学习或培训新的复杂的模型。凭借训练有素的参数，即使只有少数是可用的，将有助于WARMSTART训练过程，并希望能帮助你的模型收敛比从头训练快得多。 无论您是从装载部分 state_dict ，它缺少一些键，或加载 state_dict 与比要装载到模型更加按键，可以设置严格参数为 假 在load_state_dict（）函数忽略非匹配密钥。 如果你想从一个层对其他负载参数，但有些键不匹配，只需更改 参数键的名称state_dict 您加载以匹配你是模型的关键装入。 节省&安培;荷载模型跨设备 保存在GPU上，加载在CPU Save: torch.save(model.state_dict(), PATH) Load: device = torch.device('cpu') model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH, map_location=device)) 当加载，将其与一个GPU培养了CPU上的模型，通过torch.device（ 'CPU'）到map_location在torch.load参数（）功能。在这种情况下，张量基础的存储器使用map_location参数动态地重新映射到CPU的设备。 保存在GPU上，加载在GPU Save: torch.save(model.state_dict(), PATH) Load: device = torch.device(\"cuda\") model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH)) model.to(device) # Make sure to call input = input.to(device) on any input tensors that you feed to the model 当加载上进行训练，并且保存在GPU上GPU的模型，简单地转换初始化模型，用model.to（Torch CUDA优化模型。设备（ 'CUDA'））。另外，一定使用。要（torch.device（ 'CUDA'））功能上的所有模型输入到该模型准备数据。请注意，调用my_tensor.to（设备）HTG14]返回GPU的my_tensor新副本。它不会覆盖my_tensor [HTG23。因此，记得手动改写张量：my_tensor = my_tensor.to（torch.device（ 'CUDA'）） 。 节省CPU，装载在GPU Save: torch.save(model.state_dict(), PATH) Load: device = torch.device(\"cuda\") model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\")) # Choose whatever GPU device number you want model.to(device) # Make sure to call input = input.to(device) on any input tensors that you feed to the model 当加载上进行训练，并且保存在CPU一个GPU的模型，将map_location参数在torch.load（）函数为 CUDA：DEVICE_ID 。这将加载模型给定的GPU设备。其次，一定要打电话model.to（torch.device（ 'CUDA'））对模型的参数张量转换为CUDA张量。最后，一定要使用。要（torch.device（ 'CUDA'））功能上的所有模型输入为CUDA优化模型准备数据。请注意，调用my_tensor.to（设备）HTG20]返回GPU的my_tensor新副本。它不会覆盖my_tensor [HTG29。因此，记得手动改写张量：my_tensor = my_tensor.to（torch.device（ 'CUDA'）） 。 节省torch.nn.DataParallel模型 Save: torch.save(model.module.state_dict(), PATH) Load: # Load to whatever device you want torch.nn.DataParallel是一个模型包装，可以并行GPU利用率。要保存数据并行模型一般，保存model.module.state_dict（）。这样，您可以灵活地加载模型要你想要的任何设备的任何方式。 脚本的总运行时间： （0分钟0.000秒） Download Python source code: saving_loading_models.py Download Jupyter notebook: saving_loading_models.ipynb 通过斯芬克斯-廊产生廊 Next Previous Was this helpful? Yes No Thank you ©版权所有2017年，PyTorch。 保存和加载模型 什么是state_dict？ [HTG0例： 保存&安培;为推理荷载模型 保存/加载state_dict（推荐） 保存/加载整个模型 保存&安培;载入通用检查点用于推断和/或恢复训练 保存： 负载： 在一个文件中保存多个模型 保存： 负载： Warmstarting模型从一个不同的模型使用参数 保存： 负载： 保存&安培;荷载模型跨设备 保存在GPU，CPU负荷 节省GPU，GPU的负载 保存在CPU，GPU上负载 保存torch.nn.DataParallel模型 ![](https://www.facebook.com/tr?id=243028289693773&ev=PageView &noscript=1) 分析流量和优化经验，我们为这个站点的Cookie。通过点击或导航，您同意我们的cookies的使用。因为这个网站目前维护者，Facebook的Cookie政策的适用。了解更多信息，包括有关可用的控制：[饼干政策HTG1。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/nn_tutorial.html":{"url":"beginner/nn_tutorial.html","title":"torch.nn 到底是什么？","keywords":"","body":"torch.nn 到底是什么？ 译者：lhc741 校对：DrDavidS 作者：Jeremy Howard，fast.ai。感谢Rachel Thomas和Francisco Ingham的帮助和支持。 我们推荐使用notebook来运行这个教程，而不是脚本，点击这里下载notebook(.ipynb)文件。 Pytorch提供了torch.nn、torch.optim、Dataset和DataLoader这些设计优雅的模块和类以帮助使用者创建和训练神经网络。 为了最大化利用这些模块和类的功能，并使用它们做出适用于你所研究问题的模型，你需要真正理解他们是如何工作的。 为了做到这一点，我们首先基于MNIST数据集训练一个没有任何特征的简单神经网络。 最开始我们只会用到PyTorch中最基本的tensor功能，然后我们将会逐渐的从torch.nn，torch.optim，Dataset，DataLoader中选择一个特征加入到模型中，来展示新加入的特征会对模型产生什么样的效果，以及它是如何使模型变得更简洁或更灵活。 在这个教程中，我们假设你已经安装好了PyTorch，并且已经熟悉了基本的tensor运算。(如果你熟悉Numpy的数组运算，你将会发现这里用到的PyTorch tensor运算和numpy几乎是一样的) MNIST数据安装 我们将要使用经典的MNIST数据集，这个数据集由手写数字（0到9）的黑白图片组成。 我们将使用pathlib来处理文件路径的相关操作（python3中的一个标准库），使用request来下载数据集。 我们只会在用到相关库的时候进行引用，这样你就可以明确在每个操作中用到了哪些库。 from pathlib import Path import requests DATA_PATH = Path(\"data\") PATH = DATA_PATH / \"mnist\" PATH.mkdir(parents=True, exist_ok=True) URL = \"http://deeplearning.net/data/mnist/\" FILENAME = \"mnist.pkl.gz\" if not (PATH / FILENAME).exists(): content = requests.get(URL + FILENAME).content (PATH / FILENAME).open(\"wb\").write(content) 该数据集采用numpy数组格式，并已使用pickle存储，pickle是一个用来把数据序列化为python特定格式的库。 import pickle import gzip with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\") 每一幅图像都是28 x 28的，并被拉平成长度为784(=28x28)的一行。 我们以其中一个为例展示一下，首先需要将这个一行的数据重新变形为一个2d的数据。 %matplotlib inline from matplotlib import pyplot import numpy as np pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\") print(x_train.shape) 输出： (50000, 784) PyTorch使用torch.tensor，而不是numpy数组，所以我们需要将数据转换。 import torch x_train, y_train, x_valid, y_valid = map( torch.tensor, (x_train, y_train, x_valid, y_valid) ) n, c = x_train.shape x_train, x_train.shape, y_train.min(), y_train.max() print(x_train, y_train) print(x_train.shape) print(y_train.min(), y_train.max()) 输出： tensor([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]) tensor([5, 0, 4, ..., 8, 4, 8]) torch.Size([50000, 784]) tensor(0) tensor(9) 神经网络从零开始（不使用torch.nn） 我们先来建立一个只使用PyTorch张量运算的模型。 我们假设你已经熟悉神经网络的基础。（如果你还不熟悉，可以访问course.fast.ai进行学习）。 PyTorch提供创建随机数填充或全零填充张量的方法，我们使用该方法初始化一个简单线性模型的权重和偏置。 这两个都是普通的张量，但它们有一个特殊的附加条件：设置需要计算梯度的参数为True。这样PyTorch就会记录所有与这个张量相关的运算，使其能在反向传播阶段自动计算梯度。 对于weights而言，由于我们希望初始化张量过程中存在梯度，所以我们在初始化之后设置requires_grad。（注意：尾缀为_的方法在PyTorch中表示这个操作会被立即被执行。） 注意： 我们以Xavier初始化方法（每个元素都除以1/sqrt(n)）为例来对权重进行初始化。 import math weights = torch.randn(784, 10) / math.sqrt(784) weights.requires_grad_() bias = torch.zeros(10, requires_grad=True) 多亏了PyTorch具有自动梯度计算功能，我们可以使用Python中任何标准函数（或者可调用对象）来创建模型！ 因此，让我们编写一个普通的矩阵乘法和广播加法建立一个简单的线性模型。 我们还需要一个激活函数，所以我们编写并使用一个log_softmax函数。 请记住：尽管Pytorch提供了许多预先编写好的损失函数、激活函数等等，你仍然可以使用纯python轻松实现你自己的函数。 Pytorch甚至可以自动地为你的函数创建快速的GPU代码或向量化的CPU代码。 def log_softmax(x): return x - x.exp().sum(-1).log().unsqueeze(-1) def model(xb): return log_softmax(xb @ weights + bias) 在上面的一段代码中，@表示点积运算符。我们将调用我们的函数计算一个批次的数据（本例中为64幅图像）。 这是一次模型前向传递的过程。 请注意，因为我们使用了随机数来初始化权重，所以在这个阶段我们的预测值并不会比随机的更好。 bs = 64 # 一批数据个数 xb = x_train[0:bs] # 从x获取一小批数据 preds = model(xb) # 预测值 preds[0], preds.shape print(preds[0], preds.shape) 输出： tensor([-2.5125, -2.3091, -2.0139, -1.8648, -2.7132, -2.5598, -2.3423, -1.8809, -2.5860, -2.7542], grad_fn=) torch.Size([64, 10]) 可以从上面的结果不难看出，张量preds不仅包括了张量值，还包括了梯度函数。这个梯度函数我们可以在后面的反向传播阶段用到。 下面我们来实现一个负的对数似然函数（Negative log-likehood）作为损失函数（同样也使用纯python实现）： def nll(input, target): return -input[range(target.shape[0]), target].mean() loss_func = nll 让我们查看下随机模型的损失值，这样我们就可以确认在执行反向传播的步骤后，模型的预测效果是否有了改进。 yb = y_train[0:bs] print(loss_func(preds, yb)) 输出： tensor(2.3860, grad_fn=) 我们再来实现一个用来计算模型准确率的函数。对于每次预测，我们规定如果预测结果中概率最大的数字和图片实际对应的数字是相同的，那么这次预测就是正确的。 def accuracy(out, yb): preds = torch.argmax(out, dim=1) return (preds == yb).float().mean() 我们先来看一下被随机初始化的模型的准确率，这样我们就可以看到损失值降低的时候准确率是否提高了。 print(accuracy(preds, yb)) 输出： tensor(0.1094) 现在我们可以运行一个完整的训练步骤了，每次迭代，我们会进行以下几个操作： 从全部数据中选择一小批数据（大小为bs） 使用模型进行预测 计算当前预测的损失值 使用loss.backward()更新模型中的梯度，在这个例子中，更新的是weights和bias 现在，我们来利用计算出的梯度对权值和偏置项进行更新，因为我们不希望这一步的操作被用于下一次迭代的梯度计算，所以我们在torch.no_grad()这个上下文管理器中完成。想要了解更多关于PyTorch Autograd是如何记录操作的，可以点击这里。 接下来，我们将梯度设置为0，来为下一次循环做准备。否则我们的梯度将会记录所有已经执行过的运算（如，loss.backward()会将梯度变化值直接与变量已有值进行累加，而不是替换变量原有的值）。 小贴士 您可以使用标准python调试器对PyTorch代码进行单步调试，从而在每一步检查不同的变量值。取消下面的set_trace()注释，来尝试该功能。 from IPython.core.debugger import set_trace lr = 0.5 # 学习率 epochs = 2 # 训练的轮数 for epoch in range(epochs): for i in range((n - 1) // bs + 1): # set_trace() start_i = i * bs end_i = start_i + bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] pred = model(xb) loss = loss_func(pred, yb) loss.backward() with torch.no_grad(): weights -= weights.grad * lr bias -= bias.grad * lr weights.grad.zero_() bias.grad.zero_() 目前为止，我们已经从零开始完成了建立和训练一个最小的神经网络（因为我们建立的logistic回归模型不包含隐层）！ 现在，我们来看一下模型的损失值和准确率，并于我们之前输出的值进行比较。结果正如我们预期的，损失值下降，准确率提高。 print(loss_func(model(xb), yb), accuracy(model(xb), yb)) 输出： tensor(0.0831, grad_fn=) tensor(1.) torch.nn.functional的使用 现在，我们要对前面的代码进行重构，使代码在完成相同功能的同时，用PyTorch的nn来使代码变得更加简洁和灵活。 从现在开始，接下来的每一步我们都会使代码变得更短，更好理解或更灵活。 要进行的第一步也是最简单的一步，是使用torch.nn.functional（通过会在引用时用F表示）中的函数替换我们自己的激活函数和损失函数使代码变得更短。 这个模块包含了torch.nn库中的所有函数（这个库的其它部分是各种类），所以在这个模块中还会找到其它便于建立神经网络的函数，比如池化函数。（模块中还包含卷积函数，线性函数等等，不过在后面的内容中我们会看到，这些操作使用库中的其它部分会更好。） 如果你使用负对数似然损失和对数柔性最大值(softmax)激活函数，PyTorch有一个结合了这两个函数的简单函数F.cross_entropy供你使用，这样我们就可以删掉模型中的激活函数。 import torch.nn.functional as F loss_func = F.cross_entropy def model(xb): return xb @ weights + bias 注意在model函数中我们不再调用log_softmax。现在我们来确认一下损失值和准确率与之前相同。 print(loss_func(model(xb), yb), accuracy(model(xb), yb)) 输出： tensor(0.0822, grad_fn=) tensor(1.) 使用nn.Module进行重构 接下来，我们将会用到nn.Model和nn.Parameter来完成一个更加清晰简洁的训练循环。我们继承nn.Module(它是一个能够跟踪状态的类)。在这个例子中，我们想要新建一个类，实现存储权重，偏置和前向传播步骤中所有用到方法。nn.Module包含了许多属性和方法（比如.parameters()和.zero_grad()），我们会在后面用到。 注意 nn.Module（M大写）是一个PyTorch中特有的概念，它是一个会经常用到的类。不要和Python中module（m小写）混淆，module是一个可以被引入的Python代码文件。 from torch import nn class Mnist_Logistic(nn.Module): def __init__(self): super().__init__() self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784)) self.bias = nn.Parameter(torch.zeros(10)) def forward(self, xb): return xb @ self.weights + self.bias 既然现在我们要使用一个对象而不是函数，我们要先对模型进行实例化。 model = Mnist_Logistic() 现在我们可以像之前那样计算损失值了。注意nn.Module对象的使用方式很像函数（例如它们是可调用的），但是PyTorch将会自动调用我们的forward函数 print(loss_func(model(xb), yb)) 输出： tensor(2.2001, grad_fn=) 之前在每个训练循环中，我们通过变量名对每个变量的值进行更新，并手动的将每个变量的梯度置为0，像这样： with torch.no_grad(): weights -= weights.grad * lr bias -= bias.grad * lr weights.grad.zero_() bias.grad.zero_() 现在我们可以利用model.parameters()和model.zero_grad()（这两个都是PyTorch定义在nn.Module中的）使这些步骤变得更加简洁并且更不容易忘记更新部分参数，尤其是模型很复杂的情况： with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr model.zero_grad() 下面我们把训练循环封装进fit函数中，这样就能在后面再次运行。 def fit(): for epoch in range(epochs): for i in range((n - 1) // bs + 1): start_i = i * bs end_i = start_i + bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] pred = model(xb) loss = loss_func(pred, yb) loss.backward() with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr model.zero_grad() fit() 我们来再次检查一下我们的损失值是否下降。 print(loss_func(model(xb), yb)) 输出： tensor(0.0812, grad_fn=) 使用nn.Linear进行重构 我们继续对代码进行重构。我们将用PyTorch中的nn.Linear代替手动定义和初始化self.weights和self.bias以及计算xb @ self.weights + self.bias, 因为nn.Linear可以完成这些操作。 PyTorch中预设了很多类型的神经网络层，使用它们可以极大的简化我们的代码，通常还会带来速度上的提升。 class Mnist_Logistic(nn.Module): def __init__(self): super().__init__() self.lin = nn.Linear(784, 10) def forward(self, xb): return self.lin(xb) 我们初始化模型并像之前那样计算损失值： model = Mnist_Logistic() print(loss_func(model(xb), yb)) 输出： tensor(2.2731, grad_fn=) 我们仍然可以像之前那样使用fit函数： fit() print(loss_func(model(xb), yb)) 输出： tensor(0.0817, grad_fn=) 使用optim进行重构 PyTorch还有一个包含很多优化算法的包————torch.optim。我们可以使用优化器中的step方法执行前向传播过程中的步骤来替换手动更新每个参数。 这个方法将允许我们替换之前手动编写的优化步骤： with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr model.zero_grad() 替换后如下： opt.step() opt.zero_grad() （optim.zero_grad()将梯度重置为0，我们需要在计算下一次梯度之前调用它） from torch import optim 我们将建立模型和优化器的步骤定义为一个小函数方便将来复用。 def get_model(): model = Mnist_Logistic() return model, optim.SGD(model.parameters(), lr=lr) model, opt = get_model() print(loss_func(model(xb), yb)) for epoch in range(epochs): for i in range((n - 1) // bs + 1): start_i = i * bs end_i = start_i + bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() print(loss_func(model(xb), yb)) 输出： tensor(2.2795, grad_fn=) tensor(0.0813, grad_fn=) 使用Dataset进行重构 Pytorch包含一个Dataset抽象类。Dataset可以是任何东西，但它始终包含一个__len__函数（通过Python中的标准函数len调用）和一个用来索引到内容中的__getitem__函数。 这篇教程以创建Dataset的自定义子类FacialLandmarkDataset为例进行介绍。 PyTorch中的TensorDataset是一个封装了张量的Dataset。通过定义长度和索引的方式，是我们可以对张量的第一维进行迭代，索引和切片。这将使我们在训练中，获取同一行中的自变量和因变量更加容易。 from torch.utils.data import TensorDataset 可以把x_train和y_train中的数据合并成一个简单的TensorDataset，这样就可以方便的进行迭代和切片操作。 train_ds = TensorDataset(x_train, y_train) 之前，我们不得不分别对x和y的值进行迭代循环。 xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] 现在我们可以将这两步合二为一了。 xb,yb = train_ds[i*bs : i*bs+bs] model, opt = get_model() for epoch in range(epochs): for i in range((n - 1) // bs + 1): xb, yb = train_ds[i * bs: i * bs + bs] pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() print(loss_func(model(xb), yb)) 输出： tensor(0.0825, grad_fn=) 使用DataLoader进行重构 PyTorch的DataLoader负责批量数据管理，你可以使用任意的Dataset创建一个DataLoader。DataLoader使得对批量数据的迭代更容易。DataLoader自动的为我们提供每一小批量的数据来代替切片的方式train_ds[i*bs : i*bs+bs]。 from torch.utils.data import DataLoader train_ds = TensorDataset(x_train, y_train) train_dl = DataLoader(train_ds, batch_size=bs) 之前我们像下面这样按批(xb,yb)对数据进行迭代： for i in range((n-1)//bs + 1): xb,yb = train_ds[i*bs : i*bs+bs] pred = model(xb) 现在我们的循环变得更加简洁，因为使用了data loader来自动获取数据。 for xb,yb in train_dl: pred = model(xb) model, opt = get_model() for epoch in range(epochs): for xb, yb in train_dl: pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() print(loss_func(model(xb), yb)) 输出： tensor(0.0823, grad_fn=) 多亏PyTorch中的nn.Module，nn.Parameter，Dataset和DataLoader，我们的训练代码变得非常简洁易懂。下面我们来试着增加一些用于提高模型效率所必需的的基本特征。 增加验证集 在第一部分，我们仅仅是试着为我们的训练集构建一个合理的训练步骤，但实际上，我们始终应该有一个验证集来确认模型是否过拟合。 打乱训练数据的顺序通常是避免不同批数据中存在相关性和过拟合的重要步骤。另一方面，无论是否打乱顺序计算出的验证集损失值都是一样的。鉴于打乱顺序还会消耗额外的时间，所以打乱验证集数据是没有任何意义的。 我们在验证集上用到的每批数据的数量是训练集的两倍，这是因为在验证集上不需要进行反向传播，这样就会占用较小的内存（因为它并不需要储存梯度）。我们利用了这一点，使用了更大的batchsize，更快的计算出了损失值。 train_ds = TensorDataset(x_train, y_train) train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True) valid_ds = TensorDataset(x_valid, y_valid) valid_dl = DataLoader(valid_ds, batch_size=bs * 2) 我们将会在每轮(epoch)结束后计算并输出验证集上的损失值。 （注意：在训练前我们总是会调用model.train()函数，在推断之前调用model.eval()函数，因为这些会被nn.BatchNorm2d，nn.Dropout等层使用，确保在不同阶段的准确性。） model, opt = get_model() for epoch in range(epochs): model.train() for xb, yb in train_dl: pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() model.eval() with torch.no_grad(): valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl) print(epoch, valid_loss / len(valid_dl)) 输出： 0 tensor(0.3039) 1 tensor(0.2842) 编写fit()和get_data()函数 现在我们来重构一下我们自己的函数。 我们在计算训练集和验证集上的损失值时执行了差不多的过程两次，因此我们将这部分代码提炼成一个函数loss_batch，用来计算每个批的损失值。 我们为训练集传递一个优化器参数来执行反向传播。对于验证集我们不传优化器参数，这样就不会执行反向传播。 def loss_batch(model, loss_func, xb, yb, opt=None): loss = loss_func(model(xb), yb) if opt is not None: loss.backward() opt.step() opt.zero_grad() return loss.item(), len(xb) fit执行了训练模型的必要操作，并在每轮(epoch)结束后计算模型在训练集和测试集上的损失。 import numpy as np def fit(epochs, model, loss_func, opt, train_dl, valid_dl): for epoch in range(epochs): model.train() for xb, yb in train_dl: loss_batch(model, loss_func, xb, yb, opt) model.eval() with torch.no_grad(): losses, nums = zip( *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl] ) val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums) print(epoch, val_loss) get_data返回训练集和验证集需要使用到的dataloaders。 def get_data(train_ds, valid_ds, bs): return ( DataLoader(train_ds, batch_size=bs, shuffle=True), DataLoader(valid_ds, batch_size=bs * 2), ) 现在，我们只需要三行代码就可以获取数据、拟合模型了。 train_dl, valid_dl = get_data(train_ds, valid_ds, bs) model, opt = get_model() fit(epochs, model, loss_func, opt, train_dl, valid_dl) 输出： 0 0.3368099178314209 1 0.28037907302379605 现在你能用这三行代码训练各种各样的模型。我们来看一下能否用它们来训练一个卷积神经网络（CNN）吧！ 应用到卷积神经网络 我们现在将要创建一个包含三个卷积层的神经网络。因为前面章节中没有一个函数涉及到模型的具体形式，所以我们不需要对它们进行任何修改就可以训练一个卷积神经网络。 我们将会使用PyTorch中预先定义好的Conv2d类作为我们的卷积层。我们定义一个有三个卷积层的卷积神经网络。每个卷积层之后会执行ReLu。在最后，我们会执行一个平均池化操作。 （注意：view是PyTorch版的numpy reshape） class Mnist_CNN(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1) self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1) self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1) def forward(self, xb): xb = xb.view(-1, 1, 28, 28) xb = F.relu(self.conv1(xb)) xb = F.relu(self.conv2(xb)) xb = F.relu(self.conv3(xb)) xb = F.avg_pool2d(xb, 4) return xb.view(-1, xb.size(1)) lr = 0.1 Momentum是随机梯度下降的一个变型，它将前面步骤的更新也考虑在内，通常能够加快训练速度。 model = Mnist_CNN() opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9) fit(epochs, model, loss_func, opt, train_dl, valid_dl) 输出： 0 0.3590330636262894 1 0.24007404916286468 nn.Sequential torch.nn中还有另一个类可以方便的用来简化我们的代码：Sequential。一个Sequential对象可以序列化运行它包含的模块。这是一个更简单的搭建神经网络的方式。 想要充分利用这一优势，我们要能够使用给定的函数轻松的定义一个自定义层。比如说，PyTorch中没有view层，我们需要为我们的网络定义一个。 Lambda函数将会创建一个层，并在后面使用Sequential定义神经网络的时候用到。 class Lambda(nn.Module): def __init__(self, func): super().__init__() self.func = func def forward(self, x): return self.func(x) def preprocess(x): return x.view(-1, 1, 28, 28) 使用Sequential创建模型非常简单： model = nn.Sequential( Lambda(preprocess), nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.AvgPool2d(4), Lambda(lambda x: x.view(x.size(0), -1)), ) opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9) fit(epochs, model, loss_func, opt, train_dl, valid_dl) 输出： 0 0.340390869307518 1 0.2571977460026741 封装 DataLoader 我们的卷积神经网络已经非常简洁了，但是它只能运行在MNIST数据集上，原因如下： 它假定输入是长度为28*28的向量 它假定卷积神经网络最终输出是大小为4*4的网格（因为这是平均值池化操作时我们使用的核大小） 让我们摆脱这两种假定，这样我们的模型就可以运行在任意的2d单通道图像上。 首先，我们可以删除最初的Lambda层，并将数据预处理放在一个生成器中。 def preprocess(x, y): return x.view(-1, 1, 28, 28), y class WrappedDataLoader: def __init__(self, dl, func): self.dl = dl self.func = func def __len__(self): return len(self.dl) def __iter__(self): batches = iter(self.dl) for b in batches: yield (self.func(*b)) train_dl, valid_dl = get_data(train_ds, valid_ds, bs) train_dl = WrappedDataLoader(train_dl, preprocess) valid_dl = WrappedDataLoader(valid_dl, preprocess) 接下来，我们用nn.AdaptiveAvgPool2d替换nn.AvgPool2d，这个函数允许我们定义期望输出张量的大小，而不是定义已有输入的大小。 这样我们的模型就可以处理任意大小的输入了。 model = nn.Sequential( nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d(1), Lambda(lambda x: x.view(x.size(0), -1)), ) opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9) 我们来试一下新的模型： fit(epochs, model, loss_func, opt, train_dl, valid_dl) 输出： 0 0.44035838775634767 1 0.2957034994959831 使用你的GPU 如果你有幸拥有支持CUDA的GPU（你可以租一个，大部分云服务提供商的价格使0.5$/每小时），那你可以用GPU来加速你的代码。 首先检查一下的GPU是否可以被PyTorch调用： print(torch.cuda.is_available()) 输出： True 接下来，新建一个设备对象： dev = torch.device( \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") 然后更新一下preprocess函数将批运算移到GPU上计算 def preprocess(x, y): return x.view(-1, 1, 28, 28).to(dev), y.to(dev) train_dl, valid_dl = get_data(train_ds, valid_ds, bs) train_dl = WrappedDataLoader(train_dl, preprocess) valid_dl = WrappedDataLoader(valid_dl, preprocess) 最后，我们可以把模型移动到GPU上。 model.to(dev) opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9) 你现在应该能发现运算变快了。 fit(epochs, model, loss_func, opt, train_dl, valid_dl) 输出： 0 0.2397482816696167 1 0.2180072937965393 总结 现在我们有一个用PyTorch构建的通用数据管道和训练循环可以用来训练很多类型的模型。 想要知道训练一个模型有多么简单，可以参照mnist_sample这个例子。 当然了，你可能还想要在模型中加入很多其它的东西，比如数据扩充，超参数调整，训练监控，迁移学习等。这些特性可以在fastai库中获取到，该库使用与本教程中介绍的相同设计方法开发的，为想要扩展模型的学习者提供了合理的后续步骤。 在教程的开始部分，我们说了要通过例子对torch.nn，torch.optim，Dataset和DataLoader进行说明。 现在我们来总结一下，我们都讲了些什么： torch.nn Module：创建一个可调用的，其表现类似于函数，但又可以包含状态（比如神经网络层的权重）的对象。该对象知道它包含的Parameter（s），并可以将梯度置为0，以及对梯度进行循环以更新权重等。 Parameter：是一个对张量的封装，它告诉Module在反向传播阶段更新权重。只有设置了requires_grad属性的张量会被更新。 functional：一个包含了梯度函数、损失函数等以及一些无状态的层，如卷积层和线性层的模块（通常使用F作为导入的别名）。 torch.optim：包含了优化器，比如在反向阶段更新Parameter中权重的SGD。 Dataset：一个抽象接口，包含了__len__和__getitem__，还包含了PyTorch提供的类，如TensorDataset。 DataLoader：接受任意的Dataset并生成一个可以批量返回数据的迭代器（iterator ）。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"intermediate/torchvision_tutorial.html":{"url":"intermediate/torchvision_tutorial.html","title":"TorchVision 对象检测微调教程","keywords":"","body":"TorchVision 对象检测微调教程 TIP 为了充分利用本教程，我们建议使用此Colab版本。这将使您可以尝试以下信息。 对于本教程，我们将在Penn-Fudan数据库中对行人检测和分割的预训练Mask R-CNN模型进行微调。它包含170个图像，其中包含345个行人实例，我们将用它来说明如何在torchvision中使用新功能，以便在自定义数据集上训练实例细分模型。 定义数据集 用于训练对象检测，实例细分和人员关键点检测的参考脚本可轻松支持添加新的自定义数据集。数据集应继承自标准 torch.utils.data.Dataset类，并实现__len__和__getitem__。 我们唯一需要的特异性是数据集__getitem__应该返回： 图像：尺寸（H， W）的PIL图像 目标：包含以下字段的一个字典 盒 （FloatTensor [N， 4]）：的N 的坐标在包围盒[X0， Y0， X 1， Y1]格式中，范围从0至W和0至H 标签 （Int64Tensor [N]）：对于每个边界框的标签 image_id （Int64Tensor [1]）：图像标识符。它应该是在数据集中的所有图像之间唯一的，评估过程中使用 面积 （张量[N]）：将边界框的面积。这是通过COCO度量评估过程中使用，以分离小，中，大箱之间的度量得分。 iscrowd （UInt8Tensor [N]）：用iscrowd =真实例将被评估期间忽略。 （可选地）掩模 （UInt8Tensor [N， H， W]）：本分割掩码的每个其中一个对象 （可选地）关键点 （FloatTensor [N， K， 3]）：对于每一个中的所述一个N个对象，它包含K个关键点[X， Y， 能见度]格式中，定义的对象。能见度= 0表示所述关键点是不可见的。请注意，数据增强，翻转关键点的概念是依赖于数据表示，你可能要适应引用/检测/ transforms.py为您的新关键点表示 图像：大小为(H, W)PIL的图像 目标：包含以下字段的字典 boxes (FloatTensor[N, 4])：N个边界框的坐标，格式为[x0, y0, x1, y1]，，范围从0至W和0至H labels (Int64Tensor[N])：每个边界框的标签 image_id (Int64Tensor[1])：图像标识符。它在数据集中的所有图像之间应该是唯一的，并在评估过程中使用 area (Tensor[N])：边界框的面积。在使用COCO指标进行评估时，可使用此值来区分小盒子，中盒子和大盒子之间的指标得分。 iscrowd (UInt8Tensor[N])：评估期间将忽略iscrowd = True的实例。 （可选）masks (UInt8Tensor[N, H, W])：每个对象的分割蒙版 （可选）keypoints (FloatTensor[N, K, 3])：对于N个对象中的每个对象，它包含[x, y, visibility]格式的K个关键点，以定义对象。 visibility=0 表示关键点不可见。请注意，对于数据扩充，翻转关键点的概念取决于数据表示，并且您可能应该将references/detection/transforms.py修改为新的关键点表示形式。 如果您的模型返回上述方法，则它们将使其适用于训练和评估，并将使用pycocotools中的评估脚本。 此外，如果要在训练过程中使用长宽比分组（以便每个批次仅包含长宽比相似的图像），则建议您还实现一种get_height_and_width 方法，该方法可返回图像的高度和宽度。如果未提供此方法，我们将通过查询数据集的所有元素__getitem__，这会将图像加载到内存中，并且比提供自定义方法要慢。 为PennFudan编写自定义数据集 让我们为PennFudan数据集编写一个数据集。之后下载并解压缩zip文件，我们有以下文件夹结构： PennFudanPed/ PedMasks/ FudanPed00001_mask.png FudanPed00002_mask.png FudanPed00003_mask.png FudanPed00004_mask.png ... PNGImages/ FudanPed00001.png FudanPed00002.png FudanPed00003.png FudanPed00004.png 这是一对图像和分割蒙版的一个示例 因此，每个图像都有一个对应的分割蒙版，其中每个颜色对应一个不同的实例。让我们为此数据集编写一个torch.utils.data.Dataset类。 import os import numpy as np import torch from PIL import Image class PennFudanDataset(object): def __init__(self, root, transforms): self.root = root self.transforms = transforms # load all image files, sorting them to # ensure that they are aligned self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\")))) self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\")))) def __getitem__(self, idx): # load images ad masks img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx]) mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx]) img = Image.open(img_path).convert(\"RGB\") # note that we haven't converted the mask to RGB, # because each color corresponds to a different instance # with 0 being background mask = Image.open(mask_path) # convert the PIL Image into a numpy array mask = np.array(mask) # instances are encoded as different colors obj_ids = np.unique(mask) # first id is the background, so remove it obj_ids = obj_ids[1:] # split the color-encoded mask into a set # of binary masks masks = mask == obj_ids[:, None, None] # get bounding box coordinates for each mask num_objs = len(obj_ids) boxes = [] for i in range(num_objs): pos = np.where(masks[i]) xmin = np.min(pos[1]) xmax = np.max(pos[1]) ymin = np.min(pos[0]) ymax = np.max(pos[0]) boxes.append([xmin, ymin, xmax, ymax]) # convert everything into a torch.Tensor boxes = torch.as_tensor(boxes, dtype=torch.float32) # there is only one class labels = torch.ones((num_objs,), dtype=torch.int64) masks = torch.as_tensor(masks, dtype=torch.uint8) image_id = torch.tensor([idx]) area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # suppose all instances are not crowd iscrowd = torch.zeros((num_objs,), dtype=torch.int64) target = {} target[\"boxes\"] = boxes target[\"labels\"] = labels target[\"masks\"] = masks target[\"image_id\"] = image_id target[\"area\"] = area target[\"iscrowd\"] = iscrowd if self.transforms is not None: img, target = self.transforms(img, target) return img, target def __len__(self): return len(self.imgs) 这就是数据集的全部内容。现在，让我们定义一个可以对该数据集执行预测的模型。 定义模型 在本教程中，我们将使用基于Faster R-CNN的Mask R-CNN。更快的R-CNN是可预测图像中潜在对象的边界框和类分数的模型。 这是所有的数据集。现在让我们来定义可以在这个数据集进行预测的模型。 Mask R-CNN在Faster R-CNN中增加了一个分支，该分支还可以预测每个实例的分割掩码。 在两种常见情况下，可能要修改Torchvision modelzoo中的可用模型之一。首先是当我们想从预先训练的模型开始，然后微调最后一层时。另一个是当我们要用另一个模型替换主干时（例如，为了更快的预测）。 在以下各节中，让我们看看如何做一个或另一个。 1-通过预训练模型进行微调 假设您要从在COCO上进行预训练的模型开始，并希望针对您的特定班级对其进行微调。这是一种可行的方法： import torchvision from torchvision.models.detection.faster_rcnn import FastRCNNPredictor # load a model pre-trained pre-trained on COCO model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) # replace the classifier with a new one, that has # num_classes which is user-defined num_classes = 2 # 1 class (person) + background # get number of input features for the classifier in_features = model.roi_heads.box_predictor.cls_score.in_features # replace the pre-trained head with a new one model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) 2-修改模型以添加其他主干 import torchvision from torchvision.models.detection import FasterRCNN from torchvision.models.detection.rpn import AnchorGenerator # load a pre-trained model for classification and return # only the features backbone = torchvision.models.mobilenet_v2(pretrained=True).features # FasterRCNN needs to know the number of # output channels in a backbone. For mobilenet_v2, it's 1280 # so we need to add it here backbone.out_channels = 1280 # let's make the RPN generate 5 x 3 anchors per spatial # location, with 5 different sizes and 3 different aspect # ratios. We have a Tuple[Tuple[int]] because each feature # map could potentially have different sizes and # aspect ratios anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),)) # let's define what are the feature maps that we will # use to perform the region of interest cropping, as well as # the size of the crop after rescaling. # if your backbone returns a Tensor, featmap_names is expected to # be [0]. More generally, the backbone should return an # OrderedDict[Tensor], and in featmap_names you can choose which # feature maps to use. roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0], output_size=7, sampling_ratio=2) # put the pieces together inside a FasterRCNN model model = FasterRCNN(backbone, num_classes=2, rpn_anchor_generator=anchor_generator, box_roi_pool=roi_pooler) PennFudan数据集的实例细分模型 在我们的例子中，由于我们的数据集非常小，我们希望从预训练模型中进行微调，因此我们将遵循方法1。 在这里，我们还要计算实例分割掩码，因此我们将使用Mask R-CNN： import torchvision from torchvision.models.detection.faster_rcnn import FastRCNNPredictor from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor def get_model_instance_segmentation(num_classes): # load an instance segmentation model pre-trained pre-trained on COCO model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True) # get number of input features for the classifier in_features = model.roi_heads.box_predictor.cls_score.in_features # replace the pre-trained head with a new one model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) # now get the number of input features for the mask classifier in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels hidden_layer = 256 # and replace the mask predictor with a new one model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes) return model 就是这样，这将model准备好在您的自定义数据集上进行训练和评估。 将所有内容放在一起 在中references/detection/，我们提供了许多帮助程序功能来简化训练和评估检测模型。在这里，我们将使用 references/detection/engine.py，references/detection/utils.py和references/detection/transforms.py。只需将它们复制到您的文件夹中，然后在此处使用它们即可。 让我们写一些辅助函数来进行数据扩充/转换： import transforms as T def get_transform(train): transforms = [] transforms.append(T.ToTensor()) if train: transforms.append(T.RandomHorizontalFlip(0.5)) return T.Compose(transforms) 现在让我们编写执行训练和验证的主要功能： from engine import train_one_epoch, evaluate import utils def main(): # train on the GPU or on the CPU, if a GPU is not available device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') # our dataset has two classes only - background and person num_classes = 2 # use our dataset and defined transformations dataset = PennFudanDataset('PennFudanPed', get_transform(train=True)) dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False)) # split the dataset in train and test set indices = torch.randperm(len(dataset)).tolist() dataset = torch.utils.data.Subset(dataset, indices[:-50]) dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:]) # define training and validation data loaders data_loader = torch.utils.data.DataLoader( dataset, batch_size=2, shuffle=True, num_workers=4, collate_fn=utils.collate_fn) data_loader_test = torch.utils.data.DataLoader( dataset_test, batch_size=1, shuffle=False, num_workers=4, collate_fn=utils.collate_fn) # get the model using our helper function model = get_model_instance_segmentation(num_classes) # move model to the right device model.to(device) # construct an optimizer params = [p for p in model.parameters() if p.requires_grad] optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005) # and a learning rate scheduler lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1) # let's train it for 10 epochs num_epochs = 10 for epoch in range(num_epochs): # train for one epoch, printing every 10 iterations train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10) # update the learning rate lr_scheduler.step() # evaluate on the test dataset evaluate(model, data_loader_test, device=device) print(\"That's it!\") 您应该获得第一个时期的输出： Epoch: [0] [ 0/60] eta: 0:01:18 lr: 0.000090 loss: 2.5213 (2.5213) loss_classifier: 0.8025 (0.8025) loss_box_reg: 0.2634 (0.2634) loss_mask: 1.4265 (1.4265) loss_objectness: 0.0190 (0.0190) loss_rpn_box_reg: 0.0099 (0.0099) time: 1.3121 data: 0.3024 max mem: 3485 Epoch: [0] [10/60] eta: 0:00:20 lr: 0.000936 loss: 1.3007 (1.5313) loss_classifier: 0.3979 (0.4719) loss_box_reg: 0.2454 (0.2272) loss_mask: 0.6089 (0.7953) loss_objectness: 0.0197 (0.0228) loss_rpn_box_reg: 0.0121 (0.0141) time: 0.4198 data: 0.0298 max mem: 5081 Epoch: [0] [20/60] eta: 0:00:15 lr: 0.001783 loss: 0.7567 (1.1056) loss_classifier: 0.2221 (0.3319) loss_box_reg: 0.2002 (0.2106) loss_mask: 0.2904 (0.5332) loss_objectness: 0.0146 (0.0176) loss_rpn_box_reg: 0.0094 (0.0123) time: 0.3293 data: 0.0035 max mem: 5081 Epoch: [0] [30/60] eta: 0:00:11 lr: 0.002629 loss: 0.4705 (0.8935) loss_classifier: 0.0991 (0.2517) loss_box_reg: 0.1578 (0.1957) loss_mask: 0.1970 (0.4204) loss_objectness: 0.0061 (0.0140) loss_rpn_box_reg: 0.0075 (0.0118) time: 0.3403 data: 0.0044 max mem: 5081 Epoch: [0] [40/60] eta: 0:00:07 lr: 0.003476 loss: 0.3901 (0.7568) loss_classifier: 0.0648 (0.2022) loss_box_reg: 0.1207 (0.1736) loss_mask: 0.1705 (0.3585) loss_objectness: 0.0018 (0.0113) loss_rpn_box_reg: 0.0075 (0.0112) time: 0.3407 data: 0.0044 max mem: 5081 Epoch: [0] [50/60] eta: 0:00:03 lr: 0.004323 loss: 0.3237 (0.6703) loss_classifier: 0.0474 (0.1731) loss_box_reg: 0.1109 (0.1561) loss_mask: 0.1658 (0.3201) loss_objectness: 0.0015 (0.0093) loss_rpn_box_reg: 0.0093 (0.0116) time: 0.3379 data: 0.0043 max mem: 5081 Epoch: [0] [59/60] eta: 0:00:00 lr: 0.005000 loss: 0.2540 (0.6082) loss_classifier: 0.0309 (0.1526) loss_box_reg: 0.0463 (0.1405) loss_mask: 0.1568 (0.2945) loss_objectness: 0.0012 (0.0083) loss_rpn_box_reg: 0.0093 (0.0123) time: 0.3489 data: 0.0042 max mem: 5081 Epoch: [0] Total time: 0:00:21 (0.3570 s / it) creating index... index created! Test: [ 0/50] eta: 0:00:19 model_time: 0.2152 (0.2152) evaluator_time: 0.0133 (0.0133) time: 0.4000 data: 0.1701 max mem: 5081 Test: [49/50] eta: 0:00:00 model_time: 0.0628 (0.0687) evaluator_time: 0.0039 (0.0064) time: 0.0735 data: 0.0022 max mem: 5081 Test: Total time: 0:00:04 (0.0828 s / it) Averaged stats: model_time: 0.0628 (0.0687) evaluator_time: 0.0039 (0.0064) Accumulating evaluation results... DONE (t=0.01s). Accumulating evaluation results... DONE (t=0.01s). IoU metric: bbox Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.606 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.984 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.780 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.313 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.582 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.612 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.270 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.672 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.672 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.755 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.664 IoU metric: segm Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.704 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.979 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.871 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.325 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.488 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.727 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.316 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.748 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.749 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.673 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.758 因此，经过一个时期的训练，我们获得了60.6的COCO风格mAP和70.4的口罩mAP。 经过10个时期的训练，我得到了以下指标 IoU metric: bbox Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.799 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.969 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.935 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.349 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.592 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.831 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.324 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.844 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.844 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.400 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.777 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.870 IoU metric: segm Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.761 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.969 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.919 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.341 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.464 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.788 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.303 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.799 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.799 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.400 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.769 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.818 但是这些预测是什么样的？让我们在数据集中拍摄一张图像并验证 经过训练的模型可以在此图像中预测9个人的实例，让我们看看其中的几个： 结果看起来不错！ 总结 在本教程中，您学习了如何在自定义数据集上为实例细分模型创建自己的训练管道。为此，您编写了一个torch.utils.data.Dataset类，该类返回图像，地面真相框和分割蒙版。您还利用了在COCO train2017上预先训练的Mask R-CNN模型，以便对该新数据集执行转移学习。 对于更完整的示例（包括多机/多GPU训练），请检查references/detection/train.py在Torchvision存储库中存在的。 您可以在此处下载本教程的完整源文件 。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/finetuning_torchvision_models_tutorial.html":{"url":"beginner/finetuning_torchvision_models_tutorial.html","title":"微调Torchvision模型","keywords":"","body":"微调 TorchVision 模型 作者 Nathan Inkawhich 在本教程中，我们将更深入地研究如何微调和特征提取Torchvision模型，所有这些模型都已在1000类Imagenet数据集上进行了预训练。本教程将深入研究如何使用几种现代的CNN架构，并将建立一种直观的方法来微调任何PyTorch模型。由于每种模型的架构都不同，因此没有适用于所有场景的样板微调代码。相反，研究人员必须查看现有的体系结构，并对每个模型进行自定义调整。 在本文档中，我们将执行两种类型的迁移学习：微调和特征提取。在微调中，我们从预先训练的模型开始，并为新任务更新模型的所有参数，实质上是对整个模型进行重新训练。在特征提取中，我们从预先训练的模型开始，仅更新最终的层权重，从中得出预测值。之所以称为特征提取，是因为我们将预训练的CNN用作固定的特征提取器，并且仅更改输出层。有关转学的更多技术信息，请参见此处和此处。 通常，两种转移学习方法都遵循相同的几个步骤： 初始化预训练模型 重塑最终图层，使其输出数量与新数据集中的类数相同 为优化算法定义我们要在训练期间更新哪些参数 运行训练步骤 from __future__ import print_function from __future__ import division import torch import torch.nn as nn import torch.optim as optim import numpy as np import torchvision from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import time import os import copy print(\"PyTorch Version: \",torch.__version__) print(\"Torchvision Version: \",torchvision.__version__) Out: PyTorch Version: 1.2.0 Torchvision Version: 0.4.0 输入 这是要更改运行的所有参数。我们将使用可以在此处下载的hymenoptera_data数据集 。该数据集包含bees和ants两类，其结构使得我们可以使用 ImageFolder 数据集，而不必编写自己的自定义数据集。下载数据并将data_dir输入设置为数据集的根目录。输入的model_name是您要使用的模型的名称，必须从以下列表中进行选择： [resnet, alexnet, vgg, squeezenet, densenet, inception] 其他输入如下：num_classes是数据集中的类数，batch_size是用于训练的批次大小，可以根据您计算机的能力进行调整，num_epochs是我们要运行的训练时期的数量，以及feature_extract是一个布尔值，它定义了我们是微调还是特征提取。如果feature_extract = False，则微调模型并更新所有模型参数。 如果feature_extract = True，则仅更新最后一层参数，其他参数保持固定。 # Top level data directory. Here we assume the format of the directory conforms # to the ImageFolder structure data_dir = \"./data/hymenoptera_data\" # Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception] model_name = \"squeezenet\" # Number of classes in the dataset num_classes = 2 # Batch size for training (change depending on how much memory you have) batch_size = 8 # Number of epochs to train for num_epochs = 15 # Flag for feature extracting. When False, we finetune the whole model, # when True we only update the reshaped layer params feature_extract = True 辅助函数 在编写用于调整模型的代码之前，让我们定义一些辅助函数。 模型训练和验证码 train_model函数处理给定模型的训练和验证。作为输入，它采用PyTorch模型，数据加载器字典，损失函数，优化器，要训练和验证的指定时期数以及当模型是Inception模型时的布尔标志。 is_inception标志用于适应Inception v3模型，因为该体系结构使用辅助输出，并且总体模型损失同时考虑了辅助输出和最终输出，如此处所述。 该函数针对指定的时期数进行训练，并且在每个时期之后运行完整的验证步骤。 它还跟踪最佳模型（在验证准确性方面），并且在训练结束时返回最佳模型。 在每个时期之后，将打印训练和验证准确性。 def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False): since = time.time() val_acc_history = [] best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): print('Epoch {}/{}'.format(epoch, num_epochs - 1)) print('-' * 10) # Each epoch has a training and validation phase for phase in ['train', 'val']: if phase == 'train': model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == 'train'): # Get model outputs and calculate loss # Special case for inception because in training it has an auxiliary output. In train # mode we calculate the loss by summing the final output and the auxiliary output # but in testing we only consider the final output. if is_inception and phase == 'train': # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958 outputs, aux_outputs = model(inputs) loss1 = criterion(outputs, labels) loss2 = criterion(aux_outputs, labels) loss = loss1 + 0.4*loss2 else: outputs = model(inputs) loss = criterion(outputs, labels) _, preds = torch.max(outputs, 1) # backward + optimize only if in training phase if phase == 'train': loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) epoch_loss = running_loss / len(dataloaders[phase].dataset) epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset) print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc)) # deep copy the model if phase == 'val' and epoch_acc > best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) if phase == 'val': val_acc_history.append(epoch_acc) print() time_elapsed = time.time() - since print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) print('Best val Acc: {:4f}'.format(best_acc)) # load best model weights model.load_state_dict(best_model_wts) return model, val_acc_history 设置模型参数.requires_grad属性 当我们进行特征提取时，此辅助函数将模型中参数的.requires_grad属性设置为False。默认情况下，当我们加载预训练的模型时，所有参数都具有.requires_grad = True，如果我们从头开始或进行微调训练，这很好。但是，如果我们要进行特征提取，并且只想为新初始化的图层计算梯度，那么我们希望所有其他参数都不需要梯度。稍后将更有意义。 def set_parameter_requires_grad(model, feature_extracting): if feature_extracting: for param in model.parameters(): param.requires_grad = False 初始化和重塑网络 现在到最有趣的部分。我们在这里处理每个网络的重塑。注意，这不是自动过程，并且对于每个型号都是唯一的。回想一下，CNN模型的最后一层（通常是FC层的倍数）具有与数据集中的输出类数相同的节点数。由于所有模型都已在Imagenet上进行了预训练，因此它们都具有大小为1000的输出层，每个类一个节点。这里的目标是重塑最后一层，使其具有与以前相同的输入数量，并且具有与数据集中的类数相同的输出数量。在以下各节中，我们将讨论如何分别更改每个模型的体系结构。但是首先，有一个关于微调和特征提取之间差异的重要细节。 特征提取时，我们只想更新最后一层的参数，换句话说，我们只想更新我们要重塑的层的参数。因此，我们不需要计算不变的参数的梯度，因此为了提高效率，我们将.requires_grad属性设置为False。这很重要，因为默认情况下，此属性设置为True。然后，当我们初始化新图层时，默认情况下，新参数的值为.requires_grad = True，因此仅新图层的参数将被更新。当我们进行微调时，我们可以将所有.required_grad的设置保留为默认值True。 最后，请注意 inception_v3 要求输入大小为(299,299)，而所有其他模型都期望为(224,224)。 Resnet Resnet在用于图像识别的深度残差学习中进行了介绍。 有几种不同大小的变体，包括Resnet18，Resnet34，Resnet50，Resnet101和Resnet152，所有这些都可以从Torchvision模型中获得。 这里我们使用Resnet18，因为我们的数据集很小，只有两个类。 当我们打印模型时，我们看到最后一层是完全连接的层，如下所示： (fc): Linear(in_features=512, out_features=1000, bias=True) 因此，我们必须将model.fc重新初始化为具有512个输入要素和2个输出要素的线性层，其具有： model.fc = nn.Linear(512, num_classes) Alexnet Alexnet在《使用深度卷积神经网络的ImageNet分类》一书中进行了介绍，并且是ImageNet数据集上第一个非常成功的CNN。 当我们打印模型架构时，我们看到模型输出来自分类器的第六层 (classifier): Sequential( ... (6): Linear(in_features=4096, out_features=1000, bias=True) ) 为了将模型与我们的数据集一起使用，我们将该层重新初始化为 model.classifier[6] = nn.Linear(4096,num_classes) VGG VGG在用于大型图像识别的甚深度卷积网络中被介绍。TorchVision提供了八种不同长度的VGG版本，有些具有批归一化层。在这里，我们将VGG-11与批处理归一化一起使用。 输出层类似于Alexnet，即 (classifier): Sequential( ... (6): Linear(in_features=4096, out_features=1000, bias=True) ) 因此，我们使用相同的技术来修改输出层 model.classifier[6] = nn.Linear(4096,num_classes) Squeezenet 论文SqueezeNet中描述了Squeeznet体系结构：AlexNet级别的精度，参数减少了50倍，模型尺寸小于0.5MB，并且使用的输出结构与此处显示的任何其他模型都不相同。 TorchVision有两个版本的Squeezenet，我们使用1.0版。输出来自1x1卷积层，这是分类器的第一层： (classifier): Sequential( (0): Dropout(p=0.5) (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1)) (2): ReLU(inplace) (3): AvgPool2d(kernel_size=13, stride=1, padding=0) ) 为了修改网络，我们将Conv2d层重新初始化为深度为2的输出特征图为 model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1)) Densenet Densenet在《密集连接卷积网络》一文中进行了介绍。 TorchVision有Densenet的四个变体，但这里我们仅使用Densenet-121。输出层是具有1024个输入要素的线性层： (classifier): Linear(in_features=1024, out_features=1000, bias=True) 为了重塑网络，我们将分类器的线性层重新初始化为 model.classifier = nn.Linear(1024, num_classes) Inception V3 最后，在重新思考计算机视觉的初始架构中首次描述了Inception v3。该网络是唯一的，因为在训练时它具有两个输出层。第二个输出称为辅助输出，包含在网络的AuxLogits部分中。 主要输出是网络末端的线性层。注意，在测试时，我们仅考虑主要输出。 加载模型的辅助输出和主要输出打印为： (AuxLogits): InceptionAux( ... (fc): Linear(in_features=768, out_features=1000, bias=True) ) ... (fc): Linear(in_features=2048, out_features=1000, bias=True) 要微调此模型，我们必须重塑这两层的形状。这可以通过以下步骤完成 model.AuxLogits.fc = nn.Linear(768, num_classes) model.fc = nn.Linear(2048, num_classes) 注意，许多模型具有相似的输出结构，但是每个模型的处理方式都必须略有不同。另外，请检查重塑网络的打印模型架构，并确保输出要素的数量与数据集中的类的数量相同。 def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True): # Initialize these variables which will be set in this if statement. Each of these # variables is model specific. model_ft = None input_size = 0 if model_name == \"resnet\": \"\"\" Resnet18 \"\"\" model_ft = models.resnet18(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) num_ftrs = model_ft.fc.in_features model_ft.fc = nn.Linear(num_ftrs, num_classes) input_size = 224 elif model_name == \"alexnet\": \"\"\" Alexnet \"\"\" model_ft = models.alexnet(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) num_ftrs = model_ft.classifier[6].in_features model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes) input_size = 224 elif model_name == \"vgg\": \"\"\" VGG11_bn \"\"\" model_ft = models.vgg11_bn(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) num_ftrs = model_ft.classifier[6].in_features model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes) input_size = 224 elif model_name == \"squeezenet\": \"\"\" Squeezenet \"\"\" model_ft = models.squeezenet1_0(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1)) model_ft.num_classes = num_classes input_size = 224 elif model_name == \"densenet\": \"\"\" Densenet \"\"\" model_ft = models.densenet121(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) num_ftrs = model_ft.classifier.in_features model_ft.classifier = nn.Linear(num_ftrs, num_classes) input_size = 224 elif model_name == \"inception\": \"\"\" Inception v3 Be careful, expects (299,299) sized images and has auxiliary output \"\"\" model_ft = models.inception_v3(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) # Handle the auxilary net num_ftrs = model_ft.AuxLogits.fc.in_features model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes) # Handle the primary net num_ftrs = model_ft.fc.in_features model_ft.fc = nn.Linear(num_ftrs,num_classes) input_size = 299 else: print(\"Invalid model name, exiting...\") exit() return model_ft, input_size # Initialize the model for this run model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True) # Print the model we just instantiated print(model_ft) Out: SqueezeNet( (features): Sequential( (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2)) (1): ReLU(inplace=True) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True) (3): Fire( (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace=True) (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace=True) (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace=True) ) (4): Fire( (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace=True) (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace=True) (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace=True) ) (5): Fire( (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace=True) (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace=True) (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace=True) ) (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True) (7): Fire( (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace=True) (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace=True) (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace=True) ) (8): Fire( (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace=True) (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace=True) (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace=True) ) (9): Fire( (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace=True) (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace=True) (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace=True) ) (10): Fire( (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace=True) (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace=True) (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace=True) ) (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True) (12): Fire( (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace=True) (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace=True) (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace=True) ) ) (classifier): Sequential( (0): Dropout(p=0.5, inplace=False) (1): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1)) (2): ReLU(inplace=True) (3): AdaptiveAvgPool2d(output_size=(1, 1)) ) ) 加载数据 既然我们知道输入大小必须为多少，就可以初始化数据转换，图像数据集和数据加载器。请注意，模型已经过硬编码规范化值的预训练，如下所述。 # Data augmentation and normalization for training # Just normalization for validation data_transforms = { 'train': transforms.Compose([ transforms.RandomResizedCrop(input_size), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), 'val': transforms.Compose([ transforms.Resize(input_size), transforms.CenterCrop(input_size), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } print(\"Initializing Datasets and Dataloaders...\") # Create training and validation datasets image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']} # Create training and validation dataloaders dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']} # Detect if we have a GPU available device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") Out: Initializing Datasets and Dataloaders... 创建优化器 既然模型结构正确，那么微调和特征提取的最后一步就是创建一个仅更新所需参数的优化器。回想一下，在加载了预训练的模型之后，但是在重塑之前，如果feature_extract = True，我们将所有参数的.requires_grad属性手动设置为False。然后，默认情况下，重新初始化的图层的参数为.requires_grad = True。 因此，现在我们知道应该优化所有具有.requires_grad = True的参数。接下来，我们列出此类参数，并将此列表输入SGD算法构造函数。 要验证这一点，请查看打印的参数以进行学习。 进行微调时，此列表应该很长，并且包括所有模型参数。 但是，在提取特征时，此列表应简短，并且仅包括重塑图层的权重和偏差。 # Send the model to GPU model_ft = model_ft.to(device) # Gather the parameters to be optimized/updated in this run. If we are # finetuning we will be updating all parameters. However, if we are # doing feature extract method, we will only update the parameters # that we have just initialized, i.e. the parameters with requires_grad # is True. params_to_update = model_ft.parameters() print(\"Params to learn:\") if feature_extract: params_to_update = [] for name,param in model_ft.named_parameters(): if param.requires_grad == True: params_to_update.append(param) print(\"\\t\",name) else: for name,param in model_ft.named_parameters(): if param.requires_grad == True: print(\"\\t\",name) # Observe that all parameters are being optimized optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9) Out: Params to learn: classifier.1.weight classifier.1.bias 运行训练和验证步骤 最后，最后一步是为模型设置损失，然后针对设定的时期数运行训练和验证功能。注意，根据时期数，此步骤在CPU上可能需要一段时间。同样，默认学习率并非对所有模型都最佳，因此要获得最大的准确性，有必要分别针对每个模型进行调整。 # Setup the loss fxn criterion = nn.CrossEntropyLoss() # Train and evaluate model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\")) Out: Epoch 0/14 ---------- train Loss: 0.5200 Acc: 0.7336 val Loss: 0.3895 Acc: 0.8366 Epoch 1/14 ---------- train Loss: 0.3361 Acc: 0.8566 val Loss: 0.3015 Acc: 0.8954 Epoch 2/14 ---------- train Loss: 0.2721 Acc: 0.8770 val Loss: 0.2938 Acc: 0.8954 Epoch 3/14 ---------- train Loss: 0.2776 Acc: 0.8770 val Loss: 0.2774 Acc: 0.9150 Epoch 4/14 ---------- train Loss: 0.1881 Acc: 0.9139 val Loss: 0.2715 Acc: 0.9150 Epoch 5/14 ---------- train Loss: 0.1561 Acc: 0.9467 val Loss: 0.3201 Acc: 0.9150 Epoch 6/14 ---------- train Loss: 0.2536 Acc: 0.9016 val Loss: 0.3474 Acc: 0.9150 Epoch 7/14 ---------- train Loss: 0.1781 Acc: 0.9303 val Loss: 0.3262 Acc: 0.9150 Epoch 8/14 ---------- train Loss: 0.2321 Acc: 0.8811 val Loss: 0.3197 Acc: 0.8889 Epoch 9/14 ---------- train Loss: 0.1616 Acc: 0.9344 val Loss: 0.3161 Acc: 0.9346 Epoch 10/14 ---------- train Loss: 0.1510 Acc: 0.9262 val Loss: 0.3199 Acc: 0.9216 Epoch 11/14 ---------- train Loss: 0.1485 Acc: 0.9385 val Loss: 0.3198 Acc: 0.9216 Epoch 12/14 ---------- train Loss: 0.1098 Acc: 0.9590 val Loss: 0.3331 Acc: 0.9281 Epoch 13/14 ---------- train Loss: 0.1449 Acc: 0.9385 val Loss: 0.3556 Acc: 0.9281 Epoch 14/14 ---------- train Loss: 0.1405 Acc: 0.9303 val Loss: 0.4227 Acc: 0.8758 Training complete in 0m 20s Best val Acc: 0.934641 与从头开始训练的模型比较 只是为了好玩，让我们看看如果我们不使用转移学习，该模型将如何学习。 微调与特征提取的性能在很大程度上取决于数据集，但与从头开始训练的模型相比，总体而言，两种转移学习方法在训练时间和总体准确性方面均产生良好的结果。 # Initialize the non-pretrained version of the model used for this run scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False) scratch_model = scratch_model.to(device) scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9) scratch_criterion = nn.CrossEntropyLoss() _,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\")) # Plot the training curves of validation accuracy vs. number # of training epochs for the transfer learning method and # the model trained from scratch ohist = [] shist = [] ohist = [h.cpu().numpy() for h in hist] shist = [h.cpu().numpy() for h in scratch_hist] plt.title(\"Validation Accuracy vs. Number of Training Epochs\") plt.xlabel(\"Training Epochs\") plt.ylabel(\"Validation Accuracy\") plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\") plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\") plt.ylim((0,1.)) plt.xticks(np.arange(1, num_epochs+1, 1.0)) plt.legend() plt.show() Out: Epoch 0/14 ---------- train Loss: 0.7032 Acc: 0.5205 val Loss: 0.6931 Acc: 0.4641 Epoch 1/14 ---------- train Loss: 0.6931 Acc: 0.5000 val Loss: 0.6931 Acc: 0.4641 Epoch 2/14 ---------- train Loss: 0.6931 Acc: 0.4549 val Loss: 0.6931 Acc: 0.4641 Epoch 3/14 ---------- train Loss: 0.6931 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4641 Epoch 4/14 ---------- train Loss: 0.6931 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4641 Epoch 5/14 ---------- train Loss: 0.6931 Acc: 0.5656 val Loss: 0.6931 Acc: 0.4641 Epoch 6/14 ---------- train Loss: 0.6931 Acc: 0.4467 val Loss: 0.6931 Acc: 0.4641 Epoch 7/14 ---------- train Loss: 0.6932 Acc: 0.5123 val Loss: 0.6931 Acc: 0.4641 Epoch 8/14 ---------- train Loss: 0.6931 Acc: 0.4918 val Loss: 0.6931 Acc: 0.4641 Epoch 9/14 ---------- train Loss: 0.6931 Acc: 0.4754 val Loss: 0.6931 Acc: 0.4641 Epoch 10/14 ---------- train Loss: 0.6931 Acc: 0.4795 val Loss: 0.6931 Acc: 0.4641 Epoch 11/14 ---------- train Loss: 0.6931 Acc: 0.5205 val Loss: 0.6931 Acc: 0.4641 Epoch 12/14 ---------- train Loss: 0.6931 Acc: 0.4754 val Loss: 0.6931 Acc: 0.4641 Epoch 13/14 ---------- train Loss: 0.6932 Acc: 0.4590 val Loss: 0.6931 Acc: 0.4641 Epoch 14/14 ---------- train Loss: 0.6932 Acc: 0.5082 val Loss: 0.6931 Acc: 0.4641 Training complete in 0m 29s Best val Acc: 0.464052 最后的思考和下一步是什么 尝试运行其他一些模型，看看精度如何。另外，请注意，特征提取花费的时间更少，因为在向后传递中，我们不必计算大多数梯度。 这里有很多地方。 你可以： 使用更困难的数据集运行此代码，并查看迁移学习的更多好处 使用此处描述的方法，使用转移学习来更新不同的模型，也许是在新的领域（例如NLP，音频等） 对模型满意后，可以将其导出为ONNX模型，也可以使用混合前端对其进行跟踪以提高速度和优化机会。 脚本的总运行时间：（0分钟57.562秒） 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"intermediate/spatial_transformer_tutorial.html":{"url":"intermediate/spatial_transformer_tutorial.html","title":"空间变压器网络教程","keywords":"","body":"空间变压器网络教程 作者: Ghassen HAMROUNI 在本教程中，您将学习如何使用称为空间变换器网络的视觉注意力机制来扩充网络。 您可以在DeepMind论文中阅读有关空间变换器网络的更多信息。 空间变换器网络是对任何空间变换的可区别关注的概括。空间变换器网络（简称STN）允许神经网络学习如何对输入图像执行空间变换，以增强模型的几何不变性。例如，它可以裁剪感兴趣的区域，缩放并校正图像的方向。这可能是一个有用的机制，因为CNN不会对旋转和缩放以及更一般的仿射变换保持不变。 关于STN的最好的事情之一就是能够将其简单地插入到任何现有的CNN中，而无需进行任何修改。 # License: BSD # Author: Ghassen Hamrouni from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim import torchvision from torchvision import datasets, transforms import matplotlib.pyplot as plt import numpy as np plt.ion() # interactive mode 加载数据 在本文中，我们将尝试使用经典的MNIST数据集。使用标准卷积网络和空间变换器网络。 device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Training dataset train_loader = torch.utils.data.DataLoader( datasets.MNIST(root='.', train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=64, shuffle=True, num_workers=4) # Test dataset test_loader = torch.utils.data.DataLoader( datasets.MNIST(root='.', train=False, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=64, shuffle=True, num_workers=4) Out: Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw Processing... Done! 描绘空间变换器网络 空间变压器网络可归结为三个主要组成部分： 定位网络是常规的CNN，可以对转换参数进行回归。永远不会从该数据集中显式学习变换，而是网络会自动学习增强全局精度的空间变换。 网格生成器在输入图像中生成与来自输出图像的每个像素相对应的坐标网格。 采样器使用转换的参数，并将其应用于输入图像。 Note 我们需要包含affine_grid和grid_sample模块的最新版本的PyTorch。 class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 10, kernel_size=5) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(320, 50) self.fc2 = nn.Linear(50, 10) # Spatial transformer localization-network self.localization = nn.Sequential( nn.Conv2d(1, 8, kernel_size=7), nn.MaxPool2d(2, stride=2), nn.ReLU(True), nn.Conv2d(8, 10, kernel_size=5), nn.MaxPool2d(2, stride=2), nn.ReLU(True) ) # Regressor for the 3 * 2 affine matrix self.fc_loc = nn.Sequential( nn.Linear(10 * 3 * 3, 32), nn.ReLU(True), nn.Linear(32, 3 * 2) ) # Initialize the weights/bias with identity transformation self.fc_loc[2].weight.data.zero_() self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float)) # Spatial transformer network forward function def stn(self, x): xs = self.localization(x) xs = xs.view(-1, 10 * 3 * 3) theta = self.fc_loc(xs) theta = theta.view(-1, 2, 3) grid = F.affine_grid(theta, x.size()) x = F.grid_sample(x, grid) return x def forward(self, x): # transform the input x = self.stn(x) # Perform the usual forward pass x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(-1, 320) x = F.relu(self.fc1(x)) x = F.dropout(x, training=self.training) x = self.fc2(x) return F.log_softmax(x, dim=1) model = Net().to(device) 训练模式 现在，让我们使用SGD算法训练模型。网络正在以监督方式学习分类任务。同时，该模型以端到端的方式自动学习STN。 optimizer = optim.SGD(model.parameters(), lr=0.01) def train(epoch): model.train() for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() optimizer.step() if batch_idx % 500 == 0: print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) # # A simple test procedure to measure STN the performances on MNIST. # def test(): with torch.no_grad(): model.eval() test_loss = 0 correct = 0 for data, target in test_loader: data, target = data.to(device), target.to(device) output = model(data) # sum up batch loss test_loss += F.nll_loss(output, target, size_average=False).item() # get the index of the max log-probability pred = output.max(1, keepdim=True)[1] correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n' .format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) 可视化STN结果 现在，我们将检查学习到的视觉注意力机制的结果。 我们定义了一个小的辅助函数，以便在训练时可视化转换。 def convert_image_np(inp): \"\"\"Convert a Tensor to numpy image.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) return inp # We want to visualize the output of the spatial transformers layer # after the training, we visualize a batch of input images and # the corresponding transformed batch using STN. def visualize_stn(): with torch.no_grad(): # Get a batch of training data data = next(iter(test_loader))[0].to(device) input_tensor = data.cpu() transformed_input_tensor = model.stn(data).cpu() in_grid = convert_image_np( torchvision.utils.make_grid(input_tensor)) out_grid = convert_image_np( torchvision.utils.make_grid(transformed_input_tensor)) # Plot the results side-by-side f, axarr = plt.subplots(1, 2) axarr[0].imshow(in_grid) axarr[0].set_title('Dataset Images') axarr[1].imshow(out_grid) axarr[1].set_title('Transformed Images') for epoch in range(1, 20 + 1): train(epoch) test() # Visualize the STN transformation on some input batch visualize_stn() plt.ioff() plt.show() Out: Train Epoch: 1 [0/60000 (0%)] Loss: 2.290877 Train Epoch: 1 [32000/60000 (53%)] Loss: 0.910913 Test set: Average loss: 0.2449, Accuracy: 9312/10000 (93%) Train Epoch: 2 [0/60000 (0%)] Loss: 0.489534 Train Epoch: 2 [32000/60000 (53%)] Loss: 0.296471 Test set: Average loss: 0.1443, Accuracy: 9563/10000 (96%) Train Epoch: 3 [0/60000 (0%)] Loss: 0.410248 Train Epoch: 3 [32000/60000 (53%)] Loss: 0.355454 Test set: Average loss: 0.1019, Accuracy: 9687/10000 (97%) Train Epoch: 4 [0/60000 (0%)] Loss: 0.217658 Train Epoch: 4 [32000/60000 (53%)] Loss: 0.185522 Test set: Average loss: 0.0818, Accuracy: 9751/10000 (98%) Train Epoch: 5 [0/60000 (0%)] Loss: 0.471464 Train Epoch: 5 [32000/60000 (53%)] Loss: 0.591574 Test set: Average loss: 0.0770, Accuracy: 9760/10000 (98%) Train Epoch: 6 [0/60000 (0%)] Loss: 0.119462 Train Epoch: 6 [32000/60000 (53%)] Loss: 0.093015 Test set: Average loss: 0.0817, Accuracy: 9744/10000 (97%) Train Epoch: 7 [0/60000 (0%)] Loss: 0.074523 Train Epoch: 7 [32000/60000 (53%)] Loss: 0.414406 Test set: Average loss: 0.0944, Accuracy: 9714/10000 (97%) Train Epoch: 8 [0/60000 (0%)] Loss: 0.100317 Train Epoch: 8 [32000/60000 (53%)] Loss: 0.114539 Test set: Average loss: 0.1519, Accuracy: 9510/10000 (95%) Train Epoch: 9 [0/60000 (0%)] Loss: 0.205053 Train Epoch: 9 [32000/60000 (53%)] Loss: 0.135724 Test set: Average loss: 0.0892, Accuracy: 9749/10000 (97%) Train Epoch: 10 [0/60000 (0%)] Loss: 0.213368 Train Epoch: 10 [32000/60000 (53%)] Loss: 0.208627 Test set: Average loss: 0.0634, Accuracy: 9813/10000 (98%) Train Epoch: 11 [0/60000 (0%)] Loss: 0.078725 Train Epoch: 11 [32000/60000 (53%)] Loss: 0.099131 Test set: Average loss: 0.0580, Accuracy: 9834/10000 (98%) Train Epoch: 12 [0/60000 (0%)] Loss: 0.133572 Train Epoch: 12 [32000/60000 (53%)] Loss: 0.213358 Test set: Average loss: 0.0506, Accuracy: 9854/10000 (99%) Train Epoch: 13 [0/60000 (0%)] Loss: 0.289802 Train Epoch: 13 [32000/60000 (53%)] Loss: 0.165571 Test set: Average loss: 0.0542, Accuracy: 9842/10000 (98%) Train Epoch: 14 [0/60000 (0%)] Loss: 0.219281 Train Epoch: 14 [32000/60000 (53%)] Loss: 0.284233 Test set: Average loss: 0.0505, Accuracy: 9856/10000 (99%) Train Epoch: 15 [0/60000 (0%)] Loss: 0.218599 Train Epoch: 15 [32000/60000 (53%)] Loss: 0.055698 Test set: Average loss: 0.0507, Accuracy: 9848/10000 (98%) Train Epoch: 16 [0/60000 (0%)] Loss: 0.048718 Train Epoch: 16 [32000/60000 (53%)] Loss: 0.093410 Test set: Average loss: 0.0502, Accuracy: 9855/10000 (99%) Train Epoch: 17 [0/60000 (0%)] Loss: 0.071185 Train Epoch: 17 [32000/60000 (53%)] Loss: 0.053381 Test set: Average loss: 0.0587, Accuracy: 9829/10000 (98%) Train Epoch: 18 [0/60000 (0%)] Loss: 0.127790 Train Epoch: 18 [32000/60000 (53%)] Loss: 0.169319 Test set: Average loss: 0.0484, Accuracy: 9863/10000 (99%) Train Epoch: 19 [0/60000 (0%)] Loss: 0.224094 Train Epoch: 19 [32000/60000 (53%)] Loss: 0.175750 Test set: Average loss: 0.0628, Accuracy: 9817/10000 (98%) Train Epoch: 20 [0/60000 (0%)] Loss: 0.251131 Train Epoch: 20 [32000/60000 (53%)] Loss: 0.024119 Test set: Average loss: 0.0445, Accuracy: 9869/10000 (99%) 脚本的总运行时间： （1分钟44.448秒） Download Python source code: spatial_transformer_tutorial.py Download Jupyter notebook: spatial_transformer_tutorial.ipynb 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"advanced/neural_style_tutorial.html":{"url":"advanced/neural_style_tutorial.html","title":"使用PyTorch进行神经网络传递","keywords":"","body":"使用PyTorch进行神经网络传递 作者: Alexis Jacq 编辑: Winston Herring 简介 本教程介绍了如何实现 由Leon A. Gatys，Alexander S. Ecker和Matthias Bethge开发的神经风格算法。神经风格或神经传递，使您可以拍摄图像并以新的艺术风格对其进行再现。该算法获取三个图像，即输入图像，内容图像和样式图像，然后更改输入以使其类似于内容图像的内容和样式图像的艺术风格。 基本原理 原理很简单：我们定义了两个距离，一个用于内容 (D_S) 和一种样式 (D_S)。 (D_C) 测量两个图像之间的内容有多不同，而 (D_S) 测量两个图像之间样式的差异。然后，我们获取第三个图像输入，并将其转换为最小化与内容图像的内容距离和与样式图像的样式距离。现在我们可以导入必要的包并开始神经传递。 导入软件包并选择设备 以下是实现神经传递所需的软件包列表。 torch，torch.nn，numpy（与PyTorch神经网络包赛前必读） torch.optim （有效的梯度下降） PIL，PIL.Image，matplotlib.pyplot（加载和显示图像） torchvision.transforms （将PIL图像转换为张量） torchvision.models （训练或加载预训练模型） copy （以深层复制模型；系统包） from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from PIL import Image import matplotlib.pyplot as plt import torchvision.transforms as transforms import torchvision.models as models import copy 接下来，我们需要选择要在哪个设备上运行网络并导入内容和样式图像。 在大图像上运行神经传递算法需要更长的时间，并且在GPU上运行时会更快。 我们可以使用torch.cuda.is_available()来检测是否有GPU。 接下来，我们设置torch.device以在整个教程中使用。 .to(device)方法也用于将张量或模块移动到所需的设备。 device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") 加载图像 现在，我们将导入样式和内容图像。原始的PIL图像的值在0到255之间，但是当转换为Torch张量时，其值将转换为0到1之间。图像也需要调整大小以具有相同的尺寸。需要注意的一个重要细节是，使用从0到1的张量值对Torch库中的神经网络进行训练。如果尝试为网络提供0到255张量图像，则激活的特征图将无法感知预期的内容和风格。但是，来自Caffe库的预训练网络使用0到255张量图像进行训练。 Note 通过以下链接下载到运行教程所需的图像：picasso.jpg和dancing.jpg。下载这两个图像并将它们添加到images当前工作目录中具有名称的目录中。 # desired size of the output image imsize = 512 if torch.cuda.is_available() else 128 # use small size if no gpu loader = transforms.Compose([ transforms.Resize(imsize), # scale imported image transforms.ToTensor()]) # transform it into a torch tensor def image_loader(image_name): image = Image.open(image_name) # fake batch dimension required to fit network's input dimensions image = loader(image).unsqueeze(0) return image.to(device, torch.float) style_img = image_loader(\"./data/images/neural-style/picasso.jpg\") content_img = image_loader(\"./data/images/neural-style/dancing.jpg\") assert style_img.size() == content_img.size(), \\ \"we need to import style and content images of the same size\" 现在，让我们创建一个通过将图像的副本转换为PIL格式并使用来显示图像的功能plt.imshow。我们将尝试显示内容和样式图像，以确保正确导入它们。 unloader = transforms.ToPILImage() # reconvert into PIL image plt.ion() def imshow(tensor, title=None): image = tensor.cpu().clone() # we clone the tensor to not do changes on it image = image.squeeze(0) # remove the fake batch dimension image = unloader(image) plt.imshow(image) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated plt.figure() imshow(style_img, title='Style Image') plt.figure() imshow(content_img, title='Content Image') 损失函数 内容损失 内容损失是代表单个图层内容距离的加权版本的函数。该功能获取特征图F_{XL} 一层L在网络中处理输入X并返回加权内容距离w_{CL}.D_C^L(X,C)图像之间X和内容图片C。内容图像的特征图（F_{CL}函数必须知道）才能计算内容距离。我们将此函数作为带有构造函数的火炬模块来实现F_{CL}作为输入。距离\\|F_{XL} - F_{CL}\\|^2是两组要素图之间的均方误差，可以使用进行计算nn.MSELoss。 我们将直接在用于计算内容距离的卷积层之后添加此内容丢失模块。这样，每次向网络提供输入图像时，都会在所需层上计算内容损失，并且由于自动渐变，将计算所有梯度。现在，为了使内容丢失层透明，我们必须定义一种forward方法来计算内容丢失，然后返回该层的输入。计算出的损耗将保存为模块的参数。 class ContentLoss(nn.Module): def __init__(self, target,): super(ContentLoss, self).__init__() # we 'detach' the target content from the tree used # to dynamically compute the gradient: this is a stated value, # not a variable. Otherwise the forward method of the criterion # will throw an error. self.target = target.detach() def forward(self, input): self.loss = F.mse_loss(input, self.target) return input Note 重要细节：尽管此模块名为ContentLoss，但它不是真正的PyTorch损失函数。如果要将内容损失定义为PyTorch损失函数，则必须创建一个PyTorch autograd函数以在backward方法中手动重新计算/实现梯度。 风格损失 风格损失模块类似地实现对内容的损失模块。它将作为其计算该层的风格损失的网络中的透明层。为了计算的样式的损失，我们需要计算克矩阵G_{XL}。甲克矩阵是通过它的转置矩阵的给定矩阵相乘的结果。在本申请中给出的矩阵是特征的重整的版本映射F_{XL}层L。F_{XL}重塑形成\\hat{F}_{XL}，K X N矩阵，其中 K 是特征图中的层L和数N是任何量化特征地图F_{XL}^ķ的长度。例如，的第一行 \\hat{F}_{XL} 对应于第一量化特征地图 F_{XL}^1。 最后，克矩阵必须由在矩阵元素的总数量除以每个元素进行归一化。这种归一化是为了抵消这一事实\\hat{F}_{XL}具有大N维产量较大的革兰氏矩阵值的矩阵。这些较大的值将导致第一层（池层之前），以具有梯度下降期间产生更大的影响。风格特征往往是在网络的更深层所以这归一化步骤是至关重​​要的。 def gram_matrix(input): a, b, c, d = input.size() # a=batch size(=1) # b=number of feature maps # (c,d)=dimensions of a f. map (N=c*d) features = input.view(a * b, c * d) # resise F_XL into \\hat F_XL G = torch.mm(features, features.t()) # compute the gram product # we 'normalize' the values of the gram matrix # by dividing by the number of element in each feature maps. return G.div(a * b * c * d) 现在，样式丢失模块看起来几乎与内容丢失模块完全一样。样式距离也可以使用G_{XL} 和 G_{SL}。 class StyleLoss(nn.Module): def __init__(self, target_feature): super(StyleLoss, self).__init__() self.target = gram_matrix(target_feature).detach() def forward(self, input): G = gram_matrix(input) self.loss = F.mse_loss(G, self.target) return input 导入模型 现在我们需要导入一个预训练的神经网络。我们将使用19层VGG网络，就像本文中使用的那样。 PyTorch的VGG实现是一个模块，分为两个子 Sequential模块：（features包含卷积和池化层）和classifier（包含完全连接的层）。我们将使用该features模块，因为我们需要各个卷积层的输出来测量内容和样式损失。某些层在训练期间的行为与评估不同，因此我们必须使用将网络设置为评估模式.eval()。 cnn = models.vgg19(pretrained=True).features.to(device).eval() 另外，在图像上训练VGG网络，每个通道的均值通过 mean=[0.485，0.456，0.406]和 std=[0.229，0.224，0.225]归一化。在将其发送到网络之前，我们将使用它们对图像进行规范化。 cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device) cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device) # create a module to normalize input image so we can easily put it in a # nn.Sequential class Normalization(nn.Module): def __init__(self, mean, std): super(Normalization, self).__init__() # .view the mean and std to make them [C x 1 x 1] so that they can # directly work with image Tensor of shape [B x C x H x W]. # B is batch size. C is number of channels. H is height and W is width. self.mean = torch.tensor(mean).view(-1, 1, 1) self.std = torch.tensor(std).view(-1, 1, 1) def forward(self, img): # normalize img return (img - self.mean) / self.std Sequential模块包含子模块的有序列表。 例如，vgg19.features包含以正确的深度顺序对齐的序列（Conv2d，ReLU，MaxPool2d，Conv2d，ReLU…）。 我们需要在检测到的卷积层之后立即添加内容丢失层和样式丢失层。 为此，我们必须创建一个新Sequential模块，该模块具有正确插入的内容丢失和样式丢失模块。 # desired depth layers to compute style/content losses : content_layers_default = ['conv_4'] style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5'] def get_style_model_and_losses(cnn, normalization_mean, normalization_std, style_img, content_img, content_layers=content_layers_default, style_layers=style_layers_default): cnn = copy.deepcopy(cnn) # normalization module normalization = Normalization(normalization_mean, normalization_std).to(device) # just in order to have an iterable access to or list of content/syle # losses content_losses = [] style_losses = [] # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential # to put in modules that are supposed to be activated sequentially model = nn.Sequential(normalization) i = 0 # increment every time we see a conv for layer in cnn.children(): if isinstance(layer, nn.Conv2d): i += 1 name = 'conv_{}'.format(i) elif isinstance(layer, nn.ReLU): name = 'relu_{}'.format(i) # The in-place version doesn't play very nicely with the ContentLoss # and StyleLoss we insert below. So we replace with out-of-place # ones here. layer = nn.ReLU(inplace=False) elif isinstance(layer, nn.MaxPool2d): name = 'pool_{}'.format(i) elif isinstance(layer, nn.BatchNorm2d): name = 'bn_{}'.format(i) else: raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__)) model.add_module(name, layer) if name in content_layers: # add content loss: target = model(content_img).detach() content_loss = ContentLoss(target) model.add_module(\"content_loss_{}\".format(i), content_loss) content_losses.append(content_loss) if name in style_layers: # add style loss: target_feature = model(style_img).detach() style_loss = StyleLoss(target_feature) model.add_module(\"style_loss_{}\".format(i), style_loss) style_losses.append(style_loss) # now we trim off the layers after the last content and style losses for i in range(len(model) - 1, -1, -1): if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss): break model = model[:(i + 1)] return model, style_losses, content_losses 接下来，我们选择输入图像。您可以使用内容图像或白噪声的副本。 input_img = content_img.clone() # if you want to use white noise instead uncomment the below line: # input_img = torch.randn(content_img.data.size(), device=device) # add the original input image to the figure: plt.figure() imshow(input_img, title='Input Image') 梯度下降 正如该算法的作者Leon Gatys在此处建议的那样，我们将使用L-BFGS算法来运行我们的梯度下降。与训练网络不同，我们希望训练输入图像以最大程度地减少内容/样式损失。我们将创建一个PyTorch L-BFGS优化器optim.LBFGS，并将图像作为张量传递给它进行优化。 def get_input_optimizer(input_img): # this line to show that input is a parameter that requires a gradient optimizer = optim.LBFGS([input_img.requires_grad_()]) return optimizer 最后，我们必须定义一个执行神经传递的函数。对于网络的每次迭代，它都会被提供更新的输入并计算新的损耗。我们将运行backward每个损失模块的方法来动态计算其梯度。优化器需要“关闭”功能，该功能可以重新评估模数并返回损耗。 我们还有最后一个约束要解决。网络可能会尝试使用超出图像的0到1张量范围的值来优化输入。我们可以通过在每次网络运行时将输入值校正为0到1之间来解决此问题。 def run_style_transfer(cnn, normalization_mean, normalization_std, content_img, style_img, input_img, num_steps=300, style_weight=1000000, content_weight=1): \"\"\"Run the style transfer.\"\"\" print('Building the style transfer model..') model, style_losses, content_losses = get_style_model_and_losses(cnn, normalization_mean, normalization_std, style_img, content_img) optimizer = get_input_optimizer(input_img) print('Optimizing..') run = [0] while run[0] 最后，我们可以运行算法。 output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std, content_img, style_img, input_img) plt.figure() imshow(output, title='Output Image') # sphinx_gallery_thumbnail_number = 4 plt.ioff() plt.show() Out: Building the style transfer model.. Optimizing.. run [50]: Style Loss : 4.169304 Content Loss: 4.235329 run [100]: Style Loss : 1.145476 Content Loss: 3.039176 run [150]: Style Loss : 0.716769 Content Loss: 2.663749 run [200]: Style Loss : 0.476047 Content Loss: 2.500893 run [250]: Style Loss : 0.347092 Content Loss: 2.410895 run [300]: Style Loss : 0.263698 Content Loss: 2.358449 脚本的总运行时间： （1分钟9.573秒） Download Python source code: neural_style_tutorial.py Download Jupyter notebook: neural_style_tutorial.ipynb 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/fgsm_tutorial.html":{"url":"beginner/fgsm_tutorial.html","title":"对抗性示例生成","keywords":"","body":"对抗性实例生成 作者: Nathan Inkawhich 如果您正在阅读本文，希望您能体会到某些机器学习模型的有效性。研究不断推动ML模型更快，更准确和更高效。但是，设计和训练模型的一个经常被忽略的方面是安全性和鲁棒性，尤其是在面对想要欺骗模型的对手的情况下。 本教程将提高您对ML模型的安全漏洞的认识，并深入了解对抗性机器学习的热门话题。您可能会惊讶地发现，对图像添加无法察觉的扰动会导致模型性能大不相同。鉴于这是一个教程，我们将通过图像分类器上的示例来探讨该主题。具体来说，我们将使用第一种也是最流行的攻击方法之一，即快速梯度符号攻击（FGSM）来欺骗MNIST分类器。 威胁模型 就上下文而言，有许多类别的对抗性攻击，每种攻击者都有不同的目标和对攻击者知识的假设。但是，总的来说，总体目标是向输入数据添加最少的扰动，以引起所需的错误分类。攻击者的知识有几种假设，其中两种是：white-box和black-box。一个white-box攻击假设攻击者有充分的知识和访问模型，包括建筑，输入，输出，和权重。一个black-box攻击假设攻击者只能访问输入和模型的输出，并且一无所知底层架构或权重。目标也有几种类型，包括错误分类和源/目标错误分类。一个错误分类的目标意味着对手只希望输出分类错误，而不关心新分类是什么。一个源/目标误分类装置对手想要改变图像是特定源类的最初使得其被归类为特定的目标类。 在这种情况下，FGSM攻击是white-box攻击，目的是进行错误分类。有了这些背景信息，我们现在就可以详细讨论攻击了。 快速梯度符号攻击 迄今为止，最早的也是最流行的对抗性攻击之一被称为“ 快速梯度符号攻击”（FGSM），由Goodfellow et. al. 在解释和利用对抗例子中的运用描述。攻击非常强大，而且直观。它旨在利用神经网络的学习方式，梯度来攻击神经网络。这个想法很简单，不是根据反向传播的梯度通过调整权重来使损失最小化，而是根据相同的反向传播的梯度来调整输入数据以使损失最大化。换句话说，攻击使用输入数据的损失梯度，然后调整输入数据以使损失最大化。 在进入代码之前，让我们看一下著名的 FGSM panda示例并提取一些表示法。 从图中 x 是正确分类为 “panda” 的原始输入图像， y 是地面真相标签 x，\\mathbf{\\theta} 代表模型参数，并且 J(\\mathbf{\\theta}, \\mathbf{x}, y) 是用于训练网络的损失。攻击会将梯度反向传播回输入数据以进行计算 \\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)。然后，通过一小步调整输入数据（\\epsilon要么 0.007 在图片中）的方向（即 sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y))），这将使损失最大化。产生的扰动图像，x'然后 ，在目标网络仍明显是 “panda” 的情况下，它会被目标网络误分类为“gibbon”。 希望本教程的动机已经明确，所以让我们进入实现过程。 from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms import numpy as np import matplotlib.pyplot as plt 实现 在本节中，我们将讨论本教程的输入参数，定义受到攻击的模型，然后编写攻击代码并运行一些测试。 输入 本教程只有三个输入，定义如下： epsilons - 用于运行的epsilon值列表。在列表中保留0很重要，因为它代表原始测试集上的模型性能。同样，从直觉上讲，我们期望ε越大，扰动越明显，但是从降低模型准确性的角度来看，攻击越有效。由于这里的数据范围是[0,1]，则epsilon值不得超过1。 pretrained_model - 使用 pytorch/examples/mnist 训练的预训练MNIST模型的路径 。为简单起见，请在此处下载预训练的模型。 use_cuda - 布尔标志，如果需要和可用，则使用CUDA。请注意，具有CUDA的GPU在本教程中并不重要，因为CPU不会花费很多时间。 epsilons = [0, .05, .1, .15, .2, .25, .3] pretrained_model = \"data/lenet_mnist_model.pth\" use_cuda=True 受到攻击的模型 如前所述，受到攻击的模型与 pytorch/examples/mnist 中的MNIST模型相同 。您可以训练并保存自己的MNIST模型，也可以下载并使用提供的模型。该网的定义和测试的 DataLoader 这里已经从MNIST实例中复制。本部分的目的是定义模型和数据加载器，然后初始化模型并加载预训练的权重。 # LeNet Model definition class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 10, kernel_size=5) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(320, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(-1, 320) x = F.relu(self.fc1(x)) x = F.dropout(x, training=self.training) x = self.fc2(x) return F.log_softmax(x, dim=1) # MNIST Test dataset and dataloader declaration test_loader = torch.utils.data.DataLoader( datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([ transforms.ToTensor(), ])), batch_size=1, shuffle=True) # Define what device we are using print(\"CUDA Available: \",torch.cuda.is_available()) device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\") # Initialize the network model = Net().to(device) # Load the pretrained model model.load_state_dict(torch.load(pretrained_model, map_location='cpu')) # Set the model in evaluation mode. In this case this is for the Dropout layers model.eval() Out: Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw Processing... Done! CUDA Available: True FGSM攻击 现在，我们可以通过干扰原始输入来定义创建对抗示例的函数。该fgsm_attack函数需要三个输入，图像是原始的干净图像(x)，epsilon是像素方向的扰动量(\\epsilon)，而 data_grad 是输入图片\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)。该函数然后创建扰动图像为 perturbed\\_image = image + epsilon*sign(data\\_grad) = x + \\epsilon * sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)) 最后，为了保持数据的原始范围，将受干扰的图像裁剪到一定范围 [0,1]。 # FGSM attack code def fgsm_attack(image, epsilon, data_grad): # Collect the element-wise sign of the data gradient sign_data_grad = data_grad.sign() # Create the perturbed image by adjusting each pixel of the input image perturbed_image = image + epsilon*sign_data_grad # Adding clipping to maintain [0,1] range perturbed_image = torch.clamp(perturbed_image, 0, 1) # Return the perturbed image return perturbed_image 测试功能 最后，本教程的主要结果来自该test函数。每次调用此测试功能都会在MNIST测试集中执行完整的测试步骤，并报告最终精度。但是，请注意，此功能还需要输入epsilon。这是因为该test功能报告了受到对手强大攻击的模型的准确性\\epsilon。更具体地说，对于测试集中的每个样本，函数都会计算输入数据的损耗梯度(data_grad)，使用fgsm_attack (perturbed_data)，然后检查受干扰的示例是否具有对抗性。除了测试模型的准确性外，该函数还保存并返回一些成功的对抗示例，以供以后可视化。 def test( model, device, test_loader, epsilon ): # Accuracy counter correct = 0 adv_examples = [] # Loop over all examples in test set for data, target in test_loader: # Send the data and label to the device data, target = data.to(device), target.to(device) # Set requires_grad attribute of tensor. Important for Attack data.requires_grad = True # Forward pass the data through the model output = model(data) init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability # If the initial prediction is wrong, dont bother attacking, just move on if init_pred.item() != target.item(): continue # Calculate the loss loss = F.nll_loss(output, target) # Zero all existing gradients model.zero_grad() # Calculate gradients of model in backward pass loss.backward() # Collect datagrad data_grad = data.grad.data # Call FGSM Attack perturbed_data = fgsm_attack(data, epsilon, data_grad) # Re-classify the perturbed image output = model(perturbed_data) # Check for success final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability if final_pred.item() == target.item(): correct += 1 # Special case for saving 0 epsilon examples if (epsilon == 0) and (len(adv_examples) 运行攻击 实现的最后一部分是实际运行攻击。在这里，我们为epsilons输入中的每个epsilon值运行一个完整的测试步骤。对于每个epsilon，我们还保存最终精度，并在接下来的部分中绘制一些成功的对抗示例。请注意，随着 \\epsilon 值的增加，打印的精度如何降低。另外，请注意 \\epsilon=0 外壳代表原始的测试准确性，没有任何攻击。 accuracies = [] examples = [] # Run test for each epsilon for eps in epsilons: acc, ex = test(model, device, test_loader, eps) accuracies.append(acc) examples.append(ex) Out: Epsilon: 0 Test Accuracy = 9810 / 10000 = 0.981 Epsilon: 0.05 Test Accuracy = 9426 / 10000 = 0.9426 Epsilon: 0.1 Test Accuracy = 8510 / 10000 = 0.851 Epsilon: 0.15 Test Accuracy = 6826 / 10000 = 0.6826 Epsilon: 0.2 Test Accuracy = 4301 / 10000 = 0.4301 Epsilon: 0.25 Test Accuracy = 2082 / 10000 = 0.2082 Epsilon: 0.3 Test Accuracy = 869 / 10000 = 0.0869 结果 Accuracy vs Epsilon 第一个结果是 accuracy 与ε曲线的关系。如前所述，随着ε的增加，我们期望测试精度会降低。这是因为较大的ε意味着我们朝着将损失最大化的方向迈出了更大的一步。请注意，即使epsilon值是线性间隔的，曲线中的趋势也不是线性的。例如，在ϵ=0.05 仅比 ϵ=0 约低4％，但accuracy为 ϵ=0.2 比 ϵ=0.15 低25％。另外，请注意，对于介于 ϵ=0.25 和 ϵ=0.3。 plt.figure(figsize=(5,5)) plt.plot(epsilons, accuracies, \"*-\") plt.yticks(np.arange(0, 1.1, step=0.1)) plt.xticks(np.arange(0, .35, step=0.05)) plt.title(\"Accuracy vs Epsilon\") plt.xlabel(\"Epsilon\") plt.ylabel(\"Accuracy\") plt.show() 对抗示例 还记得没有免费午餐的想法吗？在这种情况下，随着ε的增加，测试精度降低，BUT扰动变得更容易察觉。实际上，攻击者必须考虑准确性降低和可感知性之间的权衡。在这里，我们展示了每个epsilon值的成功对抗示例。绘图的每一行显示不同的ε值。第一行是ϵ=0代表原始“干净”图像且无干扰的示例。每个图像的标题显示“原始分类->对抗分类”。请注意，扰动在以下位置开始变得明显ϵ=0.15 并且在 ϵ=0.3。然而，在所有情况下，尽管增加了噪音，人类仍然能够识别正确的类别。 # Plot several examples of adversarial samples at each epsilon cnt = 0 plt.figure(figsize=(8,10)) for i in range(len(epsilons)): for j in range(len(examples[i])): cnt += 1 plt.subplot(len(epsilons),len(examples[0]),cnt) plt.xticks([], []) plt.yticks([], []) if j == 0: plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14) orig,adv,ex = examples[i][j] plt.title(\"{} -> {}\".format(orig, adv)) plt.imshow(ex, cmap=\"gray\") plt.tight_layout() plt.show() 下一步去哪里？ 希望本教程对对抗性机器学习主题有所了解。从这里可以找到许多潜在的方向。这种攻击代表了对抗性攻击研究的最开始，并且由于随后有很多关于如何攻击和防御对手的ML模型的想法。实际上，在NIPS 2017上有一个对抗性的攻击和防御竞赛，并且本文描述了该竞赛中使用的许多方法：对抗性的攻击和防御竞赛。国防方面的工作还引发了使机器学习模型总体上更加健壮的想法，以适应自然扰动和对抗性输入。 另一个方向是不同领域的对抗性攻击和防御。对抗性研究不仅限于图像领域，请查看这种对语音到文本模型的攻击。但是，也许更多地了解对抗性机器学习的最好方法是弄脏您的手。尝试实施与NIPS 2017竞赛不同的攻击，并查看其与FGSM的不同之处。然后，尝试保护模型免受自己的攻击。 脚本的总运行时间： （2分钟57.229秒） Download Python source code: fgsm_tutorial.py Download Jupyter notebook: fgsm_tutorial.ipynb 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/dcgan_faces_tutorial.html":{"url":"beginner/dcgan_faces_tutorial.html","title":"DCGAN教程","keywords":"","body":"DCGAN 教程 作者: Nathan Inkawhich 译者：wangshuai9517 校验: 片刻 介绍 本教程将通过一个例子来介绍DCGAN。我们将使用很多真正的名人照片训练一个生成对抗网络（GAN）后，生成新的假名人照片。这里的大多数代码来自于pytorch/examples中对DCGAN的实现，并且本文档将对DCGAN的实现进行全面解释，并阐明该模型是怎样工作的以及为什么能工作。但是不要担心，我们并不需要你事先了解GAN，但是可能需要先花一些时间来弄明白实际发生了什么。此外，为了节省时间，安装一两个GPU也将有所帮助。让我们从头开始吧。 生成对抗网络 什么是生成对抗网络(GAN)? GAN是用于教授DL模型以捕获训练数据分布的框架，因此我们可以从同一分布中生成新数据。GAN是Ian Goodfellow在2014年发明的，最早在Generative Adversarial Nets一书中进行了描述。它们由两个不同的模型组成：生成器和判别器。生成器的工作是生成看起来像训练图像的“假”图像。判别器的工作是查看图像并从生成器输出它是真实的训练图像还是伪图像。在训练过程中，生成器不断尝试通过生成越来越好的伪造品而使判别器的性能超过智者，而判别器正在努力成为更好的侦探并正确地对真实和伪造图像进行分类。博弈的平衡点是当生成器生成的伪造品看起来像直接来自训练数据时，而判别器则始终猜测生成器输出是真实还是伪造品的50％置信度。 现在，让我们从判别器开始定义一些在整个教程中使用的符号。 x 表示图像数据。D(x) 表示判别网络，它的输出表示数据 x 来自与训练数据，而不是生成器。在这里，由于我们正在处理图像，因此输入 D(x)是CHW大小为3x64x64图像。 直观地说，当 x 来自训练数据时，D(x)的值应当是大的；而当 x 来自生成器时，D(x) 的值应为小的。 D(x) 也可以被认为是传统的二元分类器。 对于生成器 z 表示从标准正态分布中采样的空间矢量（本征向量）。 G(z) 表示将本征向量 z 映射到数据空间的生成器函数。 G 的目标是估计训练数据来自的分布 p_{data} ，这样就可以从估计的分布 p_g 中生成假样本。 因此，D(G(z)) 表示生成器输出G是真实图片的概率。就像在 Goodfellow’s paper中描述的那样，D 和 G 在玩一个极大极小游戏。在这个游戏中 D 试图最大化正确分类真假图片的概率 logD(x) ，G 试图最小化 D 预测其输出为假图片的概率 log(1-D(G(x))) 。文章中GAN的损失函数是 \\underset{G}{\\text{min}} \\underset{D}{\\text{max}}V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}\\big[logD(x)\\big] + \\mathbb{E}_{z\\sim p_{z}(z)}\\big[log(1-D(G(z)))\\big] 理论上，这个极小极大游戏的目标是 p_g=p_{data}，如果输入是真实的或假的，则判别器会随机猜测。 然而，GAN的收敛理论仍在积极研究中，实际上，模型并不总是能达到此目的。 什么是DCGAN? DCGAN是对上述的GAN的直接扩展，除了它分别在判别器和生成器中明确地使用卷积和卷积转置层。DCGAN是在Radford等的文章Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks中首次被提出的。判别器由卷积层、批标准化 层以及LeakyReLU 激活层组成。输入是3x64x64的图像，输出是输入图像来自实际数据的概率。生成器由转置卷积层，批标准化层以及ReLU 激活层组成。 输入是一个本征向量（latent vector） z，它是从标准正态分布中采样得到的，输出是一个3x64x64 的RGB图像。 转置卷积层能够把本征向量转换成和图像具有相同大小。 在本文中，作者还提供了一些有关如何设置优化器，如何计算损失函数以及如何初始化模型权重的建议，所有这些都将在后面的章节中进行说明。 from __future__ import print_function #%matplotlib inline import argparse import os import random import torch import torch.nn as nn import torch.nn.parallel import torch.backends.cudnn as cudnn import torch.optim as optim import torch.utils.data import torchvision.datasets as dset import torchvision.transforms as transforms import torchvision.utils as vutils import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation from IPython.display import HTML # 为了可重复性设置随机种子 manualSeed = 999 #manualSeed = random.randint(1, 10000) # 如果你想有一个不同的结果使用这行代码 print(\"Random Seed: \", manualSeed) random.seed(manualSeed) torch.manual_seed(manualSeed) 输出: Random Seed: 999 输入 为了能够运行，定义一些输入： dataroot - 数据集文件夹的路径。我们将在后面的章节中讨论更多关于数据集的内容 workers - 数据加载器DataLoader加载数据时能够使用的进程数 batch_size - 训练时的批大小。在DCGAN文献中使用的批大小是128 image_size - 训练时使用的图片大小。 这里设置默认值为 64x64\\ 。如果想使用别的大小，生成器G和判别器D的结构也要改变。 想看更多详细内容请点击这里 nc - 输入图片的颜色通道个数。彩色图片是3 nz - 本征向量的长度 ngf - 生成器使用的特征图深度 ndf - 设置判别器使用的特征图的深度 num_epochs - 一共训练多少次。训练次数多很可能产生更好的结果但是需要训练更长的时间 lr - 训练时的学习率，DCGAN文章中使用的是0.0002 beta1 - Adam优化算法的beta1超参数。文章用使用的是0.5 ngpu - 可利用的GPU数量，如果设置为0则运行在CPU模式。如果设置的大于0则再行在那些数量的GPU # 数据集根目录 dataroot = \"data/celeba\" # 数据加载器能够使用的进程数量 workers = 2 # 训练时的批大小 batch_size = 128 # 训练图片的大小，所有的图片给都将改变到该大小 # 转换器使用的大小. image_size = 64 # 训练图片的通道数，彩色图片是3 nc = 3 # 本征向量z的大小(生成器的输入大小) nz = 100 # 生成器中特征图大小 ngf = 64 # 判别器中特征图大小 ndf = 64 # 训练次数 num_epochs = 5 # 优化器学习率 lr = 0.0002 # Adam优化器的Beta1超参 beta1 = 0.5 # 可利用的GPU数量，使用0将运行在CPU模式。 ngpu = 1 数据 本教程中我将使用 Celeb-A Faces 数据集 可以在链接中下载，或者在 谷歌网盘中下载。下载该数据集名为 img_align_celeba.zip 的文件。 下载完成后，创建一个名为celeba的目录，并将zip文件解压缩到该目录中。然后，将此笔记本的dataroot输入设置为刚创建的celeba目录。结果目录结构应为： /path/to/celeba -> img_align_celeba -> 188242.jpg -> 173822.jpg -> 284702.jpg -> 537394.jpg ... 这是重要的一步，因为我们将使用ImageFolder数据集类，该类要求数据集的根文件夹中有子目录。现在，我们可以创建数据集，创建数据加载器，将设备设置为可以运行，最后可视化一些训练数据。 # 我们能够使用我们创建的数据集图片文件夹了 # 创建数据集 dataset = dset.ImageFolder(root=dataroot, transform=transforms.Compose([ transforms.Resize(image_size), transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ])) # 创建数据加载器 dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=workers) # 决定我们在哪个设备上运行 device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\") # 展示一些训练图片 real_batch = next(iter(dataloader)) plt.figure(figsize=(8,8)) plt.axis(\"off\") plt.title(\"Training Images\") plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0))) 实现（implementation） 设置好输入参数并准备好数据集后，我们现在可以进入实现了。我们将从Weigth初始化策略开始，然后详细讨论生成器，判别器，损失函数和训练循环。 权重初始化 在DCGAN论文中，作者指定所有模型权重均应从 mean=0, stdev=0.02 的正态分布中随机初始化。该weights_init函数以已初始化的模型作为输入，并重新初始化所有卷积，卷积转置和批处理规范化层，以符合此条件。初始化后立即将此功能应用于模型。 # 在netG和netD上调用的自定义权重初始化函数 def weights_init(m): classname = m.__class__.__name__ if classname.find('Conv') != -1: nn.init.normal_(m.weight.data, 0.0, 0.02) elif classname.find('BatchNorm') != -1: nn.init.normal_(m.weight.data, 1.0, 0.02) nn.init.constant_(m.bias.data, 0) 生成器 生成器 G 用于将本征向量 z 映射到数据空间。 由于我们的数据是图像，因此将 z 转换为数据空间意味着最终创建一个与训练图像大小相同的RGB图像（即3x64x64）。 实际上，这是通过一系列跨步的二维卷积转置层实现的，每个转换层与二维批标准化层和relu激活层配对。 生成器的输出通过tanh层，使其输出数据范围和输入图片一样，在 [-1, 1] 之间。 值得注意的是在转换层之后存在批标准化函数，因为这是DCGAN论文的关键贡献。 这些层有助于训练期间的梯度传播。 DCGAN论文中的生成器图片如下所示。 请注意，我们在变量定义部分 (nz, ngf 和 nc) 中设置的输入如何影响代码中的生成器体系结构。 nz是z输入向量的长度，ngf生成器要生成的特征图个数大小，nc是输出图像中的通道数（对于RGB图像，设置为3）。 下面是生成器的代码。 # 生成器代码 class Generator(nn.Module): def __init__(self, ngpu): super(Generator, self).__init__() self.ngpu = ngpu self.main = nn.Sequential( # 输入是 Z, 对Z进行卷积 nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False), nn.BatchNorm2d(ngf * 8), nn.ReLU(True), # 输入特征图大小. (ngf*8) x 4 x 4 nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 4), nn.ReLU(True), # 输入特征图大小. (ngf*4) x 8 x 8 nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 2), nn.ReLU(True), # 输入特征图大小. (ngf*2) x 16 x 16 nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf), nn.ReLU(True), # 输入特征图大小. (ngf) x 32 x 32 nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False), nn.Tanh() # 输入特征图大小. (nc) x 64 x 64 ) def forward(self, input): return self.main(input) 现在，我们可以实例化生成器并对其使用 weights_init 函数。打印出生成器模型，用以查看生成器的结构。 # 创建生成器 netG = Generator(ngpu).to(device) # 如果期望使用多个GPU，设置一下。 if (device.type == 'cuda') and (ngpu > 1): netG = nn.DataParallel(netG, list(range(ngpu))) # 使用权重初始化函数 weights_init 去随机初始化所有权重 # mean=0, stdev=0.2. netG.apply(weights_init) # 输出该模型 print(netG) Out: Generator( (main): Sequential( (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace) (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace) (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace) (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (11): ReLU(inplace) (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (13): Tanh() ) ) 判别器 如上所述，判别器 D 是一个二分类网络，它将图像作为输入并输出输入图像是真实的概率（而不是假的）。 这里，D 采用3x64x64输入图像，通过一系列Conv2d，BatchNorm2d和LeakyReLU层处理它，并通过Sigmoid激活函数输出最终概率。 如果问题需要，可以使用更多层扩展此体系结构，但使用跨步卷积，BatchNorm和LeakyReLU具有重要意义。 DCGAN论文提到使用跨步卷积而不是使用pooling下采样是一种很好的做法，因为它可以让网络学习自己的pooling功能。批标准化和LeakyReLU函数也促进了健康的梯度流动，这对于 G 和 D 的学习过程至关重要。 判别器代码 class Discriminator(nn.Module): def __init__(self, ngpu): super(Discriminator, self).__init__() self.ngpu = ngpu self.main = nn.Sequential( # 输入大小 (nc) x 64 x 64 nn.Conv2d(nc, ndf, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, inplace=True), # state size. (ndf) x 32 x 32 nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 2), nn.LeakyReLU(0.2, inplace=True), # 输入大小. (ndf*2) x 16 x 16 nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 4), nn.LeakyReLU(0.2, inplace=True), # 输入大小. (ndf*4) x 8 x 8 nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 8), nn.LeakyReLU(0.2, inplace=True), # 输入大小. (ndf*8) x 4 x 4 nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False), nn.Sigmoid() ) def forward(self, input): return self.main(input) 现在，我们可以实例化判别器并对其应用weights_init函数。查看打印的模型以查看判别器对象的结构。 # 创建判别器 netD = Discriminator(ngpu).to(device) # 如果期望使用多GPU，设置一下 if (device.type == 'cuda') and (ngpu > 1): netD = nn.DataParallel(netD, list(range(ngpu))) # 使用权重初始化函数 weights_init 去随机初始化所有权重 # mean=0, stdev=0.2. netD.apply(weights_init) # 输出该模型 print(netD) Out: Discriminator( (main): Sequential( (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): LeakyReLU(negative_slope=0.2, inplace) (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (4): LeakyReLU(negative_slope=0.2, inplace) (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): LeakyReLU(negative_slope=0.2, inplace) (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (10): LeakyReLU(negative_slope=0.2, inplace) (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False) (12): Sigmoid() ) ) 损失函数和优化器 随着对判别器 D 和生成器 G 完成了设置， 我们能够详细的叙述它们怎么通过损失函数和优化器来进行学习的。我们将使用Binary Cross Entropy loss (BCELoss) 函数，其在pyTorch中的定义如下： \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right] 需要注意的是目标函数中两个log部分是怎么提供计算的(i.e. log(D(x)) 和 log(1-D(G(z))) 。 即将介绍的训练循环中我们将详细的介绍BCE公式的怎么使用输入 y 的。但重要的是要了解我们如何通过改变 y（即GT标签）来选择我们想要计算的部分损失。 下一步，我们定义真实图片标记为1，假图片标记为0。这个标记将在计算 D 和 G 的损失函数的时候使用，这是在原始的GAN文献中使用的惯例。最后我们设置两个单独的优化器，一个给判别器 D 使用，一个给生成器 G 使用。 就像DCGAN文章中说的那样，两个Adam优化算法都是用学习率为0.0002以及Beta1为0.5。为了保存追踪生成器学习的过程，我们将生成一个批固定不变的来自于高斯分布的本征向量(例如 fixed_noise)。在训练的循环中，我们将周期性的输入这个fixed_noise到生成器 G 中， 在训练都完成后我们将看一下由fixed_noise生成的图片。 # 初始化 BCE损失函数 criterion = nn.BCELoss() # 创建一个批次的本征向量用于可视化生成器训练的过程。 fixed_noise = torch.randn(64, nz, 1, 1, device=device) # 建立一个在训练中使用的真实和假的标记 real_label = 1 fake_label = 0 # 为G和D都设置Adam优化器 optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999)) optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999)) 训练 最后，既然已经定义了GAN框架的所有部分，我们就可以对其进行训练。 请注意，训练GAN在某种程度上是一种艺术形式，因为不正确的超参数设置会导致mode collapse，而对错误的解释很少。 在这里，我们将密切关注Goodfellow的论文中的算法1，同时遵守ganhacks中显示的一些最佳实践。 也就是说，我们将“为真实和假冒”图像构建不同的小批量，并调整G的目标函数以最大化logD(G(z))。 训练分为两个主要部分。 第1部分更新判别器Discriminator，第2部分更新生成器Generator。 Part 1 - 训练判别器 回想一下，训练判别器的目的是最大化将给定输入正确分类为真实或假的概率。 就Goodfellow而言，我们希望“通过提升其随机梯度来更新判别器”。 实际上，我们想要最大化损失log(D(x))+ log(1-D(G(z)))。 由于ganhacks的单独小批量建议，我们将分两步计算。 首先，我们将从训练集中构造一批实际样本，向前通过 D，计算损失（log(D(x))），然后计算梯度 向后传递。 其次，我们将用当前的生成器构造一批假样本，通过D 转发该批次，计算损失（log(1-D(G(z)))）和 accumulate 带有向后传递。 现在，随着从全真实和全假批量累积的梯度，我们称之为Discriminator优化器的一步。 Part 2 - 训练生成器 正如原始论文所述，我们希望通过最小化 log(1-D(G(z))) 来训练生成器Generator，以便产生更好的假样本。 如上所述，Goodfellow表明这不会提供足够的梯度，尤其是在学习过程的早期阶段。 作为修改，我们希望最大化 log(D(G(z)))。 在代码中，我们通过以下方式实现此目的：使用 Discriminator 对第1部分的 Generator 输出进行分类，使用真实标签作为 GT计算G的损失，在反向传递中计算G的梯度，最后使用优化器步骤更新G的参数。使用真实标签作为损失函数的GT标签似乎是违反直觉的，但这允许我们使用BCELoss的 log(x) 部分（而不是 log(1-x) 这部分）这正是我们想要的。 最后，我们将进行一些统计报告，在每个循环结束时，我们将通过生成器推送我们的fixed_noise批次，以直观地跟踪G训练的进度。 报告的训练统计数据是： Loss_D - 判别器损失是所有真实样本批次和所有假样本批次的损失之和 log(D(x)) + log(D(G(z))) . Loss_G - 生成器损失 log(D(G(z))) D(x) - 所有真实批次的判别器的平均输出（整批）。 这应该从接近1开始，然后当G变好时理论上收敛到0.5。 想想为什么会这样。 D(G(z)) - 所有假批次的平均判别器输出。 第一个数字是在D更新之前，第二个数字是在D更新之后。 当G变好时，这些数字应该从0开始并收敛到0.5。 想想为什么会这样。 Note: 此步骤可能需要一段时间，具体取决于您运行的循环数以及是否从数据集中删除了一些数据。 # 训练循环 # 保存跟踪进度的列表 img_list = [] G_losses = [] D_losses = [] iters = 0 print(\"Starting Training Loop...\") # 每个epoh for epoch in range(num_epochs): # 数据加载器中的每个批次 for i, data in enumerate(dataloader, 0): ############################ # (1) 更新 D 网络: 最大化 log(D(x)) + log(1 - D(G(z))) ########################### ## 使用所有真实样本批次训练 netD.zero_grad() # 格式化批 real_cpu = data[0].to(device) b_size = real_cpu.size(0) label = torch.full((b_size,), real_label, device=device) # 通过D向前传递真实批次 output = netD(real_cpu).view(-1) # 对所有真实样本批次计算损失 errD_real = criterion(output, label) # 计算后向传递中D的梯度 errD_real.backward() D_x = output.mean().item() ## 使用所有假样本批次训练 # 生成本征向量批次 noise = torch.randn(b_size, nz, 1, 1, device=device) # 使用生成器G生成假图片 fake = netG(noise) label.fill_(fake_label) # 使用判别器分类所有的假批次样本 output = netD(fake.detach()).view(-1) # 计算判别器D的损失对所有的假样本批次 errD_fake = criterion(output, label) # 对这个批次计算梯度 errD_fake.backward() D_G_z1 = output.mean().item() # 把所有真样本和假样本批次的梯度加起来 errD = errD_real + errD_fake # 更新判别器D optimizerD.step() ############################ # (2) 更新 G 网络: 最大化 log(D(G(z))) ########################### netG.zero_grad() label.fill_(real_label) # 假样本的标签对于生成器成本是真的 # 因为我们之更新了D，通过D执行所有假样本批次的正向传递 output = netD(fake).view(-1) # 基于这个输出计算G的损失 errG = criterion(output, label) # 为生成器计算梯度 errG.backward() D_G_z2 = output.mean().item() # 更新生成器G optimizerG.step() # 输出训练状态 if i % 50 == 0: print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f' % (epoch, num_epochs, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2)) # 为以后画损失图，保存损失 G_losses.append(errG.item()) D_losses.append(errD.item()) # 检查生成器generator做了什么，通过保存的fixed_noise通过G的输出 if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)): with torch.no_grad(): fake = netG(fixed_noise).detach().cpu() img_list.append(vutils.make_grid(fake, padding=2, normalize=True)) iters += 1 Out: Starting Training Loop... [0/5][0/1583] Loss_D: 1.7410 Loss_G: 4.7761 D(x): 0.5343 D(G(z)): 0.5771 / 0.0136 [0/5][50/1583] Loss_D: 1.7332 Loss_G: 25.4829 D(x): 0.9774 D(G(z)): 0.7441 / 0.0000 [0/5][100/1583] Loss_D: 1.6841 Loss_G: 11.6585 D(x): 0.4728 D(G(z)): 0.0000 / 0.0000 [0/5][150/1583] Loss_D: 1.2547 Loss_G: 8.7245 D(x): 0.9286 D(G(z)): 0.5209 / 0.0044 [0/5][200/1583] Loss_D: 0.7563 Loss_G: 8.9600 D(x): 0.9525 D(G(z)): 0.4514 / 0.0003 [0/5][250/1583] Loss_D: 1.0221 Loss_G: 2.5713 D(x): 0.5274 D(G(z)): 0.0474 / 0.1177 [0/5][300/1583] Loss_D: 0.3387 Loss_G: 3.8185 D(x): 0.8431 D(G(z)): 0.1066 / 0.0461 [0/5][350/1583] Loss_D: 0.5054 Loss_G: 3.6141 D(x): 0.7289 D(G(z)): 0.0758 / 0.0535 [0/5][400/1583] Loss_D: 0.8758 Loss_G: 6.5680 D(x): 0.8097 D(G(z)): 0.4017 / 0.0031 [0/5][450/1583] Loss_D: 0.2486 Loss_G: 3.5121 D(x): 0.9035 D(G(z)): 0.1054 / 0.0717 [0/5][500/1583] Loss_D: 1.5792 Loss_G: 4.3590 D(x): 0.3457 D(G(z)): 0.0053 / 0.0379 [0/5][550/1583] Loss_D: 0.8897 Loss_G: 3.9447 D(x): 0.5350 D(G(z)): 0.0349 / 0.0386 [0/5][600/1583] Loss_D: 0.5292 Loss_G: 4.4346 D(x): 0.8914 D(G(z)): 0.2768 / 0.0233 [0/5][650/1583] Loss_D: 0.3779 Loss_G: 4.7253 D(x): 0.7868 D(G(z)): 0.0627 / 0.0174 [0/5][700/1583] Loss_D: 0.7512 Loss_G: 2.6246 D(x): 0.6112 D(G(z)): 0.0244 / 0.1493 [0/5][750/1583] Loss_D: 0.4378 Loss_G: 5.0045 D(x): 0.8614 D(G(z)): 0.2028 / 0.0108 [0/5][800/1583] Loss_D: 0.5795 Loss_G: 6.0537 D(x): 0.8693 D(G(z)): 0.2732 / 0.0066 [0/5][850/1583] Loss_D: 0.8980 Loss_G: 6.5355 D(x): 0.8465 D(G(z)): 0.4226 / 0.0048 [0/5][900/1583] Loss_D: 0.5776 Loss_G: 7.7162 D(x): 0.9756 D(G(z)): 0.3707 / 0.0009 [0/5][950/1583] Loss_D: 0.5593 Loss_G: 5.6692 D(x): 0.9560 D(G(z)): 0.3494 / 0.0080 [0/5][1000/1583] Loss_D: 0.5036 Loss_G: 5.1312 D(x): 0.7775 D(G(z)): 0.0959 / 0.0178 [0/5][1050/1583] Loss_D: 0.5192 Loss_G: 4.5706 D(x): 0.8578 D(G(z)): 0.2605 / 0.0222 [0/5][1100/1583] Loss_D: 0.5645 Loss_G: 3.1618 D(x): 0.7133 D(G(z)): 0.1138 / 0.0768 [0/5][1150/1583] Loss_D: 0.2790 Loss_G: 4.5294 D(x): 0.8541 D(G(z)): 0.0909 / 0.0207 [0/5][1200/1583] Loss_D: 0.5334 Loss_G: 4.3445 D(x): 0.8567 D(G(z)): 0.2457 / 0.0245 [0/5][1250/1583] Loss_D: 0.7318 Loss_G: 2.2779 D(x): 0.6846 D(G(z)): 0.1485 / 0.1497 [0/5][1300/1583] Loss_D: 0.6939 Loss_G: 6.1172 D(x): 0.9123 D(G(z)): 0.3853 / 0.0041 [0/5][1350/1583] Loss_D: 0.4653 Loss_G: 3.7054 D(x): 0.8208 D(G(z)): 0.1774 / 0.0404 [0/5][1400/1583] Loss_D: 1.9711 Loss_G: 3.1569 D(x): 0.2704 D(G(z)): 0.0108 / 0.1390 [0/5][1450/1583] Loss_D: 0.4427 Loss_G: 5.8683 D(x): 0.9230 D(G(z)): 0.2600 / 0.0056 [0/5][1500/1583] Loss_D: 0.4432 Loss_G: 3.3681 D(x): 0.8001 D(G(z)): 0.1510 / 0.0633 [0/5][1550/1583] Loss_D: 0.4852 Loss_G: 3.2790 D(x): 0.7532 D(G(z)): 0.1100 / 0.0661 [1/5][0/1583] Loss_D: 0.3536 Loss_G: 4.5358 D(x): 0.8829 D(G(z)): 0.1714 / 0.0173 [1/5][50/1583] Loss_D: 0.4717 Loss_G: 4.7728 D(x): 0.8973 D(G(z)): 0.2750 / 0.0142 [1/5][100/1583] Loss_D: 0.4702 Loss_G: 2.3528 D(x): 0.7847 D(G(z)): 0.1468 / 0.1385 [1/5][150/1583] Loss_D: 0.4833 Loss_G: 2.9645 D(x): 0.7893 D(G(z)): 0.1607 / 0.0867 [1/5][200/1583] Loss_D: 0.6035 Loss_G: 2.0728 D(x): 0.6646 D(G(z)): 0.0852 / 0.1806 [1/5][250/1583] Loss_D: 0.3822 Loss_G: 3.1946 D(x): 0.7969 D(G(z)): 0.1024 / 0.0656 [1/5][300/1583] Loss_D: 0.3892 Loss_G: 3.3337 D(x): 0.7848 D(G(z)): 0.0969 / 0.0525 [1/5][350/1583] Loss_D: 1.7989 Loss_G: 7.5798 D(x): 0.9449 D(G(z)): 0.7273 / 0.0011 [1/5][400/1583] Loss_D: 0.4765 Loss_G: 3.0655 D(x): 0.7479 D(G(z)): 0.1116 / 0.0687 [1/5][450/1583] Loss_D: 0.3649 Loss_G: 3.1674 D(x): 0.8603 D(G(z)): 0.1619 / 0.0627 [1/5][500/1583] Loss_D: 0.6922 Loss_G: 4.5841 D(x): 0.9235 D(G(z)): 0.4003 / 0.0175 [1/5][550/1583] Loss_D: 0.6126 Loss_G: 4.6642 D(x): 0.8761 D(G(z)): 0.3199 / 0.0180 [1/5][600/1583] Loss_D: 0.7032 Loss_G: 4.6221 D(x): 0.9463 D(G(z)): 0.4365 / 0.0154 [1/5][650/1583] Loss_D: 0.4707 Loss_G: 3.3616 D(x): 0.7664 D(G(z)): 0.1280 / 0.0617 [1/5][700/1583] Loss_D: 0.3393 Loss_G: 2.4236 D(x): 0.9120 D(G(z)): 0.1771 / 0.1280 [1/5][750/1583] Loss_D: 0.6828 Loss_G: 4.4585 D(x): 0.8647 D(G(z)): 0.3546 / 0.0191 [1/5][800/1583] Loss_D: 0.7958 Loss_G: 3.6708 D(x): 0.8386 D(G(z)): 0.3987 / 0.0403 [1/5][850/1583] Loss_D: 0.4651 Loss_G: 2.7477 D(x): 0.7602 D(G(z)): 0.1334 / 0.0900 [1/5][900/1583] Loss_D: 0.8799 Loss_G: 4.7930 D(x): 0.9050 D(G(z)): 0.4710 / 0.0201 [1/5][950/1583] Loss_D: 0.3909 Loss_G: 2.7973 D(x): 0.7730 D(G(z)): 0.0902 / 0.0838 [1/5][1000/1583] Loss_D: 0.3822 Loss_G: 3.0223 D(x): 0.8699 D(G(z)): 0.1837 / 0.0709 [1/5][1050/1583] Loss_D: 0.4689 Loss_G: 2.2831 D(x): 0.7096 D(G(z)): 0.0536 / 0.1448 [1/5][1100/1583] Loss_D: 0.6676 Loss_G: 2.2773 D(x): 0.6669 D(G(z)): 0.1386 / 0.1443 [1/5][1150/1583] Loss_D: 0.5970 Loss_G: 4.1558 D(x): 0.9166 D(G(z)): 0.3554 / 0.0240 [1/5][1200/1583] Loss_D: 0.3622 Loss_G: 3.5782 D(x): 0.8590 D(G(z)): 0.1547 / 0.0481 [1/5][1250/1583] Loss_D: 0.5234 Loss_G: 2.5915 D(x): 0.7811 D(G(z)): 0.1990 / 0.1037 [1/5][1300/1583] Loss_D: 1.3243 Loss_G: 5.5428 D(x): 0.9882 D(G(z)): 0.6572 / 0.0088 [1/5][1350/1583] Loss_D: 0.4891 Loss_G: 1.9552 D(x): 0.7686 D(G(z)): 0.1540 / 0.1910 [1/5][1400/1583] Loss_D: 0.5639 Loss_G: 3.7796 D(x): 0.9137 D(G(z)): 0.3390 / 0.0343 [1/5][1450/1583] Loss_D: 1.7329 Loss_G: 5.0373 D(x): 0.9760 D(G(z)): 0.7332 / 0.0161 [1/5][1500/1583] Loss_D: 0.7999 Loss_G: 3.7268 D(x): 0.9029 D(G(z)): 0.4550 / 0.0384 [1/5][1550/1583] Loss_D: 0.4740 Loss_G: 2.3220 D(x): 0.7824 D(G(z)): 0.1625 / 0.1327 [2/5][0/1583] Loss_D: 0.8693 Loss_G: 3.8890 D(x): 0.9376 D(G(z)): 0.4822 / 0.0339 [2/5][50/1583] Loss_D: 0.3742 Loss_G: 2.5041 D(x): 0.8148 D(G(z)): 0.1310 / 0.1151 [2/5][100/1583] Loss_D: 1.1134 Loss_G: 1.5167 D(x): 0.4248 D(G(z)): 0.0335 / 0.3023 [2/5][150/1583] Loss_D: 0.5987 Loss_G: 3.2047 D(x): 0.8536 D(G(z)): 0.3121 / 0.0555 [2/5][200/1583] Loss_D: 2.0846 Loss_G: 1.5473 D(x): 0.1919 D(G(z)): 0.0054 / 0.2899 [2/5][250/1583] Loss_D: 0.5017 Loss_G: 3.0225 D(x): 0.8965 D(G(z)): 0.2986 / 0.0626 [2/5][300/1583] Loss_D: 1.3296 Loss_G: 4.1927 D(x): 0.9444 D(G(z)): 0.6574 / 0.0270 [2/5][350/1583] Loss_D: 0.4905 Loss_G: 2.7693 D(x): 0.8049 D(G(z)): 0.2090 / 0.0863 [2/5][400/1583] Loss_D: 0.4668 Loss_G: 2.1790 D(x): 0.7160 D(G(z)): 0.0815 / 0.1529 [2/5][450/1583] Loss_D: 0.4877 Loss_G: 2.4190 D(x): 0.6943 D(G(z)): 0.0693 / 0.1254 [2/5][500/1583] Loss_D: 0.7856 Loss_G: 2.2362 D(x): 0.6148 D(G(z)): 0.1698 / 0.1489 [2/5][550/1583] Loss_D: 0.6371 Loss_G: 1.3879 D(x): 0.6164 D(G(z)): 0.0852 / 0.3041 [2/5][600/1583] Loss_D: 0.6409 Loss_G: 2.8623 D(x): 0.7658 D(G(z)): 0.2684 / 0.0790 [2/5][650/1583] Loss_D: 0.6454 Loss_G: 1.5708 D(x): 0.6293 D(G(z)): 0.0944 / 0.2706 [2/5][700/1583] Loss_D: 0.8472 Loss_G: 2.0847 D(x): 0.5071 D(G(z)): 0.0181 / 0.1937 [2/5][750/1583] Loss_D: 1.2356 Loss_G: 0.3673 D(x): 0.3606 D(G(z)): 0.0328 / 0.7270 [2/5][800/1583] Loss_D: 0.4852 Loss_G: 2.7325 D(x): 0.8670 D(G(z)): 0.2630 / 0.0877 [2/5][850/1583] Loss_D: 0.6494 Loss_G: 4.5357 D(x): 0.8899 D(G(z)): 0.3756 / 0.0158 [2/5][900/1583] Loss_D: 0.5184 Loss_G: 2.7194 D(x): 0.8377 D(G(z)): 0.2540 / 0.0871 [2/5][950/1583] Loss_D: 0.9771 Loss_G: 4.6200 D(x): 0.9596 D(G(z)): 0.5432 / 0.0176 [2/5][1000/1583] Loss_D: 0.7509 Loss_G: 2.2864 D(x): 0.5861 D(G(z)): 0.1021 / 0.1539 [2/5][1050/1583] Loss_D: 0.4512 Loss_G: 3.2484 D(x): 0.8649 D(G(z)): 0.2313 / 0.0542 [2/5][1100/1583] Loss_D: 0.6856 Loss_G: 2.2425 D(x): 0.6405 D(G(z)): 0.1333 / 0.1508 [2/5][1150/1583] Loss_D: 0.5271 Loss_G: 3.0327 D(x): 0.8385 D(G(z)): 0.2552 / 0.0639 [2/5][1200/1583] Loss_D: 0.4058 Loss_G: 2.9557 D(x): 0.8769 D(G(z)): 0.2169 / 0.0694 [2/5][1250/1583] Loss_D: 0.5564 Loss_G: 2.9065 D(x): 0.8409 D(G(z)): 0.2835 / 0.0695 [2/5][1300/1583] Loss_D: 0.4703 Loss_G: 2.7865 D(x): 0.7825 D(G(z)): 0.1680 / 0.0850 [2/5][1350/1583] Loss_D: 0.5352 Loss_G: 3.1362 D(x): 0.8260 D(G(z)): 0.2582 / 0.0606 [2/5][1400/1583] Loss_D: 0.5281 Loss_G: 2.7742 D(x): 0.7970 D(G(z)): 0.2275 / 0.0835 [2/5][1450/1583] Loss_D: 0.6558 Loss_G: 1.8152 D(x): 0.6103 D(G(z)): 0.0795 / 0.2030 [2/5][1500/1583] Loss_D: 0.9446 Loss_G: 1.1492 D(x): 0.4593 D(G(z)): 0.0356 / 0.3947 [2/5][1550/1583] Loss_D: 0.9269 Loss_G: 0.7383 D(x): 0.5226 D(G(z)): 0.1333 / 0.5205 [3/5][0/1583] Loss_D: 0.4855 Loss_G: 2.1548 D(x): 0.7157 D(G(z)): 0.1059 / 0.1568 [3/5][50/1583] Loss_D: 0.7259 Loss_G: 1.1093 D(x): 0.5804 D(G(z)): 0.0797 / 0.3894 [3/5][100/1583] Loss_D: 0.7367 Loss_G: 1.0389 D(x): 0.5515 D(G(z)): 0.0405 / 0.4190 [3/5][150/1583] Loss_D: 0.5942 Loss_G: 3.4803 D(x): 0.9290 D(G(z)): 0.3709 / 0.0432 [3/5][200/1583] Loss_D: 1.3464 Loss_G: 0.6549 D(x): 0.3261 D(G(z)): 0.0242 / 0.5949 [3/5][250/1583] Loss_D: 0.5110 Loss_G: 2.2086 D(x): 0.7263 D(G(z)): 0.1327 / 0.1457 [3/5][300/1583] Loss_D: 1.4272 Loss_G: 3.3018 D(x): 0.9230 D(G(z)): 0.6654 / 0.0635 [3/5][350/1583] Loss_D: 0.6491 Loss_G: 3.0766 D(x): 0.8124 D(G(z)): 0.3127 / 0.0607 [3/5][400/1583] Loss_D: 0.5583 Loss_G: 2.9363 D(x): 0.8233 D(G(z)): 0.2759 / 0.0666 [3/5][450/1583] Loss_D: 0.9496 Loss_G: 0.6436 D(x): 0.4958 D(G(z)): 0.1367 / 0.5538 [3/5][500/1583] Loss_D: 0.4463 Loss_G: 2.2234 D(x): 0.7776 D(G(z)): 0.1545 / 0.1371 [3/5][550/1583] Loss_D: 0.5874 Loss_G: 3.6688 D(x): 0.8478 D(G(z)): 0.2930 / 0.0348 [3/5][600/1583] Loss_D: 0.3724 Loss_G: 2.6326 D(x): 0.8673 D(G(z)): 0.1854 / 0.0891 [3/5][650/1583] Loss_D: 0.7292 Loss_G: 4.4254 D(x): 0.9081 D(G(z)): 0.4234 / 0.0200 [3/5][700/1583] Loss_D: 0.4728 Loss_G: 2.8665 D(x): 0.8189 D(G(z)): 0.2115 / 0.0774 [3/5][750/1583] Loss_D: 0.5845 Loss_G: 3.3046 D(x): 0.8977 D(G(z)): 0.3490 / 0.0463 [3/5][800/1583] Loss_D: 0.5597 Loss_G: 2.2564 D(x): 0.7088 D(G(z)): 0.1497 / 0.1300 [3/5][850/1583] Loss_D: 0.6518 Loss_G: 2.5048 D(x): 0.7195 D(G(z)): 0.2183 / 0.1053 [3/5][900/1583] Loss_D: 0.7340 Loss_G: 1.4263 D(x): 0.6285 D(G(z)): 0.1806 / 0.2818 [3/5][950/1583] Loss_D: 1.4633 Loss_G: 4.9204 D(x): 0.9792 D(G(z)): 0.7093 / 0.0143 [3/5][1000/1583] Loss_D: 0.6643 Loss_G: 2.8332 D(x): 0.8548 D(G(z)): 0.3597 / 0.0751 [3/5][1050/1583] Loss_D: 0.7741 Loss_G: 2.9355 D(x): 0.7281 D(G(z)): 0.3064 / 0.0712 [3/5][1100/1583] Loss_D: 0.7279 Loss_G: 3.2299 D(x): 0.8867 D(G(z)): 0.4193 / 0.0544 [3/5][1150/1583] Loss_D: 0.6049 Loss_G: 1.9150 D(x): 0.6917 D(G(z)): 0.1645 / 0.1912 [3/5][1200/1583] Loss_D: 0.7431 Loss_G: 3.8188 D(x): 0.9334 D(G(z)): 0.4500 / 0.0306 [3/5][1250/1583] Loss_D: 0.5061 Loss_G: 1.9905 D(x): 0.7393 D(G(z)): 0.1531 / 0.1653 [3/5][1300/1583] Loss_D: 0.6979 Loss_G: 3.0183 D(x): 0.8182 D(G(z)): 0.3421 / 0.0616 [3/5][1350/1583] Loss_D: 0.9133 Loss_G: 4.0629 D(x): 0.9198 D(G(z)): 0.5131 / 0.0261 [3/5][1400/1583] Loss_D: 0.7075 Loss_G: 4.0061 D(x): 0.9188 D(G(z)): 0.4216 / 0.0266 [3/5][1450/1583] Loss_D: 0.7704 Loss_G: 2.3802 D(x): 0.7555 D(G(z)): 0.3348 / 0.1114 [3/5][1500/1583] Loss_D: 0.6055 Loss_G: 1.8402 D(x): 0.7011 D(G(z)): 0.1643 / 0.1995 [3/5][1550/1583] Loss_D: 0.7240 Loss_G: 3.2589 D(x): 0.8747 D(G(z)): 0.4069 / 0.0528 [4/5][0/1583] Loss_D: 0.8162 Loss_G: 2.8040 D(x): 0.8827 D(G(z)): 0.4435 / 0.0870 [4/5][50/1583] Loss_D: 0.5859 Loss_G: 2.2796 D(x): 0.6782 D(G(z)): 0.1312 / 0.1309 [4/5][100/1583] Loss_D: 0.6655 Loss_G: 3.5365 D(x): 0.8178 D(G(z)): 0.3262 / 0.0394 [4/5][150/1583] Loss_D: 1.8662 Loss_G: 5.4950 D(x): 0.9469 D(G(z)): 0.7590 / 0.0113 [4/5][200/1583] Loss_D: 0.7060 Loss_G: 3.6253 D(x): 0.9215 D(G(z)): 0.4316 / 0.0364 [4/5][250/1583] Loss_D: 0.5589 Loss_G: 2.1394 D(x): 0.7108 D(G(z)): 0.1513 / 0.1548 [4/5][300/1583] Loss_D: 0.7278 Loss_G: 1.2391 D(x): 0.5757 D(G(z)): 0.0987 / 0.3454 [4/5][350/1583] Loss_D: 0.7597 Loss_G: 2.8481 D(x): 0.7502 D(G(z)): 0.3094 / 0.0843 [4/5][400/1583] Loss_D: 0.6167 Loss_G: 2.2143 D(x): 0.6641 D(G(z)): 0.1315 / 0.1405 [4/5][450/1583] Loss_D: 0.6234 Loss_G: 1.7961 D(x): 0.7303 D(G(z)): 0.2208 / 0.2007 [4/5][500/1583] Loss_D: 0.6098 Loss_G: 4.9416 D(x): 0.9442 D(G(z)): 0.3978 / 0.0104 [4/5][550/1583] Loss_D: 0.6570 Loss_G: 3.6935 D(x): 0.9180 D(G(z)): 0.4015 / 0.0312 [4/5][600/1583] Loss_D: 0.4195 Loss_G: 2.3446 D(x): 0.7798 D(G(z)): 0.1319 / 0.1211 [4/5][650/1583] Loss_D: 0.5291 Loss_G: 2.5303 D(x): 0.7528 D(G(z)): 0.1875 / 0.1075 [4/5][700/1583] Loss_D: 0.5187 Loss_G: 2.0350 D(x): 0.7174 D(G(z)): 0.1431 / 0.1547 [4/5][750/1583] Loss_D: 0.8208 Loss_G: 1.0780 D(x): 0.5665 D(G(z)): 0.1128 / 0.3844 [4/5][800/1583] Loss_D: 0.5223 Loss_G: 3.0140 D(x): 0.8708 D(G(z)): 0.2871 / 0.0612 [4/5][850/1583] Loss_D: 2.9431 Loss_G: 1.0175 D(x): 0.0914 D(G(z)): 0.0162 / 0.4320 [4/5][900/1583] Loss_D: 0.5456 Loss_G: 1.7923 D(x): 0.7489 D(G(z)): 0.1972 / 0.2038 [4/5][950/1583] Loss_D: 0.4718 Loss_G: 2.3825 D(x): 0.7840 D(G(z)): 0.1772 / 0.1172 [4/5][1000/1583] Loss_D: 0.5174 Loss_G: 2.5070 D(x): 0.8367 D(G(z)): 0.2556 / 0.1074 [4/5][1050/1583] Loss_D: 0.8214 Loss_G: 0.8055 D(x): 0.5181 D(G(z)): 0.0694 / 0.4963 [4/5][1100/1583] Loss_D: 1.3243 Loss_G: 0.7562 D(x): 0.3284 D(G(z)): 0.0218 / 0.5165 [4/5][1150/1583] Loss_D: 0.9334 Loss_G: 5.1260 D(x): 0.8775 D(G(z)): 0.4817 / 0.0088 [4/5][1200/1583] Loss_D: 0.5141 Loss_G: 2.7230 D(x): 0.8067 D(G(z)): 0.2188 / 0.0872 [4/5][1250/1583] Loss_D: 0.6007 Loss_G: 1.9893 D(x): 0.6968 D(G(z)): 0.1667 / 0.1748 [4/5][1300/1583] Loss_D: 0.4025 Loss_G: 2.3066 D(x): 0.8101 D(G(z)): 0.1471 / 0.1412 [4/5][1350/1583] Loss_D: 0.5979 Loss_G: 3.2825 D(x): 0.8248 D(G(z)): 0.3003 / 0.0509 [4/5][1400/1583] Loss_D: 0.7430 Loss_G: 3.6521 D(x): 0.8888 D(G(z)): 0.4243 / 0.0339 [4/5][1450/1583] Loss_D: 1.0814 Loss_G: 5.4255 D(x): 0.9647 D(G(z)): 0.5842 / 0.0070 [4/5][1500/1583] Loss_D: 1.7211 Loss_G: 0.7875 D(x): 0.2588 D(G(z)): 0.0389 / 0.5159 [4/5][1550/1583] Loss_D: 0.5871 Loss_G: 2.1340 D(x): 0.7332 D(G(z)): 0.1982 / 0.1518 结果 最后，让我们看看我们做的怎么样。 在这里，我们将看看三个不同的结果。 首先，我们将看到判别器D和生成器G的损失在训练期间是如何变化的。 其次，我们将在每个批次可视化生成器G的输出。 第三，我们将查看一批实际数据以及来自生成器G一批假数据。 损失与训练迭代次数关系图 下面将绘制生成器和判别器的损失和训练迭代次数关系图。 plt.figure(figsize=(10,5)) plt.title(\"Generator and Discriminator Loss During Training\") plt.plot(G_losses,label=\"G\") plt.plot(D_losses,label=\"D\") plt.xlabel(\"iterations\") plt.ylabel(\"Loss\") plt.legend() plt.show() 生成器G的训练进度 我们在每一个批次训练完成之后都保存了生成器的输出。 现在我们可以通过动画可视化生成器G的训练进度。点击播放按钮开始动画. #%%capture fig = plt.figure(figsize=(8,8)) plt.axis(\"off\") ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list] ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True) HTML(ani.to_jshtml()) 真实图像 vs. 假图像 最后，让我们一起看看一些真实的图像和假图像。 # 从数据加载器中获取一批真实图像 real_batch = next(iter(dataloader)) # 画出真实图像 plt.figure(figsize=(15,15)) plt.subplot(1,2,1) plt.axis(\"off\") plt.title(\"Real Images\") plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0))) # 画出来自最后一次训练的假图像 plt.subplot(1,2,2) plt.axis(\"off\") plt.title(\"Fake Images\") plt.imshow(np.transpose(img_list[-1],(1,2,0))) plt.show() 下一步计划 我们已经到了教程的最后，但是你可以根据此教程研究以下内容： 训练更长的时间看看能够达到多好的结果 调整此模型以适合不同的数据集，如果可能你可以更改输入图片大小以及模型的架构 看看这里其他一些很酷的GAN项目 创建一个能够产生音乐的GAN模型 脚本的总运行时间：（ 27分钟53.743秒） 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/audio_preprocessing_tutorial.html":{"url":"beginner/audio_preprocessing_tutorial.html","title":"torchaudio教程","keywords":"","body":"torchaudio教程 PyTorch是一个开源深度学习平台，提供了从研究原型到具有GPU支持的生产部署的无缝路径。 解决机器学习问题的巨大努力在于数据准备。torchaudio利用PyTorch的GPU支持，并提供许多工具来简化数据加载并使其更具可读性。在本教程中，我们将看到如何从简单的数据集中加载和预处理数据。 对于本教程，请确保matplotlib已安装该软件包, 以方便查看。 import torch import torchaudio import matplotlib.pyplot as plt 打开数据集 torchaudio支持以wav和mp3格式加载声音文件。我们将波形称为原始音频信号。 filename = \"https://pytorch.org/tutorials/_static/img/steam-train-whistle-daniel_simon-converted-from-mp3.wav\" waveform, sample_rate = torchaudio.load(filename) print(\"Shape of waveform: {}\".format(waveform.size())) print(\"Sample rate of waveform: {}\".format(sample_rate)) plt.figure() plt.plot(waveform.t().numpy()) 日期： Shape of waveform: torch.Size([2, 276858]) Sample rate of waveform: 44100 转换 torchaudio支持越来越多的 转换 Resample ：将波形重采样为其他采样率。 Spectrogram ：根据波形创建频谱图。 MelScale ：使用转换矩阵将普通STFT转换为Mel频率STFT。 AmplitudeToDB ：这将频谱图从功率/振幅标度转换为分贝标度。 MFCC ：从波形创建梅尔频率倒谱系数。 MelSpectrogram ：使用PyTorch中的STFT功能从波形创建MEL频谱图。 MuLawEncoding ：基于mu-law压扩对波形进行编码。 MuLawDecoding ：解码mu-law编码的波形。 由于所有变换都是nn.Modules或jit.ScriptModules，因此它们可以随时用作神经网络的一部分。 首先，我们可以以对数刻度查看频谱图的对数。 specgram = torchaudio.transforms.Spectrogram()(waveform) print(\"Shape of spectrogram: {}\".format(specgram.size())) plt.figure() plt.imshow(specgram.log2()[0,:,:].numpy(), cmap='gray') Out: Shape of spectrogram: torch.Size([2, 201, 1385]) 或者我们可以以对数刻度查看梅尔光谱图。 specgram = torchaudio.transforms.MelSpectrogram()(waveform) print(\"Shape of spectrogram: {}\".format(specgram.size())) plt.figure() p = plt.imshow(specgram.log2()[0,:,:].detach().numpy(), cmap='gray') Out: Shape of spectrogram: torch.Size([2, 128, 1385]) 我们可以一次对一个通道重新采样波形。 new_sample_rate = sample_rate/10 # Since Resample applies to a single channel, we resample first channel here channel = 0 transformed = torchaudio.transforms.Resample(sample_rate, new_sample_rate)(waveform[channel,:].view(1,-1)) print(\"Shape of transformed waveform: {}\".format(transformed.size())) plt.figure() plt.plot(transformed[0,:].numpy()) Out: Shape of transformed waveform: torch.Size([1, 27686]) 作为变换的另一个示例，我们可以基于Mu-Law编码对信号进行编码。但是要这样做，我们需要信号在-1和1之间。由于张量只是常规的PyTorch张量，因此我们可以在其上应用标准运算符。 # Let's check if the tensor is in the interval [-1,1] print(\"Min of waveform: {}\\nMax of waveform: {}\\nMean of waveform: {}\".format(waveform.min(), waveform.max(), waveform.mean())) Out: Min of waveform: -0.572845458984375 Max of waveform: 0.575958251953125 Mean of waveform: 9.293758921558037e-05 由于波形已经在-1和1之间，因此我们不需要对其进行归一化。 def normalize(tensor): # Subtract the mean, and scale to the interval [-1,1] tensor_minusmean = tensor - tensor.mean() return tensor_minusmean/tensor_minusmean.abs().max() # Let's normalize to the full interval [-1,1] # waveform = normalize(waveform) 让我们对波形进行编码。 transformed = torchaudio.transforms.MuLawEncoding()(waveform) print(\"Shape of transformed waveform: {}\".format(transformed.size())) plt.figure() plt.plot(transformed[0,:].numpy()) Out: Shape of transformed waveform: torch.Size([2, 276858]) 现在解码。 reconstructed = torchaudio.transforms.MuLawDecoding()(transformed) print(\"Shape of recovered waveform: {}\".format(reconstructed.size())) plt.figure() plt.plot(reconstructed[0,:].numpy()) Out: Shape of recovered waveform: torch.Size([2, 276858]) 我们最终可以将原始波形与其重构版本进行比较。 # Compute median relative difference err = ((waveform-reconstructed).abs() / waveform.abs()).median() print(\"Median relative difference between original and MuLaw reconstucted signals: {:.2%}\".format(err)) Out: Median relative difference between original and MuLaw reconstucted signals: 1.28% 从Kaldi迁移到Torchaudio 用户可能熟悉 语音识别工具包Kaldi。torchaudio在中提供与之的兼容性 torchaudio.kaldi_io。实际上，它可以通过以下方式从kaldi scp或ark文件或流中读取： read_vec_int_ark read_vec_flt_scp read_vec_flt_arkfile /流 read_mat_scp read_mat_ark torchaudio为GPU提供支持 spectrogram 并 fbank受益于Kaldi兼容的转换，请参见此处以获取更多信息。 n_fft = 400.0 frame_length = n_fft / sample_rate * 1000.0 frame_shift = frame_length / 2.0 params = { \"channel\": 0, \"dither\": 0.0, \"window_type\": \"hanning\", \"frame_length\": frame_length, \"frame_shift\": frame_shift, \"remove_dc_offset\": False, \"round_to_power_of_two\": False, \"sample_frequency\": sample_rate, } specgram = torchaudio.compliance.kaldi.spectrogram(waveform, **params) print(\"Shape of spectrogram: {}\".format(specgram.size())) plt.figure() plt.imshow(specgram.t().numpy(), cmap='gray') Out: Shape of spectrogram: torch.Size([1383, 201]) 我们还支持根据波形计算滤波器组特征，与Kaldi的实现相匹配。 fbank = torchaudio.compliance.kaldi.fbank(waveform, **params) print(\"Shape of fbank: {}\".format(fbank.size())) plt.figure() plt.imshow(fbank.t().numpy(), cmap='gray') Out: Shape of fbank: torch.Size([1383, 23]) 结论 我们使用示例原始音频信号或波形来说明如何使用torchaudio打开音频文件，以及如何预处理和转换此类波形。鉴于torchaudio是基于PyTorch构建的，则这些技术可在利用GPU的同时用作更高级音频应用（例如语音识别）的构建块。 脚本的总运行时间： （0分钟2.343秒） Download Python source code: audio_preprocessing_tutorial.py Download Jupyter notebook: audio_preprocessing_tutorial.ipynb 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"intermediate/char_rnn_classification_tutorial.html":{"url":"intermediate/char_rnn_classification_tutorial.html","title":"NLP From Scratch: 使用char-RNN对姓氏进行分类","keywords":"","body":"NLP From Scratch：使用char-RNN对姓氏进行分类 > 作者: Sean Robertson 校验: 松鼠 我们将构建和训练基本的char-RNN来对单词进行分类。本教程以及以下两个教程展示了如何“从头开始”为NLP建模进行预处理数据，尤其是不使用Torchtext的许多便利功能，因此您可以了解NLP建模的预处理是如何从低层次进行的。 char-RNN将单词作为一系列字符读取,在每个步骤输出预测和“隐藏状态”，将其先前的隐藏状态输入到每个下一步。我们将最终的预测作为输出，即单词属于哪个类别。 具体来说，我们将训练起源于18种语言的数千种姓氏，并根据拼写来预测姓氏来自哪种语言： $ python predict.py Hinton (-0.47) Scottish (-1.52) English (-3.57) Irish $ python predict.py Schmidhuber (-0.19) German (-2.48) Czech (-2.68) Dutch 建议： 假设你已经至少安装PyTorch，知道Python和理解张量： pytorch安装说明 观看《PyTorch进行深度学习：60分钟速成》来开始学习pytorch 通过实例深入学习PyTorch pytorch为前torch用户的提供的指南 下面这些是了解RNNs以及它们如何工作的相关联接： 回归神经网络展示真实生活中的一系列例子 理解LSTM网络虽然是关于LSTMs的但也对RNNs有很多详细的讲解 准备数据 Note 从data.zip\">此处下载数据，并将其解压到当前目录。 包含了在data/names目录被命名为[Language] .txt 的18个文本文件。每个文件都包含了一堆姓氏，每行一个名字，大多都已经罗马字母化了（但我们仍然需要从Unicode转换到到ASCII）。 我们将得到一个字典，列出每种语言的名称列表 。通用变量category和line（在本例中为语言和名称）用于以后的扩展。{language: [names ...]} from __future__ import unicode_literals, print_function, division from io import open import glob import os def findFiles(path): return glob.glob(path) print(findFiles('data/names/*.txt')) import unicodedata import string all_letters = string.ascii_letters + \" .,;'\" n_letters = len(all_letters) # Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427 def unicodeToAscii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in all_letters ) print(unicodeToAscii('Ślusàrski')) # Build the category_lines dictionary, a list of names per language category_lines = {} all_categories = [] # Read a file and split into lines def readLines(filename): lines = open(filename, encoding='utf-8').read().strip().split('\\n') return [unicodeToAscii(line) for line in lines] for filename in findFiles('data/names/*.txt'): category = os.path.splitext(os.path.basename(filename))[0] a\\ll_categories.append(category) lines = readLines(filename) category_lines[category] = lines n_categories = len(all_categories) 输出： ['data/names/French.txt', 'data/names/Czech.txt', 'data/names/Dutch.txt', 'data/names/Polish.txt', 'data/names/Scottish.txt', 'data/names/Chinese.txt', 'data/names/English.txt', 'data/names/Italian.txt', 'data/names/Portuguese.txt', 'data/names/Japanese.txt', 'data/names/German.txt', 'data/names/Russian.txt', 'data/names/Korean.txt', 'data/names/Arabic.txt', 'data/names/Greek.txt', 'data/names/Vietnamese.txt', 'data/names/Spanish.txt', 'data/names/Irish.txt'] Slusarski 现在，我们有了category_lines字典，将每个类别（语言）映射到行（姓氏）列表。我们还保持all_categories（只是一种语言列表）和n_categories为可追加状态，供后续的调用。 print(category_lines['Italian'][:5]) 输出: ['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni'] 将姓氏转化为张量 我们已经处理好了所有的姓氏，现在我们需要将它们转换为张量以使用它们。 为了表示单个字母，我们使用大小为的“独热向量” 。一个独热向量就是在字母索引处填充1，其他都填充为0，例，\"b\" = 为了表达一个单词，我们将一堆字母合并成2D矩阵，其中举证的大小为 额外的1维是因为PyTorch假设所有东西都是成批的-我们在这里只使用1的批处理大小。 import torch # Find letter index from all_letters, e.g. \"a\" = 0 def letterToIndex(letter): return all_letters.find(letter) # Just for demonstration, turn a letter into a Tensor def letterToTensor(letter): tensor = torch.zeros(1, n_letters) tensor[0][letterToIndex(letter)] = 1 return tensor # Turn a line into a , # or an array of one-hot letter vectors def lineToTensor(line): tensor = torch.zeros(len(line), 1, n_letters) for li, letter in enumerate(line): tensor[li][0][letterToIndex(letter)] = 1 return tensor print(letterToTensor('J')) print(lineToTensor('Jones').size()) 输出: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) torch.Size([5, 1, 57]) 创建网络 在进行自动求导之前，在Torch中创建一个递归神经网络需要在多个时间状态上克隆图的参数。图保留了隐藏状态和梯度，这些状态和梯度现在完全由图本身处理。这意味着您可以以非常“单纯”的方式将RNN作为常规的前馈网络来实现。 这个RNN模块（大部分是从PyTorch for Torch用户教程中复制的）只有2个线性层，它们在输入和隐藏状态下运行，输出之后是LogSoftmax层。 import torch.nn as nn class RNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(RNN, self).__init__() self.hidden_size = hidden_size self.i2h = nn.Linear(input_size + hidden_size, hidden_size) self.i2o = nn.Linear(input_size + hidden_size, output_size) self.softmax = nn.LogSoftmax(dim=1) def forward(self, input, hidden): combined = torch.cat((input, hidden), 1) hidden = self.i2h(combined) output = self.i2o(combined) output = self.softmax(output) return output, hidden def initHidden(self): return torch.zeros(1, self.hidden_size) n_hidden = 128 rnn = RNN(n_letters, n_hidden, n_categories) 运行网络的步骤是，首先我们需要输入（在本例中为当前字母的张量）和先前的隐藏状态（首先将其初始化为零）。我们将返回输出（每种语言的概率）和下一个隐藏状态（我们将其保留用于下一步）。 input = letterToTensor('A') hidden =torch.zeros(1, n_hidden) output, next_hidden = rnn(input, hidden) 为了提高效率，我们不想为每个步骤都创建一个新的Tensor，因此我们将使用和lineToTensor代替letterToTensorslice。这可以通过预先计算一批张量来进一步优化。 input = lineToTensor('Albert') hidden = torch.zeros(1, n_hidden) output, next_hidden = rnn(input[0], hidden) print(output) 输出: tensor([[-2.8636, -2.8199, -2.8899, -2.9073, -2.9117, -2.8644, -2.9027, -2.9334, -2.8705, -2.8383, -2.8892, -2.9161, -2.8215, -2.9996, -2.9423, -2.9116, -2.8750, -2.8862]], grad_fn=) 正如你看到的输出为的张量，其中每一个值都是该类别的可能性（数值越大可能性越高）。 训练 准备训练 在训练之前，我们需要做一些辅助函数。首先是解释网络的输出，我们知道这是每个类别的可能性。我们可以用Tensor.topk来获取最大值对应的索引： def categoryFromOutput(output): top_n, top_i = output.topk(1) category_i = top_i[0].item() return all_categories[category_i], category_i print(categoryFromOutput(output)) 输出: ('Czech', 1) 我们也将需要一个快速的方法来获得一个训练例子（姓氏和其所属语言）: import random def randomChoice(l): return l[random.randint(0, len(l) - 1)] def randomTrainingExample(): category = randomChoice(all_categories) line = randomChoice(category_lines[category]) category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long) line_tensor = lineToTensor(line) return category, line, category_tensor, line_tensor for i in range(10): category, line, category_tensor, line_tensor = randomTrainingExample() print('category =', category, '\\t // \\t line =', line) 输出: category = Dutch // line = Ryskamp category = Spanish // line = Iniguez category = Vietnamese // line = Thuy category = Italian // line = Nacar category = Vietnamese // line = Le category = French // line = Tremblay category = Russian // line = Bakhchivandzhi category = Irish // line = Kavanagh category = Irish // line = O'Shea category = Spanish // line = Losa 网络训练 现在，训练该网络所需要做的就是向它喂入大量训练样例，进行预测，并告诉它预测的是否正确。 最后因为RNN的最后一层是nn.LogSoftmax,所以我们选择损失函数nn.NLLLoss比较合适。 criterion = nn.NLLLoss() 每个循环的训练将： 创建输入和目标张量 创建一个零初始隐藏状态 读取每个字母 保持隐藏状态到下一个字母 比较最后输出和目标 进行反向传播 返回输出值和损失函数的值 learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn def train(category_tensor, line_tensor): hidden = rnn.initHidden() rnn.zero_grad() for i in range(line_tensor.size()[0]): output, hidden = rnn(line_tensor[i], hidden) loss = criterion(output, category_tensor) loss.backward() # Add parameters' gradients to their values, multiplied by learning rate for p in rnn.parameters(): p.data.add_(-learning_rate, p.grad.data) return output, loss.item() 现在，我们只需要运行大量样例。由于train函数同时返回output和loss，因此我们可以打印其猜测并跟踪绘制损失。由于有1000个样例，因此我们仅打印每个print_every样例，并对损失进行平均。 import time import math n_iters = 100000 print_every = 5000 plot_every = 1000 # Keep track of losses for plotting current_loss = 0 all_losses = [] def timeSince(since): now = time.time() s = now - since m = math.floor(s / 60) s -= m * 60 return '%dm %ds' % (m, s) start = time.time() for iter in range(1, n_iters + 1): category, line, category_tensor, line_tensor = randomTrainingExample() output, loss = train(category_tensor, line_tensor) current_loss += loss # Print iter number, loss, name and guess if iter % print_every == 0: guess, guess_i = categoryFromOutput(output) correct = '✓' if guess == category else '✗ (%s)' % category print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct)) # Add current loss avg to list of losses if iter % plot_every == 0: all_losses.append(current_loss / plot_every) current_loss = 0 输出: 5000 5% (0m 7s) 2.7482 Silje / French ✗ (Dutch) 10000 10% (0m 15s) 1.5569 Lillis / Greek ✓ 15000 15% (0m 22s) 2.7729 Burt / Korean ✗ (English) 20000 20% (0m 30s) 1.1036 Zhong / Chinese ✓ 25000 25% (0m 38s) 1.7088 Sarraf / Portuguese ✗ (Arabic) 30000 30% (0m 45s) 0.7595 Benivieni / Italian ✓ 35000 35% (0m 53s) 1.2900 Arreola / Italian ✗ (Spanish) 40000 40% (1m 0s) 2.3171 Gass / Arabic ✗ (German) 45000 45% (1m 8s) 3.1630 Stoppelbein / Dutch ✗ (German) 50000 50% (1m 15s) 1.7478 Berger / German ✗ (French) 55000 55% (1m 23s) 1.3516 Almeida / Spanish ✗ (Portuguese) 60000 60% (1m 31s) 1.8843 Hellewege / Dutch ✗ (German) 65000 65% (1m 38s) 1.7374 Moreau / French ✓ 70000 70% (1m 46s) 0.5718 Naifeh / Arabic ✓ 75000 75% (1m 53s) 0.6268 Zhui / Chinese ✓ 80000 80% (2m 1s) 2.2226 Dasios / Portuguese ✗ (Greek) 85000 85% (2m 9s) 1.3690 Walter / Scottish ✗ (German) 90000 90% (2m 16s) 0.5329 Zhang / Chinese ✓ 95000 95% (2m 24s) 3.4474 Skala / Czech ✗ (Polish) 100000 100% (2m 31s) 1.4720 Chi / Korean ✗ (Chinese) 绘制结果 从绘制all_losses的历史损失图可以看出网络的学习： import matplotlib.pyplot as plt import matplotlib.ticker as ticker plt.figure() plt.plot(all_losses) 评价结果 为了了解网络在不同类别上的表现如何，我们将创建一个混淆矩阵，包含姓氏属于的实际语言（行）和网络猜测的是哪种语言（列）。要计算混淆矩阵，将使用evaluate()通过网络来评测一些样本。 # Keep track of correct guesses in a confusion matrix confusion = torch.zeros(n_categories, n_categories) n_confusion = 10000 # Just return an output given a line def evaluate(line_tensor): hidden = rnn.initHidden() for i in range(line_tensor.size()[0]): output, hidden = rnn(line_tensor[i], hidden) return output # Go through a bunch of examples and record which are correctly guessed for i in range(n_confusion): category, line, category_tensor, line_tensor = randomTrainingExample() output = evaluate(line_tensor) guess, guess_i = categoryFromOutput(output) category_i = all_categories.index(category) confusion[category_i][guess_i] += 1 # Normalize by dividing every row by its sum for i in range(n_categories): confusion[i] = confusion[i] / confusion[i].sum() # Set up plot fig = plt.figure() ax = fig.add_subplot(111) cax = ax.matshow(confusion.numpy()) fig.colorbar(cax) # Set up axes ax.set_xticklabels([''] + all_categories, rotation=90) ax.set_yticklabels([''] + all_categories) # Force label at every tick ax.xaxis.set_major_locator(ticker.MultipleLocator(1)) ax.yaxis.set_major_locator(ticker.MultipleLocator(1)) # sphinx_gallery_thumbnail_number = 2 plt.show() 您可以从主轴上挑出一些亮点，以显示错误猜测的语言，例如，中文（朝鲜语）和西班牙语（意大利语）。它似乎与希腊语搭预测得很好，而英语预测的很差（可能是因为与其他语言重叠）。 运行用户输入 def predict(input_line, n_predictions=3): print('\\n> %s' % input_line) with torch.no_grad(): output = evaluate(lineToTensor(input_line)) # Get top N categories topv, topi = output.topk(n_predictions, 1, True) predictions = [] for i in range(n_predictions): value = topv[0][i].item() category_index = topi[0][i].item() print('(%.2f) %s' % (value, all_categories[category_index])) predictions.append([value, all_categories[category_index]]) predict('Dovesky') predict('Jackson') predict('Satoshi') Out: > Dovesky (-0.47) Russian (-1.30) Czech (-2.90) Polish > Jackson (-1.04) Scottish (-1.72) English (-1.74) Russian > Satoshi (-0.32) Japanese (-2.63) Polish (-2.71) Italian 实际PyTorch存储库中的脚本的最终版本将上述代码分成几个文件： data.py（加载文件） model.py（定义RNN） train.py（训练） predict.py（predict()与命令行参数一起运行） server.py（通过bottle.py将预测用作JSON API） 运行train.py训练并保存网络。 用predict.py脚本并加上姓氏运行以查看预测： $ python predict.py Hazaki (-0.42) Japanese (-1.39) Polish (-3.51) Czech 运行server.py，查看http://localhost:5533/Yourname 获得预测的JSON输出。 练习 尝试使用line-> category的其他数据集，例如： 任何单词->语言 名->性别 角色名称->作家 页面标题-> Blog或subreddit 通过更大和/或结构更好的网络获得更好的结果 添加更多线性层 尝试nn.LSTM和nn.GRU图层 将多个这些RNN合并为更高级别的网络 脚本的总运行时间： （2分钟42.458秒） 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"intermediate/char_rnn_generation_tutorial.html":{"url":"intermediate/char_rnn_generation_tutorial.html","title":"NLP From Scratch: 生成名称与字符级RNN","keywords":"","body":"NLP From Scratch：使用char-RNN生成姓氏 > 作者: Sean Robertson 校验: 松鼠 这是我们关于“从零开始的NLP”的三个教程中的第二个。在第一个教程中，我们使用了RNN将姓氏分类为它们的起源语言。这次，我们将从语言中生成姓氏。 python sample.py Russian RUS Rovakov Uantov Shavakov python sample.py German GER Gerren Ereng Rosher python sample.py Spanish SPA Salla Parer Allan python sample.py Chinese CHI Chan Hang Iun 我们之前还在手撸带有一些线性层的小型RNN网络。现在和之前最大的区别在于，我们不再是读取一个姓氏的所有字母来预测是什么类别，而是输入一个类别并同时输出一个字母。这种循环预测出来自语言的字符模型（这也可以用单词或其他高阶结构来完成）通常称为“语言模型”。 建议： 假设你已经至少安装PyTorch，知道Python和理解张量： pytorch安装说明 观看《PyTorch进行深度学习：60分钟速成》来开始学习pytorch 通过实例深入学习PyTorch pytorch为前torch用户的提供的指南 下面这些是了解RNNs以及它们如何工作的相关联接： 回归神经网络展示真实生活中的一系列例子 理解LSTM网络虽然是关于LSTMs的但也对RNNs有很多详细的讲解 我也建议浏览下前面的教程，NLP From Scratch：使用char-RNN对姓氏进行分类 准备数据 Note 从此处下载数据，并将其解压到当前目录。 有关此过程的更多详细信息，请参见上一教程。简而言之，有一堆纯文本文件data/names/[Language].txt，每行都有一个姓氏。我们将行拆分成一个数组，将Unicode转换为ASCII，最后得到一个dictionary{language: [names ...]} from __future__ import unicode_literals, print_function, division from io import open import glob import os import unicodedata import string all_letters = string.ascii_letters + \" .,;'-\" n_letters = len(all_letters) + 1 # Plus EOS marker def findFiles(path): return glob.glob(path) # Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427 def unicodeToAscii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in all_letters ) # Read a file and split into lines def readLines(filename): lines = open(filename, encoding='utf-8').read().strip().split('\\n') return [unicodeToAscii(line) for line in lines] # Build the category_lines dictionary, a list of lines per category category_lines = {} all_categories = [] for filename in findFiles('data/names/*.txt'): category = os.path.splitext(os.path.basename(filename))[0] all_categories.append(category) lines = readLines(filename) category_lines[category] = lines n_categories = len(all_categories) if n_categories == 0: raise RuntimeError('Data not found. Make sure that you downloaded data ' 'from https://download.pytorch.org/tutorial/data.zip and extract it to ' 'the current directory.') print('# categories:', n_categories, all_categories) print(unicodeToAscii(\"O'Néàl\")) 输出： # categories: 18 ['French', 'Czech', 'Dutch', 'Polish', 'Scottish', 'Chinese', 'English', 'Italian', 'Portuguese', 'Japanese', 'German', 'Russian', 'Korean', 'Arabic', 'Greek', 'Vietnamese', 'Spanish', 'Irish'] O'Neal 创建网络 该网络 使用类别张量的额外参数扩展了上一教程的RNN，该参数与其他张量串联在一起。类别张量是一个独热向量，就像字母输入一样。 我们将输出解释为下一个字母的概率。采样时，最有可能的输出字母用作下一个输入字母。 我们添加了第二个线性层o2o（将隐藏和输出结合在一起之后），以使它具有更多的性能可以使用。还有一个drop层，它以给定的概率（此处为0.1）将输入的一部分随机归零，通常用于模糊输入以防止过拟合。在这里，我们在网络末端使用它来故意添加一些混乱并增加采样种类。 import torch import torch.nn as nn class RNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(RNN, self).__init__() self.hidden_size = hidden_size self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size) self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size) self.o2o = nn.Linear(hidden_size + output_size, output_size) self.dropout = nn.Dropout(0.1) self.softmax = nn.LogSoftmax(dim=1) def forward(self, category, input, hidden): input_combined = torch.cat((category, input, hidden), 1) hidden = self.i2h(input_combined) output = self.i2o(input_combined) output_combined = torch.cat((hidden, output), 1) output = self.o2o(output_combined) output = self.dropout(output) output = self.softmax(output) return output, hidden def initHidden(self): return torch.zeros(1, self.hidden_size) 训练 准备训练 先，helper函数获取随机对（类别，行）： import random # Random item from a list def randomChoice(l): return l[random.randint(0, len(l) - 1)] # Get a random category and random line from that category def randomTrainingPair(): category = randomChoice(all_categories) line = randomChoice(category_lines[category]) return category, line 对于每个时间步长（即对于训练单词中的每个字母），网络的输入将为(category, current letter, hidden state) ，输出将为(next letter, next hidden state)。因此，对于每个训练集，我们都需要类别，一组输入字母和一组输出/目标字母。 由于我们正在预测每个时间步中当前字母的下一个字母，因此字母对是该行中连续的字母组,例如:\"ABCD\"我们将创建（“ A”，“ B”），（“ B”，“ C” ），（“ C”，“ D”），（“ D”，“ EOS”）。 类别张量是大小为的独热张量。训练时，我们会随时随地将其馈送到网络中。这是一种设计方式，它可能已作为初始隐藏状态或某些其他策略的一部分包含在内。 # One-hot vector for category def categoryTensor(category): li = all_categories.index(category) tensor = torch.zeros(1, n_categories) tensor[0][li] = 1 return tensor # One-hot matrix of first to last letters (not including EOS) for input def inputTensor(line): tensor = torch.zeros(len(line), 1, n_letters) for li in range(len(line)): letter = line[li] tensor[li][0][all_letters.find(letter)] = 1 return tensor # LongTensor of second letter to end (EOS) for target def targetTensor(line): letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))] letter_indexes.append(n_letters - 1) # EOS return torch.LongTensor(letter_indexes) 为了方便训练，我们将创建一个randomTrainingExample函数以获取随机（类别，行）对并将其转换为所需的（类别，输入，目标）张量。 # Make category, input, and target tensors from a random category, line pair def randomTrainingExample(): category, line = randomTrainingPair() category_tensor = categoryTensor(category) input_line_tensor = inputTensor(line) target_line_tensor = targetTensor(line) return category_tensor, input_line_tensor, target_line_tensor 网络训练 与仅使用最后一个输出的分类相反，我们在每个步骤进行预测，因此在每个步骤都计算损失。 autograd使您可以简单地将每一步的损失相加，然后在末尾调用。 criterion = nn.NLLLoss() learning_rate = 0.0005 def train(category_tensor, input_line_tensor, target_line_tensor): target_line_tensor.unsqueeze_(-1) hidden = rnn.initHidden() rnn.zero_grad() loss = 0 for i in range(input_line_tensor.size(0)): output, hidden = rnn(category_tensor, input_line_tensor[i], hidden) l = criterion(output, target_line_tensor[i]) loss += l loss.backward() for p in rnn.parameters(): p.data.add_(-learning_rate, p.grad.data) return output, loss.item() / input_line_tensor.size(0) 为了跟踪训练需要多长时间，我添加了一个timeSince(timestamp)返回人类可读字符串的函数： import time import math def timeSince(since): now = time.time() s = now - since m = math.floor(s / 60) s -= m * 60 return '%dm %ds' % (m, s) 训练通常会需要来回调用很多次，然后等待几分钟，打印每个print_every的当前时间和损失值，并保存每个样本的平均损失plot_every到all_losses供以后绘图用。 rnn = RNN(n_letters, 128, n_letters) n_iters = 100000 print_every = 5000 plot_every = 500 all_losses = [] total_loss = 0 # Reset every plot_every iters start = time.time() for iter in range(1, n_iters + 1): output, loss = train(*randomTrainingExample()) total_loss += loss if iter % print_every == 0: print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss)) if iter % plot_every == 0: all_losses.append(total_loss / plot_every) total_loss = 0 Out: 0m 17s (5000 5%) 3.5187 0m 35s (10000 10%) 2.5492 0m 53s (15000 15%) 2.2320 1m 11s (20000 20%) 3.2664 1m 29s (25000 25%) 2.2973 1m 47s (30000 30%) 1.1620 2m 5s (35000 35%) 2.8624 2m 23s (40000 40%) 1.8314 2m 41s (45000 45%) 2.3952 2m 58s (50000 50%) 2.7142 3m 16s (55000 55%) 2.4662 3m 34s (60000 60%) 2.9410 3m 53s (65000 65%) 2.5558 4m 11s (70000 70%) 2.2629 4m 29s (75000 75%) 2.3106 4m 47s (80000 80%) 2.2239 5m 5s (85000 85%) 1.4803 5m 23s (90000 90%) 2.9525 5m 42s (95000 95%) 1.9797 6m 0s (100000 100%) 2.3567 绘制损失 绘制all_losses中的历史损失值展示网络学习： import matplotlib.pyplot as plt import matplotlib.ticker as ticker plt.figure() plt.plot(all_losses) 采样网络 我们给网络一个字母，问下一个字母是什么，并将其作为下一个字母输入，并重复直到EOS标记。 创建输入类别，首个字母，空的隐藏状态的张量 创建一个以首字母开头的字符串output_name 最大输出长度， 喂入当前字母到网络 从输出中获取最可能字母，和下一个隐藏层的状态 如果当前输出是EOS标记，就停止循环 如果是一个正常的字母，就添加到output_name中并继续 返回的最终姓氏 Note 相比于给它起一个开始字母，另一种策略是在训练中包括一个“字符串开始”标记，并让网络选择自己的开始字母。 max_length = 20 # Sample from a category and starting letter def sample(category, start_letter='A'): with torch.no_grad(): # no need to track history in sampling category_tensor = categoryTensor(category) input = inputTensor(start_letter) hidden = rnn.initHidden() output_name = start_letter for i in range(max_length): output, hidden = rnn(category_tensor, input[0], hidden) topv, topi = output.topk(1) topi = topi[0][0] if topi == n_letters - 1: break else: letter = all_letters[topi] output_name += letter input = inputTensor(letter) return output_name # Get multiple samples from one category and multiple starting letters def samples(category, start_letters='ABC'): for start_letter in start_letters: print(sample(category, start_letter)) samples('Russian', 'RUS') samples('German', 'GER') samples('Spanish', 'SPA') samples('Chinese', 'CHI') 输出: Rovallov Uanovakov Sanovakov Geller Eringer Raman Salos Para Allan Chan Hang Iun 练习 尝试使用不同的数据集的category -> line，例如： 虚构系列->角色名称 词性->单词 国家->城市 使用“句子开头”标记，以便无需选择开始字母即可进行采样 通过更大和/或结构更好的网络获得更好的结果 尝试nn.LSTM和nn.GRU层 将多个这些RNN合并为更高级别的网络 脚本的总运行时间： （6分钟0.536秒） 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"intermediate/seq2seq_translation_tutorial.html":{"url":"intermediate/seq2seq_translation_tutorial.html","title":"NLP From Scratch: 基于注意力机制的 seq2seq 神经网络翻译","keywords":"","body":"NLP From Scratch: 基于注意力机制的 seq2seq 神经网络翻译 翻译：DrDavidS 翻译：mengfu188 作者 ：Sean Robertson 这是第三篇也是最后一篇“从零开始NLP”教程，我们会在其中编写自己的类与函数来处理数据，从而完成我们的NLP建模任务。我们希望在你完成本篇教程后，你可以紧接着在其后的三篇教程中继续学习 torchtext 是如何帮你完成大量的此类预处理的。 在这个项目中，我们将编写一个把法语翻译成英语的神经网络。 [KEY: > input, = target, il est en train de peindre un tableau . = he is painting a picture . pourquoi ne pas essayer ce vin delicieux ? = why not try that delicious wine ? elle n est pas poete mais romanciere . = she is not a poet but a novelist . vous etes trop maigre . = you re too skinny . … 取得了不同程度的成功 这是通过seq2seq网络来进行实现的，在这个网络中使用两个递归的神经网络（编码器网络和解码器网络）一起工作使得一段序列变成另一段序列。 编码器网络将输入序列变成一个向量，解码器网络将该向量展开为新的序列。 我们将使用注意力机制改进这个模型，它可以让解码器学会集中在输入序列的特定范围中。 推荐阅读： 我假设你至少已经了解Python，安装了PyTorch，并且了解什么是张量： https://pytorch.org/ → PyTorch安装说明 PyTorch 深度学习: 60 分钟极速入门 → 开始使用PyTorch 用例子学习 PyTorch → 更加广泛而深入地了解 PyTorch for Former Torch Users → 如果你之前是Lua Torch用户 这些内容有利于了解seq2seq网络及其工作机制： 用RNN编码器 - 解码器来学习用于统计机器翻译的短语表示 用神经网络进行seq2seq学习 通过共同学习对齐和翻译的神经机器翻译 一种神经会话模型 你还可以找到之前类似于编码器和解码器的教程，如使用字符级别特征的RNN网络生成名字和使用字符级别特征的RNN网络进行名字分类，学习这些概念也很有帮助。 更多内容请阅读以下论文： 用RNN编码器 - 解码器来学习用于统计机器翻译的短语表示 用神经网络进行seq2seq学习 通过共同学习对齐和翻译的神经机器翻译 神经会话模型 需求如下： from __future__ import unicode_literals, print_function, division from io import open import unicodedata import string import re import random import torch import torch.nn as nn from torch import optim import torch.nn.functional as F device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") 加载数据文件 这个项目的数据是一组数以千计的英语到法语的翻译用例。 这个问题在 Open Data Stack Exchange 上 点我打开翻译网址 https://tatoeba.org/ 这个网站的下载地址 https://tatoeba.org/eng/downloads - 更棒的是，有人将这些语言切分成单个文件: https://www.manythings.org/anki/ 由于翻译文件太大而不能放到repo中，请在继续往下阅读前，下载数据到 data/eng-fra.txt。该文件是一个使用制表符（table）分割的翻译列表: I am cold. J'ai froid. 注意 从 这里 下载数据和解压到相关的路径. 与character-level RNN教程中使用的字符编码类似,我们将用语言中的每个单词 作为独热向量,或者除了单个单词之外(在单词的索引处)的大的零向量. 相较于可能 存在于一种语言中仅有十个字符相比,多数都是有大量的字,因此编码向量很大. 然而,我们会欺骗性的做一些数据修剪,保证每种语言只使用几千字. 我们之后需要将每个单词对应唯一的索引作为神经网络的输入和目标.为了追踪这些索引我们使用一个帮助类 Lang 类中有 词 → 索引 (word2index) 和 索引 → 词(index2word) 的字典, 以及每个词word2count 用来替换稀疏词汇。 SOS_token = 0 EOS_token = 1 class Lang: def __init__(self, name): self.name = name self.word2index = {} self.word2count = {} self.index2word = {0: \"SOS\", 1: \"EOS\"} self.n_words = 2 # Count SOS and EOS def addSentence(self, sentence): for word in sentence.split(' '): self.addWord(word) def addWord(self, word): if word not in self.word2index: self.word2index[word] = self.n_words self.word2count[word] = 1 self.index2word[self.n_words] = word self.n_words += 1 else: self.word2count[word] += 1 这些文件全部采用Unicode编码，为了简化起见，我们将Unicode字符转换成ASCII编码，所有内容小写，并修剪大部分标点符号。 # Turn a Unicode string to plain ASCII, thanks to # https://stackoverflow.com/a/518232/2809427 def unicodeToAscii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' ) # Lowercase, trim, and remove non-letter characters def normalizeString(s): s = unicodeToAscii(s.lower().strip()) s = re.sub(r\"([.!?])\", r\" \\1\", s) s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) return s 为了读取数据文件，我们将按行分开，并将每一行分成两对来读取文件。这些文件都是英语 → 其他语言，所以如果我们想从其他语言翻译 → 英语，添加reverse标志来翻转词语对。 def readLangs(lang1, lang2, reverse=False): print(\"Reading lines...\") # Read the file and split into lines lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\ read().strip().split('\\n') # Split every line into pairs and normalize pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines] # Reverse pairs, make Lang instances if reverse: pairs = [list(reversed(p)) for p in pairs] input_lang = Lang(lang2) output_lang = Lang(lang1) else: input_lang = Lang(lang1) output_lang = Lang(lang2) return input_lang, output_lang, pairs 由于有很多例句，而且我们想要快速训练模型，因此我们将数据集修剪为长度相对较短且简单的句子。在这里，最大长度是十个单词（包括结尾标点符号），而且我们会对翻译为\"I am\" 或者 \"He is\" 形式的句子进行过滤（考虑到之前我们清理过撇号 → '）。 MAX_LENGTH = 10 eng_prefixes = ( \"i am \", \"i m \", \"he is\", \"he s \", \"she is\", \"she s \", \"you are\", \"you re \", \"we are\", \"we re \", \"they are\", \"they re \" ) def filterPair(p): return len(p[0].split(' ')) 完整的数据准备过程： 按行读取文本文件，将行拆分成对 规范文本，按长度和内容过滤 从句子中成对列出单词列表 def prepareData(lang1, lang2, reverse=False): input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse) print(\"Read %s sentence pairs\" % len(pairs)) pairs = filterPairs(pairs) print(\"Trimmed to %s sentence pairs\" % len(pairs)) print(\"Counting words...\") for pair in pairs: input_lang.addSentence(pair[0]) output_lang.addSentence(pair[1]) print(\"Counted words:\") print(input_lang.name, input_lang.n_words) print(output_lang.name, output_lang.n_words) return input_lang, output_lang, pairs input_lang, output_lang, pairs = prepareData('eng', 'fra', True) print(random.choice(pairs)) 输出： Reading lines... Read 135842 sentence pairs Trimmed to 10599 sentence pairs Counting words... Counted words: fra 4345 eng 2803 ['ils ne sont pas encore chez eux .', 'they re not home yet .'] Seq2Seq模型 归神经网络（RNN）是一种对序列进行操作并利用自己的输出作为后序输入的网络 序列到序列网络（Sequence to Sequence network）, 也叫做 seq2seq 网络, 又或者是 编码器解码器网络（Encoder Decoder network）, 是一个由两个称为编码器解码器的RNN组成的模型。编码器读取输入序列并输出一个矢量，解码器读取该矢量并产生输出序列。 与每个输入对应一个输出的单个RNN的序列预测不同，seq2seq模型将我们从序列长度和顺序中解放出来，这使得它更适合两种语言的转换。 考虑这句话“Je ne suis pas le chat noir” → “I am not the black cat”。虽然大部分情况下输入输出序列可以对单词进行比较直接的翻译，但是很多时候单词的顺序却略有不同，例如: “chat noir” 和 “black cat”。由于 “ne/pas”结构, 输入的句子中还有另外一个单词.。因此直接从输入词的序列中直接生成正确的翻译是很困难的。 使用seq2seq模型时，编码器会创建一个向量，在理想的情况下，将输入序列的实际语义编码为单个向量 - 序列的一些N维空间中的单个点。 编码器 seq2seq网络的编码器是RNN，它为输入序列中的每个单词输出一些值。 对于每个输入单词，编码器输出一个向量和一个隐状态，并将该隐状态用于下一个输入的单词。 class EncoderRNN(nn.Module): def __init__(self, input_size, hidden_size): super(EncoderRNN, self).__init__() self.hidden_size = hidden_size self.embedding = nn.Embedding(input_size, hidden_size) self.gru = nn.GRU(hidden_size, hidden_size) def forward(self, input, hidden): embedded = self.embedding(input).view(1, 1, -1) output = embedded output, hidden = self.gru(output, hidden) return output, hidden def initHidden(self): return torch.zeros(1, 1, self.hidden_size, device=device) 解码器 解码器是一个接受编码器输出向量并输出一系列单词以创建翻译的RNN。 简单解码器 在最简单的seq2seq解码器中，我们只使用编码器的最后输出。这最后一个输出有时称为上下文向量因为它从整个序列中编码上下文。该上下文向量用作解码器的初始隐藏状态。 在解码的每一步,解码器都被赋予一个输入指令和隐藏状态. 初始输入指令字符串开始的指令,第一个隐藏状态是上下文向量(编码器的最后隐藏状态). class DecoderRNN(nn.Module): def __init__(self, hidden_size, output_size): super(DecoderRNN, self).__init__() self.hidden_size = hidden_size self.embedding = nn.Embedding(output_size, hidden_size) self.gru = nn.GRU(hidden_size, hidden_size) self.out = nn.Linear(hidden_size, output_size) self.softmax = nn.LogSoftmax(dim=1) def forward(self, input, hidden): output = self.embedding(input).view(1, 1, -1) output = F.relu(output) output, hidden = self.gru(output, hidden) output = self.softmax(self.out(output[0])) return output, hidden def initHidden(self): return torch.zeros(1, 1, self.hidden_size, device=device) 我们鼓励你训练和观察这个模型的结果,但为了节省空间,我们将直入主题开始讲解注意力机制。 带有注意力机制的解码器 如果仅在编码器和解码器之间传递上下文向量,则该单个向量承担编码整个句子的负担. 注意力机制允许解码器网络针对解码器自身输出的每一步”聚焦”编码器输出的不同部分. 首先我们计算一组注意力权重. 这些将被乘以编码器输出矢量获得加权的组合. 结果(在代码中为attn_applied) 应该包含关于输入序列的特定部分的信息, 从而帮助解码器选择正确的输出单词. 注意权值的计算是用另一个前馈层attn进行的, 将解码器的输入和隐藏层状态作为输入. 由于训练数据中的输入序列（语句）长短不一,为了实际创建和训练此层, 我们必须选择最大长度的句子(输入长度,用于编码器输出),以适用于此层. 最大长度的句子将使用所有注意力权重,而较短的句子只使用前几个. class AttnDecoderRNN(nn.Module): def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH): super(AttnDecoderRNN, self).__init__() self.hidden_size = hidden_size self.output_size = output_size self.dropout_p = dropout_p self.max_length = max_length self.embedding = nn.Embedding(self.output_size, self.hidden_size) self.attn = nn.Linear(self.hidden_size * 2, self.max_length) self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size) self.dropout = nn.Dropout(self.dropout_p) self.gru = nn.GRU(self.hidden_size, self.hidden_size) self.out = nn.Linear(self.hidden_size, self.output_size) def forward(self, input, hidden, encoder_outputs): embedded = self.embedding(input).view(1, 1, -1) embedded = self.dropout(embedded) attn_weights = F.softmax( self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1) attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0)) output = torch.cat((embedded[0], attn_applied[0]), 1) output = self.attn_combine(output).unsqueeze(0) output = F.relu(output) output, hidden = self.gru(output, hidden) output = F.log_softmax(self.out(output[0]), dim=1) return output, hidden, attn_weights def initHidden(self): return torch.zeros(1, 1, self.hidden_size, device=device) 注意 还有其他通过使用相对位置方法来解决长度限制的注意力机制。 在 基于注意力机制的神经机器翻译的有效途径读一读关于“local attention” 的内容。 训练 准备训练数据 为了训练，对于每一对我们都需要 输入张量(输入句子中的词的索引)和 目标张量(目标语句中的词的索引)。 在创建这些向量时,我们会将EOS标记添加到两个序列中。 def indexesFromSentence(lang, sentence): return [lang.word2index[word] for word in sentence.split(' ')] def tensorFromSentence(lang, sentence): indexes = indexesFromSentence(lang, sentence) indexes.append(EOS_token) return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1) def tensorsFromPair(pair): input_tensor = tensorFromSentence(input_lang, pair[0]) target_tensor = tensorFromSentence(output_lang, pair[1]) return (input_tensor, target_tensor) 训练模型 为了训练我们通过编码器运行输入序列,并跟踪每个输出和最新的隐藏状态. 然后解码器被赋予 标志作为其第一个输入, 并将编码器的最后一个隐藏状态作为其第一个隐藏状态. “Teacher forcing” 是将实际目标输出用作每个下一个输入的概念,而不是将解码器的 猜测用作下一个输入.使用“Teacher forcing” 会使其更快地收敛,但是 当训练好的网络被利用时,它可能表现出不稳定性.. 您可以观察“Teacher forcing”网络的输出，这些输出阅读起来是语法连贯的，但却偏离了正确的翻译 - 直觉上它已经学会表示输出语法，并且一旦老师告诉它前几个单词就可以“提取”意义，但是 它没有正确地学习如何从翻译中创建句子。 由于PyTorch的autograd给我们的自由性，我们可以通过简单的if语句来随意选择使用或者不使用“Teacher forcing”。 调高teacher_forcing_ratio来更好地使用它。 teacher_forcing_ratio = 0.5 def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH): encoder_hidden = encoder.initHidden() encoder_optimizer.zero_grad() decoder_optimizer.zero_grad() input_length = input_tensor.size(0) target_length = target_tensor.size(0) encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device) loss = 0 for ei in range(input_length): encoder_output, encoder_hidden = encoder( input_tensor[ei], encoder_hidden) encoder_outputs[ei] = encoder_output[0, 0] decoder_input = torch.tensor([[SOS_token]], device=device) decoder_hidden = encoder_hidden use_teacher_forcing = True if random.random() 这是一个帮助函数，用于在给定当前时间和进度%的情况下打印经过的时间和估计的剩余时间。 import time import math def asMinutes(s): m = math.floor(s / 60) s -= m * 60 return '%dm %ds' % (m, s) def timeSince(since, percent): now = time.time() s = now - since es = s / (percent) rs = es - s return '%s (- %s)' % (asMinutes(s), asMinutes(rs)) 整个训练过程如下所示: 启动计时器 初始化优化器和准则 创建一组训练队 为进行绘图启动空损失数组 之后我们多次调用train函数，偶尔打印进度 (样本的百分比，到目前为止的时间，狙击的时间) 和平均损失 def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01): start = time.time() plot_losses = [] print_loss_total = 0 # Reset every print_every plot_loss_total = 0 # Reset every plot_every encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate) decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate) training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)] criterion = nn.NLLLoss() for iter in range(1, n_iters + 1): training_pair = training_pairs[iter - 1] input_tensor = training_pair[0] target_tensor = training_pair[1] loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) print_loss_total += loss plot_loss_total += loss if iter % print_every == 0: print_loss_avg = print_loss_total / print_every print_loss_total = 0 print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg)) if iter % plot_every == 0: plot_loss_avg = plot_loss_total / plot_every plot_losses.append(plot_loss_avg) plot_loss_total = 0 showPlot(plot_losses) 绘制结果 使用matplotlib完成绘图，使用plot_losses保存训练时的数组。 import matplotlib.pyplot as plt plt.switch_backend('agg') import matplotlib.ticker as ticker import numpy as np def showPlot(points): plt.figure() fig, ax = plt.subplots() # 该定时器用于定时记录时间 loc = ticker.MultipleLocator(base=0.2) ax.yaxis.set_major_locator(loc) plt.plot(points) 评估 评估与训练大部分相同,但没有目标,因此我们只是将解码器的每一步预测反馈给它自身. 每当它预测到一个单词时,我们就会将它添加到输出字符串中,并且如果它预测到我们在那里停止的EOS指令. 我们还存储解码器的注意力输出以供稍后显示. def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH): with torch.no_grad(): input_tensor = tensorFromSentence(input_lang, sentence) input_length = input_tensor.size()[0] encoder_hidden = encoder.initHidden() encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device) for ei in range(input_length): encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden) encoder_outputs[ei] += encoder_output[0, 0] decoder_input = torch.tensor([[SOS_token]], device=device) # SOS decoder_hidden = encoder_hidden decoded_words = [] decoder_attentions = torch.zeros(max_length, max_length) for di in range(max_length): decoder_output, decoder_hidden, decoder_attention = decoder( decoder_input, decoder_hidden, encoder_outputs) decoder_attentions[di] = decoder_attention.data topv, topi = decoder_output.data.topk(1) if topi.item() == EOS_token: decoded_words.append('') break else: decoded_words.append(output_lang.index2word[topi.item()]) decoder_input = topi.squeeze().detach() return decoded_words, decoder_attentions[:di + 1] 我们可以从训练集中对随机句子进行评估，并打印出输入、目标和输出，从而做出一些主观的质量判断： def evaluateRandomly(encoder, decoder, n=10): for i in range(n): pair = random.choice(pairs) print('>', pair[0]) print('=', pair[1]) output_words, attentions = evaluate(encoder, decoder, pair[0]) output_sentence = ' '.join(output_words) print(' 训练和评估 有了所有这些帮助函数(它看起来像是额外的工作，但它使运行多个实验更容易)，我们实际上可以初始化一个网络并开始训练。 请记住输入句子被已经严格过滤过了。对于这个小数据集,我们可以使用包含256个隐藏节点和单个GRU层的相对较小的网络.在 MacBook CPU 上训练约40分钟后,我们会得到一些合理的结果. 注意 如果你运行这个notebook，你可以训练,中断内核，评估，并在以后继续训练。 注释编码器和解码器初始化的行并再次运行 trainIters 。 hidden_size = 256 encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device) attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device) trainIters(encoder1, attn_decoder1, 75000, print_every=5000) 输出: 1m 47s (- 25m 8s) (5000 6%) 2.8641 3m 30s (- 22m 45s) (10000 13%) 2.2666 5m 15s (- 21m 1s) (15000 20%) 1.9537 7m 0s (- 19m 17s) (20000 26%) 1.7170 8m 46s (- 17m 32s) (25000 33%) 1.5182 10m 31s (- 15m 46s) (30000 40%) 1.3280 12m 15s (- 14m 0s) (35000 46%) 1.2137 14m 1s (- 12m 16s) (40000 53%) 1.0843 15m 48s (- 10m 32s) (45000 60%) 0.9847 17m 34s (- 8m 47s) (50000 66%) 0.8515 19m 20s (- 7m 2s) (55000 73%) 0.7940 21m 6s (- 5m 16s) (60000 80%) 0.7189 22m 53s (- 3m 31s) (65000 86%) 0.6490 24m 41s (- 1m 45s) (70000 93%) 0.5954 26m 26s (- 0m 0s) (75000 100%) 0.5257 evaluateRandomly(encoder1, attn_decoder1) 输出: > nous sommes contents que tu sois la . = we re glad you re here . > il est dependant a l heroine . = he is a heroin addict . > nous sommes les meilleurs . = we are the best . > tu es puissant . = you re powerful . > j ai peur des chauves souris . = i m afraid of bats . > tu es enseignant n est ce pas ? = you re a teacher right ? > je suis pret a tout faire pour toi . = i am ready to do anything for you . > c est desormais un homme . = he s a man now . > elle est une mere tres avisee . = she s a very wise mother . > je suis completement vanne . = i m completely exhausted . 可视化注意力 注意力机制的一个有用的特性是其高度可解释的输出。由于它用于加权输入序列的特定编码器输出，因此我们可以想象，在每个时间步骤中，查看网络最集中的位置。 你可以简单地运行 plt.matshow(attentions) 来查看显示为矩阵的注意力输出，列为输入步骤，行位输出步骤。 output_words, attentions = evaluate( encoder1, attn_decoder1, \"je suis trop froid .\") plt.matshow(attentions.numpy()) 为了获得更好的观看体验,我们将额外添加轴和标签: def showAttention(input_sentence, output_words, attentions): # Set up figure with colorbar fig = plt.figure() ax = fig.add_subplot(111) cax = ax.matshow(attentions.numpy(), cmap='bone') fig.colorbar(cax) # Set up axes ax.set_xticklabels([''] + input_sentence.split(' ') + [''], rotation=90) ax.set_yticklabels([''] + output_words) # Show label at every tick ax.xaxis.set_major_locator(ticker.MultipleLocator(1)) ax.yaxis.set_major_locator(ticker.MultipleLocator(1)) plt.show() def evaluateAndShowAttention(input_sentence): output_words, attentions = evaluate( encoder1, attn_decoder1, input_sentence) print('input =', input_sentence) print('output =', ' '.join(output_words)) showAttention(input_sentence, output_words, attentions) evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\") evaluateAndShowAttention(\"elle est trop petit .\") evaluateAndShowAttention(\"je ne crains pas de mourir .\") evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\") 输出: input = elle a cinq ans de moins que moi . output = she s five years younger than me . input = elle est trop petit . output = she s too slow . input = je ne crains pas de mourir . output = i m not scared to die . input = c est un jeune directeur plein de talent . output = he s a talented young player . 练习题 尝试使用不同的数据集 另一种语言对（language pair） 人 → 机器 (例如 IOT 命令) 聊天 → 响应 问题 → 回答 将嵌入替换为预先训练过的单词嵌入，例如word2vec或者GloVe 尝试用更多的层次，更多的隐藏单位，更多的句子。比较训练时间和结果。 如果使用一个翻译文件，其中成对有两个相同的短语(I am test \\t I am test)，您可以将其用作自动编码器。试试这个： 训练为自动编码器 只保存编码器网络 训练一种新的翻译解码器 脚本的总运行时间： （ 27 minutes 13.758 seconds） 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/text_sentiment_ngrams_tutorial.html":{"url":"beginner/text_sentiment_ngrams_tutorial.html","title":"文本分类与TorchText","keywords":"","body":"文本分类与TorchText 本教程将说明如何在torchtext中使用文本分类数据集，其中包括： - AG_NEWS, - SogouNews, - DBpedia, - YelpReviewPolarity, - YelpReviewFull, - YahooAnswers, - AmazonReviewPolarity, - AmazonReviewFull 下面的例子显示如何使用TextClassification中的数据集训练有监督的文本分类模型。 以ngrams的方式加载数据 使用ngrams可以捕捉句子中有关单词顺序的一些信息。实际上，应用二元语法或三元语法作为单词组可以比仅一个单词组成句子提供更多的好处。一个例子： \"load data with ngrams\" Bi-grams results: \"load data\", \"data with\", \"with ngrams\" Tri-grams results: \"load data with\", \"data with ngrams\" TextClassification数据集支持n元语法方法。如果NGRAMS 设置为2，数据集中的示例文本将是单个单词加上二元语法字符串的列表。 import torch import torchtext from torchtext.datasets import text_classification NGRAMS = 2 import os if not os.path.isdir('./.data'): os.mkdir('./.data') train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS']( root='./.data', ngrams=NGRAMS, vocab=None) BATCH_SIZE = 16 device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") 定义模型 该模型是由 EmbeddingBag层和线性层（参见下图）组成的。 nn.EmbeddingBag计算“袋”中embeddings的平均值。此处的文本具有不同的长度。nn.EmbeddingBag不需要填充，因为文本长度会以偏移量保存。 此外，由于nn.EmbeddingBag会动态累积嵌入中的平均值，因此nn.EmbeddingBag可以提高性能和存储效率，以处理张量序列。 import torch.nn as nn import torch.nn.functional as F class TextSentiment(nn.Module): def __init__(self, vocab_size, embed_dim, num_class): super().__init__() self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True) self.fc = nn.Linear(embed_dim, num_class) self.init_weights() def init_weights(self): initrange = 0.5 self.embedding.weight.data.uniform_(-initrange, initrange) self.fc.weight.data.uniform_(-initrange, initrange) self.fc.bias.data.zero_() def forward(self, text, offsets): embedded = self.embedding(text, offsets) return self.fc(embedded) 初始化一个实例 AG_NEWS数据集有四个标签，也就是一共有四类。分别如下： 1 : World 2 : Sports 3 : Business 4 : Sci/Tec VOCAB_SIZE等于词汇表的长度（包括单个单词和ngrams）。NUN_CLASS等于标签的数量，在AG_NEWS情况下为4。 VOCAB_SIZE = len(train_dataset.get_vocab()) EMBED_DIM = 32 NUN_CLASS = len(train_dataset.get_labels()) model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device) 生成批数据的方法 由于文本条目的长度不同，因此使用自定义函数generate_batch（）被用于生成批量数据和偏移量。该函数在torch.utils.data.DataLoader中传递给collate_fn在`。collate_fn的输入是batch_size大小的张量列表，collat​​e_fn函数把它们打包成一个小规模的批处理（mini-batch）。请注意将collate_fn`在程序顶层声明。 这样可以确保该功能在每个程序中均可用。 原始数据批处理输入中的文本条目打包到一个列表中，并作为单个张量串联在一起，作为nn.EmbeddingBag的输入。 偏移量是分解符的张量，表示文本张量中各个序列的起始索引。 Label是一个保存单个文本条目的标签张量。 def generate_batch(batch): label = torch.tensor([entry[0] for entry in batch]) text = [entry[1] for entry in batch] offsets = [0] + [len(entry) for entry in text] # torch.Tensor.cumsum returns the cumulative sum # of elements in the dimension dim. # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0) offsets = torch.tensor(offsets[:-1]).cumsum(dim=0) text = torch.cat(text) return text, offsets, label 定义功能训练模型和评估结果。 建议PyTorch用户使用torch.utils.data.DataLoader，它使数据并行加载变得容易（此处有一个教程）。 我们在这里使用DataLoader加载AG_NEWS数据集并将其发送到模型以进行训练/验证。 from torch.utils.data import DataLoader def train_func(sub_train_): # Train the model train_loss = 0 train_acc = 0 data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch) for i, (text, offsets, cls) in enumerate(data): optimizer.zero_grad() text, offsets, cls = text.to(device), offsets.to(device), cls.to(device) output = model(text, offsets) loss = criterion(output, cls) train_loss += loss.item() loss.backward() optimizer.step() train_acc += (output.argmax(1) == cls).sum().item() # Adjust the learning rate scheduler.step() return train_loss / len(sub_train_), train_acc / len(sub_train_) def test(data_): loss = 0 acc = 0 data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch) for text, offsets, cls in data: text, offsets, cls = text.to(device), offsets.to(device), cls.to(device) with torch.no_grad(): output = model(text, offsets) loss = criterion(output, cls) loss += loss.item() acc += (output.argmax(1) == cls).sum().item() return loss / len(data_), acc / len(data_) 拆分数据集和运行模型 由于原始AG_NEWS数据没有验证集，我们用的0.95（训练集）和0.05（验证集）的比例分割训练数据集。在这里，我们使用PyTorch核心库中的torch.utils.data.dataset.random_split函数。 CrossEntropyLoss将nn.LogSoftmax（）和nn.NLLLoss（）合并到一个类中。在训练多分类模型时很有用。 SGD将随机梯度下降方法用作优化程序。 初始学习率设置为4.0。 StepLR用于每轮调整学习率。 import time from torch.utils.data.dataset import random_split N_EPOCHS = 5 min_valid_loss = float('inf') criterion = torch.nn.CrossEntropyLoss().to(device) optimizer = torch.optim.SGD(model.parameters(), lr=4.0) scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9) train_len = int(len(train_dataset) * 0.95) sub_train_, sub_valid_ = \\ random_split(train_dataset, [train_len, len(train_dataset) - train_len]) for epoch in range(N_EPOCHS): start_time = time.time() train_loss, train_acc = train_func(sub_train_) valid_loss, valid_acc = test(sub_valid_) secs = int(time.time() - start_time) mins = secs / 60 secs = secs % 60 print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs)) print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)') print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)') 输出： Epoch: 1 | time in 0 minutes, 8 seconds Loss: 0.0261(train) | Acc: 84.8%(train) Loss: 0.0000(valid) | Acc: 90.4%(valid) Epoch: 2 | time in 0 minutes, 8 seconds Loss: 0.0120(train) | Acc: 93.5%(train) Loss: 0.0000(valid) | Acc: 91.2%(valid) Epoch: 3 | time in 0 minutes, 8 seconds Loss: 0.0070(train) | Acc: 96.4%(train) Loss: 0.0000(valid) | Acc: 90.8%(valid) Epoch: 4 | time in 0 minutes, 8 seconds Loss: 0.0039(train) | Acc: 98.1%(train) Loss: 0.0001(valid) | Acc: 91.0%(valid) Epoch: 5 | time in 0 minutes, 8 seconds Loss: 0.0023(train) | Acc: 99.0%(train) Loss: 0.0001(valid) | Acc: 90.9%(valid) 运行在GPU上的相关信息： Epoch：1 |时间为0分钟，11秒 ​​ Loss: 0.0263(train) | Acc: 84.5%(train) ​ Loss: 0.0001(valid) | Acc: 89.0%(valid) Epoch：2 |时间0分钟，10秒 ​​ Loss: 0.0119(train) | Acc: 93.6%(train) ​ Loss: 0.0000(valid) | Acc: 89.6%(valid) Epoch：3 |时间0分钟，9秒 ​​ Loss: 0.0069(train) | Acc: 96.4%(train) ​ Loss: 0.0000(valid) | Acc: 90.5%(valid) Epoch：4 |时间为0分钟，11秒 ​​ Loss: 0.0038(train) | Acc: 98.2%(train) ​ Loss: 0.0000(valid) | Acc: 90.4%(valid) Epoch：5 |时间为0分钟，11秒 ​​ Loss: 0.0022(train) | Acc: 99.0%(train) ​ Loss: 0.0000(valid) | Acc: 91.0%(valid) 使用测试数据集评估模型 print('Checking the results of test dataset...') test_loss, test_acc = test(test_dataset) print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)') Out: Checking the results of test dataset... Loss: 0.0002(test) | Acc: 89.3%(test) 检查测试数据集的结果... Loss: 0.0237(test) | Acc: 90.5%(test) 在随机新闻上测试 使用到目前为止最好的模型并测试高尔夫新闻。标签信息在这里可用。 import re from torchtext.data.utils import ngrams_iterator from torchtext.data.utils import get_tokenizer ag_news_label = {1 : \"World\", 2 : \"Sports\", 3 : \"Business\", 4 : \"Sci/Tec\"} def predict(text, model, vocab, ngrams): tokenizer = get_tokenizer(\"basic_english\") with torch.no_grad(): text = torch.tensor([vocab[token] for token in ngrams_iterator(tokenizer(text), ngrams)]) output = model(text, torch.tensor([0])) return output.argmax(1).item() + 1 ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\ enduring the season’s worst weather conditions on Sunday at The \\ Open on his way to a closing 75 at Royal Portrush, which \\ considering the wind and the rain was a respectable showing. \\ Thursday’s first round at the WGC-FedEx St. Jude Invitational \\ was another story. With temperatures in the mid-80s and hardly any \\ wind, the Spaniard was 13 strokes better in a flawless round. \\ Thanks to his best putting performance on the PGA Tour, Rahm \\ finished with an 8-under 62 for a three-stroke lead, which \\ was even more impressive considering he’d never played the \\ front nine at TPC Southwind.\" vocab = train_dataset.get_vocab() model = model.to(\"cpu\") print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model, vocab, 2)]) Out: This is a Sports news 这是一个体育新闻 你可以在此处找到对应的代码示例。 脚本的总运行时间： （1分钟26.424秒） 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/torchtext_translation_tutorial.html":{"url":"beginner/torchtext_translation_tutorial.html","title":"语言翻译与TorchText","keywords":"","body":"基于TorchText的语言翻译 本教程介绍如何使用torchtext的几个类来预处理英德数据集，该数据集可以用来训练seq2seq模型，既而能自动把德语句子翻译成英语。 本文基于PyTorch社区成员Ben Trevett的教程，并得到了他本人的许可。 阅读完本教程，你将能够： 使用以下torchtext的类将句子预处理为NLP建模的常用格式：： TranslationDataset Field BucketIterator Field和 TranslationDataset torchtext具有创建数据集的功能，可以轻松对其迭代以构建机器翻译模型。一个关键的类是Filed，它指定每个句子的预处理方法，另一个类是TranslationDataset ; torchtext内置了几个翻译数据集；在本教程中，我们将使用 Multi30k dataset数据集，其中包含约30000个英德句对（平均长度约13个字）。 注：本教程中的tokenization 需要使用 Spacy 。Spacy包可以帮助我们对英语以外的语言tokenization。torchtext提供了basic_english的tokenizer ，但是对于其他语言，使用Spacy对我们而言是最好的选择。 为了运行该教程，首先要使用pip或conda安装Spacy。接下来，下载英德原始数据： python -m spacy download en python -m spacy download de 安装Spacy后，以下代码将根据Field中定义的tokenizer 处理TranslationDataset中的每个句子。 from torchtext.datasets import Multi30k from torchtext.data import Field, BucketIterator SRC = Field(tokenize = \"spacy\", tokenizer_language=\"de\", init_token = '', eos_token = '', lower = True) TRG = Field(tokenize = \"spacy\", tokenizer_language=\"en\", init_token = '', eos_token = '', lower = True) train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), fields = (SRC, TRG)) 输出： downloading training.tar.gz downloading validation.tar.gz downloading mmt_task1_test2016.tar.gz 现在，我们已经定义好了train_data，torchtext的Field有一个非常有用的功能 ：我们可以使用build_vocab方法创建每个语言的词汇表。 SRC.build_vocab(train_data, min_freq = 2) TRG.build_vocab(train_data, min_freq = 2) 一旦这些代码行被运行，SRC.vocab.stoi将成为一个tokens作为key，索引作为value的词典；对应的， SRC.vocab.itos是一个交换了key和value内容相同的字典。在本教程中我们不会广泛使用此功能，但是你可能在遇到其他NLP任务有用。 BucketIterator 我们使用最后一个torchtext的特性是BucketIterator， 它以TranslationDataset作为第一个参数，所以易于使用。如文档所说：定义一个迭代器，该迭代器将相似长度的数据放在一起。产生每个新bacth时，最大程度地减少所需的填充量。 import torch device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') BATCH_SIZE = 128 train_iterator, valid_iterator, test_iterator = BucketIterator.splits( (train_data, valid_data, test_data), batch_size = BATCH_SIZE, device = device) 可以像DataLoader一样调用这些迭代器。 在下面的训练和评估函数中，它们可以简单地通过以下方式调用： for i, batch in enumerate(iterator): 每个batch于是具有SRC和TRG属性： src = batch.src trg = batch.trg 定义我们的nn.Module和Optimizer 解决了数据集的问题并为之定义好迭代器，我们剩下的任务就是定义模型和优化器完成训练过程。 具体来说，我们的模型遵循此处描述的结构。 注意：我们选择这种模型并不是因为它是目前最优的，而是因为它是机器翻译的标准模型。众所周知，目前机器翻译的最优模型是Transformers。 import random from typing import Tuple import torch.nn as nn import torch.optim as optim import torch.nn.functional as F from torch import Tensor class Encoder(nn.Module): def __init__(self, input_dim: int, emb_dim: int, enc_hid_dim: int, dec_hid_dim: int, dropout: float): super().__init__() self.input_dim = input_dim self.emb_dim = emb_dim self.enc_hid_dim = enc_hid_dim self.dec_hid_dim = dec_hid_dim self.dropout = dropout self.embedding = nn.Embedding(input_dim, emb_dim) self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True) self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim) self.dropout = nn.Dropout(dropout) def forward(self, src: Tensor) -> Tuple[Tensor]: embedded = self.dropout(self.embedding(src)) outputs, hidden = self.rnn(embedded) hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))) return outputs, hidden class Attention(nn.Module): def __init__(self, enc_hid_dim: int, dec_hid_dim: int, attn_dim: int): super().__init__() self.enc_hid_dim = enc_hid_dim self.dec_hid_dim = dec_hid_dim self.attn_in = (enc_hid_dim * 2) + dec_hid_dim self.attn = nn.Linear(self.attn_in, attn_dim) def forward(self, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tensor: src_len = encoder_outputs.shape[0] repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1) encoder_outputs = encoder_outputs.permute(1, 0, 2) energy = torch.tanh(self.attn(torch.cat(( repeated_decoder_hidden, encoder_outputs), dim = 2))) attention = torch.sum(energy, dim=2) return F.softmax(attention, dim=1) class Decoder(nn.Module): def __init__(self, output_dim: int, emb_dim: int, enc_hid_dim: int, dec_hid_dim: int, dropout: int, attention: nn.Module): super().__init__() self.emb_dim = emb_dim self.enc_hid_dim = enc_hid_dim self.dec_hid_dim = dec_hid_dim self.output_dim = output_dim self.dropout = dropout self.attention = attention self.embedding = nn.Embedding(output_dim, emb_dim) self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim) self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim) self.dropout = nn.Dropout(dropout) def _weighted_encoder_rep(self, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tensor: a = self.attention(decoder_hidden, encoder_outputs) a = a.unsqueeze(1) encoder_outputs = encoder_outputs.permute(1, 0, 2) weighted_encoder_rep = torch.bmm(a, encoder_outputs) weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2) return weighted_encoder_rep def forward(self, input: Tensor, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tuple[Tensor]: input = input.unsqueeze(0) embedded = self.dropout(self.embedding(input)) weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden, encoder_outputs) rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2) output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0)) embedded = embedded.squeeze(0) output = output.squeeze(0) weighted_encoder_rep = weighted_encoder_rep.squeeze(0) output = self.out(torch.cat((output, weighted_encoder_rep, embedded), dim = 1)) return output, decoder_hidden.squeeze(0) class Seq2Seq(nn.Module): def __init__(self, encoder: nn.Module, decoder: nn.Module, device: torch.device): super().__init__() self.encoder = encoder self.decoder = decoder self.device = device def forward(self, src: Tensor, trg: Tensor, teacher_forcing_ratio: float = 0.5) -> Tensor: batch_size = src.shape[1] max_len = trg.shape[0] trg_vocab_size = self.decoder.output_dim outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device) encoder_outputs, hidden = self.encoder(src) # first input to the decoder is the token output = trg[0,:] for t in range(1, max_len): output, hidden = self.decoder(output, hidden, encoder_outputs) outputs[t] = output teacher_force = random.random() 输出： The model has 1,856,685 trainable parameters 注：当计算模型分数尤其是翻译模型时，我们需要设置nn.CrossEntropyLoss 忽略padding。 PAD_IDX = TRG.vocab.stoi[''] criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX) 最后，我们可以训练和评价模型： import math import time def train(model: nn.Module, iterator: BucketIterator, optimizer: optim.Optimizer, criterion: nn.Module, clip: float): model.train() epoch_loss = 0 for _, batch in enumerate(iterator): src = batch.src trg = batch.trg optimizer.zero_grad() output = model(src, trg) output = output[1:].view(-1, output.shape[-1]) trg = trg[1:].view(-1) loss = criterion(output, trg) loss.backward() torch.nn.utils.clip_grad_norm_(model.parameters(), clip) optimizer.step() epoch_loss += loss.item() return epoch_loss / len(iterator) def evaluate(model: nn.Module, iterator: BucketIterator, criterion: nn.Module): model.eval() epoch_loss = 0 with torch.no_grad(): for _, batch in enumerate(iterator): src = batch.src trg = batch.trg output = model(src, trg, 0) #turn off teacher forcing output = output[1:].view(-1, output.shape[-1]) trg = trg[1:].view(-1) loss = criterion(output, trg) epoch_loss += loss.item() return epoch_loss / len(iterator) def epoch_time(start_time: int, end_time: int): elapsed_time = end_time - start_time elapsed_mins = int(elapsed_time / 60) elapsed_secs = int(elapsed_time - (elapsed_mins * 60)) return elapsed_mins, elapsed_secs N_EPOCHS = 10 CLIP = 1 best_valid_loss = float('inf') for epoch in range(N_EPOCHS): start_time = time.time() train_loss = train(model, train_iterator, optimizer, criterion, CLIP) valid_loss = evaluate(model, valid_iterator, criterion) end_time = time.time() epoch_mins, epoch_secs = epoch_time(start_time, end_time) print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s') print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}') print(f'\\t Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f}') test_loss = evaluate(model, test_iterator, criterion) print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |') 输出： Epoch: 01 | Time: 0m 36s Train Loss: 5.686 | Train PPL: 294.579 Val. Loss: 5.250 | Val. PPL: 190.638 Epoch: 02 | Time: 0m 37s Train Loss: 5.019 | Train PPL: 151.260 Val. Loss: 5.155 | Val. PPL: 173.274 Epoch: 03 | Time: 0m 37s Train Loss: 4.757 | Train PPL: 116.453 Val. Loss: 4.976 | Val. PPL: 144.824 Epoch: 04 | Time: 0m 35s Train Loss: 4.574 | Train PPL: 96.914 Val. Loss: 4.835 | Val. PPL: 125.834 Epoch: 05 | Time: 0m 35s Train Loss: 4.421 | Train PPL: 83.185 Val. Loss: 4.783 | Val. PPL: 119.414 Epoch: 06 | Time: 0m 38s Train Loss: 4.321 | Train PPL: 75.233 Val. Loss: 4.802 | Val. PPL: 121.734 Epoch: 07 | Time: 0m 38s Train Loss: 4.233 | Train PPL: 68.957 Val. Loss: 4.675 | Val. PPL: 107.180 Epoch: 08 | Time: 0m 35s Train Loss: 4.108 | Train PPL: 60.838 Val. Loss: 4.622 | Val. PPL: 101.693 Epoch: 09 | Time: 0m 34s Train Loss: 4.020 | Train PPL: 55.680 Val. Loss: 4.530 | Val. PPL: 92.785 Epoch: 10 | Time: 0m 34s Train Loss: 3.919 | Train PPL: 50.367 Val. Loss: 4.448 | Val. PPL: 85.441 | Test Loss: 4.464 | Test PPL: 86.801 | 接下来的步骤 看看Ben Trevett使用torchtext教程的其余部分 请继续关注使用其他torchtext功能以及nn.Transformer语言建模预测下一个单词的教程！ 脚本的总运行时间： （6分钟27.732秒） 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/transformer_tutorial.html":{"url":"beginner/transformer_tutorial.html","title":"序列到序列与nn.Transformer和TorchText建模","keywords":"","body":"序列对序列建模nn.Transformer和TorchText 本教程将会使用 nn.Transformer 模块训练一个序列到序列模型。 PyTorch 1.2 版本依据论文 Attention is All You Need 发布了标准的 transformer 模型。Transformer 模型已被证明在解决序列到序列问题时效果优异。 nn.Transformer 模块通过注意力机制( nn.MultiheadAttention )来取得输入与输出之间的全局相关性。nn.Transformer 模块现已高度模块化，可以直接用于构建其他模型(如 nn.TransformerEncoder)。 定义模型 在本教程中，我们训练 nn.TransformerEncoder 用于构建语言模型。语言模型的目标是对给定字/词序列打分，判断该字/词序列出现在文本中的概率。字符序列首先会被传进 embedding 层转化为向量，然后被传入位置编码层 （详见下段）。 nn.TransformerEncoder 由多个编码层nn.TransformerEncoderLayer组成。对输入序列的每一维需要施加一个自注意力权重影响。nn.TransformerEncoder 的自注意力权重只影响序列中靠前的数据，不修改之后位置的数据。在本任务中，nn.TransformerEncoder 的输出将会被送至最终的线性层，该层为一个 log-Softmax 层。 import math import torch import torch.nn as nn import torch.nn.functional as F class TransformerModel(nn.Module): def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5): super(TransformerModel, self).__init__() from torch.nn import TransformerEncoder, TransformerEncoderLayer self.model_type = 'Transformer' self.src_mask = None self.pos_encoder = PositionalEncoding(ninp, dropout) encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout) self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers) self.encoder = nn.Embedding(ntoken, ninp) self.ninp = ninp self.decoder = nn.Linear(ninp, ntoken) self.init_weights() def _generate_square_subsequent_mask(self, sz): mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1) mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)) return mask def init_weights(self): initrange = 0.1 self.encoder.weight.data.uniform_(-initrange, initrange) self.decoder.bias.data.zero_() self.decoder.weight.data.uniform_(-initrange, initrange) def forward(self, src): if self.src_mask is None or self.src_mask.size(0) != len(src): device = src.device mask = self._generate_square_subsequent_mask(len(src)).to(device) self.src_mask = mask src = self.encoder(src) * math.sqrt(self.ninp) src = self.pos_encoder(src) output = self.transformer_encoder(src, self.src_mask) output = self.decoder(output) return F.log_softmax(output, dim=-1) PositionalEncoding 模块将字/词在序列中的绝对位置或相对位置信息编码。 位置编码与嵌入层具有相同的维度，这样位置信息向量和嵌入向量可以直接相加。 这里，我们使用 sin 和 cos 函数在不同位置的值来作为位置编码的值。具体计算公式见下方代码。 class PositionalEncoding(nn.Module): def __init__(self, d_model, dropout=0.1, max_len=5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0).transpose(0, 1) self.register_buffer('pe', pe) def forward(self, x): x = x + self.pe[:x.size(0), :] return self.dropout(x) 加载和整合数据 训练过程中使用的数据机是从 torchtext 中得到的wikitext的-2数据集。词典对象基于训练数据集进行构建。batchify（） 函数把数据集中的数据排到多个列中，在划分成多个大小为 batch_size 的集合后，剩下的少于 batch_size 个数据会被丢弃。例如，对于字母序列（长度为26, batch_size 为4），将按照以下方法划分： \\[\\begin{split}\\begin{bmatrix} \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z} \\end{bmatrix} \\Rightarrow \\begin{bmatrix} \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} & \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} & \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} & \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix} \\end{bmatrix}\\end{split}\\] 对于我们的模型来说，只学习同一列中的数据的关系，不同的列各自独立。即我们的模型无法学习到 G 和 F 之间的联系，这样可以增加模型的并行度，增加学习效率。 import torchtext from torchtext.data.utils import get_tokenizer TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"), init_token='', eos_token='', lower=True) train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT) TEXT.build_vocab(train_txt) device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") def batchify(data, bsz): data = TEXT.numericalize([data.examples[0].text]) # Divide the dataset into bsz parts. nbatch = data.size(0) // bsz # Trim off any extra elements that wouldn't cleanly fit (remainders). data = data.narrow(0, 0, nbatch * bsz) # Evenly divide the data across the bsz batches. data = data.view(bsz, -1).t().contiguous() return data.to(device) batch_size = 20 eval_batch_size = 10 train_data = batchify(train_txt, batch_size) val_data = batchify(val_txt, eval_batch_size) test_data = batchify(test_txt, eval_batch_size) 输出： downloading wikitext-2-v1.zip extracting 生成训练数据(输入和目标输出)的函数 get_batch() 函数生成用于 transformer 模型的输入和目标序列。它把源数据细分为长度为 bptt 的块。对于语言模型，需要当前词的下一个词作为目标词。例如当 bptt 为2， i =0 时，该函数会产生以下数据： 张量的第0维是不同的块，块的大小与 Transformer 中的编码层大小一致。\b张量的第1维大小为 batch 大小。 bptt = 35 def get_batch(source, i): seq_len = min(bptt, len(source) - 1 - i) data = source[i:i+seq_len] target = source[i+1:i+1+seq_len].view(-1) return data, target 初始化模型 模型的超参数如下，词典大小为 vocab 数组的长度。 ntokens = len(TEXT.vocab.stoi) # the size of vocabulary emsize = 200 # embedding dimension nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder nhead = 2 # the number of heads in the multiheadattention models dropout = 0.2 # the dropout value model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device) 运行模型 模型使用交叉墒( CrossEntropyLoss )作为损失函数，使用随机梯度下降( SGD )方法更新参数。初始学习率设置为5.0。 StepLR 用于调节学习速率。在训练过程中，使用nn.utils.clipgrad_norm 函数限制梯度大小以防梯度爆炸。 criterion = nn.CrossEntropyLoss() lr = 5.0 # learning rate optimizer = torch.optim.SGD(model.parameters(), lr=lr) scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95) import time def train(): model.train() # Turn on the train mode total_loss = 0. start_time = time.time() ntokens = len(TEXT.vocab.stoi) for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)): data, targets = get_batch(train_data, i) optimizer.zero_grad() output = model(data) loss = criterion(output.view(-1, ntokens), targets) loss.backward() torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) optimizer.step() total_loss += loss.item() log_interval = 200 if batch % log_interval == 0 and batch > 0: cur_loss = total_loss / log_interval elapsed = time.time() - start_time print('| epoch {:3d} | {:5d}/{:5d} batches | ' 'lr {:02.2f} | ms/batch {:5.2f} | ' 'loss {:5.2f} | ppl {:8.2f}'.format( epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0], elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss))) total_loss = 0 start_time = time.time() def evaluate(eval_model, data_source): eval_model.eval() # Turn on the evaluation mode total_loss = 0. ntokens = len(TEXT.vocab.stoi) with torch.no_grad(): for i in range(0, data_source.size(0) - 1, bptt): data, targets = get_batch(data_source, i) output = eval_model(data) output_flat = output.view(-1, ntokens) total_loss += len(data) * criterion(output_flat, targets).item() return total_loss / (len(data_source) - 1) 在每个 epoch 结束时，若验证集的损失函数为最低则会更新一次学习率。 best_val_loss = float(\"inf\") epochs = 3 # The number of epochs best_model = None for epoch in range(1, epochs + 1): epoch_start_time = time.time() train() val_loss = evaluate(model, val_data) print('-' * 89) print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | ' 'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time), val_loss, math.exp(val_loss))) print('-' * 89) if val_loss 输出: | epoch 1 | 200/ 2981 batches | lr 5.00 | ms/batch 35.59 | loss 8.12 | ppl 3348.51 | epoch 1 | 400/ 2981 batches | lr 5.00 | ms/batch 34.57 | loss 6.82 | ppl 912.80 | epoch 1 | 600/ 2981 batches | lr 5.00 | ms/batch 34.55 | loss 6.39 | ppl 597.41 | epoch 1 | 800/ 2981 batches | lr 5.00 | ms/batch 34.59 | loss 6.25 | ppl 517.17 | epoch 1 | 1000/ 2981 batches | lr 5.00 | ms/batch 34.58 | loss 6.12 | ppl 455.67 | epoch 1 | 1200/ 2981 batches | lr 5.00 | ms/batch 34.59 | loss 6.09 | ppl 442.33 | epoch 1 | 1400/ 2981 batches | lr 5.00 | ms/batch 34.60 | loss 6.04 | ppl 421.27 | epoch 1 | 1600/ 2981 batches | lr 5.00 | ms/batch 34.59 | loss 6.05 | ppl 423.61 | epoch 1 | 1800/ 2981 batches | lr 5.00 | ms/batch 34.60 | loss 5.96 | ppl 386.26 | epoch 1 | 2000/ 2981 batches | lr 5.00 | ms/batch 34.60 | loss 5.96 | ppl 387.13 | epoch 1 | 2200/ 2981 batches | lr 5.00 | ms/batch 34.60 | loss 5.85 | ppl 347.56 | epoch 1 | 2400/ 2981 batches | lr 5.00 | ms/batch 34.60 | loss 5.89 | ppl 362.72 | epoch 1 | 2600/ 2981 batches | lr 5.00 | ms/batch 34.60 | loss 5.90 | ppl 363.70 | epoch 1 | 2800/ 2981 batches | lr 5.00 | ms/batch 34.61 | loss 5.80 | ppl 330.43 ----------------------------------------------------------------------------------------- | end of epoch 1 | time: 107.65s | valid loss 5.77 | valid ppl 321.01 ----------------------------------------------------------------------------------------- | epoch 2 | 200/ 2981 batches | lr 4.75 | ms/batch 34.78 | loss 5.81 | ppl 333.28 | epoch 2 | 400/ 2981 batches | lr 4.75 | ms/batch 34.63 | loss 5.78 | ppl 324.24 | epoch 2 | 600/ 2981 batches | lr 4.75 | ms/batch 34.62 | loss 5.61 | ppl 272.10 | epoch 2 | 800/ 2981 batches | lr 4.75 | ms/batch 34.62 | loss 5.65 | ppl 283.77 | epoch 2 | 1000/ 2981 batches | lr 4.75 | ms/batch 34.61 | loss 5.60 | ppl 269.12 | epoch 2 | 1200/ 2981 batches | lr 4.75 | ms/batch 34.63 | loss 5.62 | ppl 275.40 | epoch 2 | 1400/ 2981 batches | lr 4.75 | ms/batch 34.62 | loss 5.62 | ppl 276.93 | epoch 2 | 1600/ 2981 batches | lr 4.75 | ms/batch 34.62 | loss 5.66 | ppl 287.64 | epoch 2 | 1800/ 2981 batches | lr 4.75 | ms/batch 34.63 | loss 5.59 | ppl 268.86 | epoch 2 | 2000/ 2981 batches | lr 4.75 | ms/batch 34.62 | loss 5.63 | ppl 277.73 | epoch 2 | 2200/ 2981 batches | lr 4.75 | ms/batch 34.63 | loss 5.52 | ppl 249.01 | epoch 2 | 2400/ 2981 batches | lr 4.75 | ms/batch 34.61 | loss 5.58 | ppl 265.86 | epoch 2 | 2600/ 2981 batches | lr 4.75 | ms/batch 34.62 | loss 5.60 | ppl 269.12 | epoch 2 | 2800/ 2981 batches | lr 4.75 | ms/batch 34.63 | loss 5.51 | ppl 248.37 ----------------------------------------------------------------------------------------- | end of epoch 2 | time: 107.58s | valid loss 5.60 | valid ppl 270.75 ----------------------------------------------------------------------------------------- | epoch 3 | 200/ 2981 batches | lr 4.51 | ms/batch 34.80 | loss 5.55 | ppl 257.31 | epoch 3 | 400/ 2981 batches | lr 4.51 | ms/batch 34.63 | loss 5.56 | ppl 259.12 | epoch 3 | 600/ 2981 batches | lr 4.51 | ms/batch 34.62 | loss 5.36 | ppl 213.08 | epoch 3 | 800/ 2981 batches | lr 4.51 | ms/batch 34.63 | loss 5.44 | ppl 229.59 | epoch 3 | 1000/ 2981 batches | lr 4.51 | ms/batch 34.63 | loss 5.37 | ppl 215.90 | epoch 3 | 1200/ 2981 batches | lr 4.51 | ms/batch 34.64 | loss 5.41 | ppl 223.49 | epoch 3 | 1400/ 2981 batches | lr 4.51 | ms/batch 34.63 | loss 5.43 | ppl 228.08 | epoch 3 | 1600/ 2981 batches | lr 4.51 | ms/batch 34.62 | loss 5.47 | ppl 238.36 | epoch 3 | 1800/ 2981 batches | lr 4.51 | ms/batch 34.58 | loss 5.40 | ppl 222.43 | epoch 3 | 2000/ 2981 batches | lr 4.51 | ms/batch 34.56 | loss 5.44 | ppl 229.30 | epoch 3 | 2200/ 2981 batches | lr 4.51 | ms/batch 34.55 | loss 5.32 | ppl 204.63 | epoch 3 | 2400/ 2981 batches | lr 4.51 | ms/batch 34.54 | loss 5.39 | ppl 220.17 | epoch 3 | 2600/ 2981 batches | lr 4.51 | ms/batch 34.55 | loss 5.41 | ppl 223.92 | epoch 3 | 2800/ 2981 batches | lr 4.51 | ms/batch 34.55 | loss 5.34 | ppl 209.22 ----------------------------------------------------------------------------------------- | end of epoch 3 | time: 107.47s | valid loss 5.54 | valid ppl 253.71 ----------------------------------------------------------------------------------------- 使用测试集评价模型 使用测试集来测试模型。 test_loss = evaluate(best_model, test_data) print('=' * 89) print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format( test_loss, math.exp(test_loss))) print('=' * 89) 输出: ========================================================================================= | End of training | test loss 5.43 | test ppl 229.27 ========================================================================================= 脚本的总运行时间： （5分钟38.763秒） Download Python source code:transformer_tutorial.py Download Jupyter notebook:transformer_tutorial.ipynb Next [ Previous Was this helpful? Yes No Thank you ©版权所有2017年，PyTorch。 序列到序列与nn.Transformer和TorchText建模 定义模型 负载和批数据 函数来生成输入和目标序列 启动一个实例 运行模型 评估与所述测试数据集的模型 分析流量和优化经验，我们为这个站点的Cookie。通过点击或导航，您同意我们的cookies的使用。因为这个网站目前维护者，Facebook的Cookie政策的适用。了解更多信息，包括有关可用的控制：[饼干政策HTG1。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"intermediate/reinforcement_q_learning.html":{"url":"intermediate/reinforcement_q_learning.html","title":"强化学习（DQN）教程","keywords":"","body":"强化学习（DQN）教程 作者: Adam Paszke 翻译: wutong Zhang 本教程介绍了如何使用PyTorch训练一个Deep Q-learning（DQN）智能点（Agent）来完成OpenAI Gym中的CartPole-V0任务。 任务 智能点需要决定两种动作：向左或向右来使其上的杆保持直立。 你可以在OpenAI Gym找到一个有各种算法和可视化的官方排行榜。 当智能点观察环境的当前状态并选择动作时，环境将转换为新状态，并返回指示动作结果的奖励。在这项任务中，每增加一个时间步，奖励+1，如果杆子掉得太远或大车移动距离中心超过2.4个单位，环境就会终止。这意味着更好的执行场景将持续更长的时间，积累更大的回报。 Cartpole任务的设计为智能点输入代表环境状态（位置、速度等）的4个实际值。然而，神经网络完全可以通过观察场景来解决这个任务，所以我们将使用以车为中心的一块屏幕作为输入。因此，我们的结果无法直接与官方排行榜上的结果相比——我们的任务更艰巨。不幸的是，这会减慢训练速度，因为我们必须渲染所有帧。 严格地说，我们将以当前帧和前一个帧之间的差异来呈现状态。这将允许代理从一张图像中考虑杆子的速度。 包 首先你需要导入必须的包。我们需要 gym 作为环境 (使用 pip install gym 安装). 我们也需要 PyTorch 的如下功能: 神经网络（torch.nn） 优化（torch.optim） 自动微分（torch.autograd） 对于视觉任务工具（torchvision- 一个单独的包） import gym import math import random import numpy as np import matplotlib import matplotlib.pyplot as plt from collections import namedtuple from itertools import count from PIL import Image import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F import torchvision.transforms as T env = gym.make('CartPole-v0').unwrapped # set up matplotlib is_ipython = 'inline' in matplotlib.get_backend() if is_ipython: from IPython import display plt.ion() # if gpu is to be used device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") 回放内存 我们将使用经验回放内存来训练DQN。它存储智能点观察到的转换，允许我们稍后重用此数据。通过从中随机抽样，组成批对象的转换将被取消相关性。结果表明，这大大稳定和改进了DQN训练过程。 因此，我们需要两个类别： Transition- 一个命名的元组，表示我们环境中的单个转换。它基本上将（状态、动作）对映射到它们的（下一个状态、奖励）结果，状态是屏幕差分图像，如后面所述。 ReplayMemory- 一个有界大小的循环缓冲区，用于保存最近观察到的转换。它还实现了一个.sample()方法，用于选择一批随机转换进行训练。 Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward')) class ReplayMemory(object): def __init__(self, capacity): self.capacity = capacity self.memory = [] self.position = 0 def push(self, *args): \"\"\"Saves a transition.\"\"\" if len(self.memory) 现在我们来定义自己的模型。但首先来快速了解一下DQN。 DQN算法 我们的环境是确定的，所以这里提出的所有方程也都是确定性的，为了简单起见。在强化学习文献中，它们还包含对环境中随机转换的期望。 我们的目标是制定一项策略，试图最大化折扣、累积奖励 R_{t_0}=\\sum_{T=T_0}^{\\infty}\\gamma^{t-t_0}r_t ，其中 R_{t_0} 也被认为是返回值。 折扣， \\gamma 应该是介于 0 和 1 之间的常量，以确保和收敛。它使来自不确定的遥远未来的回报对我们的智能点来说比它在不久的将来相当有信心的回报更不重要。 Q-Learning背后的主要思想是，如果我们有一个函数 Q ^* :State \\times Action \\rightarrow \\mathbb{R} ，则如果我们在特定的状态下采取行动，那么我们可以很容易地构建一个最大化回报的策略： \\pi^*(s) = \\arg\\max_a \\ Q^*(s, a) 然而，我们并不了解世界的一切，因此我们无法访问 Q^* 。但是，由于神经网络是通用的函数逼近器，我们可以简单地创建一个并训练它类似于 Q^* 。 对对于我们的训练更新规则，我们将假设某些策略的每个 Q 函数都遵循Bellman方程： Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s')) 等式两边的差异被称为时间差误差，即 \\delta ： \\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a)) 为了尽量减少这个错误，我们将使用Huber loss。Huber损失在误差很小的情况下表现为均方误差，但在误差较大的情况下表现为平均绝对误差 —— 这使得当对 Q 的估计噪音很大时，对异常值的鲁棒性更强。我们通过从重放内存中取样的一批转换来计算 B \\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta) \\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\\frac{1}{2}{\\delta^2} & \\text{for } |\\delta| \\le 1,\\\\\\ |\\delta| - \\frac{1}{2} & \\text{otherwise.} \\end{cases} Q-网络 我们的模型将是一个卷积神经网络需要在当前和以前的屏幕补丁之间的差异。它具有两个输出端，表示 Q(S,\\mathrm{left}) 和 Q(S,\\mathrm{right}) (其中 S 是输入到网络)。实际上，网络正试图预测在给定电流输入的情况下采取每项行动的预期回报。 class DQN(nn.Module): def __init__(self, h, w, outputs): super(DQN, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2) self.bn2 = nn.BatchNorm2d(32) self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2) self.bn3 = nn.BatchNorm2d(32) # 线性输入连接的数量取决于conv2d层的输出，因此需要计算输入图像的大小。 def conv2d_size_out(size, kernel_size = 5, stride = 2): return (size - (kernel_size - 1) - 1) // stride + 1 convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w))) convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h))) linear_input_size = convw * convh * 32 self.head = nn.Linear(linear_input_size, outputs) # 使用一个元素调用以确定下一个操作，或在优化期间调用批处理。返回张量 def forward(self, x): x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) x = F.relu(self.bn3(self.conv3(x))) return self.head(x.view(x.size(0), -1)) 获取输入 下面的代码是用于从环境中提取和处理渲染图像的实用程序。它使用了torchvision包，这样就可以很容易地组合图像转换。运行单元后，它将显示它提取的示例帧。 resize = T.Compose([T.ToPILImage(), T.Resize(40, interpolation=Image.CUBIC), T.ToTensor()]) def get_cart_location(screen_width): world_width = env.x_threshold * 2 scale = screen_width / world_width return int(env.state[0] * scale + screen_width / 2.0) # MIDDLE OF CART def get_screen(): # 返回 gym 需要的400x600x3 图片, 但有时会更大，如800x1200x3. 将其转换为torch (CHW). screen = env.render(mode='rgb_array').transpose((2, 0, 1)) # 车子在下半部分，因此请剥去屏幕的顶部和底部。 screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)] view_width = int(screen_width * 0.6) cart_location = get_cart_location(screen_width) if cart_location (screen_width - view_width // 2): slice_range = slice(-view_width, None) else: slice_range = slice(cart_location - view_width // 2, cart_location + view_width // 2) # 去掉边缘，这样我们就可以得到一个以车为中心的正方形图像。 screen = screen[:, :, slice_range] # 转化为 float, 重新裁剪, 转化为 torch 张量(这并不需要拷贝) screen = np.ascontiguousarray(screen, dtype=np.float32) / 255 screen = torch.from_numpy(screen) # 重新裁剪,加入批维度 (BCHW) return resize(screen).unsqueeze(0).to(device) env.reset() plt.figure() plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(), interpolation='none') plt.title('Example extracted screen') plt.show() 训练 超参数和配置 此单元实例化模型及其优化器，并定义一些实用程序： select_action- 将根据迭代次数贪婪策略选择一个行动。简单地说，我们有时会使用我们的模型来选择动作，有时我们只会对其中一个进行统一的采样。选择随机动作的概率将从 EPS_START 开始并以指数形式向 EPS_END衰减。 EPS_DECAY 控制衰减速率。 plot_durations- 一个帮助绘制迭代次数持续时间，以及过去100迭代次数的平均值（官方评估中使用的度量）。迭代次数将在包含主训练循环的单元下方，并在每迭代之后更新。 BATCH_SIZE = 128 GAMMA = 0.999 EPS_START = 0.9 EPS_END = 0.05 EPS_DECAY = 200 TARGET_UPDATE = 10 # 获取屏幕大小，以便我们可以根据从ai-gym返回的形状正确初始化层。 # 这一点上的典型尺寸接近3x40x90，这是在get_screen（）中抑制和缩小的渲染缓冲区的结果。 init_screen = get_screen() _, _, screen_height, screen_width = init_screen.shape # Get number of actions from gym action space n_actions = env.action_space.n policy_net = DQN(screen_height, screen_width, n_actions).to(device) target_net = DQN(screen_height, screen_width, n_actions).to(device) target_net.load_state_dict(policy_net.state_dict()) target_net.eval() optimizer = optim.RMSprop(policy_net.parameters()) memory = ReplayMemory(10000) steps_done = 0 def select_action(state): global steps_done sample = random.random() eps_threshold = EPS_END + (EPS_START - EPS_END) * \\ math.exp(-1. * steps_done / EPS_DECAY) steps_done += 1 if sample > eps_threshold: with torch.no_grad(): # t.max(1) will return largest column value of each row. # second column on max result is index of where max element was # found, so we pick action with the larger expected reward. return policy_net(state).max(1)[1].view(1, 1) else: return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long) episode_durations = [] def plot_durations(): plt.figure(2) plt.clf() durations_t = torch.tensor(episode_durations, dtype=torch.float) plt.title('Training...') plt.xlabel('Episode') plt.ylabel('Duration') plt.plot(durations_t.numpy()) # Take 100 episode averages and plot them too if len(durations_t) >= 100: means = durations_t.unfold(0, 100, 1).mean(1).view(-1) means = torch.cat((torch.zeros(99), means)) plt.plot(means.numpy()) plt.pause(0.001) # pause a bit so that plots are updated if is_ipython: display.clear_output(wait=True) display.display(plt.gcf()) 训练循环 最后，训练我们的模型的代码。 在这里，你可以找到执行最优化的一个步骤的optimize_model功能。它执行优化的一个步骤。它首先对一批数据进行采样，将所有张量连接成一个张量，计算出 Q(s_t,a_t) 和 V(s_ {t + 1})= \\ max_a Q(s_ {t + 1},a) ，并将它们组合成我们的损失。根据定义，如果 s 是结束状态，我们设置 V(s) = 0 。我们还使用目标网络来计算 V(s_{t+1}) \\` 以增加稳定性。目标网络的权重大部分时间保持不变，但每隔一段时间就会更新一次策略网络的权重。这通常是一组步骤，但为了简单起见，我们将使用迭代次数。 def optimize_model(): if len(memory) 接下来，你可以找到主训练循环。开始时，我们重置环境并初始化state张量。然后，我们对一个操作进行采样，执行它，观察下一个屏幕和奖励（总是1），并对我们的模型进行一次优化。当 episode 结束（我们的模型失败）时，我们重新启动循环。 num_episodes设置得很小。你可以下载并运行更多的epsiodes，比如300+来进行有意义的持续时间改进。 num_episodes = 50 for i_episode in range(num_episodes): # 初始化环境和状态 env.reset() last_screen = get_screen() current_screen = get_screen() state = current_screen - last_screen for t in count(): # 选择并执行动作 action = select_action(state) _, reward, done, _ = env.step(action.item()) reward = torch.tensor([reward], device=device) # 观察新状态 last_screen = current_screen current_screen = get_screen() if not done: next_state = current_screen - last_screen else: next_state = None # 在内存中储存当前参数 memory.push(state, action, next_state, reward) # 进入下一状态 state = next_state # 记性一步优化 (在目标网络) optimize_model() if done: episode_durations.append(t + 1) plot_durations() break #更新目标网络, 复制在 DQN 中的所有权重偏差 if i_episode % TARGET_UPDATE == 0: target_net.load_state_dict(policy_net.state_dict()) print('Complete') env.render() env.close() plt.ioff() plt.show() 下面是一个图表，它说明了整个结果数据流。 动作可以是随机选择的，也可以是基于一个策略，从gym环境中获取下一步的样本。我们将结果记录在回放内存中，并在每次迭代中运行优化步骤。优化从重放内存中随机抽取一批来训练新策略。 “旧的”target_net也用于优化计算预期的Q值；它偶尔会更新以保持其最新。 脚本的总运行时间： （0分钟0.000秒） 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"intermediate/flask_rest_api_tutorial.html":{"url":"intermediate/flask_rest_api_tutorial.html","title":"部署PyTorch在Python经由REST API从Flask","keywords":"","body":"1.通过REST API与部署PyTorch在Python烧瓶 作者 ：阿维纳什Sajjanshetty 在本教程中，我们将使用瓶部署PyTorch模型和暴露的模型推断一个REST API。特别是，我们将部署一个预训练DenseNet 121模型检测的图像。 小费 这里使用的所有的代码是在MIT许可下发布，并可在[ Github上HTG1。 这代表了一个教程系列的第一个在生产中部署PyTorch车型。以这种方式使用瓶是目前为止最简单的方法来启动服务您PyTorch车型，但它不是一个用例性能要求较高的工作。为了那个原因： [HTG2如果你已经很熟悉TorchScript，您可以直接跳到我们的加载++ 一个TorchScript模型用C教程。 如果你首先需要在TorchScript复习，看看我们的【HTG0]介绍一个TorchScript 教程。 > API定义 首先，我们将定义我们的API端点，请求和响应类型。我们的API端点将位于/预测这需要HTTP POST请求与包含该图像的文件参数。该响应将是包含预测JSON响应的： {\"class_id\": \"n02124075\", \"class_name\": \"Egyptian_cat\"} 依赖性 安装运行下面的命令所需的依赖关系： $ pip install Flask==1.0.3 torchvision-0.3.0 简单的Web服务器 下面是一个简单的Web服务器，从瓶资料为准 from flask import Flask app = Flask(__name__) @app.route('/') def hello(): return 'Hello World!' 保存上面的代码在一个名为app.py您现在可以通过键入运行瓶开发服务器文件： $ FLASK_ENV=development FLASK_APP=app.py flask run 当您访问HTTP：//本地主机：5000 /在你的网页浏览器，你将与你好 世界打招呼！文本 我们将上述片断的轻微变化，所以它适合我们的API定义。首先，我们将重命名为预测的方法。我们将更新端点路径/预测 [HTG7。由于图像文件将通过HTTP POST请求被发送，我们会随时更新，以便它也只接受POST请求： @app.route('/predict', methods=['POST']) def predict(): return 'Hello World!' 我们也将改变响应类型，因此它返回一个包含ImageNet类ID和名称的JSON响应。更新app.py文件将是现在： from flask import Flask, jsonify app = Flask(__name__) @app.route('/predict', methods=['POST']) def predict(): return jsonify({'class_id': 'IMAGE_NET_XXX', 'class_name': 'Cat'}) 推理 在接下来的章节中，我们将集中精力编写推理代码。这将涉及到两个部分，一是我们准备的图像，以便它可以被输送到DenseNet和明年，我们将编写代码即可获得从模型的实际预测。 准备图像 DenseNet模型需要图像是尺寸224 X 224的3通道RGB图像的我们也将正常化与所需的平均和标准偏差值的图像张量。你可以阅读更多关于它的[此处HTG1。 我们将使用变换从torchvision库，并建立一个管道改造的要求，它改变我们的图像。你可以阅读更多关于变换[此处HTG9。 import io import torchvision.transforms as transforms from PIL import Image def transform_image(image_bytes): my_transforms = transforms.Compose([transforms.Resize(255), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize( [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) image = Image.open(io.BytesIO(image_bytes)) return my_transforms(image).unsqueeze(0) 上述方法需要图像数据以字节为单位，应用一系列的变换，并返回一个张量。为了检验上述方法，读取字节模式下的图像文件（第一替换 ../_static/img/sample_file.jpeg 的实际路径到计算机上的文件），看看如果你得到一个张量背部： with open(\"../_static/img/sample_file.jpeg\", 'rb') as f: image_bytes = f.read() tensor = transform_image(image_bytes=image_bytes) print(tensor) 日期： tensor([[[[ 0.4508, 0.4166, 0.3994, ..., -1.3473, -1.3302, -1.3473], [ 0.5364, 0.4851, 0.4508, ..., -1.2959, -1.3130, -1.3302], [ 0.7077, 0.6392, 0.6049, ..., -1.2959, -1.3302, -1.3644], ..., [ 1.3755, 1.3927, 1.4098, ..., 1.1700, 1.3584, 1.6667], [ 1.8893, 1.7694, 1.4440, ..., 1.2899, 1.4783, 1.5468], [ 1.6324, 1.8379, 1.8379, ..., 1.4783, 1.7352, 1.4612]], [[ 0.5728, 0.5378, 0.5203, ..., -1.3704, -1.3529, -1.3529], [ 0.6604, 0.6078, 0.5728, ..., -1.3004, -1.3179, -1.3354], [ 0.8529, 0.7654, 0.7304, ..., -1.3004, -1.3354, -1.3704], ..., [ 1.4657, 1.4657, 1.4832, ..., 1.3256, 1.5357, 1.8508], [ 2.0084, 1.8683, 1.5182, ..., 1.4657, 1.6583, 1.7283], [ 1.7458, 1.9384, 1.9209, ..., 1.6583, 1.9209, 1.6408]], [[ 0.7228, 0.6879, 0.6531, ..., -1.6476, -1.6302, -1.6476], [ 0.8099, 0.7576, 0.7228, ..., -1.6476, -1.6476, -1.6650], [ 1.0017, 0.9145, 0.8797, ..., -1.6476, -1.6650, -1.6999], ..., [ 1.6291, 1.6291, 1.6465, ..., 1.6291, 1.8208, 2.1346], [ 2.1868, 2.0300, 1.6814, ..., 1.7685, 1.9428, 2.0125], [ 1.9254, 2.0997, 2.0823, ..., 1.9428, 2.2043, 1.9080]]]]) 预测 现在将使用预训练DenseNet 121模型预测图像类。我们将使用一个从torchvision库，加载模型，并得到一个推论。虽然我们将在这个例子中使用预训练的模型，你可以使用自己的模型同样的方法。查看更多有关此 教程 加载你的模型。 from torchvision import models # Make sure to pass `pretrained`as `True`to use the pretrained weights: model = models.densenet121(pretrained=True) # Since we are using our model only for inference, switch to `eval`mode: model.eval() def get_prediction(image_bytes): tensor = transform_image(image_bytes=image_bytes) outputs = model.forward(tensor) _, y_hat = outputs.max(1) return y_hat 预测的类ID的张量y_hat将包含索引。然而，我们需要人类可读的类名。为此，我们需要一个等级ID名称映射。下载这个文件为imagenet_class_index.json，并记住您保存它（或者，如果你是以下在本教程中的具体步骤，它保存在教程/ _static ）。此文件包含ImageNet类ID来ImageNet类名的映射。我们将加载这个JSON文件，并得到预测指数的类名。 import json imagenet_class_index = json.load(open('../_static/imagenet_class_index.json')) def get_prediction(image_bytes): tensor = transform_image(image_bytes=image_bytes) outputs = model.forward(tensor) _, y_hat = outputs.max(1) predicted_idx = str(y_hat.item()) return imagenet_class_index[predicted_idx] 使用imagenet_class_index词典之前，我们首先将张量的值转换为字符串值，因为在imagenet_class_index字典中的键是字符串。我们将测试我们上面的方法： with open(\"../_static/img/sample_file.jpeg\", 'rb') as f: image_bytes = f.read() print(get_prediction(image_bytes=image_bytes)) Out: ['n02124075', 'Egyptian_cat'] 你应该得到这样的回应： ['n02124075', 'Egyptian_cat'] 在阵列中的第一项是ImageNet类ID和第二项是人类可读的名称。 Note 你有没有注意到模型变量不是get_prediction方法的一部分？为什么是模型中的全局变量？加载模型可以是在存储器和计算方面是昂贵的操作。如果我们在get_prediction方法加载模型，那么它会得到不必要加载的每一个方法被调用的时间。因为，我们正在建立一个Web服务器，有可能是每秒数千次的请求，我们不应该浪费时间冗余负载对每个推理模型。所以，我们一直在内存中加载只有一次的模型。在生产系统中，有必要提高效率你计算的使用能够在大规模服务请求，所以你一般应为请求提供服务之前加载模型。 在我们的API服务器集成模型 在这最后一部分，我们将我们的模型添加到我们的瓶API服务器。由于我们的API服务器应该采取一个图像文件，我们会随时更新我们的预测方法来读取请求的文件： from flask import request @app.route('/predict', methods=['POST']) def predict(): if request.method == 'POST': # we will get the file from the request file = request.files['file'] # convert that to bytes img_bytes = file.read() class_id, class_name = get_prediction(image_bytes=img_bytes) return jsonify({'class_id': class_id, 'class_name': class_name}) 在app.py文件现已完成。以下是完整版;与你保存的文件，它应该运行的路径替换路径： import io import json from torchvision import models import torchvision.transforms as transforms from PIL import Image from flask import Flask, jsonify, request app = Flask(__name__) imagenet_class_index = json.load(open('/imagenet_class_index.json')) model = models.densenet121(pretrained=True) model.eval() def transform_image(image_bytes): my_transforms = transforms.Compose([transforms.Resize(255), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize( [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) image = Image.open(io.BytesIO(image_bytes)) return my_transforms(image).unsqueeze(0) def get_prediction(image_bytes): tensor = transform_image(image_bytes=image_bytes) outputs = model.forward(tensor) _, y_hat = outputs.max(1) predicted_idx = str(y_hat.item()) return imagenet_class_index[predicted_idx] @app.route('/predict', methods=['POST']) def predict(): if request.method == 'POST': file = request.files['file'] img_bytes = file.read() class_id, class_name = get_prediction(image_bytes=img_bytes) return jsonify({'class_id': class_id, 'class_name': class_name}) if __name__ == '__main__': app.run() 让我们来测试我们的网络服务器！跑： $ FLASK_ENV=development FLASK_APP=app.py flask run 我们可以使用请求库发送POST请求到我们的应用程序： import requests resp = requests.post(\"http://localhost:5000/predict\", files={\"file\": open('/cat.jpg','rb')}) 印刷 resp.json（）现在会显示以下内容： {\"class_id\": \"n02124075\", \"class_name\": \"Egyptian_cat\"} 接下来的步骤 我们写的服务器是很琐碎，可能不是你所需要的生产应用程序的一切。所以，这里有一些事情可以做，以更好地使其： 端点/预测假定总是会有在该请求的图像文件。这可能不是适用于所有要求如此。我们的用户可以发送图像具有不同的参数或者根本不发送图像。 用户可以发送过多非图像类型的文件。由于我们没有处理错误，这将打破我们的服务器。并称将抛出一个异常明确的处理错误的道路，使我们能够更好地处理无效输入 尽管该模型可识别大量的图像类，也未必能够识别的所有图像。加强对办案时模型无法识别图像中的任何实施。 我们运行的发展模式，这是不适合在生产部署瓶服务器。您可以检查出本教程在生产部署瓶服务器。 您也可以通过创建与需要的图像，并显示预测的形式添加页面的UI。检查出一个类似的项目和它的源代码的[演示HTG1。 在本教程中，我们只展示了如何构建，可以在同一时间返回预测单个图像服务。我们可以修改我们的服务能马上回家多个图像的预测。此外，服务流光库自动排队请求您的服务和样品它们变成可被送入模型迷你批次。您可以检查出[本教程HTG3。 最后，我们建议您检查出部署PyTorch模型链接到页面的顶部我们的其他教程。 脚本的总运行时间： （0分钟0.925秒） 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/Intro_to_TorchScript_tutorial.html":{"url":"beginner/Intro_to_TorchScript_tutorial.html","title":"介绍TorchScript","keywords":"","body":"介绍TorchScript > 作者: James Reed (jamesreed@fb.com), Michael Suo(suo@fb.com), rev2 校对: 松鼠 本教程是TorchScript的简介，TorchScript是PyTorch模型（子类nn.Module）的中间表示，可以在高性能环境（例如C ++）中运行。 在本教程中，我们将介绍： PyTorch中的模型基础创建，包括： 模块 定义forward功能 构成模块组成模块的层次结构 将PyTorch模块转换为TorchScript（我们的高性能部署运行时）的特定方法 跟踪现有模块 使用脚本来直接编译的模块 如何组合这两种方法 保存和加载TorchScript模块 我们希望在完成本教程之后，您将继续阅读后续教程 ，该教程将引导您实际从C++调用TorchScript模型的示例。 import torch # This is all you need to use both PyTorch and TorchScript! print(torch.__version__) 输出： 1.2.0 PyTorch模型制作的基础 让我们开始定义一个简单的Module。AModule是PyTorch中组成的基本单位。它包含： 构造函数，为调用准备模块 一组Parameters和Modules。这些由构造函数初始化，并且可以在调用期间由模块使用。 forward功能。这是调用模块时运行的代码。 让我们来看看一个小例子： class MyCell(torch.nn.Module): def __init__(self): super(MyCell, self).__init__() def forward(self, x, h): new_h = torch.tanh(x + h) return new_h, new_h my_cell = MyCell() x = torch.rand(3, 4) h = torch.rand(3, 4) print(my_cell(x, h)) Out: (tensor([[0.7853, 0.8882, 0.7137, 0.3746], [0.5265, 0.8508, 0.1487, 0.9144], [0.7057, 0.8217, 0.9575, 0.6132]]), tensor([[0.7853, 0.8882, 0.7137, 0.3746], [0.5265, 0.8508, 0.1487, 0.9144], [0.7057, 0.8217, 0.9575, 0.6132]])) 因此，我们已经： 创建子类的类torch.nn.Module。 定义构造函数。构造函数没有做太多事情，只是调用的构造函数super。 定义了一个forward函数，该函数接受两个输入并返回两个输出。该forward函数的实际内容并不是很重要，但是它是一种伪造的RNN单元即，该函数应用于循环。 我们实例化了模块，并制作了x和y，它们只是3x4随机值矩阵。然后，我们使用调用单元,调用我们的forward函数my_cell(x, h) 让我们做一些更有趣的事情： class MyCell(torch.nn.Module): def __init__(self): super(MyCell, self).__init__() self.linear = torch.nn.Linear(4, 4) def forward(self, x, h): new_h = torch.tanh(self.linear(x) + h) return new_h, new_h my_cell = MyCell() print(my_cell) print(my_cell(x, h)) Out: MyCell( (linear): Linear(in_features=4, out_features=4, bias=True) ) (tensor([[0.7619, 0.7761, 0.7476, 0.0897], [0.6886, 0.4990, 0.4495, 0.2021], [0.5849, 0.5874, 0.9256, 0.0460]], grad_fn=), tensor([[0.7619, 0.7761, 0.7476, 0.0897], [0.6886, 0.4990, 0.4495, 0.2021], [0.5849, 0.5874, 0.9256, 0.0460]], grad_fn=)) 我们已经重新定义了模块MyCell，但是这次我们添加了一个 self.linear属性，并self.linear在forward函数中调用。 这里到底发生了什么？torch.nn.Linear是Module来自PyTorch标准库的。就像一样MyCell，可以使用调用语法来调用它。我们正在建立的层次结构Module们。 print上的Module会直观地表示 Module的子类层次结构。在我们的示例中，我们可以看到 Linear子类及其参数。 通过这种方式构成Module们，我们可以简洁而易读地编写具有可复用组件的模型。 您可能已经注意到输出中的grad_fn了。这是PyTorch自动区分求导给出的信息，称为autograd。简而言之，该系统允许我们通过潜在的复杂程序来计算导数。该设计为模型创作提供了极大的灵活性。 现在让我们检查一下灵活性： class MyDecisionGate(torch.nn.Module): def forward(self, x): if x.sum() > 0: return x else: return -x class MyCell(torch.nn.Module): def __init__(self): super(MyCell, self).__init__() self.dg = MyDecisionGate() self.linear = torch.nn.Linear(4, 4) def forward(self, x, h): new_h = torch.tanh(self.dg(self.linear(x)) + h) return new_h, new_h my_cell = MyCell() print(my_cell) print(my_cell(x, h)) Out: MyCell( (dg): MyDecisionGate() (linear): Linear(in_features=4, out_features=4, bias=True) ) (tensor([[ 0.9077, 0.5939, 0.6809, 0.0994], [ 0.7583, 0.7180, 0.0790, 0.6733], [ 0.9102, -0.0368, 0.8246, -0.3256]], grad_fn=), tensor([[ 0.9077, 0.5939, 0.6809, 0.0994], [ 0.7583, 0.7180, 0.0790, 0.6733], [ 0.9102, -0.0368, 0.8246, -0.3256]], grad_fn=)) 我们再次重新定义了MyCell类，但是在这里我们定义了 MyDecisionGate。该模块利用控制流程。控制流包括循环和if-statements之类的东西。 给定完整的程序表示形式，许多框架都采用计算符号派生的方法。但是，在PyTorch中，我们使用渐变色带。我们记录操作发生时的操作，并在计算衍生产品时向后回放。这样，框架不必为语言中的所有构造显式定义派生类。 autograd的工作原理 TorchScript的基础 现在，让我们以正在运行的示例为例，看看如何应用TorchScript。 简而言之，即使PyTorch具有灵活和动态的特性，TorchScript也提供了捕获模型定义的工具。让我们开始研究所谓的跟踪。 追踪Modules class MyCell(torch.nn.Module): def __init__(self): super(MyCell, self).__init__() self.linear = torch.nn.Linear(4, 4) def forward(self, x, h): new_h = torch.tanh(self.linear(x) + h) return new_h, new_h my_cell = MyCell() x, h = torch.rand(3, 4), torch.rand(3, 4) traced_cell = torch.jit.trace(my_cell, (x, h)) print(traced_cell) traced_cell(x, h) Out: TracedModule[MyCell]( (linear): TracedModule[Linear]() ) 我们来看看之前的例子。和以前一样，我们实例化了它，但是这次，我们使用torch.jit.trace方法调用了Module，并传入，然后传入了网络可能的示例输入。 这到底是做什么的？它已调用Module，记录了Module运行时发生的操作，并创建了torch.jit.ScriptModule（TracedModule的实例） TorchScript将其定义记录在中间表示（或IR）中，在深度学习中通常称为图形。我们可以检查具有以下.graph属性的图形： print(traced_cell.graph) Out: graph(%self : ClassType, %input : Float(3, 4), %h : Float(3, 4)): %1 : ClassType = prim::GetAttr[name=\"linear\"](%self) %weight : Tensor = prim::GetAttr[name=\"weight\"](%1) %bias : Tensor = prim::GetAttr[name=\"bias\"](%1) %6 : Float(4!, 4!) = aten::t(%weight), scope: MyCell/Linear[linear] # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1369:0 %7 : int = prim::Constant[value=1](), scope: MyCell/Linear[linear] # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1369:0 %8 : int = prim::Constant[value=1](), scope: MyCell/Linear[linear] # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1369:0 %9 : Float(3, 4) = aten::addmm(%bias, %input, %6, %7, %8), scope: MyCell/Linear[linear] # /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1369:0 %10 : int = prim::Constant[value=1](), scope: MyCell # /var/lib/jenkins/workspace/beginner_source/Intro_to_TorchScript_tutorial.py:188:0 %11 : Float(3, 4) = aten::add(%9, %h, %10), scope: MyCell # /var/lib/jenkins/workspace/beginner_source/Intro_to_TorchScript_tutorial.py:188:0 %12 : Float(3, 4) = aten::tanh(%11), scope: MyCell # /var/lib/jenkins/workspace/beginner_source/Intro_to_TorchScript_tutorial.py:188:0 %13 : (Float(3, 4), Float(3, 4)) = prim::TupleConstruct(%12, %12) return (%13) 但是，这是一个非常低级的表示形式，图中包含的大多数信息对最终用户没有用。相反，我们可以使用.code属性为代码提供Python语法的解释： print(traced_cell.code) Out: def forward(self, input: Tensor, h: Tensor) -> Tuple[Tensor, Tensor]: _0 = self.linear weight = _0.weight bias = _0.bias _1 = torch.addmm(bias, input, torch.t(weight), beta=1, alpha=1) _2 = torch.tanh(torch.add(_1, h, alpha=1)) return (_2, _2) 那么为什么我们要做所有这些呢？有以下几个原因： TorchScript代码可以在其自己的解释器中调用，该解释器基本上是受限制的Python解释器。该解释器不获取全局解释器锁定，因此可以在同一实例上同时处理许多请求。 这种格式使我们可以将整个模型保存到磁盘上，并将其加载到另一个环境中，例如在以Python以外的语言编写的服务器中 TorchScript为我们提供了一种表示形式，其中我们可以对代码进行编译器优化以提供更有效的执行 TorchScript允许我们与许多后端/设备运行时进行接口，这些运行时比单个操作员需要更广泛的程序视图。 我们可以看到调用traced_cell产生的结果与Python模块相同：： print(my_cell(x, h)) print(traced_cell(x, h)) Out: (tensor([[ 0.0294, 0.2921, 0.5171, 0.2689], [ 0.5859, 0.8311, 0.2553, 0.8026], [-0.4138, 0.7641, 0.4251, 0.7217]], grad_fn=), tensor([[ 0.0294, 0.2921, 0.5171, 0.2689], [ 0.5859, 0.8311, 0.2553, 0.8026], [-0.4138, 0.7641, 0.4251, 0.7217]], grad_fn=)) (tensor([[ 0.0294, 0.2921, 0.5171, 0.2689], [ 0.5859, 0.8311, 0.2553, 0.8026], [-0.4138, 0.7641, 0.4251, 0.7217]], grad_fn=), tensor([[ 0.0294, 0.2921, 0.5171, 0.2689], [ 0.5859, 0.8311, 0.2553, 0.8026], [-0.4138, 0.7641, 0.4251, 0.7217]], grad_fn=)) 使用脚本来转换模块 我们使用模块的第二个版本是有原因的，而不是使用带有控制流的子模块的一个版本。现在让我们检查一下： class MyDecisionGate(torch.nn.Module): def forward(self, x): if x.sum() > 0: return x else: return -x class MyCell(torch.nn.Module): def __init__(self, dg): super(MyCell, self).__init__() self.dg = dg self.linear = torch.nn.Linear(4, 4) def forward(self, x, h): new_h = torch.tanh(self.dg(self.linear(x)) + h) return new_h, new_h my_cell = MyCell(MyDecisionGate()) traced_cell = torch.jit.trace(my_cell, (x, h)) print(traced_cell.code) Out: def forward(self, input: Tensor, h: Tensor) -> Tuple[Tensor, Tensor]: _0 = self.linear weight = _0.weight bias = _0.bias x = torch.addmm(bias, input, torch.t(weight), beta=1, alpha=1) _1 = torch.tanh(torch.add(torch.neg(x), h, alpha=1)) return (_1, _1) 查看.code输出，可以看到if-else找不到分支！为什么？跟踪完全按照我们所说的去做：运行代码，记录发生的操作，并构造一个可以做到这一点的ScriptModule。不幸的是，诸如控制流之类的东西被抹去了。 我们如何在TorchScript中忠实地表示此模块？我们提供了一个脚本编译器，它可以直接分析您的Python源代码以将其转换为TorchScript。让我们MyDecisionGate使用脚本编译器进行转换： scripted_gate = torch.jit.script(MyDecisionGate()) my_cell = MyCell(scripted_gate) traced_cell = torch.jit.script(my_cell) print(traced_cell.code) Out: def forward(self, x: Tensor, h: Tensor) -> Tuple[Tensor, Tensor]: _0 = self.linear _1 = _0.weight _2 = _0.bias if torch.eq(torch.dim(x), 2): _3 = torch.__isnot__(_2, None) else: _3 = False if _3: bias = ops.prim.unchecked_unwrap_optional(_2) ret = torch.addmm(bias, x, torch.t(_1), beta=1, alpha=1) else: output = torch.matmul(x, torch.t(_1)) if torch.__isnot__(_2, None): bias0 = ops.prim.unchecked_unwrap_optional(_2) output0 = torch.add_(output, bias0, alpha=1) else: output0 = output ret = output0 _4 = torch.gt(torch.sum(ret, dtype=None), 0) if bool(_4): _5 = ret else: _5 = torch.neg(ret) new_h = torch.tanh(torch.add(_5, h, alpha=1)) return (new_h, new_h) 万岁！现在，我们已经忠实地捕获了我们在TorchScript中程序的行为。现在让我们尝试运行该程序： # New inputs x, h = torch.rand(3, 4), torch.rand(3, 4) traced_cell(x, h) 混合脚本和跟踪 在某些情况下，我们只需要追踪的的结果而不需要全部脚本（例如，模块具有许多条件分支，这些分支我们并不希望展现在TorchScript中）。在这种情况下，脚本可以与用以下方法跟踪：torch.jit.script。他将只会追踪方法内的脚本，不会展示方法外的脚本情况。 第一种情况的一个示例： class MyRNNLoop(torch.nn.Module): def __init__(self): super(MyRNNLoop, self).__init__() self.cell = torch.jit.trace(MyCell(scripted_gate), (x, h)) def forward(self, xs): h, y = torch.zeros(3, 4), torch.zeros(3, 4) for i in range(xs.size(0)): y, h = self.cell(xs[i], h) return y, h rnn_loop = torch.jit.script(MyRNNLoop()) print(rnn_loop.code) Out: def forward(self, xs: Tensor) -> Tuple[Tensor, Tensor]: h = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None) y = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None) y0, h0 = y, h for i in range(torch.size(xs, 0)): _0 = self.cell _1 = torch.select(xs, 0, i) _2 = _0.linear weight = _2.weight bias = _2.bias _3 = torch.addmm(bias, _1, torch.t(weight), beta=1, alpha=1) _4 = torch.gt(torch.sum(_3, dtype=None), 0) if bool(_4): _5 = _3 else: _5 = torch.neg(_3) _6 = torch.tanh(torch.add(_5, h0, alpha=1)) y0, h0 = _6, _6 return (y0, h0) 还有第二种情况的示例： class WrapRNN(torch.nn.Module): def __init__(self): super(WrapRNN, self).__init__() self.loop = torch.jit.script(MyRNNLoop()) def forward(self, xs): y, h = self.loop(xs) return torch.relu(y) traced = torch.jit.trace(WrapRNN(), (torch.rand(10, 3, 4))) print(traced.code) Out: def forward(self, argument_1: Tensor) -> Tensor: _0 = self.loop h = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None) h0 = h for i in range(torch.size(argument_1, 0)): _1 = _0.cell _2 = torch.select(argument_1, 0, i) _3 = _1.linear weight = _3.weight bias = _3.bias _4 = torch.addmm(bias, _2, torch.t(weight), beta=1, alpha=1) _5 = torch.gt(torch.sum(_4, dtype=None), 0) if bool(_5): _6 = _4 else: _6 = torch.neg(_4) h0 = torch.tanh(torch.add(_6, h0, alpha=1)) return torch.relu(h0) 这样，当情况需要它们时，可以使用脚本和跟踪并将它们一起使用。 保存和加载模型 我们提供API，以存档格式将TorchScript模块保存到磁盘或从磁盘加载TorchScript模块。这种格式包括代码，参数，属性和调试信息，这意味着归档文件是模型的独立表示形式，可以在完全独立的过程中加载。让我们保存并加载包装好的RNN模块： traced.save('wrapped_rnn.zip') loaded = torch.jit.load('wrapped_rnn.zip') print(loaded) print(loaded.code) Out: ScriptModule( (loop): ScriptModule( (cell): ScriptModule( (dg): ScriptModule() (linear): ScriptModule() ) ) ) def forward(self, argument_1: Tensor) -> Tensor: _0 = self.loop h = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None) h0 = h for i in range(torch.size(argument_1, 0)): _1 = _0.cell _2 = torch.select(argument_1, 0, i) _3 = _1.linear weight = _3.weight bias = _3.bias _4 = torch.addmm(bias, _2, torch.t(weight), beta=1, alpha=1) _5 = torch.gt(torch.sum(_4, dtype=None), 0) if bool(_5): _6 = _4 else: _6 = torch.neg(_4) h0 = torch.tanh(torch.add(_6, h0, alpha=1)) return torch.relu(h0) 正如你所看到的，序列化保留了模块层次结构和我们一直在研究的代码。例如，也可以将模型加载到C ++中以实现不依赖Python的执行。 进一步阅读 我们已经完成了教程！有关更多涉及的演示，请查看NeurIPS演示，以使用TorchScript转换机器翻译模型 脚本的总运行时间： （0分钟0.252秒） 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"advanced/cpp_export.html":{"url":"advanced/cpp_export.html","title":"在C ++中加载TorchScript模型 ","keywords":"","body":"在C++ 中加载 TorchScript 模型 译者：talengu 本教程已更新为可与PyTorch 1.2一起使用 顾名思义，PyTorch的主要接口是Python编程语言。尽管Python是许多需要动态性和易于迭代的场景的合适且首选的语言，但是在同样许多情况下，Python的这些属性恰恰是不利的。后者通常适用的一种环境是生产 -低延迟和严格部署要求的土地。对于生产场景，即使只将C ++绑定到Java，Rust或Go之类的另一种语言中，它通常也是首选语言。以下段落将概述PyTorch提供的从现有Python模型到可以加载和执行的序列化表示形式的路径 完全来自C ++，不依赖Python。 步骤1：将PyTorch模型转换为Torch脚本 PyTorch模型从Python到C ++的旅程由Torch Script启用，Torch Script是PyTorch模型的表示形式，可以由Torch Script编译器理解，编译和序列化。如果您是从使用vanilla “eager” API编写的现有PyTorch模型开始的，则必须首先将模型转换为Torch脚本。在最常见的情况下（如下所述），这只需要很少的努力。如果您已经有了Torch脚本模块，则可以跳到本教程的下一部分。 有两种将PyTorch模型转换为Torch脚本的方法。第一种称为跟踪，一种机制，通过使用示例输入对模型的结构进行一次评估，并记录这些输入在模型中的流动，从而捕获模型的结构。这适用于有限使用控制流的模型。第二种方法是在模型中添加显式批注，以告知Torch Script编译器可以根据Torch Script语言施加的约束直接解析和编译模型代码。 小贴士 您可以在官方Torch Script中找到有关这两种方法的完整文档，以及使用方法的进一步指导。 通过跟踪转换为 Torch Script 要将PyTorch模型通过跟踪转换为Torch Script，必须将模型的实例以及示例输入传递给torch.jit.trace 函数。这将产生一个torch.jit.ScriptModule对象，该对象的模型评估轨迹将嵌入在模块的forward方法中： import torch import torchvision # An instance of your model. model = torchvision.models.resnet18() # An example input you would normally provide to your model's forward() method. example = torch.rand(1, 3, 224, 224) # Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing. traced_script_module = torch.jit.trace(model, example) ScriptModule现在可以与常规PyTorch模块相同地评估被跟踪的对象： In[1]: output = traced_script_module(torch.ones(1, 3, 224, 224)) In[2]: output[0, :5] Out[2]: tensor([-0.2698, -0.0381, 0.4023, -0.3010, -0.0448], grad_fn=) 通过Annotation将Model转换为Torch Script 在某些情况下，例如，如果模型使用特定形式的控制流，如果想要直接在Torch Script中编写模型并相应地标注(annotate)模型。例如，假设有以下普通的 Pytorch模型： import torch class MyModule(torch.nn.Module): def __init__(self, N, M): super(MyModule, self).__init__() self.weight = torch.nn.Parameter(torch.rand(N, M)) def forward(self, input): if input.sum() > 0: output = self.weight.mv(input) else: output = self.weight + input return output 因为forward此模块的方法使用取决于输入的控制流，所以它不适合跟踪。相反，我们可以将其转换为ScriptModule。为了将模块转换为ScriptModule，需要按以下方式编译模块torch.jit.script： class MyModule(torch.nn.Module): def __init__(self, N, M): super(MyModule, self).__init__() self.weight = torch.nn.Parameter(torch.rand(N, M)) def forward(self, input): if input.sum() > 0: output = self.weight.mv(input) else: output = self.weight + input return output my_module = MyModule(10,20) sm = torch.jit.script(my_module) 如果您需要排除某些方法，nn.Module 因为它们使用的是TorchScript不支持的Python功能，则可以使用以下方法注释这些方法@torch.jit.ignore my_module是ScriptModule已经准备好进行序列化的实例 。 步骤2：将脚本模块序列化为文件 一旦有了对ScriptModule PyTorch模型的跟踪或注释，就可以将其序列化为文件了。稍后，您将能够使用C++从此文件加载模块并执行它，而无需依赖Python。假设我们要序列化ResNet18先前在跟踪示例中显示的模型。要执行此序列化，只需 在模块上调用 save 并传递一个文件名即可： traced_script_module.save(\"traced_resnet_model.pt\") 这将traced_resnet_model.pt在您的工作目录中生成一个文件。如果您还想序列化my_module，请回调my_module.save(\"my_module_model.pt\")我们现在已经正式离开Python领域，并准备跨入C ++领域。 步骤3：在C++中加载脚本模块 要在C ++中加载序列化的PyTorch模型，您的应用程序必须依赖于 PyTorch C++ API（也称为LibTorch）。LibTorch发行版包含共享库，头文件和CMake构建配置文件的集合。虽然CMake不是依赖LibTorch的要求，但它是推荐的方法，并且将来会得到很好的支持。对于本教程，我们将使用CMake和LibTorch构建一个最小的C ++应用程序，该应用程序简单地加载并执行序列化的PyTorch模型。 最小的C ++应用程序 让我们从讨论加载模块的代码开始。以下将已经做： #include // One-stop header. #include #include int main(int argc, const char* argv[]) { if (argc != 2) { std::cerr \\n\"; return -1; } torch::jit::script::Module module; try { // Deserialize the ScriptModule from a file using torch::jit::load(). module = torch::jit::load(argv[1]); } catch (const c10::Error& e) { std::cerr 该首标包括由运行示例所必需的库LibTorch所有相关包括。我们的应用程序接受序列化的PyTorch的文件路径ScriptModule作为其唯一的命令行参数，然后使用该torch::jit::load()函数继续反序列化该模块，该函数将该文件路径作为输入。作为回报，我们收到一个torch::jit::script::Module对象。我们将稍后讨论如何执行它。 取决于LibTorch和构建应用程序 假设我们将上述代码存储到名为的文件中example-app.cpp。最小CMakeLists.txt的构建看起来可能很简单： cmake_minimum_required(VERSION 3.0 FATAL_ERROR) project(custom_ops) find_package(Torch REQUIRED) add_executable(example-app example-app.cpp) target_link_libraries(example-app \"${TORCH_LIBRARIES}\") set_property(TARGET example-app PROPERTY CXX_STANDARD 11) 建立示例应用程序的最后一件事是LibTorch发行版。您可以随时从PyTorch网站的下载页面上获取最新的稳定版本。如果下载并解压缩最新的归档文件，则应收到具有以下目录结构的文件夹： libtorch/ bin/ include/ lib/ share/ lib/ 文件夹包含您必须链接的共享库， include/ 文件夹包含程序需要包含的头文件， share/ 文件夹包含必要的CMake配置，以启用find_package(Torch)上面的简单命令。 小贴士 在Windows上，调试和发行版本不兼容ABI。如果您打算以调试模式构建项目，请尝试使用LibTorch的调试版本。 最后一步是构建应用程序。为此，假定示例目录的布局如下： example-app/ CMakeLists.txt example-app.cpp 现在，我们可以运行以下命令从example-app/文件夹中构建应用程序 ： mkdir build cd build cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch .. make 这里/path/to/libtorch应该是解压的LibTorch分布的完整路径。如果一切顺利，它将看起来像这样： root@4b5a67132e81:/example-app# mkdir build root@4b5a67132e81:/example-app# cd build root@4b5a67132e81:/example-app/build# cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch .. -- The C compiler identification is GNU 5.4.0 -- The CXX compiler identification is GNU 5.4.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Configuring done -- Generating done -- Build files have been written to: /example-app/build root@4b5a67132e81:/example-app/build# make Scanning dependencies of target example-app [ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o [100%] Linking CXX executable example-app [100%] Built target example-app 如果将生成的跟踪ResNet18模型的路径提供给traced_resnet_model.pt生成的example-app二进制文件，我们应该得到友好的“确定”。请注意，如果尝试与my_module_model.pt您一起运行此示例，则会收到一条错误消息，提示您输入的形状不兼容。my_module_model.pt期望使用1D而不是4D。 root@4b5a67132e81:/example-app/build# ./example-app /traced_resnet_model.pt ok 步骤4：在C ++中执行脚本模块 成功加载了ResNet18用C++编写的序列化代码后，现在离执行它仅几行代码了！让我们将这些行添加到C++应用程序的main()函数中： // Create a vector of inputs. std::vector inputs; inputs.push_back(torch::ones({1, 3, 224, 224})); // Execute the model and turn its output into a tensor. at::Tensor output = module.forward(inputs).toTensor(); std::cout 前两行设置了我们模型的输入。我们创建一个向量 torch::jit::IValue（类型擦除的值类型script::Module方法接受并返回），并添加单个输入。要创建输入张量，我们使用 torch::ones()，等效torch.ones于C++ API。然后，我们运行script::Module的forward方法，并将创建的输入向量传递给它。作为回报，我们得到一个新的IValue，通过调用将其转换为张量toTensor()。 小贴士 要大致了解诸如torch::ones PyTorch C ++ API之类的功能，请参阅 https://pytorch.org/cppdocs 上的文档。PyTorch C ++ API提供了与Python API几乎相同的功能奇偶校验，使您可以像在Python中一样进一步操纵和处理张量。 在最后一行中，我们打印输出的前五个条目。由于在本教程前面的部分中，我们向Python中的模型提供了相同的输入，因此理想情况下，我们应该看到相同的输出。让我们通过重新编译应用程序并使用相同的序列化模型运行它来进行尝试： Scanning dependencies of target example-app [ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o [100%] Linking CXX executable example-app [100%] Built target example-app root@4b5a67132e81:/example-app/build# ./example-app traced_resnet_model.pt -0.2698 -0.0381 0.4023 -0.3010 -0.0448 [ Variable[CPUFloatType]{1,5} ] 作为参考，之前Python代码的输出是： tensor([-0.2698, -0.0381, 0.4023, -0.3010, -0.0448], grad_fn=) 看起来很不错！ 小贴士 要将模型移至GPU内存，可以编写model.to(at::kCUDA);。通过调用来确保模型的输入也位于CUDA内存中tensor.to(at::kCUDA)，这将在CUDA内存中返回新的张量。 步骤5：获取帮助并探索API 希望本教程使您对PyTorch模型从Python到C++的路径有一个大致的了解。使用本教程中描述的概念，您应该能够从原始的“eager”的PyTorch模型，ScriptModule用Python 编译，在磁盘上序列化的文件，以及（关闭循环）到script::Module C++ 的可执行文件。 当然，有许多我们没有介绍的概念。例如，您可能会发现自己想要扩展ScriptModule使用C++或CUDA中实现的自定义运算符，并希望ScriptModule在纯C++生产环境中加载的内部执行此自定义运算符 。好消息是：这是可能的，并且得到了很好的支持！现在，您可以浏览此文件夹中的示例，我们将很快提供一个教程。目前，以下链接通常可能会有所帮助： Torch Script 参考: https://pytorch.org/docs/master/jit.html PyTorch C++ API 文档: https://pytorch.org/cppdocs/ PyTorch Python API 文档: https://pytorch.org/docs/ 与往常一样，如果您遇到任何问题或疑问，可以使用我们的 论坛或GitHub问题进行联系。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"advanced/super_resolution_with_onnxruntime.html":{"url":"advanced/super_resolution_with_onnxruntime.html","title":"（可选）将模型从PyTorch导出到ONNX并使用ONNX Runtime运行    ","keywords":"","body":"4.（可选）从导出到PyTorch一个ONNX模型并使用运行它ONNX运行时 在本教程中，我们将介绍如何在PyTorch定义的模型转换成ONNX格式，然后用ONNX运行时运行它。 ONNX运行时是ONNX模型，跨多个平台和硬件（在Windows，Linux和Mac和两个CPU和GPU）有效地推论一个注重性能的发动机。 ONNX运行时已被证明大大增加了多种型号的性能，解释此处 在本教程中，你需要安装 ONNX 和[ ONNX运行[HTG3。你可以得到的二进制建立ONNX和ONNX运行与点子 安装 onnx onnxruntime [HTG13。需要注意的是ONNX运行与Python版本3.5到3.7兼容。](https://github.com/microsoft/onnxruntime) 注：本教程需要PyTorch主分支可通过以下的说明这里被安装 # Some standard imports import io import numpy as np from torch import nn import torch.utils.model_zoo as model_zoo import torch.onnx 超分辨率越来越多的图像，视频分辨率的方式，被广泛应用于图像处理和视频编辑。在本教程中，我们将使用一个小的超分辨率模型。 首先，让我们创建一个PyTorch超分辨模型。该模型采用在中描述的“实时单幅图像和视频超分辨率采用高效的子像素卷积神经网络”的高效子像素卷积层 - 石等用于提高图像的分辨率由高档的因素。该模型预计的图像作为输入的所述YCbCr的Y成分，并且输出在超分辨率放大的Y分量。 该模型直接从PyTorch的例子来不加修改： # Super Resolution model definition in PyTorch import torch.nn as nn import torch.nn.init as init class SuperResolutionNet(nn.Module): def __init__(self, upscale_factor, inplace=False): super(SuperResolutionNet, self).__init__() self.relu = nn.ReLU(inplace=inplace) self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2)) self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1)) self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1)) self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1)) self.pixel_shuffle = nn.PixelShuffle(upscale_factor) self._initialize_weights() def forward(self, x): x = self.relu(self.conv1(x)) x = self.relu(self.conv2(x)) x = self.relu(self.conv3(x)) x = self.pixel_shuffle(self.conv4(x)) return x def _initialize_weights(self): init.orthogonal_(self.conv1.weight, init.calculate_gain('relu')) init.orthogonal_(self.conv2.weight, init.calculate_gain('relu')) init.orthogonal_(self.conv3.weight, init.calculate_gain('relu')) init.orthogonal_(self.conv4.weight) # Create the super-resolution model by using the above model definition. torch_model = SuperResolutionNet(upscale_factor=3) 通常情况下，你现在会训练这个模型;然而，在本教程中，我们反而会下载一些预训练的权重。请注意，这种模式并没有良好的精度全面培训，在这里仅用于演示目的。 它调用torch_model.eval（）或torch_model.train（假）导出模型前，把该模型是非常重要的推论模式。既然喜欢在不同的推断和训练模式辍学或batchnorm运营商的行为，这是必需的。 # Load pretrained model weights model_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth' batch_size = 1 # just a random number # Initialize model with the pretrained weights map_location = lambda storage, loc: storage if torch.cuda.is_available(): map_location = None torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location)) # set the model to inference mode torch_model.eval() 导出在PyTorch模型通过跟踪或脚本作品。这个教程将作为一个例子使用由跟踪导出的模型。要导出模型，我们称之为torch.onnx.export（）功能。这将执行模式，记录的是什么运营商来计算输出跟踪。因为出口运行模型，我们需要提供一个输入张量× [HTG11。只要它是正确的类型和尺寸在此的值可以是随机的。注意，输入尺寸将被固定在导出ONNX图形用于将输入的所有维的，除非指定为动态轴。在这个例子中，我们用的batch_size 1的输入导出模型，但然后指定所述第一尺寸为动态在dynamic_axes参数torch.onnx.export （） 。由此导出的模型将接受尺寸的输入[batch_size时，1，224，224]，其中的batch_size可以是可变的。 要了解PyTorch的出口接口的详细信息，请查看[ torch.onnx文献HTG1。 # Input to the model x = torch.randn(batch_size, 1, 224, 224, requires_grad=True) torch_out = torch_model(x) # Export the model torch.onnx.export(torch_model, # model being run x, # model input (or a tuple for multiple inputs) \"super_resolution.onnx\", # where to save the model (can be a file or file-like object) export_params=True, # store the trained parameter weights inside the model file opset_version=10, # the ONNX version to export the model to do_constant_folding=True, # wether to execute constant folding for optimization input_names = ['input'], # the model's input names output_names = ['output'], # the model's output names dynamic_axes={'input' : {0 : 'batch_size'}, # variable lenght axes 'output' : {0 : 'batch_size'}}) 我们还计算torch_out，该模型，我们将用它来验证ONNX运行中运行时，我们出口的模型计算相同的值后输出。 但在验证模型与ONNX运行时输出之前，我们将检查与ONNX的API的ONNX模型。首先，onnx.load（“super_resolution.onnx”）将加载保存的模型和将输出一个onnx.ModelProto结构（用于捆绑一个ML一个顶层文件/容器格式模型。详细信息 onnx.proto文档）。然后，onnx.checker.check_model（onnx_model）HTG8]将验证模型的结构，并确认该模型有一个有效的模式。所述ONNX图表的有效性是通过检查模型的版本，图的结构，以及作为节点，其输入和输出验证。 import onnx onnx_model = onnx.load(\"super_resolution.onnx\") onnx.checker.check_model(onnx_model) 现在，让我们计算使用ONNX运行的Python的API的输出。这一部分通常可以在一个单独的进程或另一台机器上完成，但我们会继续以同样的过程，使我们可以验证ONNX运行和PyTorch被计算为网络相同的值。 为了运行与ONNX运行模式，我们需要与所选择的配置参数（在这里我们使用默认配置）创建模型推断会话。一旦会话创建，我们评估使用的run（）API模型。这个调用的输出是含有ONNX运行时计算出的模型的输出列表。 import onnxruntime ort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\") def to_numpy(tensor): return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy() # compute ONNX Runtime output prediction ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)} ort_outs = ort_session.run(None, ort_inputs) # compare ONNX Runtime and PyTorch results np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05) print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\") 我们应该看到，PyTorch和ONNX运行时的输出数值上运行，与之相匹配的给定精度（RTOL = 1E-03和蒂= 1E-05）。作为一个侧面说明，如果他们不匹配，则有在ONNX出口的问题，请与我们联系在这种情况下。 运行使用图像上的模型ONNX运行时 到目前为止，我们已经从PyTorch导出的模型，并展示了如何加载和运行ONNX与伪张量作为输入运行它。 在本教程中，我们将使用广泛使用的一个著名的猫形象，它看起来像下面 首先，让我们使用标准的PIL Python库加载图像，预先对其进行处理。请注意，这是预处理的数据处理培训/测试神经网络的标准做法。 我们首先调整图像的大小，以适应模型的输入（224x224）的大小。然后我们图象分成了Y，Cb和Cr分量。这些组件代表灰度图像（Y）和蓝色差（Cb）和红色差（Cr）的色度分量。 Y分量是对人眼更敏感，我们感兴趣的是这部分，我们将改造。提取Y分量后，我们把它转换成这将是我们模型的输入张量。 from PIL import Image import torchvision.transforms as transforms img = Image.open(\"./_static/img/cat.jpg\") resize = transforms.Resize([224, 224]) img = resize(img) img_ycbcr = img.convert('YCbCr') img_y, img_cb, img_cr = img_ycbcr.split() to_tensor = transforms.ToTensor() img_y = to_tensor(img_y) img_y.unsqueeze_(0) 现在，作为下一步，让我们代表灰度调整猫形象的张量和运行ONNX运行超高分辨率模型如前所述。 ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)} ort_outs = ort_session.run(None, ort_inputs) img_out_y = ort_outs[0] 在这一点上，该模型的输出是一个张量。现在，我们将处理模型的输出从输出张建设回来的最终输出图像，并保存图像。后处理步骤已经从PyTorch实现超高分辨率模型此处采用。 img_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode='L') # get the output image follow post-processing step from PyTorch implementation final_img = Image.merge( \"YCbCr\", [ img_out_y, img_cb.resize(img_out_y.size, Image.BICUBIC), img_cr.resize(img_out_y.size, Image.BICUBIC), ]).convert(\"RGB\") # Save the image, we will compare this with the output image from mobile device final_img.save(\"./_static/img/cat_superres_with_ort.jpg\") ONNX运行时是一个跨平台的引擎，可以跨多个平台和两个CPU和GPU运行它。 ONNX运行时也可以部署到云中使用Azure的机器学习Services模型推理。更多信息[此处HTG1。 关于ONNX运行时的性能此处更多信息。 有关ONNX运行此处更多信息。 脚本的总运行时间： （0分钟0.000秒） Download Python source code: super_resolution_with_onnxruntime.py Download Jupyter notebook: super_resolution_with_onnxruntime.ipynb 通过斯芬克斯-廊产生廊 Next Previous Was this helpful? Yes No Thank you ©版权所有2017年，PyTorch。 [HTG0 （可选）将模型从PyTorch导出到ONNX并使用ONNX Runtime运行 运行使用ONNX运行时的图像上的模型 ![](https://www.facebook.com/tr?id=243028289693773&ev=PageView &noscript=1) 分析流量和优化经验，我们为这个站点的Cookie。通过点击或导航，您同意我们的cookies的使用。因为这个网站目前维护者，Facebook的Cookie政策的适用。了解更多信息，包括有关可用的控制：[饼干政策HTG1。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"intermediate/model_parallel_tutorial.html":{"url":"intermediate/model_parallel_tutorial.html","title":"模型并行化最佳实践","keywords":"","body":"模型并行化最佳实践 作者: Shen Li 译者: Hamish 模型并行化在分布式训练技术中被广泛使用。先前的文章已经解释了如何使用DataParallel在多个GPU上训练神经网络。此功能将相同的模型复制到所有GPU，其中每个GPU负责消化输入数据的不同部分。尽管它可以极大地加快训练过程，但不适用于某些模型太大而无法被单个GPU容纳的用例。这篇文章展示了如何通过使用模型化来解决该问题，与DataParallel相比，模型并行化将单个模型拆分到不同的GPU上，而不是在每个GPU上复制整个模型（具体来说，模型m包含10层：使用DataParallel时，每个GPU都具有这10层中每个层的副本，而当在两个GPU上使用模型并行化时，每个GPU可以承载5层。 模型并行化的高级思想是将模型的不同子网络放置在不同的设备上，并相应地实现forward方法，以在设备之间传递中间输出。由于只需模型的一部分就能能在任何独立设备上运行，因此一组设备可以共同为更大的模型服务。在本文中，我们不会尝试构建庞大的模型并将其压缩到有限数量的GPU中。相反，本文着重展示模型并行化的思想。读者可以将这些想法应用到实际应用中。 基本用法 让我们从包含两个线性层的玩具模型开始。要在两个GPU上运行此模型，只需将每个线性层放在不同的GPU上，然后将输入和中间输出传递到匹配的层设备。 import torch import torch.nn as nn import torch.optim as optim class ToyModel(nn.Module): def __init__(self): super(ToyModel, self).__init__() self.net1 = torch.nn.Linear(10, 10).to('cuda:0') self.relu = torch.nn.ReLU() self.net2 = torch.nn.Linear(10, 5).to('cuda:1') def forward(self, x): x = self.relu(self.net1(x.to('cuda:0'))) return self.net2(x.to('cuda:1')) 请注意，除去五个to(device)调用将线性层和张量放置在适当的设备上之外，上面的ToyModel看起来非常类似于在单个GPU上实现它的方式。那是模型中唯一需要更改的地方。backward()和torch.optim将自动处理梯度，就像模型在一个GPU上一样。你只需确保调用损失函数时标签与输出在同一设备上。 model = ToyModel() loss_fn = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.001) optimizer.zero_grad() outputs = model(torch.randn(20, 10)) labels = torch.randn(20, 5).to('cuda:1') loss_fn(outputs, labels).backward() optimizer.step() 在现有模块上应用模型并行化 只需更改几行，就可以在多个GPU上运行现有的单GPU模块。以下代码显示了如何将torchvision.models.reset50()分解到两个GPU上。基本想法是从现有的ResNet模块继承，并在构建过程中将层划分到两个GPU。然后，通过传递对应的中间输出，覆写forward方法以将两个子网络拼合。 from torchvision.models.resnet import ResNet, Bottleneck num_classes = 1000 class ModelParallelResNet50(ResNet): def __init__(self, *args, **kwargs): super(ModelParallelResNet50, self).__init__( Bottleneck, [3, 4, 6, 3], num_classes=num_classes, *args, **kwargs) self.seq1 = nn.Sequential( self.conv1, self.bn1, self.relu, self.maxpool, self.layer1, self.layer2 ).to('cuda:0') self.seq2 = nn.Sequential( self.layer3, self.layer4, self.avgpool, ).to('cuda:1') self.fc.to('cuda:1') def forward(self, x): x = self.seq2(self.seq1(x).to('cuda:1')) return self.fc(x.view(x.size(0), -1)) 对于模型太大而无法放入单个GPU的情况，上述实现解决了该问题。但是，你可能已经注意到，if your model fits，这一实现将比在单个GPU上运行还要慢。这是因为在任何时间点，两个GPU中只有一个在工作，而另一个在那儿什么也没做。而由于中间输出需要在layer2和layer3之间从cuda:0复制到cuda:1，这导致性能进一步下降。 让我们进行实验以更定量地了解执行时间。在本实验中，我们通过运行随机输入和标签来训练ModelParallelResNet50和现有的torchvision.models.reset50()。训练后，模型不会产生任何有用的预测，但是我们可以对执行时间有一个合理的了解。 import torchvision.models as models num_batches = 3 batch_size = 120 image_w = 128 image_h = 128 def train(model): model.train(True) loss_fn = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.001) one_hot_indices = torch.LongTensor(batch_size) \\ .random_(0, num_classes) \\ .view(batch_size, 1) for _ in range(num_batches): # generate random inputs and labels inputs = torch.randn(batch_size, 3, image_w, image_h) labels = torch.zeros(batch_size, num_classes) \\ .scatter_(1, one_hot_indices, 1) # run forward pass optimizer.zero_grad() outputs = model(inputs.to('cuda:0')) # run backward pass labels = labels.to(outputs.device) loss_fn(outputs, labels).backward() optimizer.step() 上述train(model)方法使用nn.MSELoss作为损失函数，并使用optim.SGD作为优化器。它模拟了对128 X 128图像的训练，这些图像分为3个batch，每个batch包含120幅图像。然后，我们使用timeit来运行train(model)方法10次，并绘制带有标准差的执行时间。 import matplotlib.pyplot as plt plt.switch_backend('Agg') import numpy as np import timeit num_repeat = 10 stmt = \"train(model)\" setup = \"model = ModelParallelResNet50()\" # globals arg is only available in Python 3. In Python 2, use the following # import __builtin__ # __builtin__.__dict__.update(locals()) mp_run_times = timeit.repeat( stmt, setup, number=1, repeat=num_repeat, globals=globals()) mp_mean, mp_std = np.mean(mp_run_times), np.std(mp_run_times) setup = \"import torchvision.models as models;\" + \\ \"model = models.resnet50(num_classes=num_classes).to('cuda:0')\" rn_run_times = timeit.repeat( stmt, setup, number=1, repeat=num_repeat, globals=globals()) rn_mean, rn_std = np.mean(rn_run_times), np.std(rn_run_times) def plot(means, stds, labels, fig_name): fig, ax = plt.subplots() ax.bar(np.arange(len(means)), means, yerr=stds, align='center', alpha=0.5, ecolor='red', capsize=10, width=0.6) ax.set_ylabel('ResNet50 Execution Time (Second)') ax.set_xticks(np.arange(len(means))) ax.set_xticklabels(labels) ax.yaxis.grid(True) plt.tight_layout() plt.savefig(fig_name) plt.close(fig) plot([mp_mean, rn_mean], [mp_std, rn_std], ['Model Parallel', 'Single GPU'], 'mp_vs_rn.png') 结果表明，模型并行实现的执行时间比现有的单GPU实现长4.02 / 3.75-1 = 7％。因此，我们可以得出结论，在GPU之间来回复制张量大约有7％的开销。这里有改进的空间，因为我们知道两个GPU之一在整个执行过程中处于空闲状态。一种选择是将每个批次进一步划分为拆分流水线，以便当一个拆分到达第二子网络时，可以将下一个拆分投入第一子网络。这样，两个连续的拆分可以在两个GPU上同时运行。 用流水线输入加速 在以下实验中，我们将每个120图像batch进一步划分为20图像分割。当PyTorch异步启动CUDA操作时，该实现无需生成多个线程即可实现并发。 class PipelineParallelResNet50(ModelParallelResNet50): def __init__(self, split_size=20, *args, **kwargs): super(PipelineParallelResNet50, self).__init__(*args, **kwargs) self.split_size = split_size def forward(self, x): splits = iter(x.split(self.split_size, dim=0)) s_next = next(splits) s_prev = self.seq1(s_next).to('cuda:1') ret = [] for s_next in splits: # A. s_prev runs on cuda:1 s_prev = self.seq2(s_prev) ret.append(self.fc(s_prev.view(s_prev.size(0), -1))) # B. s_next runs on cuda:0, which can run concurrently with A s_prev = self.seq1(s_next).to('cuda:1') s_prev = self.seq2(s_prev) ret.append(self.fc(s_prev.view(s_prev.size(0), -1))) return torch.cat(ret) setup = \"model = PipelineParallelResNet50()\" pp_run_times = timeit.repeat( stmt, setup, number=1, repeat=num_repeat, globals=globals()) pp_mean, pp_std = np.mean(pp_run_times), np.std(pp_run_times) plot([mp_mean, rn_mean, pp_mean], [mp_std, rn_std, pp_std], ['Model Parallel', 'Single GPU', 'Pipelining Model Parallel'], 'mp_vs_rn_vs_pp.png') 请注意，设备到设备的张量复制操作在源设备和目标设备上的当前线程上同步。如果创建多个线程，则必须确保复制操作正确同步。在完成复制操作之前写入源张量或读取/写入目标张量可能导致不确定的行为。上面的实现仅在源设备和目标设备上都使用默认线程，因此没有必要另外强制执行同步。 实验结果表明，对并行ResNet50进行建模的流水线输入可将训练过程大致加快3.75 / 2.51-1 = 49％。距离理想的100％加速还有很长的路要走。由于我们在管道并行实现中引入了新参数split_sizes，因此尚不清楚新参数如何影响整体训练时间。直观地讲，使用较小的split_size会导致许多小的CUDA内核启动，而使用较大的split_size结果会导致在第一次和最后一次拆分期间有较长的空闲时间。都不是最佳选择。对于此特定实验，可能会有最佳的split_size配置。让我们尝试通过使用几个不同的split_size值进行实验来找到它。 means = [] stds = [] split_sizes = [1, 3, 5, 8, 10, 12, 20, 40, 60] for split_size in split_sizes: setup = \"model = PipelineParallelResNet50(split_size=%d)\" % split_size pp_run_times = timeit.repeat( stmt, setup, number=1, repeat=num_repeat, globals=globals()) means.append(np.mean(pp_run_times)) stds.append(np.std(pp_run_times)) fig, ax = plt.subplots() ax.plot(split_sizes, means) ax.errorbar(split_sizes, means, yerr=stds, ecolor='red', fmt='ro') ax.set_ylabel('ResNet50 Execution Time (Second)') ax.set_xlabel('Pipeline Split Size') ax.set_xticks(split_sizes) ax.yaxis.grid(True) plt.tight_layout() plt.savefig(\"split_size_tradeoff.png\") plt.close(fig) 结果表明，将split_size设置为12可获得最快的训练速度，从而导致3.75 / 2.43-1 = 54％的加速比。仍有机会进一步加快训练过程。例如，对cuda：0的所有操作都放在其默认线程上。这意味着下一个拆分的计算不能与上一个拆分的复制操作重叠。但是，由于上一个和下一个拆分是不同的张量，因此将一个的计算与另一个的赋值操作重叠是没有问题的。实现需要在两个GPU上使用多个线程，并且不同的子网络结构需要不同的线程管理策略。由于没有通用的多线程解决方案适用于所有模型并行化用例，因此在本教程中将不再讨论。 注： 这篇文章显示了几个性能指标。在您自己的计算机上运行相同的代码时，您可能会看到不同的数字，因为结果取决于底层的硬件和软件。为了使您的环境获得最佳性能，一种正确的方法是首先生成曲线以找出最佳split_size，然后将该split_size用于流水线输入。 脚本的总运行时间： （5分钟51.519秒） 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"intermediate/ddp_tutorial.html":{"url":"intermediate/ddp_tutorial.html","title":"入门分布式数据并行","keywords":"","body":"分布式数据并行(DDP)入门 作者：Shen Li 译者：Hamish DistributedDataParallel(DDP)在模块级别实现数据并行性。它使用torch.distributed包中的通信集合体来同步梯度，参数和缓冲区。并行性在流程内和跨流程均可用。在一个过程中，DDP将输入模块复制到device_ids中指定的设备，相应地沿批处理维度分散输入，并将输出收集到output_device，这与DataParallel相似。在整个过程中，DDP在正向传递中插入必要的参数同步，在反向传递中插入梯度同步。用户可以将进程映射到可用资源，只要进程不共享GPU设备即可。推荐的方法（通常是最快的方法）是为每个模块副本创建一个过程，即在一个过程中不进行任何模块复制。本教程中的代码在8-GPU服务器上运行，但可以轻松地推广到其他环境。 DataParallel和DistributedDataParallel之间的比较 在深入研究之前，让我们澄清一下为什么，尽管增加了复杂性，您还是会考虑使用DistributedDataParallel而不是DataParallel： 首先，回想一下之前的教程，如果模型太大，无法被单个GPU容纳，则必须使用模型并行化将其拆分至多个GPU。DistributedDataParallel可以与模型并行化一起工作；DataParallel此时不工作。 DataParallel是单进程、多线程的，并且只在一台机器上工作；而DistributedDataParallel是多进程的，可用于单机和多机训练。因此，即使对于单机训练，数据足够小，可以放在一台机器上，DistributedDataParallel也会比DataParallel更快。DistributedDataParallel还可以预先复制模型，而不是在每次迭代时复制模型，从而可以避免全局解释器锁定。 如果您的数据太大，无法在一台机器上容纳，并且您的模型也太大，无法在单个GPU上容纳，则可以将模型并行化（跨多个GPU拆分单个模型）与DistributedDataParallel结合起来。在这种机制下，每个DistributedDataParallel进程都可以使用模型并行化，同时所有进程都可以使用数据并行。 基本用例 要创建DDP模块，请首先正确设置进程组。更多细节可以在使用PyTorch编写分布式应用程序中找到。 import os import tempfile import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel as DDP def setup(rank, world_size): os.environ['MASTER_ADDR'] = 'localhost' os.environ['MASTER_PORT'] = '12355' # initialize the process group dist.init_process_group(\"gloo\", rank=rank, world_size=world_size) # Explicitly setting seed to make sure that models created in two processes # start from same random weights and biases. torch.manual_seed(42) def cleanup(): dist.destroy_process_group() 现在，让我们创建一个玩具模块，用DDP包装它，并用一些虚拟输入数据给它输入。请注意，如果训练是从随机参数开始的，您可能需要确保所有DDP进程使用相同的初始值。否则，全局梯度同步将没有意义。 class ToyModel(nn.Module): def __init__(self): super(ToyModel, self).__init__() self.net1 = nn.Linear(10, 10) self.relu = nn.ReLU() self.net2 = nn.Linear(10, 5) def forward(self, x): return self.net2(self.relu(self.net1(x))) def demo_basic(rank, world_size): setup(rank, world_size) # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and # rank 2 uses GPUs [4, 5, 6, 7]. n = torch.cuda.device_count() // world_size device_ids = list(range(rank * n, (rank + 1) * n)) # create model and move it to device_ids[0] model = ToyModel().to(device_ids[0]) # output_device defaults to device_ids[0] ddp_model = DDP(model, device_ids=device_ids) loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) optimizer.zero_grad() outputs = ddp_model(torch.randn(20, 10)) labels = torch.randn(20, 5).to(device_ids[0]) loss_fn(outputs, labels).backward() optimizer.step() cleanup() def run_demo(demo_fn, world_size): mp.spawn(demo_fn, args=(world_size,), nprocs=world_size, join=True) 如您所见，DDP包装了较低级别的分布式通信细节，并提供了一个干净的API，就好像它是一个本地模型一样。对于基本用例，DDP只需要几个loc来设置流程组。在将DDP应用于更高级的用例时，需要注意一些注意事项。 不均衡的处理速度 在DDP中，构造函数、前向方法和输出的微分是分布式同步点。不同的进程将以相同的顺序到达同步点，并在大致相同的时间进入每个同步点。否则，快速进程可能会提前到达，并在等待散乱的进程时超时。因此，用户需要负责跨进程平衡工作负载的分配。有时，由于网络延迟、资源竞争、不可预测的工作量高峰，不均衡的处理速度是不可避免的。要避免在这些情况下超时，请确保在调用init_process_group时传递足够大的timeout值。 保存和载入检查点 在训练过程中，经常使用torch.save和torch.load为模块创建检查点，以及从检查点恢复。有关的详细信息，请参见保存和加载模型。在使用DDP时，一种优化方法是只在一个进程中保存模型，然后将其加载到所有进程中，从而减少写开销。这是正确的，因为所有进程都是从相同的参数开始的，并且梯度在反向过程中是同步的，因此优化器应该将参数设置为相同的值。如果使用这种优化方法，请确保在保存完成之前，所有进程都不会开始加载。此外，加载模块时，需要提供适当的map_location参数，以防止进程进入其他设备。如果缺少map_location，torch.load将首先将模块加载到CPU，然后将每个参数复制到其保存的位置，这将导致同一台计算机上的所有进程使用同一组设备。 def demo_checkpoint(rank, world_size): setup(rank, world_size) # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and # rank 2 uses GPUs [4, 5, 6, 7]. n = torch.cuda.device_count() // world_size device_ids = list(range(rank * n, (rank + 1) * n)) model = ToyModel().to(device_ids[0]) # output_device defaults to device_ids[0] ddp_model = DDP(model, device_ids=device_ids) loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\" if rank == 0: # All processes should see same parameters as they all start from same # random parameters and gradients are synchronized in backward passes. # Therefore, saving it in one process is sufficient. torch.save(ddp_model.state_dict(), CHECKPOINT_PATH) # Use a barrier() to make sure that process 1 loads the model after process # 0 saves it. dist.barrier() # configure map_location properly rank0_devices = [x - rank * len(device_ids) for x in device_ids] device_pairs = zip(rank0_devices, device_ids) map_location = {'cuda:%d' % x: 'cuda:%d' % y for x, y in device_pairs} ddp_model.load_state_dict( torch.load(CHECKPOINT_PATH, map_location=map_location)) optimizer.zero_grad() outputs = ddp_model(torch.randn(20, 10)) labels = torch.randn(20, 5).to(device_ids[0]) loss_fn = nn.MSELoss() loss_fn(outputs, labels).backward() optimizer.step() # Use a barrier() to make sure that all processes have finished reading the # checkpoint dist.barrier() if rank == 0: os.remove(CHECKPOINT_PATH) cleanup() 结合DDP与模型并行化 DDP也适用于多GPU模型，但不支持进程内的复制。您需要为每个模块副本创建一个进程，这通常会比每个进程创建多个副本带来更好的性能。当使用大量数据训练大型模型时，DDP包装多GPU模型尤其有用。使用此功能时，需要小心地实现多GPU模型，以避免硬编码设备，因为不同的模型副本将被放置到不同的设备上。 class ToyMpModel(nn.Module): def __init__(self, dev0, dev1): super(ToyMpModel, self).__init__() self.dev0 = dev0 self.dev1 = dev1 self.net1 = torch.nn.Linear(10, 10).to(dev0) self.relu = torch.nn.ReLU() self.net2 = torch.nn.Linear(10, 5).to(dev1) def forward(self, x): x = x.to(self.dev0) x = self.relu(self.net1(x)) x = x.to(self.dev1) return self.net2(x) 将多GPU模型传递给DDP时，不能设置device_ids和output_device。输入和输出数据将由应用程序或模型forward()方法放置在适当的设备中。 def demo_model_parallel(rank, world_size): setup(rank, world_size) # setup mp_model and devices for this process dev0 = rank * 2 dev1 = rank * 2 + 1 mp_model = ToyMpModel(dev0, dev1) ddp_mp_model = DDP(mp_model) loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001) optimizer.zero_grad() # outputs will be on dev1 outputs = ddp_mp_model(torch.randn(20, 10)) labels = torch.randn(20, 5).to(dev1) loss_fn(outputs, labels).backward() optimizer.step() cleanup() if __name__ == \"__main__\": run_demo(demo_basic, 2) run_demo(demo_checkpoint, 2) if torch.cuda.device_count() >= 8: run_demo(demo_model_parallel, 4) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"intermediate/dist_tuto.html":{"url":"intermediate/dist_tuto.html","title":"PyTorch编写分布式应用","keywords":"","body":"3. PyTorch编写分布式应用程序 作者 ： SEB阿诺德 在这个简短的教程中，我们将要在PyTorch的分布式包。我们将看到如何建立分布式设置，使用不同的沟通策略，走在包装件的一些内部结构。 设定 包括在PyTorch分布式包（即，torch.distributed `）使研究人员和从业人员跨进程和机器的集群来容易并行化的计算。为了这样做，它利用了消息传递语义允许每个进程进行数据通信的任何其他进程的。而不是在多处理（ torch.multiprocessing`）包，过程可以使用不同的通信后端和不限于在同一台机器上被执行。 为了开始，我们需要同时运行多个流程的能力。如果你有机会到计算机集群，你应该用你的本地系统管理员检查或使用自己喜欢的协调工具。 （例如， PDSH ， clustershell 或人）对于本教程的目的，我们将使用一台机器和叉的多个进程使用以下模板。 \"\"\"run.py:\"\"\" #!/usr/bin/env python import os import torch import torch.distributed as dist from torch.multiprocessing import Process def run(rank, size): \"\"\" Distributed function to be implemented later. \"\"\" pass def init_processes(rank, size, fn, backend='tcp'): \"\"\" Initialize the distributed environment. \"\"\" os.environ['MASTER_ADDR'] = '127.0.0.1' os.environ['MASTER_PORT'] = '29500' dist.init_process_group(backend, rank=rank, world_size=size) fn(rank, size) if __name__ == \"__main__\": size = 2 processes = [] for rank in range(size): p = Process(target=init_processes, args=(rank, size, run)) p.start() processes.append(p) for p in processes: p.join() 上述脚本派生两个过程谁将每个设置的分布式环境中，初始化处理组（dist.init_process_group），最后执行运行给定的功能。 让我们来看看init_processes功能。它确保每一道工序将能够通过一个主协调，使用相同的IP地址和端口。请注意，我们使用的是TCP后端，但我们也可以使用 MPI 或 GLOO 代替。 （参见 5.1节），我们会在魔术dist.init_process_group在本教程的最后发生的事情，但它基本上可以让进程间通信其他通过分享他们的位置。 点对点通信 传送和recv 数据从一个处理A转移到另一个被称为点 - 点通信。这些通过取得的发送和的recv的功能或它们的 立即 反份，isend和irecv。 \"\"\"Blocking point-to-point communication.\"\"\" def run(rank, size): tensor = torch.zeros(1) if rank == 0: tensor += 1 # Send the tensor to process 1 dist.send(tensor=tensor, dst=1) else: # Receive tensor from process 0 dist.recv(tensor=tensor, src=0) print('Rank ', rank, ' has data ', tensor[0]) 在上面的例子，这两个过程以零开始张量，然后处理增量0张量，并将其发送到处理1，使得它们都结束了1.0。请注意，过程1只需要以存储将接收数据分配内存。 还要注意，发送/ 的recv是 阻断 ：两个过程停止，直到通信完成。在另一方面的立即被 非阻塞 ;脚本将继续其执行和方法都返回一个DistributedRequest对象后，我们可以选择等待（）。 \"\"\"Non-blocking point-to-point communication.\"\"\" def run(rank, size): tensor = torch.zeros(1) req = None if rank == 0: tensor += 1 # Send the tensor to process 1 req = dist.isend(tensor=tensor, dst=1) print('Rank 0 started sending') else: # Receive tensor from process 0 req = dist.irecv(tensor=tensor, src=0) print('Rank 1 started receiving') req.wait() print('Rank ', rank, ' has data ', tensor[0]) 当使用的立即，我们必须小心我们的发送和接收的张量的使用。因为我们不知道什么时候的数据将被传递给其它工艺做的，我们不应该修改发张量也不req.wait（）完成之前访问接收到的张量。换一种说法， 写张量 ``dist.isend后（）将导致未定义的行为。 从读取张量 ``dist.irecv后（）将导致未定义的行为。 然而，req.wait（）已被执行之后，我们保证了通信发生了，并且，存储在张量的值[0]是1.0。 点至点，当我们想在我们的流程的通信进行细粒度的控制通信是有益的。它们可以被用来实现花哨的算法，如百度的DeepSpeech 或[ Facebook的大规模实验HTG3。所使用的（c.f。第4.1节） 集体通信 散点图 | 收集 ---|--- 降低 | 全减少 广播 | 全收集 相对于点对点通信电子，集体允许对 组 中所有进程的通信模式。 A组是我们所有进程的一个子集。要创建一个组，我们可以通过职级为dist.new_group（集团）名单 [HTG5。默认情况下，集体的对所有进程执行，也被称为 **世界[HTG7。例如，为了获得在所有进程都张量的总和，我们可以使用dist.all_reduce（张量， 运算， 组） 集体。** \"\"\" All-Reduce example.\"\"\" def run(rank, size): \"\"\" Simple point-to-point communication. \"\"\" group = dist.new_group([0, 1]) tensor = torch.ones(1) dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group) print('Rank ', rank, ' has data ', tensor[0]) 既然我们要在组中的所有张量的总和，我们使用dist.reduce_op.SUM为降低运营商。一般来说，任何可交换的数学运算，可以作为运营商。外的开箱，PyTorch配备了4个这样的运营商，都在逐元素级别工作： dist.reduce_op.SUM dist.reduce_op.PRODUCT dist.reduce_op.MAX dist.reduce_op.MIN。 除了dist.all_reduce（张量， 运算， 组），有一个总的目前PyTorch实现6个集体。 dist.broadcast（张量， SRC， 组）：复制张量从SRC到所有其它过程。 dist.reduce（张量， DST， 运算， 组）：应用OP所有结果张量，并存储在DST。 dist.all_reduce（张量， 运算， 组）：同降低，但其结果被存储在所有进程。 dist.scatter（张量， SRC， scatter_list， 组）：复制 \\（ I ^ {\\文本{第}} \\）张量scatter_list [I]到 \\（I ^ {\\文本{第}} \\）过程。 dist.gather（张量， DST， gather_list， 组）：复制张量从DST所有进程。 dist.all_gather（tensor_list， 张量， 组）：复制张量从所有流程，以tensor_list上的所有进程。 dist.barrier（组）：块组的所有进程，直至每一个已经进入该功能。 分布式训练 注： 你可以在这个GitHub的库本节的示例脚本。 现在我们明白了分布式模块是如何工作的，让我们写的东西与它有用。我们的目标是复制的 DistributedDataParallel 的功能。当然，这将是一个说教的例子，在现实世界situtation你应该使用官方的，经过严格测试和精心优化的版本上面链接。 简单地说，我们要实现的随机梯度下降一个分布式的版本。我们的脚本将让所有的进程都计算在他们的批量数据的他们的模型的梯度，然后平均的梯度。为了改变进程的数目时，以确保类似的收敛结果，我们首先要分区我们的数据。 （你也可以使用 tnt.dataset.SplitDataset ，而不是片段下方。） \"\"\" Dataset partitioning helper \"\"\" class Partition(object): def __init__(self, data, index): self.data = data self.index = index def __len__(self): return len(self.index) def __getitem__(self, index): data_idx = self.index[index] return self.data[data_idx] class DataPartitioner(object): def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234): self.data = data self.partitions = [] rng = Random() rng.seed(seed) data_len = len(data) indexes = [x for x in range(0, data_len)] rng.shuffle(indexes) for frac in sizes: part_len = int(frac * data_len) self.partitions.append(indexes[0:part_len]) indexes = indexes[part_len:] def use(self, partition): return Partition(self.data, self.partitions[partition]) 通过上述片段中，我们现在可以简单地使用下面的几行分区中的任何数据集： \"\"\" Partitioning MNIST \"\"\" def partition_dataset(): dataset = datasets.MNIST('./data', train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])) size = dist.get_world_size() bsz = 128 / float(size) partition_sizes = [1.0 / size for _ in range(size)] partition = DataPartitioner(dataset, partition_sizes) partition = partition.use(dist.get_rank()) train_set = torch.utils.data.DataLoader(partition, batch_size=bsz, shuffle=True) return train_set, bsz 假设我们有2个副本，那么每个进程将具有train_set60000/2 = 30000个样本。我们还除以副本的数量批量大小，以保持的128 总体 批量大小。 现在，我们可以写我们通常前后，优化训练码，并添加一个函数调用来平均我们的模型的梯度。 （下面是从官方 PyTorch MNIST例如很大程度上启发。） \"\"\" Distributed Synchronous SGD Example \"\"\" def run(rank, size): torch.manual_seed(1234) train_set, bsz = partition_dataset() model = Net() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) num_batches = ceil(len(train_set.dataset) / float(bsz)) for epoch in range(10): epoch_loss = 0.0 for data, target in train_set: optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) epoch_loss += loss.item() loss.backward() average_gradients(model) optimizer.step() print('Rank ', dist.get_rank(), ', epoch ', epoch, ': ', epoch_loss / num_batches) 它仍然实现average_gradients（型号）功能，它只是发生在一个模型，在整个世界平均水平的梯度。 \"\"\" Gradient averaging. \"\"\" def average_gradients(model): size = float(dist.get_world_size()) for param in model.parameters(): dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM) param.grad.data /= size 的Et瞧 ！我们成功地实施分布式同步新元，并可能培养了大量的计算机集群上的任何模型。 注：[HTG1虽然最后一句是 技术上 真实的，有[很多更多的技巧HTG5】实行同步SGD的生产级的落实需要。再次，用什么[已经过测试和优化HTG7。 我们自己的戒指，Allreduce 作为一个额外的挑战，假设我们要落实DeepSpeech的高效环allreduce。这是使用点至点集体相当容易实现。 \"\"\" Implementation of a ring-reduce with addition. \"\"\" def allreduce(send, recv): rank = dist.get_rank() size = dist.get_world_size() send_buff = th.zeros(send.size()) recv_buff = th.zeros(send.size()) accum = th.zeros(send.size()) accum[:] = send[:] left = ((rank - 1) + size) % size right = (rank + 1) % size for i in range(size - 1): if i % 2 == 0: # Send send_buff send_req = dist.isend(send_buff, right) dist.recv(recv_buff, left) accum[:] += recv[:] else: # Send recv_buff send_req = dist.isend(recv_buff, right) dist.recv(send_buff, left) accum[:] += send[:] send_req.wait() recv[:] = accum[:] 另外，在上述脚本中，allreduce（发送， 的recv）函数具有比PyTorch的那些稍微不同的签名。它需要一个的recv张量，将所有发张量的总和存储在里面。作为一个练习留给读者，还有我们的版本和一个在DeepSpeech之间的一个区别：它们的实现划分梯度张成 块 ，从而以最佳方式利用通信带宽。 （提示： torch.chunk ） 高级主题 我们现在就可以发现一些torch.distributed更先进的功能性。因为有很多覆盖，本节分为两个小节： 通讯后端：我们学习如何使用MPI和GLOO的GPU-GPU通信。 初始化方法：在我们了解如何最好地设置在dist.init_process_group初始协调阶段（） [HTG3。 通信后端 其中的最优雅的方面torch.distributed是它的抽象能力和建立在不同的后端之上。正如前面提到的，有目前有三个在后端实现PyTorch：TCP，MPI和GLOO。他们每个人都有不同的规格和权衡，根据所需的用例。支持的函数的比较表可以发现这里。需要注意的是第四后端，NCCL，已自创立本教程的补充。参见本部分中的torch.distributed文档有关其使用和值的详细信息的。 TCP后端 到目前为止，我们已经取得了TCP后端的广泛使用。这是作为一个开发平台非常方便，因为它是保证在大多数计算机和操作系统上运行。它还支持所有点至点和集体功能的CPU。然而，对于GPU和它的通信程序并不作为优化的MPI一个不支持。 GLOO后端 的 GLOO后端提供了一种优化的实施 集体 通信过程，无论对CPU和GPU。它特别照在GPU的，因为它可以在不使用 GPUDirect 将数据传送到CPU的存储器进行通信。另外，也能够使用 NCCL 执行快速节点内的通信，并实现其[自己的算法HTG9用于节点间的例程。 自从0.2.0版本中，GLOO后台自动包含PyTorch的预编译的二进制文件。正如你一定会注意到，如果你把模型在GPU上我们的分布式SGD例如不工作。让我们从第一替换后端= 'GLOO' 修复在init_processes（秩， 大小， FN， 后端= 'TCP'）。在这一点上，该脚本将仍然在CPU上运行，但使用的幕后GLOO后端。为了使用多GPU，让我们也做如下修改： init_processes（秩， 大小， FN， 后端= 'TCP'）\\（\\ RIGHTARROW \\） init_processes（秩， 大小， FN， 后端= 'GLOO'） 使用装置 = torch.device（ “CUDA：{}”。格式（评级）） 模型 = 净（）\\（\\ RIGHTARROW \\） 模型 = 净（）。到（装置） 使用数据， 目标 = data.to（装置）， target.to（装置） 通过上述修改，我们的模型现在的训练在两个GPU和您可以监控他们与利用观看 NVIDIA-SMI [HTG5。 MPI后端 消息传递接口（MPI）是从高性能计算领域标准化的工具。它允许做点至点和集体沟通，是为torch.distributed该API的主要灵感。存在MPI的若干实施方式（例如，开放-MPI ， MVAPICH2 ，英特尔MPI ），每个用于不同的目的进行了优化。使用MPI后端的优势在于MPI的广泛可用性 - 和优化的高层次 - 大型计算机集群。 [HTG10一些 最近 实现也能够利用CUDA IPC和GPU直接的技术，以便通过CPU来避免存储副本。 不幸的是，PyTorch的可执行文件可以不包括MPI实现，我们必须手工重新编译。幸运的是，这个过程是相当简单的因为在编译时，PyTorch看起来 本身 一个可用的MPI实现。下面的步骤安装MPI后端，通过从源安装PyTorch 。 创建并激活您的蟒蛇环境，安装所有下面的导的先决条件，但 不是 运行巨蟒 setup.py 安装呢。 选择并安装自己喜欢的MPI实现。请注意，启用CUDA感知MPI可能需要一些额外的步骤。在我们的例子中，我们将坚持开放MPI 无 GPU的支持：畅达 安装 -c 康达锻 的openmpi 现在，去你的克隆PyTorch回购和执行巨蟒 setup.py 安装 [HTG7。 为了测试我们新安装的后端，则需要进行一些修改。 更换下含量如果 __name__ == '__main__'：与init_processes （0， 0， 运行， 后端= 'MPI'）。 运行的mpirun -N 4 蟒 myscript.py。 究其原因，这些变化是，MPI需要产卵的过程之前创建自己的环境。 MPI也将产生其自己的过程，并执行在初始化方法所述的握手，使得秩和大小的参数init_process_group多余的。这实际上是相当强大的，你可以通过额外的参数的mpirun [HTG17为了调整计算资源，为每个进程。 （比如像每个进程内核，手工分配机器特定列数和[一些更](https://www.open- mpi.org/faq/?category=running#mpirun-hostfile)）这样做，则应该得到相同的熟悉输出与其它通信后端。 初始化方法 为了完成本教程，让我们来谈谈我们称为第一个函数：dist.init_process_group（后端， init_method）HTG4] [HTG5。特别是，我们会在不同的初始化方法，这是负责每道工序之间的协调最初一步。这些方法允许你定义这种协调是如何实现的。根据您的硬件设置，这些方法之一应该是自然比其他人更适合。除了下面的部分，你也应该有一个看看[官方文档[HTG7。](https://pytorch.org/docs/stable/distributed.html#initialization) 跳水进入初始化方法之前，让我们快速浏览一下背后init_process_group从C / C ++的角度会发生什么。 首先，参数解析和验证。 后端经由name2channel.at（）功能解决。 A 频道类被返回，并且将用于进行该数据传输。 的GIL被丢弃，并THDProcessGroupInit（）被调用。此实例化信道，并增加了主节点的地址。 用列0的过程中会执行主过程，而所有其他等级将是工人。 大师 创建为所有工人插座。 所有工人等待连接。 发送他们有关的其他进程的位置信息。 每个工人 创建一个套接字的主人。 将自己的位置信息。 接收有关的其他工作人员的信息。 打开一个插座和握手与所有其他工人。 初始化完成后，每个人都被连接到每一个人。 环境变量 我们一直在使用本教程的环境变量初始化方法。通过设置所有计算机上的以下四个环境变量，所有进程将能够正确地连接到主，获取有关的其他进程的信息，并最终与他们握手。 MASTER_PORT：将与等级0宿主的过程中机器上的空闲端口。 MASTER_ADDR：将与等级0宿主的过程中机器的IP地址。 WORLD_SIZE：总数的工艺，使主知道有多少工人等待。 RANK：每个处理的等级，所以他们会知道它是否是一个工人的主人。 共享文件系统 共享文件系统需要的所有进程能够访问共享文件系统，并协调将通过共享文件。这意味着，每个进程将打开该文件，写入其信息，并等待，直到每个人都这样做了。以后有什么需要的所有信息将随时提供给所有的进程。为了避免竞态条件，则文件系统必须支持通过的fcntl 锁定。请注意，您可以手动指定行列或让流程弄清楚自己。可以定义一个独特的组名每次作业你可以使用相同的文件路径为多个作业，然后安全地避免冲突。 dist.init_process_group(init_method='file:///mnt/nfs/sharedfile', world_size=4, group_name='mygroup') TCP初始化 &安培;组播 通过TCP初始化可以用两种不同的方式来实现： 通过提供过程中的IP地址与等级0和世界大小。 通过提供 任何 有效的IP 多播地址和世界的大小。 在第一种情况下，所有工人将能够与秩0连接至该过程，并按照上面描述的过程。 dist.init_process_group(init_method='tcp://10.1.1.20:23456', rank=args.rank, world_size=4) 在第二种情况下，多播地址指定组节点谁可能潜在地是活性和协调可以通过允许每个进程遵循上面的程序之前，有一个初始握手处理的。此外TCP组播初始化还支持组名参数（与共享文件的方法），从而允许多个作业要在同一群集中调度。 dist.init_process_group(init_method='tcp://[ff15:1e18:5d4c:4cf0:d02d:b659:53ba:b0a7]:23456', world_size=4) 致谢 我想感谢PyTorch开发人员就其执行，文档和测试做这样一个好工作。当代码不清楚，我总能指望文档或测试找到答案。我特别要感谢Soumith Chintala，亚当Paszke，和Natalia Gimelshein提供有见地的意见和回答有关初稿的问题。 Next Previous Was this helpful? Yes No Thank you ©版权所有2017年，PyTorch。 3. PyTorch编写分布式应用 安装 点对点通讯 集群通信 [HTG0分布式训练 我们自己的戒指，Allreduce 高级主题 通信后端 初始化方法 ![](https://www.facebook.com/tr?id=243028289693773&ev=PageView &noscript=1) 分析流量和优化经验，我们为这个站点的Cookie。通过点击或导航，您同意我们的cookies的使用。因为这个网站目前维护者，Facebook的Cookie政策的适用。了解更多信息，包括有关可用的控制：[饼干政策HTG1。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"beginner/aws_distributed_training_tutorial.html":{"url":"beginner/aws_distributed_training_tutorial.html","title":"（高级）PyTorch 1.0分布式训练与Amazon AWS","keywords":"","body":"（高级）PyTorch 1.0分布式训练与Amazon AWS 作者 ：弥敦道Inkawhich 由 编辑：腾力 在本教程中，我们将介绍如何设置，代码，并在两个多GPU亚马逊AWS节点运行PyTorch 1.0分布式教练。我们将与描述为分布式教练的AWS设置，那么PyTorch环境配置，最后的代码开始。希望你会发现，有您当前的训练码扩展到分布式应用程序实际上需要很少的代码变化，大部分工作是在一次环境设置。 亚马逊AWS设定 在本教程中，我们将运行在两个多GPU节点的分布式训练。在本节中，我们将首先介绍如何创建节点，那么如何设置安全组，这样的节点可以与海誓山盟沟通。 创建结点 在亚马逊AWS，有七个步骤来创建一个实例。要开始，登录并选择 启动实例[HTG1。 [HTG0步骤1：选择一个亚马逊机器映像（AMI） - 在这里，我们将选择深 学习 AMI （Ubuntu的） 版 14.0。如上所述，这种情况下附带了许多安装了最流行的深学习框架，并进行了预配置CUDA，cuDNN和NCCL。正是由于这个教程一个很好的起点。 步骤2：选择一个实例类型 - 称为现在，选择GPU计算单元p2.8xlarge。通知，每个这些实例都具有不同的成本，但这种情况下，每个节点提供了8个NVIDIA特斯拉K80 GPU和提供了用于多GPU分布式训练良好的体系结构。 步骤3：配置实例详细说明 - 改变这里的唯一设置在增加的实例的 Number来2.所有其它配置可以在默认留。 第四步：添加存储 - 请注意，默认情况下，这些节点不来了大量的存储空间（只有75 GB）。对于本教程，因为我们只使用STL-10数据集，这是充足的存储空间。但是，如果你想在一个更大的数据集，如ImageNet训练，你将不得不增加更多的存储只是为了适应数据集，并要保存任何训练的模型。 [HTG0步骤5：添加标签 - 什么可以在这里完成，只是继续前进。 [HTG0步骤6：配置安全组 - 这是在配置过程中的关键步骤。默认情况下，同一安全组中的两个节点将不能够在分布式训练环境进行通信。在这里，我们要创建一个 新 为两个节点安全组是在，但我们无法完成配置在这一步。现在，只记得你的新的安全组的名称（例如，推出的向导-12），然后转移到步骤7。 [HTG0步骤7：回顾实例启动 - 在这里，查看实例，然后启动它。默认情况下，会自动启动初始化两个实例。您可以监视从仪表板的初始化进度。 配置安全组 回想一下，我们无法创建实例时正确配置安全组。一旦你启动实例，选择 网络 &放;安全& GT ;安全组在EC2仪表板选项卡。这将显示您可以访问安全组的列表。选择在步骤6中创建的新的安全组（即启动的向导-12），这将打开的选项卡被称为 说明，入站，出站，和标签 。首先，选择 入境 选项卡和 编辑 从“源”推出的向导-12安全组中添加规则允许“所有流量”。然后选择 出境 选项卡，然后做同样的事情。现在，我们已经有效地使推出的向导-12安全组中的节点之间的所有类型的所有入站和出站流量。 必要信息 在继续之前，我们必须找到并记住两个节点的IP地址。在EC2仪表板找到你正在运行的实例。对于这两种情况下，写下 IPv4公网IP 和 私人IP地址[HTG3。对于文档的其余部分，我们将把这些作为 NODE0-publicIP ， NODE0-privateIP ， 节点1-publicIP 和 node1- privateIP 。公共IP地址是我们将使用SSH连接的地址和私有地址将被用于节点间通信。 环境设置 下一关键步骤是在每个节点的设置。不幸的是，我们不能同时设置两个节点，所以这个过程必须在每个节点上分别进行。然而，这是一个时间的设置，所以一旦你正确配置的节点，您将不必重新配置为未来分布式培训项目。 第一步骤中，一旦登录到节点，是与蟒3.6和numpy的创建一个新的康达环境。一旦创建启动环境。 $ conda create -n nightly_pt python=3.6 numpy $ source activate nightly_pt 接下来，我们将安装Cuda的9.0启用PyTorch的每晚构建与在畅达环境点子。 $ pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu90/torch_nightly.html 我们还必须安装torchvision所以我们可以使用torchvision模型和数据集。此时，由于PIP安装将在默认情况下，我们刚刚安装的每晚构建的顶部安装了旧版本PyTorch的，我们必须从源代码编译torchvision。 $ cd $ git clone https://github.com/pytorch/vision.git $ cd vision $ python setup.py install 最后， 非常重要[HTG1步骤是为NCCL插座设置网络接口的名称。这被设定为环境变量NCCL_SOCKET_IFNAME。为了得到正确的名称，该节点上运行使用ifconfig命令，查看对应的接口名称节点的 privateIP [HTG11（例如ens3）。然后设置环境变量 $ export NCCL_SOCKET_IFNAME=ens3 请记住，这样做两个节点上。您也可以考虑加入NCCLSOCKET_IFNAME设置为你的 .bashrc中[HTG1。一个重要的观察是，我们没有设置节点之间共享的文件系统。因此，每个节点必须有代码的副本和所述数据集的副本。有关设置节点之间的共享的网络文件系统的更多信息，请参见这里。_ 分布式训练码 随着运行的实例和环境设置，我们现在可以进入训练码。这里的大多数代码的已采取从 PyTorch ImageNet实施例这也支持分布式训练。此代码提供了一个自定义的教练一个很好的起点，因为它有很多的样板训练循环，确认循环和准确性跟踪功能。但是，您会注意到参数解析和其他非必要的功能已被剥离出来的简单性。 在这个例子中，我们将使用 torchvision.models.resnet18 模式，并将训练它的 torchvision.datasets.STL10 数据集。为了适应对STL-10与Resnet18维数不匹配，我们将每个图像尺寸调整到224x224通过转换。请注意，模型和数据集的选择是正交的分布式训练码，你可以使用任何你想要的数据集，模型和过程是相同的。让我们得到由第一处理进口和谈论了一些辅助功能启动。然后，我们将定义火车和测试功能，这已经在很大程度上从ImageNet实施例作出。最后，我们将构建一个处理分布式训练设置的代码的主要部分。最后，我们将讨论如何实际运行代码。 进口 最重要的分布式训练特定这里进口 torch.nn.parallel ，torch.distributed ，torch.utils.data.distributed 和Torch 。多处理。同样重要的是，设置多启动方法为 菌种 或 forkserver （仅在Python 3支持），作为默认的是 叉 这可能引起死锁时使用多个工作进行dataloading处理。 import time import sys import torch if __name__ == '__main__': torch.multiprocessing.set_start_method('spawn') import torch.nn as nn import torch.nn.parallel import torch.distributed as dist import torch.optim import torch.utils.data import torch.utils.data.distributed import torchvision.transforms as transforms import torchvision.datasets as datasets import torchvision.models as models from torch.multiprocessing import Pool, Process 辅助函数 我们还必须定义一些辅助函数和类将会使培训更容易。在AverageMeter类曲目培训统计资料，例如精度和迭代次数。在精度函数计算并返回模型的前k精度，所以我们可以跟踪学习进度。两者都提供了方便训练但是没有分配具体的培训。 class AverageMeter(object): \"\"\"Computes and stores the average and current value\"\"\" def __init__(self): self.reset() def reset(self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 def update(self, val, n=1): self.val = val self.sum += val * n self.count += n self.avg = self.sum / self.count def accuracy(output, target, topk=(1,)): \"\"\"Computes the precision@k for the specified values of k\"\"\" with torch.no_grad(): maxk = max(topk) batch_size = target.size(0) _, pred = output.topk(maxk, 1, True, True) pred = pred.t() correct = pred.eq(target.view(1, -1).expand_as(pred)) res = [] for k in topk: correct_k = correct[:k].view(-1).float().sum(0, keepdim=True) res.append(correct_k.mul_(100.0 / batch_size)) return res 列车功能 为了简化主循环中，最好是分离出训练时期步骤到一个名为列车功能。该功能用于训练的 train_loader 一个历元输入模型。在此功能仅分布训练神器的数据和标签张量 non_blocking 属性设置为前直传真 [HTG11。这使得数据传输含义异步GPU拷贝可以与计算重叠。该功能还输出沿途培训统计，所以我们可以跟踪整个时代的进步。 其它功能在这里定义为adjust_learning_rate，其衰减在一个固定的时间表初始学习速率。这又是一个样板教练功能是训练精确的模型非常有用。 def train(train_loader, model, criterion, optimizer, epoch): batch_time = AverageMeter() data_time = AverageMeter() losses = AverageMeter() top1 = AverageMeter() top5 = AverageMeter() # switch to train mode model.train() end = time.time() for i, (input, target) in enumerate(train_loader): # measure data loading time data_time.update(time.time() - end) # Create non_blocking tensors for distributed training input = input.cuda(non_blocking=True) target = target.cuda(non_blocking=True) # compute output output = model(input) loss = criterion(output, target) # measure accuracy and record loss prec1, prec5 = accuracy(output, target, topk=(1, 5)) losses.update(loss.item(), input.size(0)) top1.update(prec1[0], input.size(0)) top5.update(prec5[0], input.size(0)) # compute gradients in a backward pass optimizer.zero_grad() loss.backward() # Call step of optimizer to update model params optimizer.step() # measure elapsed time batch_time.update(time.time() - end) end = time.time() if i % 10 == 0: print('Epoch: [{0}][{1}/{2}]\\t' 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' 'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t' 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t' 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t' 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format( epoch, i, len(train_loader), batch_time=batch_time, data_time=data_time, loss=losses, top1=top1, top5=top5)) def adjust_learning_rate(initial_lr, optimizer, epoch): \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\" lr = initial_lr * (0.1 ** (epoch // 30)) for param_group in optimizer.param_groups: param_group['lr'] = lr 验证函数 为了跟踪推广性能和简化主回路还我们还可以提取所述验证步骤到一个名为函数验证。该函数运行在输入验证的DataLoader输入模型的一个完整的验证步骤，并返回所述验证集的模型的顶部-1的精度。同样，你会发现这里唯一的分布式训练功能设置non_blocking =真训练数据和标签它们传递给模型前。 def validate(val_loader, model, criterion): batch_time = AverageMeter() losses = AverageMeter() top1 = AverageMeter() top5 = AverageMeter() # switch to evaluate mode model.eval() with torch.no_grad(): end = time.time() for i, (input, target) in enumerate(val_loader): input = input.cuda(non_blocking=True) target = target.cuda(non_blocking=True) # compute output output = model(input) loss = criterion(output, target) # measure accuracy and record loss prec1, prec5 = accuracy(output, target, topk=(1, 5)) losses.update(loss.item(), input.size(0)) top1.update(prec1[0], input.size(0)) top5.update(prec5[0], input.size(0)) # measure elapsed time batch_time.update(time.time() - end) end = time.time() if i % 100 == 0: print('Test: [{0}/{1}]\\t' 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t' 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t' 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format( i, len(val_loader), batch_time=batch_time, loss=losses, top1=top1, top5=top5)) print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}' .format(top1=top1, top5=top5)) return top1.avg 输入 随着辅助函数的方式进行，现在我们已经到了有趣的部分。这里我们将定义运行的输入。一些输入是标准模型的培训投入，如批量大小和训练时期的数量，有些是专门针对我们的分布式训练任务。所需的输入是： 的batch_size - 批量大小为 分布式训练组中的每个 过程。在分布式模型总批量大小的batch_size是* world_size 工人 - 中的每个进程与所述dataloaders使用的工作进程数 num_epochs - 历元用于训练的总次数 starting_lr - 开始进行训练学习速率 world_size - 过程在分布式训练环境数 dist_backend - 后端用于分布式训练的通信（即NCCL，GLOO，MPI，等）。在本教程中，由于我们使用几个多GPU节点，NCCL建议。 dist_url - URL来指定处理组的初始化方法。这可以包含rank0处理的IP地址和端口或者是一个共享的文件系统上的不存在的文件。这里，因为我们没有一个共享文件系统，这将包括在NODE0使用 NODE0-privateIP 和端口。 print(\"Collect Inputs...\") # Batch Size for training and testing batch_size = 32 # Number of additional worker processes for dataloading workers = 2 # Number of epochs to train for num_epochs = 2 # Starting Learning Rate starting_lr = 0.1 # Number of distributed processes world_size = 4 # Distributed backend type dist_backend = 'nccl' # Url used to setup distributed training dist_url = \"tcp://172.31.22.234:23456\" 初始化进程组 一个在PyTorch分布式训练的最重要的部分是正确设置进程组，这是在初始化torch.distributed包 第一 步骤。要做到这一点，我们将使用torch.distributed.init_process_group功能，需要几个输入。首先， 后端 输入指定后端使用（即NCCL，GLOO，MPI，等）。一个 init_method 输入其是含有rank0机器的地址和端口或共享文件系统上的一个不存在的文件的路径的URL。注意，使用文件initmethod，所有机器必须能够访问该文件，同样的网址的方法，所有机器必须能够在网络上进行通信，从而确保配置任何防火墙和网络设置，以适应。的 _init_process_group 功能也需要 秩 和 world_size 用于指定运行时该方法的秩和的过程中集体的数量，分别参数。的 init_method 输入也可以是“ENV：//”。 MASTERADDR，MASTER_PORT：在这种情况下，rank0机器的地址和端口号会从以下两个环境变量分别读取。 RANK，WORLD_SIZE：如果 位次 和 _world_size 未在 init_process_group 功能指定的参数，它们都可以从以下两个环境变量分别也被读取。 另一个重要的步骤，特别是当每一个节点具有多个GPU是设置 local_rank 该方法的。例如，如果有两个节点，每个节点8个GPU和希望与他们的训练然后 \\（世界\\ _size = 16 \\），并且每个节点将与本地秩0-的处理7。此local_rank用于设置该装置（即要使用的GPU）的过程和随后用于创建分布式数据并行模型时设置该装置。此外，还建议使用NCCL后端在这个假设的环境NCCL是优选的多GPU节点。 print(\"Initialize Process Group...\") # Initialize Process Group # v1 - init with url dist.init_process_group(backend=dist_backend, init_method=dist_url, rank=int(sys.argv[1]), world_size=world_size) # v2 - init with file # dist.init_process_group(backend=\"nccl\", init_method=\"file:///home/ubuntu/pt-distributed-tutorial/trainfile\", rank=int(sys.argv[1]), world_size=world_size) # v3 - init with environment variables # dist.init_process_group(backend=\"nccl\", init_method=\"env://\", rank=int(sys.argv[1]), world_size=world_size) # Establish Local Rank and set device on this node local_rank = int(sys.argv[2]) dp_device_ids = [local_rank] torch.cuda.set_device(local_rank) 初始化模型 下一个主要步骤是初始化进行培训的模式。在这里，我们将使用torchvision.models一个resnet18模式，但可以使用任何模型。首先，我们初始化模式，并把它放在GPU内存。接下来，我们使模型DistributedDataParallel，其处理数据的分布和模型，是分布式训练的关键。在DistributedDataParallel模块还可以处理世界各地的梯度的平均，所以我们没有在训练步骤明确平均梯度。 要注意，这是一个阻塞功能，这意味着程序执行将在此函数等到 world_size 工艺已经加入了处理组是重要的。另外，还要注意我们传递的设备ID列表，其中包含了本地等级（即GPU），我们正在使用的参数。最后，我们确定损失的功能和优化训练与模型。 print(\"Initialize Model...\") # Construct Model model = models.resnet18(pretrained=False).cuda() # Make model DistributedDataParallel model = torch.nn.parallel.DistributedDataParallel(model, device_ids=dp_device_ids, output_device=local_rank) # define loss function (criterion) and optimizer criterion = nn.CrossEntropyLoss().cuda() optimizer = torch.optim.SGD(model.parameters(), starting_lr, momentum=0.9, weight_decay=1e-4) 初始化Dataloaders 在训练准备的最后一步是指定要使用的数据集。这里我们使用 torchvision.datasets.STL10 中的[ STL-10数据集HTG1。所述STL10数据集是96x96px彩色图像的10类数据集。对于我们的模型的使用，我们调整图像224x224px在变换。本节中的一个分布式训练特定项是使用的DistributedSampler对于训练集，其被设计为与DistributedDataParallel一起使用车型。这个对象处理跨分布式环境中，这样并非所有型号都在相同的数据集，这将是适得其反的训练数据集的划分。最后，我们创建的DataLoader的其负责馈送的数据的处理。 如果它们不存在的STL-10数据集将自动节点上下载。如果你想使用自己的数据集，你应该下载的数据，编写自己的数据集的处理程序，并在这里建立了您的数据集的DataLoader。 print(\"Initialize Dataloaders...\") # Define the transform for the data. Notice, we must resize to 224x224 with this dataset and model. transform = transforms.Compose( [transforms.Resize(224), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # Initialize Datasets. STL10 will automatically download if not present trainset = datasets.STL10(root='./data', split='train', download=True, transform=transform) valset = datasets.STL10(root='./data', split='test', download=True, transform=transform) # Create DistributedSampler to handle distributing the dataset across nodes when training # This can only be called after torch.distributed.init_process_group is called train_sampler = torch.utils.data.distributed.DistributedSampler(trainset) # Create the Dataloaders to feed data to the training and validation steps train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=(train_sampler is None), num_workers=workers, pin_memory=False, sampler=train_sampler) val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=workers, pin_memory=False) 训练循环 最后一步是界定训练循环。我们已经完成了大部分的工作，为建立分布式训练所以这不是分布式训练具体。唯一的细节是设置在DistributedSampler，作为取样洗牌的数据要每个进程确定性地基于历元的当前历元计数。更新采样后，循环运行完整的训练时期，然后运行一个完整的验证步骤打印对表现最好的模型当前模型的性能至今。对于num_epochs训练结束后，退出循环和教程结束。请注意，因为这是我们没有保存模型的工作，但不妨一跟踪性能最佳的模型，然后将其保存在训练结束（见此处）。 best_prec1 = 0 for epoch in range(num_epochs): # Set epoch count for DistributedSampler train_sampler.set_epoch(epoch) # Adjust learning rate according to schedule adjust_learning_rate(starting_lr, optimizer, epoch) # train for one epoch print(\"\\nBegin Training Epoch {}\".format(epoch+1)) train(train_loader, model, criterion, optimizer, epoch) # evaluate on validation set print(\"Begin Validation @ Epoch {}\".format(epoch+1)) prec1 = validate(val_loader, model, criterion) # remember best prec@1 and save checkpoint if desired # is_best = prec1 > best_prec1 best_prec1 = max(prec1, best_prec1) print(\"Epoch Summary: \") print(\"\\tEpoch Accuracy: {}\".format(prec1)) print(\"\\tBest Accuracy: {}\".format(best_prec1)) 运行代码 与其他大多数PyTorch教程，这些代码可能无法直接从这款笔记本的运行。要运行，下载此文件的版本的.py（或使用这个将其转换）并上传一份给两个节点。细心的读者会注意到，我们硬编码了 NODE0-privateIP 和 \\（世界\\ size = 4 \\）HTG5]，但输入 位次 和 _local_rank 输入作为ARG [1]和Arg [2]的命令行参数，分别。上传后，打开两个SSH终端到每个节点。 论NODE0第一终端，运行$ 蟒 main.py 0 0 论NODE0运行第二终端$ 蟒 main.py 1 1 对节点1的第一终端，运行$ 蟒 main.py 2 0 对节点1运行第二终端$ 蟒 main.py 3 1 该程序将启动，并打印“初始化模式......”所有四个流程加盟流程组后等待。注意不重复的第一个参数，因为这是过程的独特的全球性排名。重复第二个参数，因为这是在该节点上运行的进程的本地秩。如果你运行NVIDIA-SMI每个节点上，你会看到每个节点上的两个过程，一个是关于GPU0运行，一个在GPU1。 现在我们已经完成了分布式训练的例子！希望你可以看到你将如何使用此教程，以帮助培养你自己的数据集自己的模型，即使您不使用完全相同的分布式环境不受。如果你正在使用AWS，不要忘了，如果你不使用它们 关闭NODES 或者你可以在月底发现的令人不安的大账单。 下一步去哪里 退房启动公用踢关闭运行的不同方式 检查出的[ torch.multiprocessing.spawn实用程序HTG1用于开球多个分布式方法的另一个简单的方法。 PyTorch ImageNet实施例已将其实现，并且可以显示如何使用它。 如果可能的话，设置一个NFS所以你只需要数据集中的一个副本 脚本的总运行时间： （0分钟0.000秒） Download Python source code: aws_distributed_training_tutorial.py Download Jupyter notebook: aws_distributed_training_tutorial.ipynb 通过斯芬克斯-廊产生廊 Next Previous Was this helpful? Yes No Thank you ©版权所有2017年，PyTorch。 [HTG0 （高级）PyTorch 1.0分布式训练与Amazon AWS 亚马逊AWS设定 创建节点 配置安集团 必要的信息 环境设置 [HTG0分布式训练码 进口 辅助函数 列车功能 验证函数 输入 初始化处理组 初始化模型 初始化Dataloaders 培训环 运行代码 ![](https://www.facebook.com/tr?id=243028289693773&ev=PageView &noscript=1) 分析流量和优化经验，我们为这个站点的Cookie。通过点击或导航，您同意我们的cookies的使用。因为这个网站目前维护者，Facebook的Cookie政策的适用。了解更多信息，包括有关可用的控制：[饼干政策HTG1。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"advanced/torch_script_custom_ops.html":{"url":"advanced/torch_script_custom_ops.html","title":"使用自定义 C++ 扩展算TorchScript ","keywords":"","body":"使用自定义C ++算延伸TorchScript 该PyTorch 1.0版本中引入的一种新的编程模型PyTorch称为[ TorchScript HTG1。 TorchScript是可解析的，编译和优化由TorchScript编译Python编程语言的子集。此外，编译TorchScript模型有被序列化到磁盘上的文件格式，它可以随后加载和从纯C ++（以及Python）的用于推理运行选项。 TorchScript支持由Torch 提供包操作的相当大的一部分，让你表达多种复杂模型的纯粹从PyTorch的“标准库”等一系列张量操作。不过，也有可能是时候，你需要有一个自定义的C ++或CUDA功能扩展TorchScript的发现自己。虽然我们建议您只能求助于这个选项，如果你的想法不能被表达（足够有效），作为一个简单的Python函数，我们提供了一个非常友好和简单的界面使用定义自定义C ++和CUDA内核 ATEN ，PyTorch的高性能C ++库张。一旦绑定到TorchScript，您可以嵌入这些自定义内核（或“OPS”）到您的TorchScript模型，无论是在Python和直接的序列化的形式在C ++中执行。 下面的段落给出写TorchScript定制运算来调入的OpenCV ，计算机视觉库用C ++编写的一个例子。我们将讨论如何在C ++中，张量工作，如何有效地将它们转换为第三方张量格式（在这种情况下，OpenCV的 Mats），如何注册与TorchScript运行，最后如何您的运营商编译操作和Python和C ++使用它。 实施自定义操作员在C ++ 对于本教程，我们将暴露 warpPerspective 函数，它适用于透视变换的图像，从到的OpenCV作为TorchScript自定义操作符。第一步是写我们在C ++运营商定制的实现。让我们把这个实现op.cpp，使它看起来像这样的文件： #include #include torch::Tensor warp_perspective(torch::Tensor image, torch::Tensor warp) { cv::Mat image_mat(/*rows=*/image.size(0), /*cols=*/image.size(1), /*type=*/CV_32FC1, /*data=*/image.data()); cv::Mat warp_mat(/*rows=*/warp.size(0), /*cols=*/warp.size(1), /*type=*/CV_32FC1, /*data=*/warp.data()); cv::Mat output_mat; cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{8, 8}); torch::Tensor output = torch::from_blob(output_mat.ptr(), /*sizes=*/{8, 8}); return output.clone(); } 这个操作符的代码很短。在该文件的顶部，我们包括OpenCV的头文件，opencv2 / opencv.hpp，沿着torch/ script.h头部暴露从PyTorch的C ++ API所需的所有东西，我们需要编写自定义TorchScript运营商。我们的函数warp_perspective采用两个参数：输入图像和经线变换矩阵我们希望应用到图像。的类型的这些输入是torch::张量，在C PyTorch的张量类型++（其也是基础类型在Python所有张量）。我们的返回类型warp_perspective功能也将是一个Torch ::张量 [HTG31。 小费 参见本说明约ATEN，它提供了张量类PyTorch库的更多信息。此外，本教程描述了如何分配和用C初始化新张量对象++（不需要这个操作符）。 注意 所述编译器TorchScript理解的类型的固定号码。只有这些类型可以作为参数传递给您的自定义操作。目前，这些类型是：HTG0] Torch ::张量 ，Torch ::标量，双，的int64_t和的std ::矢量这些类型的第需要注意的是 只有 双和 不是 浴液HTG30]和 只有 的int64_t和 不 其他整数类型如INT，短或长的支持。 里面我们的功能，我们需要做的第一件事就是转变我们的PyTorch张量到OpenCV的矩阵，为的OpenCV的warpPerspective预计CV ::垫对象作为输入。幸运的是，有一种方法来做到这一点 而不复制任何 数据。在第几行， cv::Mat image_mat(/*rows=*/image.size(0), /*cols=*/image.size(1), /*type=*/CV_32FC1, /*data=*/image.data()); 我们呼吁此构造 OpenCV的垫类的给我们的张量转换为垫对象。我们通过它最初的图像张量，数据类型（我们定为FLOAT32为行数和列数本实施例中），最后一个原始指针到底层数据 - A 浮动*。有什么特别之处这个构造函数垫类的是，它不会复制的输入数据。相反，它会简单地引用该内存上的垫执行的所有操作。如果在image_mat进行就地操作，这将反映原始图像张量（反之亦然在）。这使我们可以调用随后OpenCV的程序与库的本地矩阵型，即使我们实际上是存储在PyTorch张量的数据。我们重复这个过程将经 PyTorch张量转换为warp_matOpenCV的矩阵： cv::Mat warp_mat(/*rows=*/warp.size(0), /*cols=*/warp.size(1), /*type=*/CV_32FC1, /*data=*/warp.data()); 接下来，我们准备调用，我们是如此渴望在TorchScript使用OpenCV的函数：warpPerspective [HTG3。为此，我们通过OpenCV的函数中的image_mat和warp_mat矩阵，以及被称为空输出矩阵output_mat 。我们还指定大小DSIZE我们要输出矩阵（图像）是。它是硬编码为8 × 8在这个例子中： cv::Mat output_mat; cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{8, 8}); 在我们的运营商定制实现的最后一步是转换的output_mat [HTG3重新站到PyTorch张量，这样我们就可以进一步PyTorch使用它。这是惊人地相似，我们先前做在其他方向转换。在这种情况下，PyTorch提供了 torch:: fromblob`方法。在这种情况下，A 一滴_ 是指一些不透明的，平坦的内存指针，我们要解释成PyTorch张量。为Torch :: from_blob调用看起来是这样的：` torch::from_blob(output_mat.ptr(), /*sizes=*/{8, 8}) 我们使用.ptr & LT ;浮子& GT ;（）上OpenCV的垫[方法HTG6]类来获得原始指针到底层数据（就像。数据& LT ;浮子& GT ;（）为PyTorch张量更早）。我们还指定张量的输出的形状，这我们硬编码为8 × 8。的输出torch:: from_blob于是为torch::张量，指向由OpenCV的基质所拥有的存储器。 从我们的运营商实现返回，这个张量之前，我们必须调用.clone（）对张进行基础数据的内存拷贝。这样做的原因是，Torch :: from_blob返回没有自己的数据的张量。在这一点上，该数据仍然由OpenCV的矩阵拥有。然而，这OpenCV的矩阵将走出去的范围，并在函数结束时被释放。如果我们返回输出张量-是，它会指向由我们使用它的功能之外的时间无效的内存。调用.clone（）返回与新张拥有自身的原始数据的副本，新的张量。因此安全回到外面的世界。 注册运营商定制与TorchScript 现在，已经在C ++中实现我们的运营商定制，我们需要 与TorchScript运行时和编译注册 它。这将允许TorchScript编译器来解决我们在TorchScript代码运营商定制引用。注册非常简单。对于我们的情况，我们需要这样写： static auto registry = torch::RegisterOperators(\"my_ops::warp_perspective\", &warp_perspective); 在某处我们的op.cpp文件的全局范围。这将创建一个全局变量注册表，这将在其构造函数注册我们的TorchScript操作（即只出现一次，每个程序）。我们指定的经营者的名称和一个指向它的实现（我们前面写的函数）。名称由两个部分组成：一个 命名空间 （my_ops），用于我们正在注册的特定运营商和一个名称（warp_perspective）。命名空间和运营商名称是由两个冒号（::）分离。 Tip 如果你想注册多个运营商，您可以链接调用.OP（）构造函数后： static auto registry = torch::RegisterOperators(\"my_ops::warp_perspective\", &warp_perspective) .op(\"my_ops::another_op\", &another_op) .op(\"my_ops::and_another_op\", &and_another_op); 在幕后，RegisterOperators将执行一些相当复杂的C ++模板元编程魔术推断函数指针的参数和返回值类型，我们把它传递（&安培; warp_perspective）。该信息被用于形成 功能架构 为我们的运营商。函数模式是运营商的结构化表示 - 一种“签名”或“原型”的 - 使用的TorchScript编译器来验证TorchScript程序的正确性。 构建自定义操作 现在，我们已经实现了我们的运营商定制的C ++及书面登记代码，它是时间来建立操作成（共享）库，我们可以在任何的Python的研究和实验加载到Python或成C ++的推理环境。存在多种方式来打造我们的运营商，使用纯CMake的，或Python的替代品如setuptools的 [HTG3。为了简便起见，下面仅段落讨论CMake的方法。本教程的附录潜入基于Python的替代品。 与CMake的构建 为了建立我们的运营商定制到一个共享库使用 CMake的构建系统，我们需要写一个简短的的CMakeLists.txt文件，并与我们以前的[放置HTG6] op.cpp 文件。对于这一点，让我们在一个目录结构，看起来像这样一致认为： warp-perspective/ op.cpp CMakeLists.txt 此外，请一定要抓住最新版本的LibTorch分布，包PyTorch的C ++库和CMake的构建文件中，在[ pytorch.org HTG1。请将解压分布在文件系统中的某个地方访问。下面的段落将参考该位置为/路径/到/ libtorch。我们的的CMakeLists.txt文件应该然后是以下内容： cmake_minimum_required(VERSION 3.1 FATAL_ERROR) project(warp_perspective) find_package(Torch REQUIRED) find_package(OpenCV REQUIRED) # Define our library target add_library(warp_perspective SHARED op.cpp) # Enable C++11 target_compile_features(warp_perspective PRIVATE cxx_range_for) # Link against LibTorch target_link_libraries(warp_perspective \"${TORCH_LIBRARIES}\") # Link against OpenCV target_link_libraries(warp_perspective opencv_core opencv_imgproc) 警告 这种设置使一些假设关于构建环境，特别是什么属于安装的OpenCV。上述的CMakeLists.txt文件被运行Ubuntu Xenial与libopencv-dev的通过[HTG9安装一个泊坞容器内测试]易于。如果它不为你工作，你觉得卡住，请使用Dockerfile中的伴随教程库建立一个隔离的，可重复的环境在其中扮演周围从本教程中的代码。如果碰上进一步的麻烦，请在本教程的库文件中的一个问题或张贴在我们的论坛的问题。 到现在建立我们的运营商，我们可以从warp_perspective文件夹中运行以下命令： $ mkdir build $ cd build $ cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch .. -- The C compiler identification is GNU 5.4.0 -- The CXX compiler identification is GNU 5.4.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Found torch: /libtorch/lib/libtorch.so -- Configuring done -- Generating done -- Build files have been written to: /warp_perspective/build $ make -j Scanning dependencies of target warp_perspective [ 50%] Building CXX object CMakeFiles/warp_perspective.dir/op.cpp.o [100%] Linking CXX shared library libwarp_perspective.so [100%] Built target warp_perspective 这将放置在构建文件夹中的libwarp_perspective.so共享库文件。在上面的cmake的命令，则应更换/路径/到/ libtorch与路径解压缩后的LibTorch分布。 我们将探讨如何使用和下面进一步呼吁我们的运营商中的细节，但要获得成功的早期感觉，我们可以尝试在Python运行下面的代码： >>> import torch >>> torch.ops.load_library(\"/path/to/libwarp_perspective.so\") >>> print(torch.ops.my_ops.warp_perspective) 在这里，/path/to/libwarp_perspective.so应到libwarp_perspective.so共享库的相对或绝对路径我们只是建成。如果一切顺利的话，这应该打印像 这是Python的功能，我们将在以后使用调用我们的自定义操作。 在Python使用TorchScript运营商定制 一旦我们的运营商定制内置共享库，我们准备在我们在Python TorchScript车型使用此运算符。有两个部分，以这样的：第一加载操作到Python和第二使用TorchScript代码操作。 你已经看到了如何导入您的运营商引入Python：torch.ops.load_library（） [HTG3。此功能将路径包含运营商定制的共享库，并将其加载到当前进程。加载共享库也将执行全局RegisterOperators 对象，就放到我们的运营商定制实现文件的构造函数。这将注册我们的运营商定制与TorchScript编译器，并允许我们使用该运营商在TorchScript代码。 你可以参考你的加载运营商为torch.ops & LT []命名空间& GT ; [ - ] LT ;。函数[ - - ] GT ;，其中& LT ;命名空间& GT ;是命名空间的一部分您的操作员姓名，以及& LT ;函数& GT ;您的操作者的功能名称。对于我们上面写的操作，命名空间为my_ops和函数名warp_perspective，这意味着我们的运营商可为torch.ops.my_ops.warp_perspective。虽然这个功能可以在脚本或跟踪TorchScript模块一起使用，我们也可以只用它在香草渴望PyTorch并将其传递规律PyTorch张量： >>> import torch >>> torch.ops.load_library(\"libwarp_perspective.so\") >>> torch.ops.my_ops.warp_perspective(torch.randn(32, 32), torch.rand(3, 3)) tensor([[0.0000, 0.3218, 0.4611, ..., 0.4636, 0.4636, 0.4636], [0.3746, 0.0978, 0.5005, ..., 0.4636, 0.4636, 0.4636], [0.3245, 0.0169, 0.0000, ..., 0.4458, 0.4458, 0.4458], ..., [0.1862, 0.1862, 0.1692, ..., 0.0000, 0.0000, 0.0000], [0.1862, 0.1862, 0.1692, ..., 0.0000, 0.0000, 0.0000], [0.1862, 0.1862, 0.1692, ..., 0.0000, 0.0000, 0.0000]]) 注意 会发生什么幕后是你第一次访问torch.ops.namespace.function在Python中，TorchScript编译器（在C ++的土地）可以看到，如果一个函数命名空间::函数已经被注册，如果是这样，则返回一个Python句柄这个功能，我们可以随后使用调入从Python中我们的C ++运算符实现。这是TorchScript运营商定制和C ++扩展之间的一个显着的差异：C ++扩展结合使用pybind11手动，而TorchScript定制OPS是在由PyTorch本身飞约束。 Pybind11为您提供了更多的灵活性，以什么样的类型和类可以绑定到Python和因此建议对纯渴望代码的问候，但它不支持TorchScript欢声笑语。 从这里开始，您可以使用脚本或代码追踪您的自定义操作，就像你从Torch 等功能包。事实上，“标准库”的功能，如torch.matmul经历大致相同的注册路径作为运营商定制，这使得运营商定制真正一流的公民，当谈到如何和在那里他们可以TorchScript使用。 使用自定义操作与跟踪 让我们在跟踪功能嵌入我们的运营商开始。回想一下，跟踪，我们先从一些香草Pytorch代码： def compute(x, y, z): return x.matmul(y) + torch.relu(z) 然后在其上调用torch.jit.trace。我们进一步通过torch.jit.trace例如一些投入，它将转发给我们的实现记录为输入流过它发生的操作顺序。这样做的结果是有效的渴望PyTorch程序，其中TorchScript编译器可以进一步分析，优化和序列化的“冻结”的版本： >>> inputs = [torch.randn(4, 8), torch.randn(8, 5), torch.randn(4, 5)] >>> trace = torch.jit.trace(compute, inputs) >>> print(trace.graph) graph(%x : Float(4, 8) %y : Float(8, 5) %z : Float(4, 5)) { %3 : Float(4, 5) = aten::matmul(%x, %y) %4 : Float(4, 5) = aten::relu(%z) %5 : int = prim::Constant[value=1]() %6 : Float(4, 5) = aten::add(%3, %4, %5) return (%6); } 现在，激动人心的启示是，我们可以简单的丢弃我们的运营商定制到我们PyTorch痕迹，好像它是torch.relu或任何其他Torch函数： torch.ops.load_library(\"libwarp_perspective.so\") def compute(x, y, z): x = torch.ops.my_ops.warp_perspective(x, torch.eye(3)) return x.matmul(y) + torch.relu(z) 然后跟踪它像以前一样： >>> inputs = [torch.randn(4, 8), torch.randn(8, 5), torch.randn(8, 5)] >>> trace = torch.jit.trace(compute, inputs) >>> print(trace.graph) graph(%x.1 : Float(4, 8) %y : Float(8, 5) %z : Float(8, 5)) { %3 : int = prim::Constant[value=3]() %4 : int = prim::Constant[value=6]() %5 : int = prim::Constant[value=0]() %6 : int[] = prim::Constant[value=[0, -1]]() %7 : Float(3, 3) = aten::eye(%3, %4, %5, %6) %x : Float(8, 8) = my_ops::warp_perspective(%x.1, %7) %11 : Float(8, 5) = aten::matmul(%x, %y) %12 : Float(8, 5) = aten::relu(%z) %13 : int = prim::Constant[value=1]() %14 : Float(8, 5) = aten::add(%11, %12, %13) return (%14); } 整合TorchScript定制OPS成追溯到PyTorch代码，因为这容易！ 使用自定义操作与脚本 除了跟踪，另一种方式在PyTorch程序的TorchScript表示到达是直接写你的 TorchScript代码[HTG0。 TorchScript主要是Python语言的一个子集，有一些限制，使得它更容易为TorchScript编译器推理程序。您可以通过使用// @标注它torch.jit.script免费功能和@ torch.jit.script_method [关闭你的常规PyTorch代码到TorchScript HTG9一种用于在类方法（其也必须从torch.jit.ScriptModule派生 ）。参见[此处](https://pytorch.org/docs/master/jit.html)关于TorchScript注释的更多细节。 而不是使用跟踪TorchScript一个特别的原因是追踪无法捕捉PyTorch代码控制流。因此，让我们考虑这个功能，不使用控制流： def compute(x, y): if bool(x[0][0] == 42): z = 5 else: z = 10 return x.matmul(y) + z 从香草PyTorch到TorchScript转换这个功能，我们用将其标注为@ torch.jit.script： @torch.jit.script def compute(x, y): if bool(x[0][0] == 42): z = 5 else: z = 10 return x.matmul(y) + z 这将刚刚在时间编译计算函数成图形表示，这是我们可以在compute.graph属性检查： >>> compute.graph graph(%x : Dynamic %y : Dynamic) { %14 : int = prim::Constant[value=1]() %2 : int = prim::Constant[value=0]() %7 : int = prim::Constant[value=42]() %z.1 : int = prim::Constant[value=5]() %z.2 : int = prim::Constant[value=10]() %4 : Dynamic = aten::select(%x, %2, %2) %6 : Dynamic = aten::select(%4, %2, %2) %8 : Dynamic = aten::eq(%6, %7) %9 : bool = prim::TensorToBool(%8) %z : int = prim::If(%9) block0() { -> (%z.1) } block1() { -> (%z.2) } %13 : Dynamic = aten::matmul(%x, %y) %15 : Dynamic = aten::add(%13, %z, %14) return (%15); } 而现在，就像之前，我们可以用我们的运营商定制等我们的脚本代码中任何其他功能： torch.ops.load_library(\"libwarp_perspective.so\") @torch.jit.script def compute(x, y): if bool(x[0] == 42): z = 5 else: z = 10 x = torch.ops.my_ops.warp_perspective(x, torch.eye(3)) return x.matmul(y) + z 当TorchScript编译器看到参考torch.ops.my_ops.warp_perspective，它会找到我们通过RegisterOperators注册的实现对象在C ++中，并将其编译成其图形表示： >>> compute.graph graph(%x.1 : Dynamic %y : Dynamic) { %20 : int = prim::Constant[value=1]() %16 : int[] = prim::Constant[value=[0, -1]]() %14 : int = prim::Constant[value=6]() %2 : int = prim::Constant[value=0]() %7 : int = prim::Constant[value=42]() %z.1 : int = prim::Constant[value=5]() %z.2 : int = prim::Constant[value=10]() %13 : int = prim::Constant[value=3]() %4 : Dynamic = aten::select(%x.1, %2, %2) %6 : Dynamic = aten::select(%4, %2, %2) %8 : Dynamic = aten::eq(%6, %7) %9 : bool = prim::TensorToBool(%8) %z : int = prim::If(%9) block0() { -> (%z.1) } block1() { -> (%z.2) } %17 : Dynamic = aten::eye(%13, %14, %2, %16) %x : Dynamic = my_ops::warp_perspective(%x.1, %17) %19 : Dynamic = aten::matmul(%x, %y) %21 : Dynamic = aten::add(%19, %z, %20) return (%21); } 特别是通知所述参照my_ops :: warp_perspective在图的结尾。 Attention 所述TorchScript图表示仍然可能发生变化。不要依赖于它看起来像这样。 这就是真正的它，当它涉及到使用Python中我们的运营商定制。总之，你导入使用torch.ops.load_library包含您的运营商（S）的图书馆，并呼吁像任何其他Torch 自定义运算从您的追溯或脚本代码TorchScript操作。 在C使用自定义TorchScript算++ TorchScript的一个有用的功能是序列化模型到磁盘上的文件的能力。该文件可以通过线路被发送，存储在文件系统，或者更重要的是，动态地解串行化和执行，而无需保留原始源代码周围。这是可能在Python，而且在C ++。对于这一点，PyTorch提供[纯C ++ API HTG1用于反串行化以及执行TorchScript模型。如果你还没有，请阅读在C对加载和运行的系列化TorchScript模型教程++ ，在其未来数段将建成。 总之，运营商定制可以像从文件反序列化，即使和用C ++运行规则torch运营商来执行。这个唯一的要求就是我们前面在我们执行模型中的C ++应用程序构建运营商定制共享库链接。在Python中，这个工作只是调用torch.ops.load_library [HTG7。在C ++中，你需要的共享库，在任何的构建系统使用的是主应用程序链接。下面的例子将展示这一点使用CMake的。 Note 从技术上讲，你还可以动态加载共享库复制到运行时你的C ++应用程序中的多，我们这样做是在Python一样。在Linux上，你可以使用dlopen 做到这一点。存在着在其他平台上的等价物。 上面链接的C ++执行教程的基础上，让我们开始用最小的C ++应用程序在一个文件中，的main.cpp从我们的运营商定制不同的文件夹，即加载并执行一个序列化TorchScript模型： #include // One-stop header. #include #include int main(int argc, const char* argv[]) { if (argc != 2) { std::cerr \\n\"; return -1; } // Deserialize the ScriptModule from a file using torch::jit::load(). std::shared_ptr module = torch::jit::load(argv[1]); std::vector inputs; inputs.push_back(torch::randn({4, 8})); inputs.push_back(torch::randn({8, 5})); torch::Tensor output = module->forward(std::move(inputs)).toTensor(); std::cout 随着小的CMakeLists.txt文件： cmake_minimum_required(VERSION 3.1 FATAL_ERROR) project(example_app) find_package(Torch REQUIRED) add_executable(example_app main.cpp) target_link_libraries(example_app \"${TORCH_LIBRARIES}\") target_compile_features(example_app PRIVATE cxx_range_for) 在这一点上，我们应该能够构建应用程序： $ mkdir build $ cd build $ cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch .. -- The C compiler identification is GNU 5.4.0 -- The CXX compiler identification is GNU 5.4.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Found torch: /libtorch/lib/libtorch.so -- Configuring done -- Generating done -- Build files have been written to: /example_app/build $ make -j Scanning dependencies of target example_app [ 50%] Building CXX object CMakeFiles/example_app.dir/main.cpp.o [100%] Linking CXX executable example_app [100%] Built target example_app 而没有通过模型只是还没有运行它： $ ./example_app usage: example_app 接下来，让我们序列化，我们写的脚本函数较早使用我们的自定义操作： torch.ops.load_library(\"libwarp_perspective.so\") @torch.jit.script def compute(x, y): if bool(x[0][0] == 42): z = 5 else: z = 10 x = torch.ops.my_ops.warp_perspective(x, torch.eye(3)) return x.matmul(y) + z compute.save(\"example.pt\") 最后一行将序列化脚本函数到一个名为“example.pt”文件。如果我们再通过这个序列化的模型来我们的C ++应用程序，我们可以运行它立刻： $ ./example_app example.pt terminate called after throwing an instance of 'torch::jit::script::ErrorReport' what(): Schema not found for node. File a bug report. Node: %16 : Dynamic = my_ops::warp_perspective(%0, %19) 或者可能不是。也许不是现在。当然！我们没有链接与我们的应用运营商定制库呢。现在，让我们这样做的权利，并做正确，让我们稍微更新我们的文件组织，如下所示： example_app/ CMakeLists.txt main.cpp warp_perspective/ CMakeLists.txt op.cpp 这将允许我们添加warp_perspective库CMake的目标作为我们的应用目标的子目录。顶层的CMakeLists.txt中的example_app文件夹应该是这样的： cmake_minimum_required(VERSION 3.1 FATAL_ERROR) project(example_app) find_package(Torch REQUIRED) add_subdirectory(warp_perspective) add_executable(example_app main.cpp) target_link_libraries(example_app \"${TORCH_LIBRARIES}\") target_link_libraries(example_app -Wl,--no-as-needed warp_perspective) target_compile_features(example_app PRIVATE cxx_range_for) 这个基本的CMake的配置看起来很像之前，除了我们添加warp_perspective CMake的建设作为一个子目录。一旦它的CMake的代码运行时，我们用warp_perspective共享库链接我们的example_app应用。 Attention 有嵌入在上面的例子中一个关键的细节：-Wl， - 无按需前缀到warp_perspective链接线。这是必需的，因为我们实际上不会调用在我们的应用程序代码中的warp_perspective共享库的任何功能。我们只需要在全球RegisterOperators对象的构造函数运行。麻烦的是，这混淆了连接器，并使其认为它可以只是完全跳过链接到的库。在Linux上，`轮候册， 无按需 标记强制发生的链接（注：这个标志是具体到Linux！）。还有其他的变通办法此。最简单的就是定义 _一些函数_ 在您需要从主应用程序调用操作库。这可能是作为简单的函数作废 的init（）[]在一些头，然后将其定义为空隙声明 初始化（） { } 在操作库。调用此的init（） `在主应用程序的功能将会给连接器的印象，这是值得链接到的库。不幸的是，这是我们无法控制的，我们宁可让你知道原因和简单的解决方法为这个比交给你一些不透明宏在代码噗通。 现在，因为我们现在找到Torch包在最顶层，在的CMakeLists.txt文件中的warp_perspective子目录可以缩短一个位。它应该是这样的： find_package(OpenCV REQUIRED) add_library(warp_perspective SHARED op.cpp) target_compile_features(warp_perspective PRIVATE cxx_range_for) target_link_libraries(warp_perspective PRIVATE \"${TORCH_LIBRARIES}\") target_link_libraries(warp_perspective PRIVATE opencv_core opencv_photo) 让我们重新构建我们的示例应用程序，这也将与运营商定制库链接。在顶层example_app目录： $ mkdir build $ cd build $ cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch .. -- The C compiler identification is GNU 5.4.0 -- The CXX compiler identification is GNU 5.4.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Found torch: /libtorch/lib/libtorch.so -- Configuring done -- Generating done -- Build files have been written to: /warp_perspective/example_app/build $ make -j Scanning dependencies of target warp_perspective [ 25%] Building CXX object warp_perspective/CMakeFiles/warp_perspective.dir/op.cpp.o [ 50%] Linking CXX shared library libwarp_perspective.so [ 50%] Built target warp_perspective Scanning dependencies of target example_app [ 75%] Building CXX object CMakeFiles/example_app.dir/main.cpp.o [100%] Linking CXX executable example_app [100%] Built target example_app 如果我们现在运行example_app二进制，并把它我们序列化模型，我们应该在一个快乐的结局到达： $ ./example_app example.pt 11.4125 5.8262 9.5345 8.6111 12.3997 7.4683 13.5969 9.0850 11.0698 9.4008 7.4597 15.0926 12.5727 8.9319 9.0666 9.4834 11.1747 9.0162 10.9521 8.6269 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 [ Variable[CPUFloatType]{8,5} ] 成功！您现在可以推论了。 结论 本教程走你扔了如何实现在C ++中的自定义TorchScript运营商，如何将它建设成一个共享库，如何在Python中使用它来定义TorchScript模型，最后如何将其加载到用于推断工作量C ++应用程序。您现在可以使用C ++运算符与第三方C ++库接口扩展您的TorchScript模型，编写自定义的高性能CUDA内核，或实现需要Python，TorchScript和C ++之间的界限顺利融入任何其他使用情况。 与往常一样，如果您遇到任何问题或有任何疑问，您可以使用我们的论坛或 GitHub的问题取得联系。此外，我们的常见问题（FAQ）页可能有帮助的信息。 附录A：建筑运营商定制的更多方法 “建设运营商定制”一节中介绍如何构建一个运营商定制成使用CMake的共享库。本附录概述了编译另外两个方法。他们都使用Python作为“驾驶员”或“接口”的编译过程。此外，两个重复使用现有的基础设施 PyTorch提供 C ++扩展 ，它们是香草（渴望）PyTorch等效TorchScript运营商定制的依赖于 pybind11 为选自C的函数“明确的”结合++成Python。 第一种方法使用C ++的扩展方便刚刚在实时（JIT）编译接口编译代码在你PyTorch脚本的后台运行它的第一次。第二种方法依赖于古老setuptools的包和涉及编写单独的setup.py文件。这允许更高级的配置以及整合与其他setuptools的为基础的项目。我们将探讨在下面详细两种方法。 与JIT编译馆 由PyTorch C ++扩展工具包中提供的JIT编译特征允许嵌入自定义操作的汇编直接进入Python代码，例如在你的训练脚本的顶部。 Note “JIT编译”这里没有什么做的JIT编译发生在TorchScript编译器优化你的程序。它只是意味着你的运营商定制的C ++代码将一个文件夹在你的系统的 / tmp目录目录下的第一次导入它，就好像你自己事先编编就。 这JIT编译功能有两种形式。在第一个，你还是留着你的运营商实现在一个单独的文件（op.cpp），然后用torch.utils.cpp_extension.load（）编译你的扩展。通常情况下，这个函数将返回Python模块暴露你的C ++的扩展。然而，由于我们没有编制我们的运营商定制到自己的Python模块，我们只是编译一个普通的共享库。幸运的是，torch.utils.cpp_extension.load（）有一个参数is_python_module，我们可以设置为假表明，我们只建立一个共享库，而不是一个Python模块感兴趣。 torch.utils.cpp_extension.load（）将然后编译和共享库也加载到当前进程，就象torch.ops.load_library以前那样： import torch.utils.cpp_extension torch.utils.cpp_extension.load( name=\"warp_perspective\", sources=[\"op.cpp\"], extra_ldflags=[\"-lopencv_core\", \"-lopencv_imgproc\"], is_python_module=False, verbose=True ) print(torch.ops.my_ops.warp_perspective) 这应该大约打印： JIT编译的第二香味可以让你通过源代码为您定制TorchScript运营商作为一个字符串。对于这一点，使用torch.utils.cpp_extension.load_inline： import torch import torch.utils.cpp_extension op_source = \"\"\" #include #include torch::Tensor warp_perspective(torch::Tensor image, torch::Tensor warp) { cv::Mat image_mat(/*rows=*/image.size(0), /*cols=*/image.size(1), /*type=*/CV_32FC1, /*data=*/image.data()); cv::Mat warp_mat(/*rows=*/warp.size(0), /*cols=*/warp.size(1), /*type=*/CV_32FC1, /*data=*/warp.data()); cv::Mat output_mat; cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{64, 64}); torch::Tensor output = torch::from_blob(output_mat.ptr(), /*sizes=*/{64, 64}); return output.clone(); } static auto registry = torch::RegisterOperators(\"my_ops::warp_perspective\", &warp_perspective); \"\"\" torch.utils.cpp_extension.load_inline( name=\"warp_perspective\", cpp_sources=op_source, extra_ldflags=[\"-lopencv_core\", \"-lopencv_imgproc\"], is_python_module=False, verbose=True, ) print(torch.ops.my_ops.warp_perspective) 当然，最好的做法是只使用torch.utils.cpp_extension.load_inline如果你的源代码是相当短的。 请注意，如果你在一个Jupyter笔记本电脑用这个，你不应该因为每次执行注册一个新的图书馆，并重新注册运营商定制执行与登记多次的细胞。如果您需要重新执行它，请事先重新启动笔记本的Python的内核。 与setuptools的构建 专门从Python的建设我们的运营商定制的第二种方法是使用setuptools的 [HTG3。这具有setuptools的 具有用于建筑用C ++编写Python模块一个相当强大的和广泛的接口的优点。然而，由于setuptools的 真的打算用于建筑Python模块和非纯共享库（不具有必要的入口点Python从一个模块期望），这条路线可以稍微古怪。这就是说，你需要的是到位的，看起来像这样的 的CMakeLists.txtAsetup.py文件： from setuptools import setup from torch.utils.cpp_extension import BuildExtension, CppExtension setup( name=\"warp_perspective\", ext_modules=[ CppExtension( \"warp_perspective\", [\"example_app/warp_perspective/op.cpp\"], libraries=[\"opencv_core\", \"opencv_imgproc\"], ) ], cmdclass={\"build_ext\": BuildExtension.with_options(no_python_abi_suffix=True)}, ) 请注意，我们启用了no_python_abi_suffix中的BuildExtension在底部的选项。这指示setuptools的省略任何的Python-3特异性ABI后缀在所产生的共享库的名称。否则，关于Python 3.7例如，库可以被称为warp_perspective.cpython-37m-x86_64-linux-gnu.so其中CPython的-37M-x86_64的-linux-GNU是ABI标签，但我们真的只是希望它被称为warp_perspective.so 如果我们现在运行巨蟒 setup.py 建 从文件夹内发展在终端中setup.py坐落，我们应该看到： $ python setup.py build develop running build running build_ext building 'warp_perspective' extension creating build creating build/temp.linux-x86_64-3.7 gcc -pthread -B /root/local/miniconda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/TH -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/THC -I/root/local/miniconda/include/python3.7m -c op.cpp -o build/temp.linux-x86_64-3.7/op.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=warp_perspective -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11 cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++ creating build/lib.linux-x86_64-3.7 g++ -pthread -shared -B /root/local/miniconda/compiler_compat -L/root/local/miniconda/lib -Wl,-rpath=/root/local/miniconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/op.o -lopencv_core -lopencv_imgproc -o build/lib.linux-x86_64-3.7/warp_perspective.so running develop running egg_info creating warp_perspective.egg-info writing warp_perspective.egg-info/PKG-INFO writing dependency_links to warp_perspective.egg-info/dependency_links.txt writing top-level names to warp_perspective.egg-info/top_level.txt writing manifest file 'warp_perspective.egg-info/SOURCES.txt' reading manifest file 'warp_perspective.egg-info/SOURCES.txt' writing manifest file 'warp_perspective.egg-info/SOURCES.txt' running build_ext copying build/lib.linux-x86_64-3.7/warp_perspective.so -> Creating /root/local/miniconda/lib/python3.7/site-packages/warp-perspective.egg-link (link to .) Adding warp-perspective 0.0.0 to easy-install.pth file Installed /warp_perspective Processing dependencies for warp-perspective==0.0.0 Finished processing dependencies for warp-perspective==0.0.0 这将产生所谓的共享库warp_perspective.so，其中我们可以通过torch.ops.load_library 正如我们前面所做为让我们的运营商看到TorchScript： >>> import torch >>> torch.ops.load_library(\"warp_perspective.so\") >>> print(torch.ops.custom.warp_perspective) Next Previous Was this helpful? Yes No Thank you ©版权所有2017年，PyTorch。 使用自定义C ++扩展算TorchScript 用C实现运营商定制++ 与TorchScript注册自定义操作 构建自定义操作 与CMake的构建 在Python使用TorchScript运营商定制 使用运营商定制与跟踪 使用运营商定制与脚本 在C使用自定义TorchScript算++ 结论 [HTG0附录A：建筑运营商定制的更多方法 与JIT编译馆 与setuptools的构建 ![](https://www.facebook.com/tr?id=243028289693773&ev=PageView &noscript=1) 分析流量和优化经验，我们为这个站点的Cookie。通过点击或导航，您同意我们的cookies的使用。因为这个网站目前维护者，Facebook的Cookie政策的适用。了解更多信息，包括有关可用的控制：[饼干政策HTG1。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"advanced/numpy_extensions_tutorial.html":{"url":"advanced/numpy_extensions_tutorial.html","title":"用 numpy 和 scipy 创建扩展","keywords":"","body":"用 numpy 和 scipy 创建扩展 作者 ：Adam Paszke 修订者: Adam Dziedzic 译者：Foxerlee、cangyunye 校验：Foxerlee、FontTian 在本教程中，我们需要完成两个任务： 创建一个无参数神经网络层。 这里需要调用 numpy 包作为实现的一部分。 创建一个权重自主优化的神经网络层。 这里需要调用 Scipy 包作为实现的一部分。 import torch from torch.autograd import Function 无参数神经网络层示例 该层并没有做任何有用的或数学上正确的事情。 它只是被恰当的命名为 BadFFTFunction 本层的实现方式 from numpy.fft import rfft2, irfft2 class BadFFTFunction(Function): def forward(self, input): numpy_input = input.detach().numpy() result = abs(rfft2(numpy_input)) return input.new(result) def backward(self, grad_output): numpy_go = grad_output.numpy() result = irfft2(numpy_go) return grad_output.new(result) # 由于本层没有任何参数，我们可以简单的声明为一个函数， # 而不是当做 nn.Module 类 def incorrect_fft(input): return BadFFTFunction()(input) 创建无参数神经网络层的示例方法: input = torch.randn(8, 8, requires_grad=True) result = incorrect_fft(input) print(result) result.backward(torch.randn(result.size())) print(input) 输出: tensor([[ 0.4073, 11.6080, 7.4098, 18.1538, 3.4384], [ 4.9980, 3.5935, 6.9132, 3.8621, 6.1521], [ 5.2876, 6.2480, 9.3535, 5.1881, 9.5353], [ 4.5351, 2.3523, 6.9937, 4.2700, 2.6574], [ 0.7658, 7.8288, 3.9512, 5.2703, 15.0991], [ 4.5351, 4.9517, 7.7959, 17.9770, 2.6574], [ 5.2876, 11.0435, 4.1705, 0.9899, 9.5353], [ 4.9980, 11.1055, 5.8031, 3.1775, 6.1521]], grad_fn=) tensor([[-1.4503, -0.6550, 0.0648, 0.2886, 1.9357, -1.2299, -1.7474, 0.6866], [-0.2466, -1.0292, 0.3109, -0.4289, -0.3620, 1.1854, -1.3372, -0.2717], [ 0.0828, 0.9115, 0.7877, -0.5776, 1.6676, -0.5576, -0.2321, -0.3273], [ 0.1632, 0.3835, 0.5422, -0.9144, 0.2871, 0.1441, -1.8333, 1.4951], [-0.2183, -0.5220, 0.9151, 0.0540, -1.0642, 0.4409, 0.7906, -1.2262], [ 0.4039, 0.3374, 1.0567, -0.8190, 0.7870, -0.6152, -0.2887, 1.3878], [ 1.6407, 0.0220, 1.4984, -1.9722, 0.3797, -0.0180, -0.7096, -0.2454], [ 0.7194, 2.3345, -0.0780, -0.2043, -0.4576, -0.9087, -2.4926, 0.9283]], requires_grad=True) 参数化示例 在深度学习的文献中，这一层被误解的称作卷积 convolution，尽管该层的实际操作是交叉-关联性 cross-correlation (唯一的区别是滤波器 filter 是为了卷积而翻转，而不是为了交叉关联)。 本层的可自优化权重的实现，依赖于交叉-关联 cross-correlation 一个表示权重的滤波器。 后向传播函数 backward 计算的是输入数据的梯度以及滤波器的梯度。 from numpy import flip import numpy as np from scipy.signal import convolve2d, correlate2d from torch.nn.modules.module import Module from torch.nn.parameter import Parameter class ScipyConv2dFunction(Function): @staticmethod def forward(ctx, input, filter, bias): # detach so we can cast to NumPy input, filter, bias = input.detach(), filter.detach(), bias.detach() result = correlate2d(input.numpy(), filter.numpy(), mode='valid') result += bias.numpy() ctx.save_for_backward(input, filter, bias) return torch.as_tensor(result, dtype=input.dtype) @staticmethod def backward(ctx, grad_output): grad_output = grad_output.detach() input, filter, bias = ctx.saved_tensors grad_output = grad_output.numpy() grad_bias = np.sum(grad_output, keepdims=True) grad_input = convolve2d(grad_output, filter.numpy(), mode='full') # the previous line can be expressed equivalently as: # grad_input = correlate2d(grad_output, flip(flip(filter.numpy(), axis=0), axis=1), mode='full') grad_filter = correlate2d(input.numpy(), grad_output, mode='valid') return torch.from_numpy(grad_input), torch.from_numpy(grad_filter).to(torch.float), torch.from_numpy(grad_bias).to(torch.float) class ScipyConv2d(Module): def __init__(self, filter_width, filter_height): super(ScipyConv2d, self).__init__() self.filter = Parameter(torch.randn(filter_width, filter_height)) self.bias = Parameter(torch.randn(1, 1)) def forward(self, input): return ScipyConv2dFunction.apply(input, self.filter, self.bias) 示例: module = ScipyConv2d(3, 3) print(\"Filter and bias: \", list(module.parameters())) input = torch.randn(10, 10, requires_grad=True) output = module(input) print(\"Output from the convolution: \", output) output.backward(torch.randn(8, 8)) print(\"Gradient for the input map: \", input.grad) 输出： Filter and bias: [Parameter containing: tensor([[ 0.6693, -0.2222, 0.4118], [-0.3676, -0.9931, 0.2691], [-0.1429, 1.8659, -0.7335]], requires_grad=True), Parameter containing: tensor([[-1.3466]], requires_grad=True)] Output from the convolution: tensor([[ 0.5250, -4.8840, -0.5804, -0.4413, -0.2209, -5.1590, -2.2587, -3.5415], [ 0.1437, -3.4806, 2.8613, -2.5486, -0.6023, 0.8587, 0.6923, -3.9129], [-6.2535, 2.7522, -2.5025, 0.0493, -3.2200, 1.2887, -2.4957, 1.6669], [ 1.6953, -0.9312, -4.6079, -0.9992, -1.4760, 0.2594, -3.8285, -2.9756], [ 1.2716, -5.1037, -0.2461, -1.1965, -1.6461, -0.6712, -3.1600, -0.9869], [-2.0643, -1.1037, 1.0145, -0.4984, 1.6899, -1.2842, -3.5010, 0.8348], [-2.6977, 0.7242, -5.2932, -2.1470, -4.0301, -2.8247, -1.4165, 0.0572], [-1.1560, 0.8500, -3.5242, 0.0686, -1.9708, 0.8417, 2.1091, -4.5537]], grad_fn=) Gradient for the input map: tensor([[ 0.2475, -1.0357, 0.9908, -1.5128, 0.9041, 0.0582, -0.5316, 1.0466, -0.4844, 0.2972], [-1.5626, 1.4143, -0.3199, -0.9362, 1.0149, -1.6612, -0.1623, 1.0273, -0.8157, 0.4636], [ 1.1604, 2.5787, -5.6081, 4.6548, -2.7051, 1.4152, 1.0695, -5.0619, 1.9227, -1.4557], [ 0.8890, -5.4601, 5.3478, 0.3287, -3.0955, 1.7628, 1.3722, 0.9022, 4.6063, -1.7763], [ 0.4180, -1.4749, 1.9056, -6.5754, 1.1695, -0.3068, -2.7579, -1.2399, -3.2611, 1.7447], [-1.5550, 1.0767, 0.5541, 0.5231, 3.7888, -2.4053, 0.4745, 4.5228, -5.2254, 0.7871], [ 0.8094, 5.9939, -4.4974, 1.9711, -4.6029, -0.7072, 0.8058, -1.0656, 1.7967, -0.5905], [-1.1218, -4.8356, -3.5650, 2.0387, 0.6232, 1.4451, 0.9014, -1.1660, -0.5986, 0.7368], [ 0.4346, 3.4302, 5.3058, -3.0440, 1.0593, -3.6538, -1.7829, -0.0543, -0.4385, 0.2770], [ 0.2144, -2.5117, -2.6153, 1.1894, -0.6176, 1.9013, -0.7186, 0.4952, 0.6256, -0.3308]]) 检查梯度: from torch.autograd.gradcheck import gradcheck moduleConv = ScipyConv2d(3, 3) input = [torch.randn(20, 20, dtype=torch.double, requires_grad=True)] test = gradcheck(moduleConv, input, eps=1e-6, atol=1e-4) print(\"Are the gradients correct: \", test) 输出： Are the gradients correct: True 脚本的总运行时间：（0分钟 4.128秒） 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"advanced/cpp_extension.html":{"url":"advanced/cpp_extension.html","title":"自定义 C++ 和CUDA扩展","keywords":"","body":"自定义 C++ 和 CUDA 扩展 作者 ：彼得戈尔兹伯勒 译者：Foxerlee 校验：Foxerlee PyTorch 提供了大量与神经网络、随机张量代数（arbitrary tensor algebra）、数据整合（data wrangling）以及其他目的相关的操作。但是，您仍然可能会发现自己需要更多自定义操作。例如，您可能想使用在论文中发现的新的激活函数，或者实现您在研究过程中所开发的新的运算。 在 PyTorch 中整合这样的自定义操作最简单的方法是利用 Python 编写扩展的函数（Funciton）和模型（Module），如此处所描写的那样。这让您可以充分地利用自动微分（automatic differentiation）（使你不需要自己编写派生函数）与 Python 在通常情况下的表现力。然而，在有些时候您的一些操作可以使用 C++ 以获得更佳的效果。比如，您的代码在模型当中会被十分 频繁地调用，或者即便调用次数较少也会带来昂贵的开销。另一个可能的原因是您的代码依赖于一些 C 和 C++ 库，或者需要与它们交互。为了解决这种情况，PyTorch 提供了一种非常简单的编写自定义 C++ 扩展 的方法。 C++ 扩展是一种我们开发的以允许用户（您）创建一些包含的资源 之外的 PyTorch 运算符，例如，与 PyTorch 后端分离开来。此方法与原生的 PyTorch 操作的实现方式不同。C++ 扩展旨在为您提供大量与 PyTorch 后端集成在一起相关的样板（boilerplate），同时为基于 PyTorch 的项目提供高度的灵活性。但是，一旦将操作定义为 C++ 扩展，将其转换为原生 PyTorch 函数在很大程度上取决于您的代码组织结构，如果您决定在较早阶段进行操作，则可以解决这个问题。 动机和例子 本篇文章的其余部分将逐步介绍一个编写和使用 C++（和CUDA）扩展的实际示例。如果您一直在被催促，或者在今天结束前仍未完成该扩展您就会被开除，那么可以跳过本节，直接进入下一部分的实施细节。 假设您想出了一种新型的循环单元，与现有技术相比，它具有更好的性能。该循环单元与 LSTM 相似，但不同之处在于，它没有遗忘门，并使用指数线性单元（ELU）作为其内部激活函数。由于此单元永远不会忘记，因此我们将其称为 LLTM 或长长期记忆（Long-Long-Term-Memory）单元。 由于 LLTM 和 LSTM 两者的区别过于明显，以至于我们不能通过修改 PyTorch 中的 LSTMCell 来实验我们的目标，因此我们需要创建一个自定义单元。解决这个问题的第一种也是最简单的一种 -- 并且在所有情况下都是最好的一步 -- 是使用 Python 在原生的 PyTorch 中实现我们所需的功能。为此，我们需要继承 torch.nn.Module 并实现LLTM的前向传播。 代码如下： class LLTM(torch.nn.Module): def __init__(self, input_features, state_size): super(LLTM, self).__init__() self.input_features = input_features self.state_size = state_size # 3 * state_size for input gate, output gate and candidate cell gate. # input_features + state_size because we will multiply with [input, h]. self.weights = torch.nn.Parameter( torch.empty(3 * state_size, input_features + state_size)) self.bias = torch.nn.Parameter(torch.empty(3 * state_size)) self.reset_parameters() def reset_parameters(self): stdv = 1.0 / math.sqrt(self.state_size) for weight in self.parameters(): weight.data.uniform_(-stdv, +stdv) def forward(self, input, state): old_h, old_cell = state X = torch.cat([old_h, input], dim=1) # Compute the input, output and candidate cell gates with one MM. gate_weights = F.linear(X, self.weights, self.bias) # Split the combined gate weight matrix into its components. gates = gate_weights.chunk(3, dim=1) input_gate = torch.sigmoid(gates[0]) output_gate = torch.sigmoid(gates[1]) # Here we use an ELU instead of the usual tanh. candidate_cell = F.elu(gates[2]) # Compute the new cell state. new_cell = old_cell + candidate_cell * input_gate # Compute the new hidden state and output. new_h = torch.tanh(new_cell) * output_gate return new_h, new_cell 单元的调用方式如预期那样： import torch X = torch.randn(batch_size, input_features) h = torch.randn(batch_size, state_size) C = torch.randn(batch_size, state_size) rnn = LLTM(input_features, state_size) new_h, new_C = rnn(X, (h, C)) 当然，如果可能的话，您应该使用如下方法扩展 PyTorch。由于 PyTorch 在 NVIDIA cuDNN，Intel MKL 或 NNPACK 等库的支持下对其 CPU 和 GPU 的操作进行了高度优化的实现，因此前述的 PyTorch 代码通常足够快。但是，我们还是可以发现，在某些情况下为什么性能仍然有进一步改进的空间。最明显的原因是 PyTorch 不了解您要实现的算法。它仅知道您用于组成算法的单个操作。因此，PyTorch 必须逐个执行您的操作。由于对操作的实现（或内核）的每个单独调用（可能涉及启动CUDA内核）都具有一定的开销，因此该开销在许多函数调用中可能变得十分明显。此外，运行我们代码的 Python 解释器本身也可能会使我们的程序变慢。 一种明显的加速方法是用 C++（或CUDA）重写这部分代码并融合 特定的操作组。 融合是指将许多函数的实现组合到一个函数中，这可以从两个方面受益：更少的内核启动，以及在提高全局数据流可见性的情况下执行的其他优化。 让我们看看如何使用 C++ 扩展来实现 LLTM 的融合 版本。我们将从使用支持 PyTorch 大部分后端功能的 ATen 库以原生 C++ 编写代码开始，然后看看它是如何让我们轻松转换 Python 代码的。然后，我们将模型的各个部分移至 CUDA 内核，以从 GPU 提供的大规模并行处理中受益，从而进一步加快处理速度。 编写一个 C++ 扩展 C++ 扩展有两种形式：可以使用 setuptools “提前”构建，也可以通过 torch.utils.cpp_extension.load() “即时”构建。 我们将从第一种方法开始，稍后再讨论后者。 使用 setuptools 进行构建 为了实现“提前”构建，我们编写一个 setup.py 脚本来构建 C++ 扩展，其使用 setuptools 来编译我们的 C++ 代码。对于 LLTM，脚本十分简单，如下所示： from setuptools import setup, Extension from torch.utils import cpp_extension setup(name='lltm_cpp', ext_modules=[cpp_extension.CppExtension('lltm_cpp', ['lltm.cpp'])], cmdclass={'build_ext': cpp_extension.BuildExtension}) 在这部分代码中，CppExtension 是 setuptools.Extension 的一个便利的包装器（wrapper），它传递正确的引用路径，并且将扩展包语言设置为 c++。等效的泛化版 setuptools 简单代码如下所示： Extension( name='lltm_cpp', sources=['lltm.cpp'], include_dirs=cpp_extension.include_paths(), language='c++') BuildExtension 执行并检查许多必需的配置步骤，并且在混合使用 C++ / CUDA 扩展的情况下管理混合编译。这就是我们目前真正需要了解的有关构建 C++ 扩展的全部信息！现在让我们看一下 lltm.cpp 中的 C++ 扩展的实现。 编写 c++ 操作 现在让我们开始利用 c++ 实现 LLTM！我们后向传播需要的一个函数是 Sigmoid 的导数。 这是一小段代码，用于讨论编写 C++ 扩展时可供我们使用的总体环境： #include #include torch::Tensor d_sigmoid(torch::Tensor z) { auto s = torch::sigmoid(z); return (1 - s) * s; } 是一站式（one-stop）头文件，其中包括编写 C++ 扩展所有必需的 PyTorch 扩展。 这包括： ATen 库，它是我们张量计算的主要 API， pybind11，用于实现我们的 C++ 代码的 Python 衔接方法， 其他管理 ATen 和 pybind11 交互细节的头文件。 d_sigmoid() 的实现展示了如何使用 ATen API。PyTorch 的张量和变量接口是由 ATen 库自动生成的，因此我们可以或多或少地实现将 Python 以 1：1 的形式转换为 C++。我们用于所有计算的主要数据类型将是 torch::Tensor。它的完整 API 可以在这里查到。注意，我们可以包含 或任何其他 C 或 C++ 头文件 -- 我们可以使用 C++11 的全部功能。 前向传播 接下来，我们可以将整个前向传播部分移植为 C++ 代码： #include std::vector lltm_forward( torch::Tensor input, torch::Tensor weights, torch::Tensor bias, torch::Tensor old_h, torch::Tensor old_cell) { auto X = torch::cat({old_h, input}, /*dim=*/1); auto gate_weights = torch::addmm(bias, X, weights.transpose(0, 1)); auto gates = gate_weights.chunk(3, /*dim=*/1); auto input_gate = torch::sigmoid(gates[0]); auto output_gate = torch::sigmoid(gates[1]); auto candidate_cell = torch::elu(gates[2], /*alpha=*/1.0); auto new_cell = old_cell + candidate_cell * input_gate; auto new_h = torch::tanh(new_cell) * output_gate; return {new_h, new_cell, input_gate, output_gate, candidate_cell, X, gate_weights}; } 后向传播 C++ 扩展 API 当前不提供为我们自动生成后向传播函数的方法。因此，我们必须要自己实现 LLTM 的后向传播，其将计算每个前向传播的输入的导数。最终，我们前向传播和后向传播函数加入 torch.autograd.Function 中以建立一个不错的 Python 衔接。后向传播的复杂度较高，因此我们不深入研究代码（如果您感兴趣，可以阅读 Alex Graves 的论文，以获得更多有关此方面的信息： // tanh'(z) = 1 - tanh^2(z) torch::Tensor d_tanh(torch::Tensor z) { return 1 - z.tanh().pow(2); } // elu'(z) = relu'(z) + { alpha * exp(z) if (alpha * (exp(z) - 1)) 0).type_as(z) + mask.type_as(z) * (alpha * e); } std::vector lltm_backward( torch::Tensor grad_h, torch::Tensor grad_cell, torch::Tensor new_cell, torch::Tensor input_gate, torch::Tensor output_gate, torch::Tensor candidate_cell, torch::Tensor X, torch::Tensor gate_weights, torch::Tensor weights) { auto d_output_gate = torch::tanh(new_cell) * grad_h; auto d_tanh_new_cell = output_gate * grad_h; auto d_new_cell = d_tanh(new_cell) * d_tanh_new_cell + grad_cell; auto d_old_cell = d_new_cell; auto d_candidate_cell = input_gate * d_new_cell; auto d_input_gate = candidate_cell * d_new_cell; auto gates = gate_weights.chunk(3, /*dim=*/1); d_input_gate *= d_sigmoid(gates[0]); d_output_gate *= d_sigmoid(gates[1]); d_candidate_cell *= d_elu(gates[2]); auto d_gates = torch::cat({d_input_gate, d_output_gate, d_candidate_cell}, /*dim=*/1); auto d_weights = d_gates.t().mm(X); auto d_bias = d_gates.sum(/*dim=*/0, /*keepdim=*/true); auto d_X = d_gates.mm(weights); const auto state_size = grad_h.size(1); auto d_old_h = d_X.slice(/*dim=*/1, 0, state_size); auto d_input = d_X.slice(/*dim=*/1, state_size); return {d_old_h, d_input, d_weights, d_bias, d_old_cell}; } 衔接到 Python 一旦您用 C++ 和 ATen 编写了计算，可以使用 pybind11 以非常简单的方式将 C++ 函数或类衔接到 Python 中。关于 PyTorch 的 C++ 扩展的这一部分的问题或疑问您可以参考 pybind11 文档来解决。 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) { m.def(\"forward\", &lltm_forward, \"LLTM forward\"); m.def(\"backward\", &lltm_backward, \"LLTM backward\"); } 这里要注意的一点是宏 TORCH_EXTENSION_NAME。torch 的扩展程序构建会将其定义为您在 setup.py 脚本中为扩展程序指定的名称。在本教程中，TORCH_EXTENSION_NAME 的值为 “lltm”。这是为了避免在两个位置（构建脚本和您的 C++ 代码）都维护扩展名，因为两者之间的不匹配会导致令人讨厌且难以跟踪的问题。 使用您的扩展 现在，我们准备将扩展名导入 PyTorch 中。 此时，目录结构可能如下所示： pytorch/ lltm-extension/ lltm.cpp setup.py 现在，运行 python setup.py install 安装你的扩展。终端的输入应该如下： running install running bdist_egg running egg_info creating lltm_cpp.egg-info writing lltm_cpp.egg-info/PKG-INFO writing dependency_links to lltm_cpp.egg-info/dependency_links.txt writing top-level names to lltm_cpp.egg-info/top_level.txt writing manifest file 'lltm_cpp.egg-info/SOURCES.txt' reading manifest file 'lltm_cpp.egg-info/SOURCES.txt' writing manifest file 'lltm_cpp.egg-info/SOURCES.txt' installing library code to build/bdist.linux-x86_64/egg running install_lib running build_ext building 'lltm_cpp' extension creating build creating build/temp.linux-x86_64-3.7 gcc -pthread -B ~/local/miniconda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I~/local/miniconda/lib/python3.7/site-packages/torch/include -I~/local/miniconda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I~/local/miniconda/lib/python3.7/site-packages/torch/include/TH -I~/local/miniconda/lib/python3.7/site-packages/torch/include/THC -I~/local/miniconda/include/python3.7m -c lltm.cpp -o build/temp.linux-x86_64-3.7/lltm.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=lltm_cpp -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++11 cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++ creating build/lib.linux-x86_64-3.7 g++ -pthread -shared -B ~/local/miniconda/compiler_compat -L~/local/miniconda/lib -Wl,-rpath=~/local/miniconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/lltm.o -o build/lib.linux-x86_64-3.7/lltm_cpp.cpython-37m-x86_64-linux-gnu.so creating build/bdist.linux-x86_64 creating build/bdist.linux-x86_64/egg copying build/lib.linux-x86_64-3.7/lltm_cpp.cpython-37m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg creating stub loader for lltm_cpp.cpython-37m-x86_64-linux-gnu.so byte-compiling build/bdist.linux-x86_64/egg/lltm_cpp.py to lltm_cpp.cpython-37.pyc creating build/bdist.linux-x86_64/egg/EGG-INFO copying lltm_cpp.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO copying lltm_cpp.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO copying lltm_cpp.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO copying lltm_cpp.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt zip_safe flag not set; analyzing archive contents... __pycache__.lltm_cpp.cpython-37: module references __file__ creating 'dist/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it removing 'build/bdist.linux-x86_64/egg' (and everything under it) Processing lltm_cpp-0.0.0-py3.7-linux-x86_64.egg removing '~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg' (and everything under it) creating ~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg Extracting lltm_cpp-0.0.0-py3.7-linux-x86_64.egg to ~/local/miniconda/lib/python3.7/site-packages lltm-cpp 0.0.0 is already the active version in easy-install.pth Installed ~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg Processing dependencies for lltm-cpp==0.0.0 Finished processing dependencies for lltm-cpp==0.0.0 关于编译器的一个小注意事项：由于 ABI 版本问题，用于构建 C++ 扩展的编译器必须与 ABI 兼容，并且这里的编译器是必须是与构建 PyTorch 时采用的编译器一样的。实际上，这意味着您必须在 Linux 上使用 GCC 4.9 及更高版本。 对于 Ubuntu 16.04 和其他较新的 Linux 发行版，这应该已经是默认的编译器。 在最坏的情况下，您可以使用编译器从源代码构建 PyTorch ，然后使用相同的编译器构建扩展。 扩展程序构建完成后，您可以使用在 setup.py 脚本中指定的名称，简单地将其导入 Python。只需要确保优先调用 import torch，因为这将解析一些动态链接器必须能够看到的标志： In [1]: import torch In [2]: import lltm_cpp In [3]: lltm_cpp.forward Out[3]: 如果我们对函数或者模块调用 help() 函数，我们可以看到，其签名符合我们的 C++ 代码： In[4] help(lltm_cpp.forward) forward(...) method of builtins.PyCapsule instance forward(arg0: torch::Tensor, arg1: torch::Tensor, arg2: torch::Tensor, arg3: torch::Tensor, arg4: torch::Tensor) -> List[torch::Tensor] LLTM forward 由于我们现在能够从 Python 中调用我们的 C++ 函数，我们可以使用 torch.autograd.Function和 torch.nn.Module 来包装（warp）它们，使它们成为 PyTorch 中的最顶层的类（first class citizens，关键的一部分）： import math import torch # Our module! import lltm_cpp class LLTMFunction(torch.autograd.Function): @staticmethod def forward(ctx, input, weights, bias, old_h, old_cell): outputs = lltm_cpp.forward(input, weights, bias, old_h, old_cell) new_h, new_cell = outputs[:2] variables = outputs[1:] + [weights] ctx.save_for_backward(*variables) return new_h, new_cell @staticmethod def backward(ctx, grad_h, grad_cell): outputs = lltm_cpp.backward( grad_h.contiguous(), grad_cell.contiguous(), *ctx.saved_variables) d_old_h, d_input, d_weights, d_bias, d_old_cell = outputs return d_input, d_weights, d_bias, d_old_h, d_old_cell class LLTM(torch.nn.Module): def __init__(self, input_features, state_size): super(LLTM, self).__init__() self.input_features = input_features self.state_size = state_size self.weights = torch.nn.Parameter( torch.empty(3 * state_size, input_features + state_size)) self.bias = torch.nn.Parameter(torch.empty(3 * state_size)) self.reset_parameters() def reset_parameters(self): stdv = 1.0 / math.sqrt(self.state_size) for weight in self.parameters(): weight.data.uniform_(-stdv, +stdv) def forward(self, input, state): return LLTMFunction.apply(input, self.weights, self.bias, *state) 性能比较 现在我们可以使用 PyTorch 调用 C++ 函数，我们可以运行一个小的基准测试，以查看通过用 C++ 重写函数获得的性能。我们将调用 LLTM 的前向传播和后向传播函数几次，并且记录耗时： import time import torch batch_size = 16 input_features = 32 state_size = 128 X = torch.randn(batch_size, input_features) h = torch.randn(batch_size, state_size) C = torch.randn(batch_size, state_size) rnn = LLTM(input_features, state_size) forward = 0 backward = 0 for _ in range(100000): start = time.time() new_h, new_C = rnn(X, (h, C)) forward += time.time() - start start = time.time() (new_h.sum() + new_C.sum()).backward() backward += time.time() - start print('Forward: {:.3f} us | Backward {:.3f} us'.format(forward * 1e6/1e5, backward * 1e6/1e5)) 如果我们使用本文开头用原生 Python 编写的原始 LLTM 来运行此代码，则会得到以下结果（在我的机器上）： Forward: 506.480 us | Backward 444.694 us 而我们的新的 C++ 版本结果： Forward: 349.335 us | Backward 443.523 us 我们可以看到前向传播已经有一个明显的速度提升（超过 30%）。对于后向传播，速度提升是可见的，尽管并不是最明显的那个。我上面所写的后向传播没有特别优化，可以肯定代码仍然能够改进。另外，PyTorch 的自动微分引擎可以自动并行化计算图，可以是一个更高效的操作流，并且也可以用 C++ 实现，因此可以预见我们的代码速度能够更快。当然，这已经是一个很好的开始了。 GPU 设备上的性能 关于 PyTorch 的 ATen 后端的一个美妙事实是，它可以抽象化您正在运行的计算设备。这意味着我们为 CPU 编写的相同代码也可以在 GPU 上运行，并且各个操作将相应地分派到 GPU 优化的实现。对于某些运算，如矩阵乘法（例如 mm 或者 addmm），这将会是一个很大的提升。让我们看一下使用 CUDA 张量运行 C++ 代码所获得的性能。 无需更改实现，我们只需要将张量在 Python 中加入 GPU 内存，即可在开始时添加 device = cuda_device 参数，或者在创建后使用 .to(cuda_device)。 import torch assert torch.cuda.is_available() cuda_device = torch.device(\"cuda\") # device object representing GPU batch_size = 16 input_features = 32 state_size = 128 # Note the device=cuda_device arguments here X = torch.randn(batch_size, input_features, device=cuda_device) h = torch.randn(batch_size, state_size, device=cuda_device) C = torch.randn(batch_size, state_size, device=cuda_device) rnn = LLTM(input_features, state_size).to(cuda_device) forward = 0 backward = 0 for _ in range(100000): start = time.time() new_h, new_C = rnn(X, (h, C)) torch.cuda.synchronize() forward += time.time() - start start = time.time() (new_h.sum() + new_C.sum()).backward() torch.cuda.synchronize() backward += time.time() - start print('Forward: {:.3f} us | Backward {:.3f} us'.format(forward * 1e6/1e5, backward * 1e6/1e5)) 再次将原始的 PyTorch 代码与 C++ 版本（现在都在 CUDA 设备上运行）进行比较，我们又看到了性能提升。 对于 Python / PyTorch： Forward: 187.719 us | Backward 410.815 us 而 C++ / ATen： Forward: 149.802 us | Backward 393.458 us 与非 CUDA 代码相比，这可以大大提高整体速度。但是，通过编写自定义 CUDA 内核，我们可以利用 C++ 获得更多性能，我们很快将深入其中。 JIT 编译扩展 在此之前，让我们讨论构建 C++ 扩展的另一种方法。在介绍了前者之后，让我们详细介绍后者。在介绍了前者之后，让我们详细介绍后者。JIT 编译机制通过调用 PyTorch API 中一个称为 torch.utils.cpp_extension.load() 的简单函数，为您提供了一种动态编译和加载扩展的方式。 对于LLTM，这看起来像这样简单： from torch.utils.cpp_extension import load lltm_cpp = load(name=\"lltm_cpp\", sources=[\"lltm.cpp\"]) 在这里，我们为函数提供与 setuptools 相同的信息。 在后端，其将执行以下操作： 创建一个临时目录 /tmp/torch_extensions/lltm， 将 Ninja 构建文件发送到该临时目录中， 将您的源文件编译到共享库中， 将此共享库导入为 Python 模块。 实际上，如果将变量 verbose = True 传递给 cpp_extension.load()，该进程会在运行过程中告诉你： Using /tmp/torch_extensions as PyTorch extensions root... Emitting ninja build file /tmp/torch_extensions/lltm_cpp/build.ninja... Building extension module lltm_cpp... Loading extension module lltm_cpp... 生成的 Python 模块将与 setuptools 生成的模块完全相同，但是避免了必须维护单独的 setup.py 构建文件的要求。如果您的设置更加复杂，并且确实需要 setuptools 的全部功能，你的确可以编写自己的 setup.py -- 但在许多情况下，这种 JIT 技术就足够了。第一次运行此行时，将需要一些时间，因为扩展程序是在后台编译的。由于我们使用 Ninja 构建系统来构建您的源代码，重新编译是以增量的形式，因此如果您不更改扩展程序的源文件，那您第二次运行 Python 模块重新加载扩展程序时会十分快捷，开销很低。 编写一个 C++/CUDA 混合扩展 为了将我们的实现提升到一个新的水平，我们可以使用自定义 CUDA 内核来手写向前和向后传递的部分内容。对于 LLTM，其提升空间将会十分明显，因为 LLTM 有大量按顺序进行的逐点计算，所有这些计算都可以在单个 CUDA 内核中融合和并行化。 让我们看看如何编写这种 CUDA 内核，并使用此扩展机制将其与 PyTorch 集成。 编写 CUDA 扩展的一般策略是先编写一个 C++ 文件，该文件定义将从 Python 调用的函数，然后使用 pybind11 将这些函数衔接到 Python。此外，此文件还将声明在CUDA（.cu）文件中定义的函数。然后，C++ 函数将进行一些检查，并最终将其调用转发给 CUDA 函数。在 CUDA 文件中，我们编写实际的 CUDA 内核。然后，cpp_extension 包将负责使用 gcc 之类的 C++ 编译器来编译 C++ 源代码，并使用 NVIDIA 的 nvcc 编译器来编译 CUDA 源代码。这样可以确保每个编译器都编译地它最了解的文件。最终，它们将被链接到一个共享库中，该库可以从 Python 代码中获得。 我们将从 C++ 文件开始，我们将其称为 lltm_cuda.cpp，例如： #include #include // CUDA forward declarations std::vector lltm_cuda_forward( torch::Tensor input, torch::Tensor weights, torch::Tensor bias, torch::Tensor old_h, torch::Tensor old_cell); std::vector lltm_cuda_backward( torch::Tensor grad_h, torch::Tensor grad_cell, torch::Tensor new_cell, torch::Tensor input_gate, torch::Tensor output_gate, torch::Tensor candidate_cell, torch::Tensor X, torch::Tensor gate_weights, torch::Tensor weights); // C++ interface #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\") #define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\") #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x) std::vector lltm_forward( torch::Tensor input, torch::Tensor weights, torch::Tensor bias, torch::Tensor old_h, torch::Tensor old_cell) { CHECK_INPUT(input); CHECK_INPUT(weights); CHECK_INPUT(bias); CHECK_INPUT(old_h); CHECK_INPUT(old_cell); return lltm_cuda_forward(input, weights, bias, old_h, old_cell); } std::vector lltm_backward( torch::Tensor grad_h, torch::Tensor grad_cell, torch::Tensor new_cell, torch::Tensor input_gate, torch::Tensor output_gate, torch::Tensor candidate_cell, torch::Tensor X, torch::Tensor gate_weights, torch::Tensor weights) { CHECK_INPUT(grad_h); CHECK_INPUT(grad_cell); CHECK_INPUT(input_gate); CHECK_INPUT(output_gate); CHECK_INPUT(candidate_cell); CHECK_INPUT(X); CHECK_INPUT(gate_weights); CHECK_INPUT(weights); return lltm_cuda_backward( grad_h, grad_cell, new_cell, input_gate, output_gate, candidate_cell, X, gate_weights, weights); } PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) { m.def(\"forward\", &lltm_forward, \"LLTM forward (CUDA)\"); m.def(\"backward\", &lltm_backward, \"LLTM backward (CUDA)\"); } 正如您所看到的，它主要是样板文件，检查并转发到我们将在 CUDA 文件中定义的功能。我们将其命名为 lltm_cuda_kernel.cu（注意扩展名为 .cu！）NVCC 可以聪明地编译 C++11，因此我们仍然可以使用 ATen 和 C++ 标准库（但不提供 torch.h）。请注意， setuptools 无法处理具有相同名称但扩展名不同的文件，因此，如果您使用 setup.py 方法而不是 JIT 方法，则必须为 CUDA 文件指定与 C++ 文件不同的名称（对于 JIT 方法则可以正常区分 lltm.cpp 和 lltm.cu）。让我们简单看一下该文件： #include #include #include #include template __device__ __forceinline__ scalar_t sigmoid(scalar_t z) { return 1.0 / (1.0 + exp(-z)); } 在这里，我们看到了刚刚提到的头文件，以及我们正在使用的特定于 CUDA 的声明（例如 __device__ 和 __forceinline__）以及函数（例如 exp）。让我们继续添加一些我们需要的辅助函数： template __device__ __forceinline__ scalar_t d_sigmoid(scalar_t z) { const auto s = sigmoid(z); return (1.0 - s) * s; } template __device__ __forceinline__ scalar_t d_tanh(scalar_t z) { const auto t = tanh(z); return 1 - (t * t); } template __device__ __forceinline__ scalar_t elu(scalar_t z, scalar_t alpha = 1.0) { return fmax(0.0, z) + fmin(0.0, alpha * (exp(z) - 1.0)); } template __device__ __forceinline__ scalar_t d_elu(scalar_t z, scalar_t alpha = 1.0) { const auto e = exp(z); const auto d_relu = z 现在，要真正实现一个函数，我们还需要两个函数：一个函数执行我们不希望手工编写并调用 CUDA 内核的操作，另一个是要加速部分的实际 CUDA 内核。对于前向传播，第一个函数应如下所示： std::vector lltm_cuda_forward( torch::Tensor input, torch::Tensor weights, torch::Tensor bias, torch::Tensor old_h, torch::Tensor old_cell) { auto X = torch::cat({old_h, input}, /*dim=*/1); auto gates = torch::addmm(bias, X, weights.transpose(0, 1)); const auto batch_size = old_cell.size(0); const auto state_size = old_cell.size(1); auto new_h = torch::zeros_like(old_cell); auto new_cell = torch::zeros_like(old_cell); auto input_gate = torch::zeros_like(old_cell); auto output_gate = torch::zeros_like(old_cell); auto candidate_cell = torch::zeros_like(old_cell); const int threads = 1024; const dim3 blocks((state_size + threads - 1) / threads, batch_size); AT_DISPATCH_FLOATING_TYPES(gates.type(), \"lltm_forward_cuda\", ([&] { lltm_cuda_forward_kernel>>( gates.data(), old_cell.data(), new_h.data(), new_cell.data(), input_gate.data(), output_gate.data(), candidate_cell.data(), state_size); })); return {new_h, new_cell, input_gate, output_gate, candidate_cell, X, gates}; } 这里的主要关注点是 AT_DISPATCH_FLOATING_TYPES 宏和内核启动代码（由 >> 表示）。尽管 ATen 提取了我们处理的张量的设备和数据类型，但张量在运行时仍将由具体设备上的具体类型的内存支持。因此，我们需要一种在运行时确定张量是什么类型，然后有选择地调用具有相应正确类型签名的函数的方法。手动完成后，（在概念上）将如下所示： switch (tensor.type().scalarType()) { case torch::ScalarType::Double: return function(tensor.data()); case torch::ScalarType::Float: return function(tensor.data()); ... } AT_DISPATCH_FLOATING_TYPES 的目的是为我们处理此调度。它需要一个类型（在我们的例子中是 gates.type()），一个名称（用于错误消息）和一个 lambda 函数。在此 lambda 函数中，类型别名 scalar_t 可用，并且定义为该上下文中张量实际上在运行时的类型。这样，如果我们有一个模板函数（CUDA 内核将会使用的），则可以使用此 scalar_t 别名实例化它，然后将调用正确的函数。在这种情况下，我们还希望检索张量的数据指针作为该 scalar_t 类型的指针。 如果想分派所有类型，而不仅是浮点类型（Float 和 Double），则可以使用 AT_DISPATCH_ALL_TYPES。 请注意，我们使用基本的 ATen 执行一些操作。 这些操作仍将在 GPU 上运行，但使用 ATen 的默认实现。这是有道理的，因为 ATen 将使用高度优化的例程来处理矩阵乘法（例如 addmm）或卷积之类的操作，而这些将很难由我们自己实现和改善。 至于内核启动本身，我们在这里指定每个 CUDA 块将具有1024个线程，并且整个 GPU 网格被分成尽可能多的 1x1024 线程块，并以一组一个线程的方式填充我们的矩阵。例如，如果我们的状态大小为 2048，批处理大小为 4，则我们将以每个 1024 个线程启动，总共 4x2=8 个块。 如果您以前从未听说过 CUDA 的“块”或“网格”，那么这篇有关 CUDA 的介绍性阅读可能会有所帮助。 实际的 CUDA 内核十分简单（如果您以往编写过 GPU）： template __global__ void lltm_cuda_forward_kernel( const scalar_t* __restrict__ gates, const scalar_t* __restrict__ old_cell, scalar_t* __restrict__ new_h, scalar_t* __restrict__ new_cell, scalar_t* __restrict__ input_gate, scalar_t* __restrict__ output_gate, scalar_t* __restrict__ candidate_cell, size_t state_size) { const int column = blockIdx.x * blockDim.x + threadIdx.x; const int index = blockIdx.y * state_size + column; const int gates_row = blockIdx.y * (state_size * 3); if (column 这里有趣的内容是，我们能够让门矩阵中的每个单独组件完全并行地计算所有逐点操作。想象一下，如果要用一个巨型的 for 循环遍历一百万个元素来完成这个操作，您就可以明白为什么这样做会更快了。 使用存取器（accessors） 您可以在 CUDA 内核中看到，我们直接处理具有正确类型的指针。实际上，直接在 cuda 内核内部使用高级类型不可知张量是非常低效的。 但是，这是以易于使用和可读性为代价的，特别是对于高维数据。在我们的示例中，我们知道连续门张量具有3个维度： 批处理，大小为 batch_size，步长为 3*state_size 行，大小为 3，步长为 state_size 索引，大小为 state_size，步长为 1 那么，我们如何访问内核内部的元素 gates[n][row][column]？事实上，您只需要通过一些简单的算法，就可以利用步长访问您的元素。 gates.data()[n*3*state_size + row*state_size + column] 除了冗长之外，此表达式还需要明确知道步长的值，并且通过参数将其传递给内核函数。 您会发现，在内核函数接受具有不同大小的多个张量的情况下，您将得到很长的参数列表。 幸运的是，ATen 提供了一种可以动态检查张量类型和维度的存取器。它利用一个 API，可以有效地访问张量元素，而不需要转换为单个指针： torch::Tensor foo = torch::rand({12, 12}); // assert foo is 2-dimensional and holds floats. auto foo_a = foo.accessor(); float trace = 0; for(int i = 0; i 访问器对象具有相对较高级别的接口，如 .size() 和 .srtide() 方法和多维索引。.accseeor<> 旨在在 cpu 张量上有效地访问数据。针对 cuda 张量的等效函数是 packed_accessor64<> 和 packed_accessor32<>，它们分别提供具有 64 位或 32 位整数索引的打包的存取器。 与普通的存取器的根本区别在于，打包的存取器在其结构内部复制大小和跨度数据，而不是指向它。它允许我们将其传递给 CUDA 内核函数并在其中使用其接口。 我们可以设计一个使用打包的存取器而不是指针的函数。 __global__ void lltm_cuda_forward_kernel( const torch::PackedTensorAccessor32 gates, const torch::PackedTensorAccessor32 old_cell, torch::PackedTensorAccessor32 new_h, torch::PackedTensorAccessor32 new_cell, torch::PackedTensorAccessor32 input_gate, torch::PackedTensorAccessor32 output_gate, torch::PackedTensorAccessor32 candidate_cell) 让我们分解一下这里使用的模板。前两个参数 scalar_t 和 2 与常规存取器相同。参数 torch::RestrictPtrTraits 指示必须使用 __restrict__ 关键字。请注意，我们使用了 PackedAccessor32 变量，该变量将大小和步长存储为 int32_t 类型。这一点很重要，因为使用 64 位变量（PackedAccessor64）会使内核变慢。 该函数声明变成了 template __global__ void lltm_cuda_forward_kernel( const torch::PackedTensorAccessor32 gates, const torch::PackedTensorAccessor32 old_cell, torch::PackedTensorAccessor32 new_h, torch::PackedTensorAccessor32 new_cell, torch::PackedTensorAccessor32 input_gate, torch::PackedTensorAccessor32 output_gate, torch::PackedTensorAccessor32 candidate_cell) { //batch index const int n = blockIdx.y; // column index const int c = blockIdx.x * blockDim.x + threadIdx.x; if (c 该实现更具可读性！然后我们可以通过在主函数内使用 .packed_accessor32<> 方法创建打包的存取器来调用此函数。 std::vector lltm_cuda_forward( torch::Tensor input, torch::Tensor weights, torch::Tensor bias, torch::Tensor old_h, torch::Tensor old_cell) { auto X = torch::cat({old_h, input}, /*dim=*/1); auto gate_weights = torch::addmm(bias, X, weights.transpose(0, 1)); const auto batch_size = old_cell.size(0); const auto state_size = old_cell.size(1); auto gates = gate_weights.reshape({batch_size, 3, state_size}); auto new_h = torch::zeros_like(old_cell); auto new_cell = torch::zeros_like(old_cell); auto input_gate = torch::zeros_like(old_cell); auto output_gate = torch::zeros_like(old_cell); auto candidate_cell = torch::zeros_like(old_cell); const int threads = 1024; const dim3 blocks((state_size + threads - 1) / threads, batch_size); AT_DISPATCH_FLOATING_TYPES(gates.type(), \"lltm_forward_cuda\", ([&] { lltm_cuda_forward_kernel>>( gates.packed_accessor32(), old_cell.packed_accessor32(), new_h.packed_accessor32(), new_cell.packed_accessor32(), input_gate.packed_accessor32(), output_gate.packed_accessor32(), candidate_cell.packed_accessor32()); })); return {new_h, new_cell, input_gate, output_gate, candidate_cell, X, gates}; } 向后传播遵循几乎相同的模式，我就不再进一步阐述： template __global__ void lltm_cuda_backward_kernel( torch::PackedTensorAccessor32 d_old_cell, torch::PackedTensorAccessor32 d_gates, const torch::PackedTensorAccessor32 grad_h, const torch::PackedTensorAccessor32 grad_cell, const torch::PackedTensorAccessor32 new_cell, const torch::PackedTensorAccessor32 input_gate, const torch::PackedTensorAccessor32 output_gate, const torch::PackedTensorAccessor32 candidate_cell, const torch::PackedTensorAccessor32 gate_weights) { //batch index const int n = blockIdx.y; // column index const int c = blockIdx.x * blockDim.x + threadIdx.x; if (c lltm_cuda_backward( torch::Tensor grad_h, torch::Tensor grad_cell, torch::Tensor new_cell, torch::Tensor input_gate, torch::Tensor output_gate, torch::Tensor candidate_cell, torch::Tensor X, torch::Tensor gates, torch::Tensor weights) { auto d_old_cell = torch::zeros_like(new_cell); auto d_gates = torch::zeros_like(gates); const auto batch_size = new_cell.size(0); const auto state_size = new_cell.size(1); const int threads = 1024; const dim3 blocks((state_size + threads - 1) / threads, batch_size); AT_DISPATCH_FLOATING_TYPES(X.type(), \"lltm_forward_cuda\", ([&] { lltm_cuda_backward_kernel>>( d_old_cell.packed_accessor32(), d_gates.packed_accessor32(), grad_h.packed_accessor32(), grad_cell.packed_accessor32(), new_cell.packed_accessor32(), input_gate.packed_accessor32(), output_gate.packed_accessor32(), candidate_cell.packed_accessor32(), gates.packed_accessor32()); })); auto d_gate_weights = d_gates.reshape({batch_size, 3*state_size}); auto d_weights = d_gate_weights.t().mm(X); auto d_bias = d_gate_weights.sum(/*dim=*/0, /*keepdim=*/true); auto d_X = d_gate_weights.mm(weights); auto d_old_h = d_X.slice(/*dim=*/1, 0, state_size); auto d_input = d_X.slice(/*dim=*/1, state_size); return {d_old_h, d_input, d_weights, d_bias, d_old_cell, d_gates}; } 将 C++/CUDA 操作与 PyTorch 集成 同样，将支持 CUDA 的操作与 PyTorch 集成非常简单。 如果要编写 setup.py 脚本，它可能看起来像这样： from setuptools import setup from torch.utils.cpp_extension import BuildExtension, CUDAExtension setup( name='lltm', ext_modules=[ CUDAExtension('lltm_cuda', [ 'lltm_cuda.cpp', 'lltm_cuda_kernel.cu', ]) ], cmdclass={ 'build_ext': BuildExtension }) 现在，我们使用 CUDAExtension() 代替 CppExtension()。 我们只需要指定 .cu 文件和 .cpp 文件，该库将替您处理所有麻烦部分。JIT 机制甚至更简单： from torch.utils.cpp_extension import load lltm = load(name='lltm', sources=['lltm_cuda.cpp', 'lltm_cuda_kernel.cu']) 性能比较 我们希望并行化与融合我们代码与 CUDA 的逐点操作将改善我们的 LLTM 的性能。让我们看看是否成立。我们可以运行在前面列出的代码来进行基准测试。我们之前的最快版本是基于 CUDA 的 C++ 代码： Forward: 149.802 us | Backward 393.458 us 现在使用我们的自定义 CUDA 内核： Forward: 129.431 us | Backward 304.641 us 性能得到了更多的提升！ 结论 你现在应该对 PyTorch 的 C++ 扩展机制以及使用它们的动机有一个很好的大致上的了解了。你可以在此处中找到本文中显示的代码示例。如果你有任何疑问，请使用 PyTorch 论坛。如果你遇到任何问题，请务必查看我们的 FAQ。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"advanced/cpp_frontend.html":{"url":"advanced/cpp_frontend.html","title":"使用PyTorch C++ 前端","keywords":"","body":"使用PyTorch C ++前端 所述PyTorch C ++前端是一个纯粹的C ++接口到PyTorch机器学习框架。而到PyTorch主接口自然是Python中，这个Python的API上面坐大幅C ++代码库提供基本的数据结构和功能性，诸如张量和自动分化。 C ++的前端暴露出扩展与机器学习培训和推理所需的工具这一潜在的C ++代码库纯C ++ 11 API。这包括神经网络建模的通用组件的内置集合;一个API来扩展此集合与自定义模块;的流行优化算法如随机梯度下降这样的库;与API并行数据加载定义和数据集负载;系列化例程等等。 本教程将指导您完成训练模式与C ++前端的终端到终端的例子。具体而言，我们将训练 DCGAN - 一种生成模式 - 生成的数字MNIST图像。虽然概念上一个简单的例子，它应该足够给你PyTorch C ++前端的旋风概述和湿你的胃口训练更复杂的模型。我们将与你为什么会想使用C ++前端开始与一些激励的话开始，然后潜水直接进入定义和训练我们的模型。 小费 腕表从CppCon 2018 这个闪电谈话对C ++的前端快速（幽默）的介绍。 Tip 本说明提供C ++前端的部件和设计理念的清扫概述。 Tip 为PyTorch C ++生态系统文档可在https://pytorch.org/cppdocs。在那里，你可以找到高水平的描述以及API级文档。 动机 我们踏上我们的甘斯和MNIST数字令人兴奋的旅程之前，让我们退后一步，并讨论你为什么会想使用C ++前端，而不是Python的一个开始。我们（PyTorch队）创造了C ++前端，以使研究中的Python不能使用，或者是根本就没有为工作的工具环境。对于这样的环境的例子包括： 低延迟系统 ：您可能想要做的强化学习研究在高帧每秒和低延迟要求一个纯C ++游戏引擎。使用纯C ++库是一个更好的拟合比Python库这样的环境。蟒蛇可能不听话，因为在所有的Python解释器的缓慢的。 高多线程环境 ：由于全局解释器锁（GIL），Python不能上同时运行多个系统线程。多是一种选择，但不作为可扩展的，具有显著的缺点。 C ++有没有这样的约束和线程易于使用和创造。需要重并行化，像那些在使用的模型深Neuroevolution ，可以受益于这种。 现有的C ++代码库 ：您可以是现有的C ++应用程序在后端服务器网页服务中的照片编辑软件渲染3D图形做任何事情的所有者，并希望机器学习方法集成到系统中。 C ++的前端可以让你留在C ++和饶了自己的结合来回Python和C ++之间的麻烦，同时保留大部分的传统PyTorch（Python）的经验，灵活性和直观性。 C ++的前端不打算使用Python前端竞争。它的目的是补充。我们知道，研究人员和工程师都喜欢PyTorch它的简单性，灵活性和直观的API。我们的目标是确保你能在每一个可能的环境中充分利用这些核心设计原理的优势，包括上述的那些。如果这些情况之一描述你的使用情况很好，或者如果你对此有兴趣或好奇，就像我们在下面的段落中探索C ++详细前端跟随一起。 Tip C ++的前端试图提供尽可能接近到了Python前端的API。如果您正在使用Python的前端有经验和不断问自己“我怎么做X与C ++前端？”，写你的代码在Python会的方式，往往不是同一个函数和方法将在C ++中如在Python（只记得，以取代双冒号点）。 编写基本应用 首先，让我们写一个最小的C ++应用程序来验证我们对我们的设置在同一页上，并建立环境。首先，你需要抢 LibTorch 分布的副本 - 我们是包中的所有相关标题，库和CMake的构建使用C ++前端所需的文件准备建造的zip压缩包。该LibTorch分布可供下载 PyTorch网站适用于Linux，MacOS和窗户上。本教程将承担基本的Ubuntu Linux操作系统环境的其余部分，但是你可以自由沿在Mac OS或Windows跟随了。 Tip 上安装PyTorch的C ++分布的说明更详细地描述的以下步骤。 Tip 在Windows中，调试和发布版本ABI不兼容。如果您计划建立在调试模式下你的项目，请尝试LibTorch的调试版本。 第一步是在本地下载LibTorch分布，通过从PyTorch网站检索到的链接。对于香草Ubuntu Linux操作系统的环境中，这意味着运行： # If you need e.g. CUDA 9.0 support, please replace \"cpu\" with \"cu90\" in the URL below. wget https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip unzip libtorch-shared-with-deps-latest.zip 接下来，让我们写所谓的dcgan.cpp一个微小的C ++文件，其中包括Torch / torch.h现在来看只是打印出来三乘三个矩阵： #include #include int main() { torch::Tensor tensor = torch::eye(3); std::cout 要构建这个小应用程序，以及我们全面的培训脚本，稍后我们将使用这个的CMakeLists.txt文件： cmake_minimum_required(VERSION 3.0 FATAL_ERROR) project(dcgan) find_package(Torch REQUIRED) add_executable(dcgan dcgan.cpp) target_link_libraries(dcgan \"${TORCH_LIBRARIES}\") set_property(TARGET dcgan PROPERTY CXX_STANDARD 11) 注意 虽然CMake的是LibTorch推荐的构建系统，它不是一个硬性要求。您还可以使用Visual Studio项目文件，QMAKE，普通的Makefile或者你觉得舒服的任何其他构建环境。但是，我们不提供这个外的现成支持。 记在上述文件的CMake线4：find_package（Torch REQUIRED）。这CMake的指示找到了LibTorch库的构建配置。为了让CMake的了解 ，其中 找到这些文件，必须设定CMAKE_PREFIX_PATH当调用cmake的。我们这样做之前，让我们对我们的dcgan应用下面的目录结构达成一致意见： dcgan/ CMakeLists.txt dcgan.cpp 此外，我将提到的路径，解压缩后的LibTorch分发/路径/到/ libtorch。请注意，这 必须是绝对路径[HTG5。特别是，设置CMAKE_PREFIX_PATH喜欢的东西../../libtorch会以意想不到的方式打破。相反，写$ PWD /../../ libtorch来获得相应的绝对路径。现在，我们准备建立我们的应用程序： root@fa350df05ecf:/home# mkdir build root@fa350df05ecf:/home# cd build root@fa350df05ecf:/home/build# cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch .. -- The C compiler identification is GNU 5.4.0 -- The CXX compiler identification is GNU 5.4.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Found torch: /path/to/libtorch/lib/libtorch.so -- Configuring done -- Generating done -- Build files have been written to: /home/build root@fa350df05ecf:/home/build# make -j Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcgan 以上，我们首先创建了dcgan目录内建文件夹，进入该文件夹，跑cmake的命令产生必要的建立（MAKE）文件，并最终通过运行让 -j编制的项目成功。我们目前都在集中执行我们最小的二进制和完成基本的项目配置本节： root@fa350df05ecf:/home/build# ./dcgan 1 0 0 0 1 0 0 0 1 [ Variable[CPUFloatType]{3,3} ] 看起来像一个单位矩阵给我！ 定义神经网络模型 现在，我们已经配置了基本的环境中，我们可以潜入本教程的更有趣的部分。首先，我们将讨论如何定义，并与在C ++前端模块交互。我们将使用由C ++前端提供内置模块的扩展库基本的，小规模的例子模块开始，然后实现一个完整的甘。 模块API基础 与Python接口线的基础上，C ++前端神经网络是由所谓的 模块 可重复使用的构建块。存在来自所有其他模块来源的基本模块类。在Python，这个类是torch.nn.Module和在C ++中它是torch::ン::模块。除了一个向前（）实施模块封装，模块通常包含任何三种子对象的算法方法：参数，缓冲剂和子模块。 参数和缓冲区存储在张量的形式状态。参数的记录梯度，而缓冲器不会。参数通常是你的神经网络训练的权重。缓冲剂的实例包括装置和用于批标准化变化。为了重复使用逻辑和状态的特定块时，PyTorch API允许模块被嵌套。嵌套模块被称为 子模块 。 参数，缓冲区和模块必须明确登记。一旦注册，如参数）的方法（或缓冲剂（）可用于检索所有参数的容器，在整个（嵌套的）模块的层次结构。类似地，如方法（......），其中例如至（torch:: kCUDA）移动的所有参数和缓冲器从CPU到CUDA存储器，工作对整个模块的层次结构。 定义模块和注册参数 为了把这些话转换成代码，让我们考虑用Python接口这个简单的模块： import torch class Net(torch.nn.Module): def __init__(self, N, M): super(Net, self).__init__() self.W = torch.nn.Parameter(torch.randn(N, M)) self.b = torch.nn.Parameter(torch.randn(M)) def forward(self, input): return torch.addmm(self.b, input, self.W) 在C ++中，它应该是这样的： #include struct Net : torch::nn::Module { Net(int64_t N, int64_t M) { W = register_parameter(\"W\", torch::randn({N, M})); b = register_parameter(\"b\", torch::randn(M)); } torch::Tensor forward(torch::Tensor input) { return torch::addmm(b, input, W); } torch::Tensor W, b; }; 就像在Python中，我们定义了一个名为网（为简单起见这里类，而不是A结构 类），并从模块基类派生它。构造函数中，我们创建一个使用Torch 张量:: randn就像我们使用torch.randn在Python。一个有趣的差异是我们如何注册的参数。在Python，我们包裹张量与torch.nn.Parameter类，而在C ++我们通过传递张量的register_parameter方法来代替。这样做的原因是，Python API中可以检测到一个属性是类型torch.nn.Parameter的和自动注册这样张量。在C ++中，反射是非常有限的，因此提供了一种更传统的（和更小神奇）的方法。 注册子模和遍历模块层次结构 以同样的方式，我们可以注册参数，我们还可以注册子模块。在Python，子模块被自动检测和注册时它们被分配作为一个模块的属性： class Net(torch.nn.Module): def __init__(self, N, M): super(Net, self).__init__() # Registered as a submodule behind the scenes self.linear = torch.nn.Linear(N, M) self.another_bias = torch.nn.Parameter(torch.rand(M)) def forward(self, input): return self.linear(input) + self.another_bias 这允许，例如，使用参数（）方法递归地访问在我们的模块层次中的所有参数： >>> net = Net(4, 5) >>> print(list(net.parameters())) [Parameter containing: tensor([0.0808, 0.8613, 0.2017, 0.5206, 0.5353], requires_grad=True), Parameter containing: tensor([[-0.3740, -0.0976, -0.4786, -0.4928], [-0.1434, 0.4713, 0.1735, -0.3293], [-0.3467, -0.3858, 0.1980, 0.1986], [-0.1975, 0.4278, -0.1831, -0.2709], [ 0.3730, 0.4307, 0.3236, -0.0629]], requires_grad=True), Parameter containing: tensor([ 0.2038, 0.4638, -0.2023, 0.1230, -0.0516], requires_grad=True)] 在C ++寄存器子模块，使用恰当地命名为register_module（）方法注册等torch的模块::ン::线性： struct Net : torch::nn::Module { Net(int64_t N, int64_t M) : linear(register_module(\"linear\", torch::nn::Linear(N, M))) { another_bias = register_parameter(\"b\", torch::randn(M)); } torch::Tensor forward(torch::Tensor input) { return linear(input) + another_bias; } torch::nn::Linear linear; torch::Tensor another_bias; }; Tip 你可以找到可用的内置模块一样的完整列表Torch :: NN ::线性，Torch :: NN ::差或Torch :: NN :: Conv2d中的Torch 的文档:: NN命名空间这里。 关于上述代码的一个微妙之处就是为什么子模块是在构造函数的初始化列表中创建的，而参数是在构造函数体内部创建的。有一个很好的理由，我们将在对C ++前端的 所有权模式 下面进一步的部分在此碰。最终的结果，但是，我们可以递归访问我们的模块树的参数，就像在Python。主叫参数（）返回的std ::矢量& LT ;torch::张量& GT ;，我们可以遍历： int main() { Net net(4, 5); for (const auto& p : net.parameters()) { std::cout 其打印： root@fa350df05ecf:/home/build# ./dcgan 0.0345 1.4456 -0.6313 -0.3585 -0.4008 [ Variable[CPUFloatType]{5} ] -0.1647 0.2891 0.0527 -0.0354 0.3084 0.2025 0.0343 0.1824 -0.4630 -0.2862 0.2500 -0.0420 0.3679 -0.1482 -0.0460 0.1967 0.2132 -0.1992 0.4257 0.0739 [ Variable[CPUFloatType]{5,4} ] 0.01 * 3.6861 -10.1166 -45.0333 7.9983 -20.0705 [ Variable[CPUFloatType]{5} ] 有三个参数，就像在Python。还看到这些参数的名称，在C ++ API提供了named_pa​​rameters（）方法，它返回一个OrderedDict就像在Python ： Net net(4, 5); for (const auto& pair : net.named_parameters()) { std::cout 我们可以再次执行看到的输出： root@fa350df05ecf:/home/build# make && ./dcgan 11:13:48 Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcgan b: -0.1863 -0.8611 -0.1228 1.3269 0.9858 [ Variable[CPUFloatType]{5} ] linear.weight: 0.0339 0.2484 0.2035 -0.2103 -0.0715 -0.2975 -0.4350 -0.1878 -0.3616 0.1050 -0.4982 0.0335 -0.1605 0.4963 0.4099 -0.2883 0.1818 -0.3447 -0.1501 -0.0215 [ Variable[CPUFloatType]{5,4} ] linear.bias: -0.0250 0.0408 0.3756 -0.2149 -0.3636 [ Variable[CPUFloatType]{5} ] Note 的文档为torch::ン::模块包含的，关于模块的层次结构进行操作的方法的完整列表。 在正向模式下运行的网络 为了执行在C ++中的网络中，我们只需调用向前（）方法中，我们定义自己： int main() { Net net(4, 5); std::cout 它打印是这样的： root@fa350df05ecf:/home/build# ./dcgan 0.8559 1.1572 2.1069 -0.1247 0.8060 0.8559 1.1572 2.1069 -0.1247 0.8060 [ Variable[CPUFloatType]{2,5} ] 模块所有权 在这一点上，我们知道如何定义在C ++模块，注册参数，注册子模块，通过像参数）的方法（穿越模块的层次结构，并最终运行模块的向前（）方法。虽然还有更多的方法，类和主题的C ++ API中吞噬，我会向您推荐文档完整的菜单。我们也将触及一些概念，我们实现在短短一秒钟DCGAN模型和终端到终端的培训渠道。在这样做之前，让当 所有权模式 C ++的前端提供了Torch 的子类，我简要地谈谈:: NN ::模块 [HTG15。 为了便于讨论，所有权模式是指模块存储并通过周围的方式 - 它决定谁或什么 拥有 特定模块实例。在Python，对象总是动态分配（在堆上），并且具有引用语义。这是非常易于使用和易于理解。事实上，在Python中，你可以在很大程度上忘记对象居住在哪里以及如何他们得到引用，并专注于做事情。 C ++，作为一个较低级别的语言，提供了在此领域的更多选项。这增加了复杂性和严重影响了设计和C ++前端的人体工程学设计。特别地，对于在C ++前端模块，我们有使用 为 值语义 或 引用语义的选项。第一种情况是最简单的和实施例中迄今为止被证明：模块对象被分配在栈上，并传递给函数时，既可以复制，移动（与的std ::移动）或采取引用或指针： struct Net : torch::nn::Module { }; void a(Net net) { } void b(Net& net) { } void c(Net* net) { } int main() { Net net; a(net); a(std::move(net)); b(net); c(&net); } 对于第二种情况 - 参考语义 - 我们可以使用的std :: shared_ptr的。引用传递的好处是，像在Python，它减少想着模块必须如何传递给函数和参数必须如何申报的认知开销（假设你使用shared_ptr的到处）。 struct Net : torch::nn::Module {}; void a(std::shared_ptr net) { } int main() { auto net = std::make_shared(); a(net); } 在我们的经验中，研究人员从动态语言来非常喜欢引用语义过值语义，即使后者更是“原生”到C ++。同样重要的是要注意，Torch :: NN ::模块的设计，以贴近了Python API的人体工程学设计，依赖于共享所有权 [HTG3。例如，利用网 我们先前的（这里缩短）的定义： struct Net : torch::nn::Module { Net(int64_t N, int64_t M) : linear(register_module(\"linear\", torch::nn::Linear(N, M))) { } torch::nn::Linear linear; }; 为了使用线性子模块，我们希望直接存储在我们班。但是，我们也希望在模块的基类来了解并有机会获得这个子模块。为此，它必须保存到该子模块的参考。在这一点上，我们已经到达了需要共享所有权。两者torch::ン::模块类和混凝土净类需要到子模块的引用。由于这个原因，基类存储模块为的shared_ptrS，因此，混凝土类必须太。 可是等等！我没有看到的在上面的代码中的shared_ptr任何提及！这是为什么？那么，因为的std :: shared_ptr的& LT ; MyModule的& GT ;是很多类型的地狱。为了使我们的研究人员生产力，我们想出了一个精心设计的方案，以隐藏提的shared_ptr - 一个好处通常保留值语义 - 同时保持引用语义。要理解这是如何工作的，我们可以看看在Torch 的简化定义:: NN ::线性模块中的核心库（完整的定义是在这里）： struct LinearImpl : torch::nn::Module { LinearImpl(int64_t in, int64_t out); Tensor forward(const Tensor& input); Tensor weight, bias; }; TORCH_MODULE(Linear); 简而言之：将模块不叫线性，但LinearImpl。宏，TORCH_MODULE然后定义实际线性类。这个“而生成”类实际上是在一个包装一的std :: shared_ptr的& LT ; LinearImpl & GT ;。这是一个包装，而不是一个简单的typedef，这样，除其他事项外，还构造如预期，即你仍然可以写Torch :: NN ::线性（3， 4 ）而非的std :: make_shared & LT ; LinearImpl & GT ;（3， 4）。我们呼吁由宏模块 持有者 创建的类。像（共享的）指针，则使用箭头操作者访问底层对象（如模型 - & GT ;向前（......））。最终的结果是所有权模式，类似于Python的API的颇有渊源。引用语义成为默认，但没有额外的输入的std :: shared_ptr的或的std :: make_shared [HTG45。对于我们的网 ，使用模块底座API看起来是这样的： struct NetImpl : torch::nn::Module {}; TORCH_MODULE(Net); void a(Net net) { } int main() { Net net; a(net); } 有一个值得在这里提到一个微妙的问题。默认构造的std :: shared_ptr的是“空的”，即，包含一个空指针。在默认构造什么线性或网？嗯，这是一个棘手的选择。我们可以说，它应该是一个空（NULL）的std :: shared_ptr的& LT ; LinearImpl & GT ; [HTG15。然而，回想线性（3， 4） 是与的std :: make_shared & LT [; ] LinearImpl & GT ;（3， 4） 。这意味着，如果我们已经决定，线性 线性; 应该是一个空指针，则就没有办法来构造的模块不采取任何构造函数的参数，或者默认所有的人。出于这个原因，目前的API中，默认的构建模块保持器（如线性（） ）调用底层模块的默认构造（LinearImpl（） ）。如果底层模块不会有一个默认的构造函数，你得到一个编译错误。为了构建，而不是空的持有人，你可以通过nullptr到支架的构造。 在实践中，这意味着你可以使用子模块要么喜欢早些时候，当模块注册，并在 初始化列表 构造图所示： struct Net : torch::nn::Module { Net(int64_t N, int64_t M) : linear(register_module(\"linear\", torch::nn::Linear(N, M))) { } torch::nn::Linear linear; }; 或者你可以先建立持有人一个空指针，然后分配给它的构造器（用于Pythonistas比较熟悉）： struct Net : torch::nn::Module { Net(int64_t N, int64_t M) { linear = register_module(\"linear\", torch::nn::Linear(N, M)); } torch::nn::Linear linear{nullptr}; // construct an empty holder }; 结论：哪个所有制模式 - 其语义 - 你应该使用？在C ++前端的API最好的支持模块保持所提供的所有权模式。这个机制的唯一缺点是模块声明以下样板的一个额外的行。这就是说，最简单的模型是静止在介绍C ++模块中示出的值语义模型。对于小型，简单的脚本，你可以逃脱它。但你会发现早晚，由于技术原因，它并不总是支持。例如，串行化API（torch::保存和torch::负载）仅支持模块保持器（或纯shared_ptr的）。这样，模块保持器API是定义与C ++前端模块的推荐的方法，我们将在本教程此后使用该API。 限定DCGAN模块 我们现在有必要的背景介绍和定义，我们希望在这个岗位，解决了机器学习任务模块。要回顾一下：我们的任务是生成从 MNIST数据集的数字图像。我们要使用生成对抗网络（GAN）HTG3]来解决这个任务。特别是，我们将使用 DCGAN架构 - 第一，它的那种简单的，但完全足以完成这个任务之一。 Tip 您可以在此存储库在本教程提出了完整的源代码。 什么是甘阿甘？ 甲GAN包括两个不同的神经网络模型：一个 发生器 和a 鉴别 。发电机从噪声分布接收样本，且其目的是将每个噪声采样转变成类似于那些目标分布的图像 - 在我们的情况下，MNIST数据集。反过来鉴别从所述数据集MNIST接收任一 真实 的图像，或者从发电机 假 图像。它被要求发射的概率判断如何真实（越接近1）或假（越接近0）的特定图像是。从如何真正由发电机产生的图像被用来训练发生器鉴别反馈。如何很好的真实性眼睛的鉴别已经被用于优化鉴别反馈。从理论上讲，在发电机和鉴别器之间的微妙平衡使得它们在串联提高，导致发生器产生从目标分布没有区别的图像，欺骗鉴别的（通过随后）优异的眼成发光的0.5的概率两个真假图像。对我们来说，最终的结果是接收噪声作为输入并产生数字作为其输出逼真的图像的机器。 所述发生器模块 我们首先定义发生器模块，它由一系列换位2D卷积，一批归一化和激活RELU单位的。像在Python，PyTorch这里提供了一种用于模型定义两个API：功能性的，其中输入通过连续函数过去了，更之一，我们构建含有序贯模块的面向对象的整个模型的子模块。让我们来看看我们的发电机的外观与任何API，你可以自己决定你更喜欢哪一个。首先，使用序贯： using namespace torch; nn::Sequential generator( // Layer 1 nn::Conv2d(nn::Conv2dOptions(kNoiseSize, 256, 4) .with_bias(false) .transposed(true)), nn::BatchNorm(256), nn::Functional(torch::relu), // Layer 2 nn::Conv2d(nn::Conv2dOptions(256, 128, 3) .stride(2) .padding(1) .with_bias(false) .transposed(true)), nn::BatchNorm(128), nn::Functional(torch::relu), // Layer 3 nn::Conv2d(nn::Conv2dOptions(128, 64, 4) .stride(2) .padding(1) .with_bias(false) .transposed(true)), nn::BatchNorm(64), nn::Functional(torch::relu), // Layer 4 nn::Conv2d(nn::Conv2dOptions(64, 1, 4) .stride(2) .padding(1) .with_bias(false) .transposed(true)), nn::Functional(torch::tanh)); Tip A 序贯模块简单地执行功能的组合物。该第一子模块的输出变成第二输入，第三的输出变为第四等的输入。 特定模块选择，如NN :: Conv2d和NN :: BatchNorm，如下前面概括的结构。的kNoiseSize常数决定输入噪声向量的大小和设置为100。还要注意的是，我们使用Torch :: NN ::功能模块为我们的激活功能，通过它Torch :: RELU为内层和torch::的tanh作为最终活化。超参数进行，当然，通过研究生血统找到。 Note Python的前端具有用于每个激活功能一个模块，如torch.nn.ReLU或torch.nn.Tanh。在C ++中，我们不是仅提供功能模块中，向其中可以传递任何C ++函数，将内部被称为功能“S 向前（）方法。 注意 没有研究生在超参数的发现受到伤害。他们定期喂食Soylent。 对于第二种方法，我们明确地通过模块之间的输入（以功能性方式）在向前（）的模块的方法，我们定义自己： struct GeneratorImpl : nn::Module { GeneratorImpl(int kNoiseSize) : conv1(nn::Conv2dOptions(kNoiseSize, 256, 4) .with_bias(false) .transposed(true)), batch_norm1(256), conv2(nn::Conv2dOptions(256, 128, 3) .stride(2) .padding(1) .with_bias(false) .transposed(true)), batch_norm2(128), conv3(nn::Conv2dOptions(128, 64, 4) .stride(2) .padding(1) .with_bias(false) .transposed(true)), batch_norm3(64), conv4(nn::Conv2dOptions(64, 1, 4) .stride(2) .padding(1) .with_bias(false) .transposed(true)), batch_norm4(64), conv5(nn::Conv2dOptions(64, 1, 4) .stride(2) .padding(1) .with_bias(false) .transposed(true)) { // register_module() is needed if we want to use the parameters() method later on register_module(\"conv1\", conv1); register_module(\"conv2\", conv2); register_module(\"conv3\", conv3); register_module(\"conv4\", conv4); register_module(\"batch_norm1\", batch_norm1); register_module(\"batch_norm2\", batch_norm1); register_module(\"batch_norm3\", batch_norm1); } torch::Tensor forward(torch::Tensor x) { x = torch::relu(batch_norm1(conv1(x))); x = torch::relu(batch_norm2(conv2(x))); x = torch::relu(batch_norm3(conv3(x))); x = torch::tanh(conv4(x)); return x; } nn::Conv2d conv1, conv2, conv3, conv4; nn::BatchNorm batch_norm1, batch_norm2, batch_norm3; }; TORCH_MODULE(Generator); Generator generator; 我们使用哪种方法，我们现在可以调用向前（）关于发生器映射一个噪声样本的图像。 Note 在途中选项的简要字被传递给内置模块等Conv2d在C ++前端：每个模块具有一些所需的选项，如特征为[数HTG5] BatchNorm。如果你只需要配置所需选项，你可以直接将它们传递到模块的构造，如BatchNorm（128）或降（0.5）或Conv2d（8， 4， 2）（用于输入信道数，输出信道数，和内核大小）。但是，如果你需要修改的其他选项，这通常是默认，如with_bias对Conv2d，你需要构建并传递一个 项 对象。在C ++前端每个模块都有一个相关联的选项结构，称为ModuleOptions其中模块是模块的名称，如LinearOptions为线性。这就是我们的Conv2d上述模块做。 所述鉴别器模块 鉴别器是同样的卷积，批次归一和激活的序列。然而，盘旋现在一些约定俗成的，而不是换位，我们用一个漏水的RELU为0.2，而不是香草RELU的alpha值。另外，最终活化变为乙状结肠，其南瓜值成范围在0和1之间然后，我们可以解释这些压扁值作为鉴别器分配给图像是真实的概率： nn::Sequential discriminator( // Layer 1 nn::Conv2d( nn::Conv2dOptions(1, 64, 4).stride(2).padding(1).with_bias(false)), nn::Functional(torch::leaky_relu, 0.2), // Layer 2 nn::Conv2d( nn::Conv2dOptions(64, 128, 4).stride(2).padding(1).with_bias(false)), nn::BatchNorm(128), nn::Functional(torch::leaky_relu, 0.2), // Layer 3 nn::Conv2d( nn::Conv2dOptions(128, 256, 4).stride(2).padding(1).with_bias(false)), nn::BatchNorm(256), nn::Functional(torch::leaky_relu, 0.2), // Layer 4 nn::Conv2d( nn::Conv2dOptions(256, 1, 3).stride(1).padding(0).with_bias(false)), nn::Functional(torch::sigmoid)); Note 当该功能我们通过功能需要更多的参数比单个张量，我们可以将它们传递到功能构造，这将它们转发到每个函数调用。对于上面的泄漏RELU，这意味着torch:: leaky_relu（previous_output_tensor， 0.2）是调用。 载入数据 现在，我们已经定义了发电机和鉴别模型，我们需要一些数据，我们可以一起训练这些模型。 C ++的前端，诸如Python之一，带有一个强大的并行数据加载器。该数据加载器可以读取一个数据集的数据批次（你可以自己定义），并提供了许多配置旋钮。 Note 虽然Python数据加载程序使用多处理中，C ++数据加载器是真正的多线程和不启动任何新的过程。 数据加载是C ++前端的数据API，包含在torch::数据::命名空间的一部分。这个API是由几个不同的部分组成： 数据加载器类， 用于定义数据集的API， 用于限定 变换 的API，其可以被应用到数据集， 用于限定 取样 ，它产生与数据集编索引的索引的API， 现有数据集，转换器和采样库。 在本教程中，我们可以使用自带的C ++前端的MNIST数据集。让我们来实例化一个Torch ::数据::数据集:: MNIST对于这一点，并应用两个转变：一是标准化的图像，使它们在范围-1-至+1（从原始范围至[0 `HTG21] 1 ）。其次，我们应用堆栈 `整理 ，这需要一批张量，并将它们堆叠成沿着第一维度单一张量： auto dataset = torch::data::datasets::MNIST(\"./mnist\") .map(torch::data::transforms::Normalize<>(0.5, 0.5)) .map(torch::data::transforms::Stack<>()); 需要注意的是MNIST数据集应位于./mnist相对于无论你执行从训练二进制文件目录。您可以使用这个脚本下载MNIST数据集。 接下来，我们创建了一个数据加载器，并通过它这个数据集。为了使新的数据加载，我们使用Torch ::数据:: make_data_loader，它返回的一个的std ::的unique_ptr正确的类型（这取决于数据集，采样器和其他一些实施细节的类型的类型）： auto data_loader = torch::data::make_data_loader(std::move(dataset)); 数据装载的确有很多的选择。您可以检查整套此处[HTG1。例如，为了加快数据加载，我们可以增加工人的数量。默认号码是零，这意味着主线程将被使用。如果我们设置工人至2，两个线程将同时催生该负载数据。我们还应该从它的默认增加批量大小1 HTG12]的东西比较合理，喜欢64（值kBatchSize）。因此，让我们创建一个DataLoaderOptions对象，并设置相应的属性： auto data_loader = torch::data::make_data_loader( std::move(dataset), torch::data::DataLoaderOptions().batch_size(kBatchSize).workers(2)); 现在，我们可以写一个循环来加载数据，我们将只打印到控制台现在的批次： for (torch::data::Example<>& batch : *data_loader) { std::cout () 由数据装入程序在这种情况下返回的类型是torch::数据::实施例。这种类型是一个简单的结构与用于数据的数据字段和一个标签目标字段。因为我们应用了堆栈核对之前，则数据加载器仅返回一个这样的例子。如果我们没有施加核对，数据加载器将产生的std ::矢量& LT ;torch::数据::实施例& LT ; [ - - ] GT ; & GT ;代替，以在每批次例如一个元素。 如果重建并运行这段代码，你会看到这样的事情： root@fa350df05ecf:/home/build# make Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcgan root@fa350df05ecf:/home/build# make [100%] Built target dcgan root@fa350df05ecf:/home/build# ./dcgan Batch size: 64 | Labels: 5 2 6 7 2 1 6 7 0 1 6 2 3 6 9 1 8 4 0 6 5 3 3 0 4 6 6 6 4 0 8 6 0 6 9 2 4 0 2 8 6 3 3 2 9 2 0 1 4 2 3 4 8 2 9 9 3 5 8 0 0 7 9 9 Batch size: 64 | Labels: 2 2 4 7 1 2 8 8 6 9 0 2 2 9 3 6 1 3 8 0 4 4 8 8 8 9 2 6 4 7 1 5 0 9 7 5 4 3 5 4 1 2 8 0 7 1 9 6 1 6 5 3 4 4 1 2 3 2 3 5 0 1 6 2 Batch size: 64 | Labels: 4 5 4 2 1 4 8 3 8 3 6 1 5 4 3 6 2 2 5 1 3 1 5 0 8 2 1 5 3 2 4 4 5 9 7 2 8 9 2 0 6 7 4 3 8 3 5 8 8 3 0 5 8 0 8 7 8 5 5 6 1 7 8 0 Batch size: 64 | Labels: 3 3 7 1 4 1 6 1 0 3 6 4 0 2 5 4 0 4 2 8 1 9 6 5 1 6 3 2 8 9 2 3 8 7 4 5 9 6 0 8 3 0 0 6 4 8 2 5 4 1 8 3 7 8 0 0 8 9 6 7 2 1 4 7 Batch size: 64 | Labels: 3 0 5 5 9 8 3 9 8 9 5 9 5 0 4 1 2 7 7 2 0 0 5 4 8 7 7 6 1 0 7 9 3 0 6 3 2 6 2 7 6 3 3 4 0 5 8 8 9 1 9 2 1 9 4 4 9 2 4 6 2 9 4 0 Batch size: 64 | Labels: 9 6 7 5 3 5 9 0 8 6 6 7 8 2 1 9 8 8 1 1 8 2 0 7 1 4 1 6 7 5 1 7 7 4 0 3 2 9 0 6 6 3 4 4 8 1 2 8 6 9 2 0 3 1 2 8 5 6 4 8 5 8 6 2 Batch size: 64 | Labels: 9 3 0 3 6 5 1 8 6 0 1 9 9 1 6 1 7 7 4 4 4 7 8 8 6 7 8 2 6 0 4 6 8 2 5 3 9 8 4 0 9 9 3 7 0 5 8 2 4 5 6 2 8 2 5 3 7 1 9 1 8 2 2 7 Batch size: 64 | Labels: 9 1 9 2 7 2 6 0 8 6 8 7 7 4 8 6 1 1 6 8 5 7 9 1 3 2 0 5 1 7 3 1 6 1 0 8 6 0 8 1 0 5 4 9 3 8 5 8 4 8 0 1 2 6 2 4 2 7 7 3 7 4 5 3 Batch size: 64 | Labels: 8 8 3 1 8 6 4 2 9 5 8 0 2 8 6 6 7 0 9 8 3 8 7 1 6 6 2 7 7 4 5 5 2 1 7 9 5 4 9 1 0 3 1 9 3 9 8 8 5 3 7 5 3 6 8 9 4 2 0 1 2 5 4 7 Batch size: 64 | Labels: 9 2 7 0 8 4 4 2 7 5 0 0 6 2 0 5 9 5 9 8 8 9 3 5 7 5 4 7 3 0 5 7 6 5 7 1 6 2 8 7 6 3 2 6 5 6 1 2 7 7 0 0 5 9 0 0 9 1 7 8 3 2 9 4 Batch size: 64 | Labels: 7 6 5 7 7 5 2 2 4 9 9 4 8 7 4 8 9 4 5 7 1 2 6 9 8 5 1 2 3 6 7 8 1 1 3 9 8 7 9 5 0 8 5 1 8 7 2 6 5 1 2 0 9 7 4 0 9 0 4 6 0 0 8 6 ... 这意味着我们能够成功地从MNIST数据集加载数据。 写作训练循环 现在，让我们完成我们的例子中的算法部分和实施发电机和鉴别之间微妙的舞蹈。首先，我们将创建两个优化，一个发电机和一个用于鉴别。在优化我们使用实现亚当算法： torch::optim::Adam generator_optimizer( generator->parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5)); torch::optim::Adam discriminator_optimizer( discriminator->parameters(), torch::optim::AdamOptions(5e-4).beta1(0.5)); Note 在撰写本文时，C ++的前端提供了实施Adagrad，亚当，LBFGS，RMSprop和SGD优化。在文档有向上的最新名单。 接下来，我们需要更新我们的训练循环。我们将增加一个外环用尽数据加载每个时间段，然后写GAN训练码： for (int64_t epoch = 1; epoch & batch : *data_loader) { // Train discriminator with real images. discriminator->zero_grad(); torch::Tensor real_images = batch.data; torch::Tensor real_labels = torch::empty(batch.data.size(0)).uniform_(0.8, 1.0); torch::Tensor real_output = discriminator->forward(real_images); torch::Tensor d_loss_real = torch::binary_cross_entropy(real_output, real_labels); d_loss_real.backward(); // Train discriminator with fake images. torch::Tensor noise = torch::randn({batch.data.size(0), kNoiseSize, 1, 1}); torch::Tensor fake_images = generator->forward(noise); torch::Tensor fake_labels = torch::zeros(batch.data.size(0)); torch::Tensor fake_output = discriminator->forward(fake_images.detach()); torch::Tensor d_loss_fake = torch::binary_cross_entropy(fake_output, fake_labels); d_loss_fake.backward(); torch::Tensor d_loss = d_loss_real + d_loss_fake; discriminator_optimizer.step(); // Train generator. generator->zero_grad(); fake_labels.fill_(1); fake_output = discriminator->forward(fake_images); torch::Tensor g_loss = torch::binary_cross_entropy(fake_output, fake_labels); g_loss.backward(); generator_optimizer.step(); std::printf( \"\\r[%2ld/%2ld][%3ld/%3ld] D_loss: %.4f | G_loss: %.4f\", epoch, kNumberOfEpochs, ++batch_index, batches_per_epoch, d_loss.item(), g_loss.item()); } } 上面，我们首先评估真实图像，它应该指定一个高概率的鉴别。对于这一点，我们使用torch::空（batch.data.size（0））。uniform_（0.8， 1.0）作为目标概率。 Note 我们挑选到处都在为了使鉴别训练更强大的0.8和1.0，而不是1.0之间均匀分布的随机值。这一招被称为 标签平滑[HTG1。 评估鉴别之前，我们归零其参数的梯度。计算损失后，我们通过网络通过调用d_loss.backward（）来计算新的梯度回传播。我们重复这个高谈阔论的假像。而不是使用图片来自数据集的，我们让发电机通过喂养它了一批随机噪声的创建这种假像。然后，我们这些假图像转发到鉴别。这一次，我们要鉴别发出低概率，最好全部为零。一旦我们计算了两个批次的真实与虚假批图像的鉴别损失，我们可以以更新其参数一步进展鉴别的优化。 为了训练发电机，我们再次先零的梯度，然后重新评估的假图像鉴别。然而，这一次我们要鉴别分配的概率非常接近，这表明该发生器可以产生这种欺骗鉴别以为他们实际上是（从数据集）的真实图像。为此，我们填补fake_labels全部为一张量。我们终于步发电机的优化也更新其参数。 现在，我们应该准备训练CPU上我们的模型。我们没有任何代码尚未捕获状态或样品产出，但我们会在短短的时刻添加此。现在，就让我们看到，我们的模型是做 东西 - 我们将根据生成的图像这个东西是否是有意义的再验证。重新构建和运行应打印是这样的： root@3c0711f20896:/home/build# make && ./dcgan Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcga [ 1/10][100/938] D_loss: 0.6876 | G_loss: 4.1304 [ 1/10][200/938] D_loss: 0.3776 | G_loss: 4.3101 [ 1/10][300/938] D_loss: 0.3652 | G_loss: 4.6626 [ 1/10][400/938] D_loss: 0.8057 | G_loss: 2.2795 [ 1/10][500/938] D_loss: 0.3531 | G_loss: 4.4452 [ 1/10][600/938] D_loss: 0.3501 | G_loss: 5.0811 [ 1/10][700/938] D_loss: 0.3581 | G_loss: 4.5623 [ 1/10][800/938] D_loss: 0.6423 | G_loss: 1.7385 [ 1/10][900/938] D_loss: 0.3592 | G_loss: 4.7333 [ 2/10][100/938] D_loss: 0.4660 | G_loss: 2.5242 [ 2/10][200/938] D_loss: 0.6364 | G_loss: 2.0886 [ 2/10][300/938] D_loss: 0.3717 | G_loss: 3.8103 [ 2/10][400/938] D_loss: 1.0201 | G_loss: 1.3544 [ 2/10][500/938] D_loss: 0.4522 | G_loss: 2.6545 ... 移动到GPU 虽然我们当前的脚本可以运行在CPU上就好了，大家都知道卷积都在GPU快了很多。让我们快速讨论如何我们的培训走上了GPU。我们需要为这个做两件事情：一个GPU设备规范传递给我们分配自己张量，并明确通过以（）法任何其他张量复制到所有GPU张量和模块在C ++前端有。实现这两个最简单的方法是在我们的训练脚本的顶层创建Torch ::设备的实例，然后将该设备传递给张厂的功能，如torch::零以及至（）方法。我们可以通过与CPU设备这样开始： // Place this somewhere at the top of your training script. torch::Device device(torch::kCPU); 新张量分配像 torch::Tensor fake_labels = torch::zeros(batch.data.size(0)); 应该被更新为取装置作为最后一个参数： torch::Tensor fake_labels = torch::zeros(batch.data.size(0), device); 对于张量其创建是不在我们手里，像那些从MNIST数据集的到来，我们必须插入明确的以（）通话。这意味着 torch::Tensor real_images = batch.data; 变 torch::Tensor real_images = batch.data.to(device); 而且我们的模型参数应该被移动到正确的设备： generator->to(device); discriminator->to(device); Note 如果张量已经住供应到在设备上（），该呼叫是一个空操作。无需额外的副本。 在这一点上，我们只是做我们以前的CPU-居住代码更加明确。不过，现在也很容易对设备更改为CUDA设备： torch::Device device(torch::kCUDA) 而现在所有的张量将住在GPU上，调入快速CUDA内核的所有操作，不用我们无需更改任何代码的下游。如果我们想要指定一个特定的设备索引，它可以作为第二个参数设备构造函数传递。如果我们想要不同张量住在不同设备上，我们可以（CUDA装置0和其他CUDA装置1上例如一种）通过单独的装置的实例。我们甚至可以这样做动态配置，这往往是有益的，使我们的培训脚本更便于携带： torch::Device device = torch::kCPU; if (torch::cuda::is_available()) { std::cout 甚至 torch::Device device(torch::cuda::is_available() ? torch::kCUDA : torch::kCPU); 检查点和恢复训练状况 最后的增强，我们应该对我们的训练脚本定期保存我们的模型参数，我们优化的状态，以及一些生成的图像样本的状态。如果我们的电脑是在训练过程中间崩溃，前两个将使我们能够恢复训练状态。对于长期的培训课程，这是绝对必要的。幸运的是，C ++前端提供了一个API来序列和反序列化两者模型和优化器状态，​​以及个别张量。 核心API因为这是Torch ::保存（的东西，文件名）HTG2]和Torch ::负载（的东西，文件名）HTG6]其中事情可以是torch::ン::模块亚类或类似的亚当的优化实例对象，我们在我们的培训讲稿。让我们来更新我们的训练循环检查点在一定的时间间隔模型和优化状态： if (batch_index % kCheckpointEvery == 0) { // Checkpoint the model and optimizer state. torch::save(generator, \"generator-checkpoint.pt\"); torch::save(generator_optimizer, \"generator-optimizer-checkpoint.pt\"); torch::save(discriminator, \"discriminator-checkpoint.pt\"); torch::save(discriminator_optimizer, \"discriminator-optimizer-checkpoint.pt\"); // Sample the generator and save the images. torch::Tensor samples = generator->forward(torch::randn({8, kNoiseSize, 1, 1}, device)); torch::save((samples + 1.0) / 2.0, torch::str(\"dcgan-sample-\", checkpoint_counter, \".pt\")); std::cout checkpoint \" 其中kCheckpointEvery是一个整数设置为类似100检查点每100批次和checkpoint_counter是一个反撞我们每一个检查点的时间。 要恢复训练状态，可以将所有的模型之后添加这样的诗句和优化创建，但训练循环之前： torch::optim::Adam generator_optimizer( generator->parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5)); torch::optim::Adam discriminator_optimizer( discriminator->parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5)); if (kRestoreFromCheckpoint) { torch::load(generator, \"generator-checkpoint.pt\"); torch::load(generator_optimizer, \"generator-optimizer-checkpoint.pt\"); torch::load(discriminator, \"discriminator-checkpoint.pt\"); torch::load( discriminator_optimizer, \"discriminator-optimizer-checkpoint.pt\"); } int64_t checkpoint_counter = 0; for (int64_t epoch = 1; epoch & batch : *data_loader) { 检查生成的图像 我们的培训剧本现已完成。我们准备训练我们的甘，无论是在CPU或GPU。要检查我们的训练过程的中介输出，为此我们添加的代码，以图像样本定期保存到“dcgan-sample-xxx.pt”文件，我们可以写一个小Python脚本加载张量，并与matplotlib显示它们： from __future__ import print_function from __future__ import unicode_literals import argparse import matplotlib.pyplot as plt import torch parser = argparse.ArgumentParser() parser.add_argument(\"-i\", \"--sample-file\", required=True) parser.add_argument(\"-o\", \"--out-file\", default=\"out.png\") parser.add_argument(\"-d\", \"--dimension\", type=int, default=3) options = parser.parse_args() module = torch.jit.load(options.sample_file) images = list(module.parameters())[0] for index in range(options.dimension * options.dimension): image = images[index].detach().cpu().reshape(28, 28).mul(255).to(torch.uint8) array = image.numpy() axis = plt.subplot(options.dimension, options.dimension, 1 + index) plt.imshow(array, cmap=\"gray\") axis.get_xaxis().set_visible(False) axis.get_yaxis().set_visible(False) plt.savefig(options.out_file) print(\"Saved \", options.out_file) 现在让我们来训练我们的模型大约30时期： root@3c0711f20896:/home/build# make && ./dcgan 10:17:57 Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcgan CUDA is available! Training on GPU. [ 1/30][200/938] D_loss: 0.4953 | G_loss: 4.0195 -> checkpoint 1 [ 1/30][400/938] D_loss: 0.3610 | G_loss: 4.8148 -> checkpoint 2 [ 1/30][600/938] D_loss: 0.4072 | G_loss: 4.36760 -> checkpoint 3 [ 1/30][800/938] D_loss: 0.4444 | G_loss: 4.0250 -> checkpoint 4 [ 2/30][200/938] D_loss: 0.3761 | G_loss: 3.8790 -> checkpoint 5 [ 2/30][400/938] D_loss: 0.3977 | G_loss: 3.3315 ... -> checkpoint 120 [30/30][938/938] D_loss: 0.3610 | G_loss: 3.8084 并显示出图的图像： root@3c0711f20896:/home/build# python display.py -i dcgan-sample-100.pt Saved out.png 这应该是这个样子： 数字！万岁！现在球在你的场内：您可以改进模型，使数字更好看？ 结论 本教程希望能够给您的PyTorch C ++前端的消化消化。机器学习库像PyTorch必然具有非常广阔和丰富的API。因此，有很多的概念，我们没有时间或空间在这里讨论。不过，我鼓励你尝试了API，当你遇到问题请教我们的文档，特别是库API 部分。此外，请记住，你可以期望的C ++前端遵循的设计和Python的前端的语义，只要我们能做到这一点，那么你可以利用这一点来提高你的学习速度。 Tip You can find the full source code presented in this tutorial in this repository. 与往常一样，如果您遇到任何问题或有任何疑问，您可以使用我们的论坛或 GitHub的问题取得联系。 Previous Was this helpful? Yes No Thank you ©版权所有2017年，PyTorch。 使用PyTorch C ++前端 动机 编写基本应用 定义神经网络模型 模块API基础 定义模块和注册参数 注册子模和遍历模块层次结构 在正向模式中运行的网络 模块所有权 定义DCGAN模块 什么是甘阿甘？ [HTG0所述发生器模块 [HTG0所述鉴别器模块 加载数据 写作训练循环 移动到GPU 和点校验恢复训练状况 检查生成的图像 结论 ![](https://www.facebook.com/tr?id=243028289693773&ev=PageView &noscript=1) 分析流量和优化经验，我们为这个站点的Cookie。通过点击或导航，您同意我们的cookies的使用。因为这个网站目前维护者，Facebook的Cookie政策的适用。了解更多信息，包括有关可用的控制：[饼干政策HTG1。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes/autograd.html":{"url":"notes/autograd.html","title":"自动求导机制","keywords":"","body":"自动求导机制 译者：冯宝宝校验：AlexJakin 本说明将概述autograd（自动求导）如何工作并记录每一步操作。了解这些并不是绝对必要的，但我们建议您熟悉它，因为它将帮助你编写更高效，更清晰的程序，并可以帮助您进行调试。 反向排除子图 每个张量都有一个标志：requires_grad，允许从梯度计算中细致地排除子图，并可以提高效率。 requires_grad 只要有单个输入进行梯度计算操作，则其输出也需要梯度计算。相反，只有当所有输入都不需要计算梯度时，输出才不需要梯度计算。如果其中所有的张量都不需要进行梯度计算，后向计算不会在子图中执行。 >>> x = torch.randn(5, 5) # requires_grad=False by default >>> y = torch.randn(5, 5) # requires_grad=False by default >>> z = torch.randn((5, 5), requires_grad=True) >>> a = x + y >>> a.requires_grad False >>> b = a + z >>> b.requires_grad True 当你想要冻结部分模型或者事先知道不会使用某些参数的梯度时，这个requires_grad标志非常有用。例如，如果要微调预训练的CNN，只需在冻结的基础中切换requires_grad标志就够了，并且直到计算到达最后一层，才会保存中间缓冲区，，其中仿射变换将使用所需要梯度的权重 ，网络的输出也需要它们。 model = torchvision.models.resnet18(pretrained=True) for param in model.parameters(): param.requires_grad = False # Replace the last fully-connected layer # Parameters of newly constructed modules have requires_grad=True by default model.fc = nn.Linear(512, 100) # Optimize only the classifier optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9) 自动求导是如何记录编码历史的 自动求导是反向自动分化系统。从概念上讲，自动求导会记录一个图形，记录在执行操作时创建数据的所有操作，为您提供有向无环图，其叶子是输入张量，根节点是输出张量。通过从根到叶跟踪此图，您可以使用链法则自动计算梯度。 在内部，autograd将此图表示为Function对象（实际表达式）的图形，可以用来计算评估图形的结果。 当计算前向传播时，自动求导同时执行所请求的计算并建立表示计算梯度的函数的图形（每个torch.Tensor的.grad_fn属性是该图的入口点）。当前向传播完成时，我们在后向传播中评估该图以计算梯度。 需要注意的一点是，在每次迭代时都会从头开始重新创建计算图，这正是允许使用任意Python控制流语句的原因，它可以在每次迭代时更改图形的整体形状和大小。 在开始训练之前，不必编码所有可能的路径 - 您运行的是您所区分的部分。 使用autograd进行in-place操作 在autograd中支持in-place操作是一件很难的事情，大多数情况下，我们不鼓励使用它们。Autograd积极的缓冲区释放和重用使其非常高效，实际上在in-place操作会大幅降低内存使用量的情况也非常少。除非在巨大的内存压力下运行，否则你可能永远不需要使用它们。 限制in-place操作适用性的主要原因有两个： 这个操作可能会覆盖梯度计算所需的值。 实际上，每个in-place操作需要重写计算图。out-of-place版本只是分配新对象并保留对旧图的引用，而in-place操作则需要将所有输入的creator更改为表示此操作的Function。这就比较麻烦，特别是如果有许多变量引用同一存储（例如通过索引或转置创建的），并且如果被修改输入的存储被任何其他张量引用，这样的话，in-place函数会抛出错误。 In-place正确性检查 每一个张量都有一个版本计算器，每次在任何操作中标记都会递增。 当Function保存任何用于后向传播的张量时，也会保存包含张量的版本计数器。一旦访问self.saved_tensors后，它将被检查，如果它大于保存的值，则会引发错误。这可以确保如果您使用in-place函数而没有看到任何错误，则可以确保计算出的梯度是正确的。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes/broadcasting.html":{"url":"notes/broadcasting.html","title":"广播语义","keywords":"","body":"广播语义 译者：冯宝宝 校验：AlexJakin 许许多多的PyTorch操作都支持NumPy Broadcasting Semantics。 简而言之，如果PyTorch操作支持广播，那么它的Tensor参数可以自动扩展为相同的类型大小（不需要复制数据）。 一般语义 如果遵守以下规则，则两个张量是“可广播的”： 每个张量至少有一个维度； 遍历张量维度大小时，从末尾随开始遍历，两个张量的维度满足以下条件之一：大小必须相等、它们其中一个为1、他们其中一个不存在。 例如： >>> x=torch.empty(5,7,3) >>> y=torch.empty(5,7,3) # 相同形状的张量可以被广播(上述规则总是成立的) >>> x=torch.empty((0,)) >>> y=torch.empty(2,2) # x和y不能被广播,因为x没有维度 # can line up trailing dimensions >>> x=torch.empty(5,3,4,1) >>> y=torch.empty( 3,1,1) # x和y能够广播. # 1st trailing dimension: both have size 1 # 2nd trailing dimension: y has size 1 # 3rd trailing dimension: x size == y size # 4th trailing dimension: y dimension doesn't exist # 但是: >>> x=torch.empty(5,2,4,1) >>> y=torch.empty( 3,1,1) # x和y不能被广播 （ ） 如果x,y两个张量是可以广播的，则通过计算得到的张量大小遵循以下原则： 如果x和y的维数不相等，则在维度较小的张量的前面增加1个维度，使它们的长度相等。 然后,生成新张量维度的大小是x和y在每个维度上的最大值。 例如： # can line up trailing dimensions to make reading easier >>> x=torch.empty(5,1,4,1) >>> y=torch.empty( 3,1,1) >>> (x+y).size() torch.Size([5, 3, 4, 1]) # but not necessary: >>> x=torch.empty(1) >>> y=torch.empty(3,1,7) >>> (x+y).size() torch.Size([3, 1, 7]) >>> x=torch.empty(5,2,4,1) >>> y=torch.empty(3,1,1) >>> (x+y).size() RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1 In - place 语义 一个复杂因素是in-place操作不允许in-place张量像广播那样改变形状。 例如： >>> x=torch.empty(5,3,4,1) >>> y=torch.empty(3,1,1) >>> (x.add_(y)).size() torch.Size([5, 3, 4, 1]) # but: >>> x=torch.empty(1,3,1) >>> y=torch.empty(3,1,7) >>> (x.add_(y)).size() RuntimeError: The expanded size of the tensor (1) must match the existing size (7) at non-singleton dimension 2. 向后兼容性 PyTorch的早期版本允许某些逐点函数在具有不同形状的张量上执行，只要每个张量中的元素数量相等即可。 然后通过将每个张量视为1维来执行逐点运算。PyTorch现在支持广播，并且“1维”逐点行为被认为已弃用，并且在张量不可广播但具有相同数量的元素的情况下将生成Python警告。 注意，在两个张量不具有相同形状但是可广播并且具有相同数量元素的情况下，广播的引入可能导致向后不兼容。例如： >>> torch.add(torch.ones(4,1), torch.randn(4)) 以前可能会产生一个torch.Size（[4,1]）的Tensor，但现在会产生一个torch.Size（[4,4]）这样的Tensor。 为了帮助识别代码中可能存在广播引起的向后不兼容性的情况，您可以设置torch.utils.backcompat.broadcast_warning.enabled 为 True，在这种情况下会产生python警告。 >>> torch.utils.backcompat.broadcast_warning.enabled=True >>> torch.add(torch.ones(4,1), torch.ones(4)) __main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements. Changing behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional. 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes/cpu_threading_torchscript_inference.html":{"url":"notes/cpu_threading_torchscript_inference.html","title":"CPU线程和TorchScript推理","keywords":"","body":"CPU线程和TorchScript推理 PyTorch允许TorchScript模型推理过程中使用多个CPU线程。下图显示了不同程度的并行人们会发现在一个典型的应用： 一个或多个线程推断在给定的输入，执行一个模型的直传。每个推理线程调用JIT解释执行模型内嵌的OPS，一个接一个。模型可以利用一个叉 TorchScript原语发起异步任务。在一次分叉几个操作导致在并行执行的任务。的叉操作者返回一个未来可用于稍后同步上，例如对象： @torch.jit.script def compute_z(x): return torch.mm(x, self.w_z) @torch.jit.script def forward(x): # launch compute_z asynchronously: fut = torch.jit._fork(compute_z, x) # execute the next operation in parallel to compute_z: y = torch.mm(x, self.w_y) # wait for the result of compute_z: z = torch.jit._wait(fut) return y + z PyTorch使用单个线程池的-OP间并行性，这个线程池是由在应用过程中的分叉的所有任务进行推演共享。 除了- OP间并行性，也PyTorch可以利用OPS（帧内运算并行）内的多个线程。这可能是在许多情况下，包括大张量等元素方面的OPS，卷积，GEMMS，嵌入查找和有用的。 构建选项 PyTorch使用内部ATEN库来实现欢声笑语。除此之外，PyTorch还可以与支持外部库，如 MKL 和 MKL-DNN ，加快对CPU计算的建造。 ATEN，MKL和MRL-DNN支持内部运算的并行和取决于以下并行库来实现它： 的OpenMP - 一个标准（和图书馆，通常有一个编译器运），广泛用于外部库; > TBB - 一个较新的并行库基于任务的并行性和并发环境优化。 > > OpenMP的历史已经使用了大量的库。它是著名的相对易用性和支持基于循环的并行性和其他原语。与此同时OpenMP是不知道与应用程序使用的其他线程库一个良好的互操作性。特别是，OpenMP的并不能保证一个每个进程内部运算的线程池会在应用程序中使用。相反，两个不同的运算线程间可能会使用内部的运算工作不同OpenMP的线程池。这可能会导致大量的应用程序所使用的线程。 TBB是用来在外部库在较小程度上，但，在同一时间，为并发环境进行了优化。 PyTorch的TBB后端保证有一个独立的，单一的，每个进程内部运算线程通过所有在运行的应用程序的OPS的使用池。 根据不同的使用情况下，可能会发现一个或另一个并行库在他们的应用程序更好的选择。 PyTorch允许在构建时用下面的生成选项使用宏正和其他库并行后端的选择： 图书馆 | 构建选项 | 值 | 笔记 ---|---|---|--- ATEN | ATEN_THREADING | OMP（默认），TBB | MKL | MKL_THREADING | （相同） | 为了使MKL使用BLAS = MKL MRL-DNN | MKLDNN_THREADING | (same) | 为了使MKL-DNN用USE_MKLDNN = 1 强烈建议不要一个构建中混合使用OpenMP和TBB。 任何TBB值的上述要求USE_TBB = 1建立设定（缺省值：OFF）。一个单独的设置USE_OPENMP = 1（默认值：ON）需要将OpenMP并行。 运行时API 下面的API是用来控制线的设置： 并行的类型 | 设置 | Notes ---|---|--- 跨运算的并行 | 在:: set_num_interop_threads，在:: get_num_interop_threads（C ++） set_num_interop_threads，get_num_interop_threads（Python中， torch 模块） | 设定*功能只能使用一次，并且只有在启动期间调用时，实际的运算符之前运行; 线程的默认编号：CPU核心数量。 内部运算的并行 | 在:: set_num_threads，在:: get_num_threads（C ++）set_num_threads`` get_num_threads（Python中， torch模块） 环境变量：OMP_NUM_THREADS和MKL_NUM_THREADS 对于内部运算的并行设置，在:: set_num_threads，torch.set_num_threads总是优先于环境变量，MKL_NUM_THREADS变量优先于OMP_NUM_THREADS。 注意 可用于调试parallel_info关于线程设置和工具打印信息。类似的输出也可以在Python与torch.__配置得到__。parallel_info（）调用。 Next Previous ©版权所有2019年，Torch 贡献者。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes/cuda.html":{"url":"notes/cuda.html","title":"CUDA语义","keywords":"","body":"CUDA 语义 译者：片刻 校验：AlexJakin torch.cuda 用于设置和运行 CUDA 操作。它会跟踪当前选定的GPU，并且默认情况下会在该设备上创建您分配的所有 CUDA tensors。可以使用 torch.cuda.device 上下文管理器更改所选设备。 但是，一旦分配了 tensor，就可以对其进行操作而不管所选择的设备如何，结果将始终与 tensor 放在同一设备上。 默认情况下不允许跨 GPU 操作，除了 copy_() 具有类似复制功能的其他方法，例如 to() 和 cuda()。除非您启用点对点内存访问，否则任何尝试在不同设备上传播的 tensor 上启动操作都会引发错误。 下面我们用一个小例子来展示: cuda = torch.device('cuda') # Default CUDA device cuda0 = torch.device('cuda:0') cuda2 = torch.device('cuda:2') # GPU 2 (these are 0-indexed) x = torch.tensor([1., 2.], device=cuda0) # x.device is device(type='cuda', index=0) y = torch.tensor([1., 2.]).cuda() # y.device is device(type='cuda', index=0) with torch.cuda.device(1): # allocates a tensor on GPU 1 a = torch.tensor([1., 2.], device=cuda) # transfers a tensor from CPU to GPU 1 b = torch.tensor([1., 2.]).cuda() # a.device and b.device are device(type='cuda', index=1) # You can also use ``Tensor.to`` to transfer a tensor: b2 = torch.tensor([1., 2.]).to(device=cuda) # b.device and b2.device are device(type='cuda', index=1) c = a + b # c.device is device(type='cuda', index=1) z = x + y # z.device is device(type='cuda', index=0) # even within a context, you can specify the device # (or give a GPU index to the .cuda call) d = torch.randn(2, device=cuda2) e = torch.randn(2).to(cuda2) f = torch.randn(2).cuda(cuda2) # d.device, e.device, and f.device are all device(type='cuda', index=2) 异步执行 默认情况下，GPU 操作是异步的。当您调用使用 GPU 的函数时，操作将排入特定设备，但不一定要在以后执行。这允许我们并行执行更多计算，包括在 CPU 或其他 GPU 上的操作。 通常，异步计算的效果对于调用者是不可见的，因为 (1) 每个设备按照它们排队的顺序执行操作，以及 (2) PyTorch 在 CPU 和 GPU 之间或两个 GPU 之间复制数据时自动执行必要的同步。因此，计算将如同每个操作同步执行一样进行。 您可以通过设置环境变量强制进行同步计算 CUDA_LAUNCH_BLOCKING=1。这在 GPU 上发生错误时非常方便。（使用异步执行时，直到实际执行操作后才会报告此类错误，因此堆栈跟踪不会显示请求的位置。） 异步计算的结果是没有同步的时间测量是不精确的。要获得精确的测量结果，应该在测量之前调用torch.cuda.synchronize()，或者使用torch.cuda.Event记录时间如下： start_event = torch.cuda.Event(enable_timing=True) end_event = torch.cuda.Event(enable_timing=True) start_event.record() # 在这里执行一些操作 end_event.record() torch.cuda.synchronize() # Wait for the events to be recorded! elapsed_time_ms = start_event.elapsed_time(end_event) 作为一个例外，有几个函数，例如 to() 和 copy_() 允许一个显式 non_blocking 参数，它允许调用者在不需要时绕过同步。另一个例外是 CUDA streams，如下所述。 CUDA streams CUDA stream 是执行的线性序列属于特定的设备。您通常不需要显式创建一个：默认情况下，每个设备使用自己的 “default” stream。 每个流内的操作按创建顺序进行序列化，但不同流的操作可以按任何相对顺序同时执行，除非使用显式同步功能（如 synchronize() 或 wait_stream() ）。例如，以下代码不正确: cuda = torch.device('cuda') s = torch.cuda.Stream() # Create a new stream. A = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0) with torch.cuda.stream(s): # sum() may start execution before normal_() finishes! B = torch.sum(A) 当 “current stream” 是 default stream 时，PyTorch 在数据移动时自动执行必要的同步，如上所述。但是，使用 non-default streams 时，用户有责任确保正确同步。 内存管理 PyTorch 使用缓存内存分配器来加速内存分配。这允许在没有设备同步的情况下快速释放内存。但是，分配器管理的未使用内存仍将显示为使用 nvidia-smi。您可以使用 memory_allocated() 和 max_memory_allocated() 监视张量占用的内存，并使用 memory_cached() 和 max_memory_cached() 监视缓存分配器管理的内存。调用 empty_cache() 可以从 PyTorch 释放所有 unused 的缓存内存，以便其他 GPU 应用程序可以使用它们。但是，tensor 占用的 GPU 内存不会被释放，因此无法增加 PyTorch 可用的 GPU 内存量。 最佳做法 设备无关的代码 由于 PyTorch 的结构，您可能需要显式编写设备无关（CPU或GPU）代码; 一个例子可能是创建一个新的张量作为递归神经网络的初始隐藏状态。 第一步是确定是否应该使用GPU。常见的模式是使用Python的 argparse 模块读入用户参数，并有一个可用于禁用 CUDA 的标志，并结合使用 is_available()。在下文中，args.device 结果 torch.device 可以用于将 tensor 移动到 CPU 或 CUDA 的对象。 import argparse import torch parser = argparse.ArgumentParser(description='PyTorch Example') parser.add_argument('--disable-cuda', action='store_true', help='Disable CUDA') args = parser.parse_args() args.device = None if not args.disable_cuda and torch.cuda.is_available(): args.device = torch.device('cuda') else: args.device = torch.device('cpu') 现在 args.device 我们可以使用它在所需的设备上创建 Tensor。 x = torch.empty((8, 42), device=args.device) net = Network().to(device=args.device) 这可以在许多情况下用于产生与设备无关的代码。以下是使用 dataloader 时的示例: cuda0 = torch.device('cuda:0') # CUDA GPU 0 for i, x in enumerate(train_loader): x = x.to(cuda0) 在系统上使用多个 GPU 时，可以使用 CUDA_VISIBLE_DEVICES 环境标志来管理 PyTorch 可用的 GPU。如上所述，要手动控制创建张量的GPU，最佳做法是使用 torch.cuda.device 上下文管理器。 print(\"Outside device is 0\") # On device 0 (default in most scenarios) with torch.cuda.device(1): print(\"Inside device is 1\") # On device 1 print(\"Outside device is still 0\") # On device 0 如果你有一个 tensor 并且想在同一个设备上创建一个相同类型的新 tensor，那么你可以使用一个 torch.Tensor.new_* 方法（参见参考资料 torch.Tensor）。虽然前面提到的 torch.* factory 函数（ Creation Ops ）依赖于当前 GPU 上下文和您传入的属性参数，但 torch.Tensor.new_* 方法会保留设备和 tensor 的其他属性。 在创建在前向传递期间需要在内部创建新 tensor 的模块时，这是建议的做法。 cuda = torch.device('cuda') x_cpu = torch.empty(2) x_gpu = torch.empty(2, device=cuda) x_cpu_long = torch.empty(2, dtype=torch.int64) y_cpu = x_cpu.new_full([3, 2], fill_value=0.3) print(y_cpu) tensor([[ 0.3000, 0.3000], [ 0.3000, 0.3000], [ 0.3000, 0.3000]]) y_gpu = x_gpu.new_full([3, 2], fill_value=-5) print(y_gpu) tensor([[-5.0000, -5.0000], [-5.0000, -5.0000], [-5.0000, -5.0000]], device='cuda:0') y_cpu_long = x_cpu_long.new_tensor([[1, 2, 3]]) print(y_cpu_long) tensor([[ 1, 2, 3]]) 如果你想创建一个与另一个 tensor 相同类型和大小的 tensor，并用一个或零填充它，ones_like() 或 zeros_like() 作为方便的辅助函数（也保留 Tensor 的 torch.device 和 torch.dtype ）提供。 x_cpu = torch.empty(2, 3) x_gpu = torch.empty(2, 3) y_cpu = torch.ones_like(x_cpu) y_gpu = torch.zeros_like(x_gpu) 使用固定内存缓冲区 当源自固定（页面锁定）内存时，主机到 GPU 副本的速度要快得多。CPU tensor 和存储器公开一种 pin_memory() 方法，该方法返回对象的副本，数据放在固定区域中。 此外，一旦您固定张量或存储，您就可以使用异步GPU副本。只需将一个额外的 non_blocking=True 参数传递给一个 cuda() 调用。这可以用于通过计算重叠数据传输。 您可以 DataLoader 通过传递 pin_memory=True 给构造函数使返回批处理放置在固定内存中。 使用 nn.DataParallel 而不是多处理 涉及批量输入和多个 GPU 的大多数用例应默认 DataParallel 使用多个GPU。即使使用GIL，单个 Python 进程也可以使多个 GPU 饱和。 从版本 0.1.9 开始，可能无法充分利用大量 GPUs (8+)。但是，这是一个正在积极开发的已知问题。一如既往，测试您的用例。 使用带有 multiprocessing 的 CUDA 模型有一些重要的注意事项 ; 除非注意完全满足数据处理要求，否则您的程序可能会有不正确或未定义的行为。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes/extending.html":{"url":"notes/extending.html","title":"扩展PyTorch","keywords":"","body":"扩展PyTorch 译者：PEGASUS1993 本章中，将要介绍使用我们的C库如何扩展torch.nn，torch.autograd和编写自定义的C扩展工具。 扩展torch.autograd 添加操作autograd需要Function为每个操作实现一个新的子类。回想一下，Function使用autograd来计算结果和梯度，并对操作历史进行编码。每个新功能都需要您实现两种方法： forward() - 执行操作的代码。如果您指定了默认值，则可以根据需求使用任意参数，其中一些参数可选。这里支持各种Python对象。Variable参数在调用之前会被转换Tensor，并且它们的使用情况将在graph中注册。请注意，此逻辑不会遍历lists/dicts/和其他任何数据的结构，并且只考虑被直接调用的Variables参数。如果有多个输出你可以返回单个Tensor或Tensor格式的元组。另外，请参阅Function文档查找只能被forward()调用的有用方法的说明。 backward() - 计算梯度的公式. 它将被赋予与输出一样多的Variable参数, 其中的每一个表示对应梯度的输出. 它应该返回与输入一样多的Variable, 其中的每一个表示都包含其相应输入的梯度. 如果输入不需要计算梯度 (请参阅needs_input_grad属性),或者是非Variable对象,则可返回None类.此外,如果你在forward()方法中有可选的参数,则可以返回比输入更多的梯度,只要它们都是None类型即可. 你可以从下面的代码看到torch.nn模块的Linear函数, 以及注解 # Inherit from Function class Linear(Function): # bias is an optional argument def forward(self, input, weight, bias=None): self.save_for_backward(input, weight, bias) output = input.mm(weight.t()) if bias is not None: output += bias.unsqueeze(0).expand_as(output) return output # This function has only a single output, so it gets only one gradient def backward(self, grad_output): # This is a pattern that is very convenient - at the top of backward # unpack saved_tensors and initialize all gradients w.r.t. inputs to # None. Thanks to the fact that additional trailing Nones are # ignored, the return statement is simple even when the function has # optional inputs. input, weight, bias = self.saved_tensors grad_input = grad_weight = grad_bias = None # These needs_input_grad checks are optional and there only to # improve efficiency. If you want to make your code simpler, you can # skip them. Returning gradients for inputs that don't require it is # not an error. if self.needs_input_grad[0]: grad_input = grad_output.mm(weight) if self.needs_input_grad[1]: grad_weight = grad_output.t().mm(input) if bias is not None and self.needs_input_grad[2]: grad_bias = grad_output.sum(0).squeeze(0) return grad_input, grad_weight, grad_bias 现在，为了更方便使用这些自定义操作，推荐使用apply方法： linear = LinearFunction.apply 我们下面给出一个由非变量参数进行参数化的函数的例子: class MulConstant(Function): @staticmethod def forward(ctx, tensor, constant): # ctx is a context object that can be used to stash information # for backward computation ctx.constant = constant return tensor * constant @staticmethod def backward(ctx, grad_output): # We return as many input gradients as there were arguments. # Gradients of non-Tensor arguments to forward must be None. return grad_output * ctx.constant, None 注意 向后输入，即grad_output，也可以是跟踪历史的张量。因此，如果使用可微运算来实现向后运算（例如，调用另一个自定义函数），则更高阶导数将起作用。 你可能想检测你刚刚实现的backward方法是否正确的计算了梯度。你可以使用小的有限差分法(Finite Difference)进行数值估计。 from torch.autograd import gradcheck # gradcheck takes a tuple of tensors as input, check if your gradient # evaluated with these tensors are close enough to numerical # approximations and returns True if they all verify this condition. input = (Variable(torch.randn(20,20).double(), requires_grad=True), Variable(torch.randn(30,20).double(), requires_grad=True),) test = gradcheck(Linear.apply, input, eps=1e-6, atol=1e-4) print(test) 有关有限差分梯度比较的更多详细信息，请参见数值梯度检查。 扩展 torch.nn nn模块包含两种接口 - modules和他们的功能版本。你可以用两种方法扩展它,但是我们建议，在扩展layer的时候使用modules， 因为modules保存着参数和buffer。如果使用无参数操作的话，那么建议使用激活函数，池化等函数。 在上面的章节中,添加操作的功能版本已经介绍过了。 增加一个Module。 由于nn大量使用autograd。所以， 添加一个新的Module类需要实现一个Function类, 它会执行对应的操作并且计算梯度。我们只需要很少的代码就可以实现上面Linear模块的功能。现在，我们需要实现两个函数： __init__ (optional) - 接收kernel sizes内核大小，特征数量等参数，并初始化parameters参数和buffers缓冲区。 forward() - 实例化Function并使用它来执行操作。它与上面显示的functional wrapper非常相似。 下面是实现Linear模块的方式： class Linear(nn.Module): def __init__(self, input_features, output_features, bias=True): super(Linear, self).__init__() self.input_features = input_features self.output_features = output_features # nn.Parameter is a special kind of Variable, that will get # automatically registered as Module's parameter once it's assigned # as an attribute. Parameters and buffers need to be registered, or # they won't appear in .parameters() (doesn't apply to buffers), and # won't be converted when e.g. .cuda() is called. You can use # .register_buffer() to register buffers. # nn.Parameters require gradients by default. self.weight = nn.Parameter(torch.Tensor(output_features, input_features)) if bias: self.bias = nn.Parameter(torch.Tensor(output_features)) else: # You should always register all possible parameters, but the # optional ones can be None if you want. self.register_parameter('bias', None) # Not a very smart way to initialize weights self.weight.data.uniform_(-0.1, 0.1) if bias is not None: self.bias.data.uniform_(-0.1, 0.1) def forward(self, input): # See the autograd section for explanation of what happens here. return LinearFunction.apply(input, self.weight, self.bias) def extra_repr(self): # (Optional)Set the extra information about this module. You can test # it by printing an object of this class. return 'in_features={}, out_features={}, bias={}'.format( self.in_features, self.out_features, self.bias is not None ) 编写自定义的C++扩展 有关详细说明和示例，请参阅此PyTorch教程。 文档可在torch.utils.cpp_extension获得。 编写自定义的C扩展 可用示例可以在这个Github仓库里面查看参考。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes/faq.html":{"url":"notes/faq.html","title":"常见问题","keywords":"","body":"常见问题解答 译者：冯宝宝 我的模型报告“cuda runtime error(2): out of memory” 正如错误消息所示，您的GPU显存已耗尽。由于经常在PyTorch中处理大量数据，因此小错误会迅速导致程序耗尽所有GPU资源; 幸运的是，这些情况下的修复通常很简单。这里有一些常见点需要检查： 不要在训练循环中积累历史记录。 默认情况下，涉及需要梯度计算的变量将保留历史记录。这意味着您应该避免在计算中使用这些变量，因为这些变量将超出您的训练循环，例如，在跟踪统计数据时。相反，您应该分离变量或访问其基础数据。 有时，当可微分变量发生时，它可能是不明显的。考虑以下训练循环（从源代码中删除）： total_loss = 0 for i in range(10000): optimizer.zero_grad() output = model(input) loss = criterion(output) loss.backward() optimizer.step() total_loss += loss 在这里，total_loss在您的训练循环中累积历史记录，因为丢失是具有自动记录历史的可微分变量。 您可以通过编写total_loss + = float（loss）来解决此问题。 此问题的其他实例：1。 不要抓住你不需要的张量或变量。 如果将张量或变量分配给本地，则在本地超出范围之前，Python不会解除分配。您可以使用del x释放此引用。 同样，如果将张量或向量分配给对象的成员变量，则在对象超出范围之前不会释放。如果您没有保留不需要的临时工具，您将获得最佳的内存使用量。 本地规模大小可能比您预期的要大。 例如： for i in range(5): intermediate = f(input[i]) result += g(intermediate) output = h(result) return output 在这里，即使在执行h时，中间变量仍然存在，因为它的范围超出了循环的末尾。要提前释放它，你应该在完成它时使用del。 不要在太大的序列上运行RNN。 通过RNN反向传播所需的存储量与RNN的长度成线性关系; 因此，如果您尝试向RNN提供过长的序列，则会耗尽内存。 这种现象的技术术语是随着时间的推移而反向传播，并且有很多关于如何实现截断BPTT的参考，包括在单词语言模型示例中; 截断由重新打包功能处理，如本论坛帖子中所述。 不要使用太大的线性图层。 线性层nn.Linear（m，n）使用O(nm)存储器：也就是说，权重的存储器需求与特征的数量成比例。 以这种方式很容易占用你的存储（并且记住，你将至少需要两倍存储权值的内存量，因为你还需要存储梯度。） My GPU memory isn’t freed properly PyTorch使用缓存内存分配器来加速内存分配。 因此，nvidia-smi中显示的值通常不会反映真实的内存使用情况。 有关GPU内存管理的更多详细信息，请参阅内存管理 。 如果在Python退出后你的GPU内存仍旧没有被释放，那么很可能是一些Python子进程仍处于活动状态。你可以通过ps -elf |grep python找到它们并用kill -9 [pid]手动结束这些进程。 My data loader workers return identical random numbers 您可能正在数据集中使用其他库来生成随机数。 例如，当通过fork启动工作程序子进程时，NumPy的RNG会重复。有关如何使用worker_init_fn选项在工作程序中正确设置随机种子的文档，请参阅torch.utils.data.DataLoader文档。 My recurrent network doesn’t work with data parallelism 在具有DataParallel或data_parallel()的模块中使用pack sequence -> recurrent network -> unpack sequence模式时有一个非常微妙的地方。每个设备上的forward()的输入只会是整个输入的一部分。由于默认情况下，解包操作torch.nn.utils.rnn.pad_packed_sequence()仅填充到其所见的最长输入，即该特定设备上的最长输入，所以在将结果收集在一起时会发生尺寸的不匹配。因此，您可以利用pad_packed_sequence()的 total_length参数来确保forward()调用返回相同长度的序列。例如，你可以写： from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence class MyModule(nn.Module): # ... __init__, 以及其他访求 # padding_input 的形状是[B x T x *]（batch_first 模式），包含按长度排序的序列 # B 是批量大小 # T 是最大序列长度 def forward(self, padded_input, input_lengths): total_length = padded_input.size(1) # get the max sequence length packed_input = pack_padded_sequence(padded_input, input_lengths, batch_first=True) packed_output, _ = self.my_lstm(packed_input) output, _ = pad_packed_sequence(packed_output, batch_first=True, total_length=total_length) return output m = MyModule().cuda() dp_m = nn.DataParallel(m) 另外，在批量的维度为dim 1（即 batch_first = False ）时需要注意数据的并行性。在这种情况下，pack_padded_sequence 函数的的第一个参数 padding_input 维度将是 [T x B x *] ，并且应该沿dim 1 （第1轴）分散，但第二个参数 input_lengths 的维度为 [B]，应该沿dim 0 （第0轴）分散。需要额外的代码来操纵张量的维度。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes/large_scale_deployments.html":{"url":"notes/large_scale_deployments.html","title":"对于大规模部署的特点","keywords":"","body":"对于大规模部署的特点 舰队宽操作者剖析 API使用记录 将元数据附加到保存TorchScript模型 构建环境的考虑 普通的扩展点 本说明有关一个更大的系统内运行PyTorch或操作在一个更大的组织使用PyTorch多个系统时可能有用的几个扩展点和技巧讲座。 它不包括生产部署模型的主题。检查 torch.jit或相应的教程。 该说明假定您无论是从源组织中的建PyTorch或有静态链接用于PyTorch时加载额外的代码的能力。因此，许多钩的公开为C ++的API，可以一次在一个集中的地方来触发，例如在静态初始化代码。 舰队宽操作仿形 PyTorch自带torch.autograd.profiler能够测量按需采取个体经营者时间的。人们可以使用相同的机制做“永远在线”的三围运行PyTorch任何进程。这可能是收集有关在给定的过程或整组机器的运行PyTorch工作负荷的信息是有用的。 对于任何运营商调用回调新可以用添加Torch :: autograd ::探查:: pushCallback [HTG3。钩将与被称为Torch :: autograd ::探查:: RecordFunction结构描述调用上下文（例如，名称）。如果启用，RecordFunction ::输入（） 包含表示为torch:: IValue变体类型的函数的自变量。请注意，该输入记录是比较昂贵的，因此，必须明确启用。 调用回调增加了一些开销，所以通常它只是随机采样操作调用有用。这可以在每个回调基础上通过Torch :: autograd ::探查:: setSamplingProbability 中指定的全局采样率启用。 请注意，pushCallback和setSamplingProbability不是线程安全的，没有PyTorch操作运行，只有当可以被调用。通常情况下，这是一个好主意，在初始化时调用了它们一次。 下面是一个例子： // Called somewhere in the program beginning void init() { // Sample one in a hundred operator runs randomly torch::autograd::setSamplingProbability(0.01); pushCallback( &onFunctionEnter, &onFunctionExit, /* needs_inputs */ true, /* sampled */ true ); } void onFunctionEnter(const RecordFunction& fn) { std::cerr API使用登录 当在更广泛的生态系统中运行，例如在管理作业调度程序，但是这是要跟踪的二进制文件调用特定的API PyTorch。存在于触发一个给定的回调几个重要的API点注射简单的仪器。因为通常PyTorch是一次性的Python脚本调用，回调火灾只有一次针对每个API的规定的处理。 C10 :: SetAPIUsageHandler可用于注册API使用的仪器处理程序。传递的参数将是一个“API密钥”识别用于点，例如python.import [HTG7用于PyTorch延长进口或torch.script.compile [HTG11如果TorchScript编译被触发。`` SetAPIUsageLogger([](const std::string& event_name) { std::cerr 注意为开发新的API的触发点可以在代码被添加与C10_LOG_API_USAGE_ONCE（ “my_api”）在C ++或torch._C._log_api_usage_once（“我的。 API“）在Python。 将元数据附加到保存TorchScript模型 TorchScript模块可以保存为捆绑串行化参数和模块代码作为TorchScript存档文件（见 torch.jit.save（） ）。这是很方便的与该模型一起捆绑的其他信息，例如，模型制作者或辅助的工件的描述。 它可以通过使_extra_files参数为 torch.jit.save（）和[HTG10来实现] Torch :: JIT ::负载 存储和保存过程中检索任意的二进制块。由于TorchScript文件定期ZIP档案，额外的信息被存储作为普通的文件归档的额外/目录内。 还有一个全局钩子允许额外的文件附加到当前进程产生的任何TorchScript存档。这可能是与制片人的元数据，类似于数码相机产生的JPEG元数据标签模型有用。用法示例可能类似于： SetExportModuleExtraFilesHook([](const script::Module&) { script::ExtraFilesMap files; files[\"producer_info.json\"] = \"{\\\"user\\\": \\\"\" + getenv(\"USER\") + \"\\\"}\"; return files; }); 构建环境的考虑 TorchScript的编译需要访问原来的Python文件，因为它使用python的inspect.getsource通话。在某些生产环境，可能需要与预编译.pyc文件沿着明确部署的.py文件。 普通的扩展点 PyTorch的API通常是松散耦合的，很容易与专门的版本替换部件。常见的扩展点包括： 用C语言实现运营商定制++ - 参见了解详情教程。 自定义数据读取经常可以通过调用相应的Python库直接集成。的 现有功能torch.utils.data可以通过扩展被利用 数据集或 IterableDataset。 Next Previous ©版权所有2019年，Torch 贡献者。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes/multiprocessing.html":{"url":"notes/multiprocessing.html","title":"多处理最佳实践","keywords":"","body":"多进程最佳实践 译者：cvley torch.multiprocessing 是 Python 的 multiprocessing 的直接替代模块。它支持完全相同的操作，但进行了扩展，这样所有的张量就可以通过一个 multiprocessing.Queue 进行传递，将数据移动到共享内存并只将句柄传递到另一个进程。 注意 当一个 Tensor 传递到另一个进程时，Tensor 的数据是共享的。如果 torch.Tensor.grad 不是 None, 也会被共享。在一个没有 torch.Tensor.grad 域的 Tensor 被送到其他进程时，一个标准的进程专用的 .grad Tensor 会被创建，而它在所有的进程中不会自动被共享，与 Tensor 数据的共享方式不同。 这就允许实现各种训练方法，比如 Hogwild、A3C，或者其他那些需要异步操作的方法。 共享 CUDA 张量 进程间共享 CUDA 张量仅支持 Python 3，使用的是 spawn 或者 forkserver 启动方法。Python 2 中的 multiprocessing 仅使用 fork 来创建子进程，而 CUDA 运行时不支持该方法。 警告 CUDA API 需要分配给其他进程的显存在它们还在使用的情况下一直有效。你需要仔细确保共享的 CUDA 张量若非必须，不会超出使用范围。这对于共享模型参数不会是一个问题，但传递其他类型的数据时需要谨慎。注意该限制并不适用于共享 CPU 内存。 也可以参考：使用 nn.DataParallel 替代 multiprocessing 最佳实践和提示 避免和处理死锁 当创建一个新进程时，很多情况会发生，最常见的就是后台线程间的死锁。如果任何一个线程有锁的状态或者引入了一个模块，然后调用了fork，子进程很有可能处于中断状态，并以另外的方式死锁或者失败。注意即使你没这么做，Python 内建的库也有可能这么做——无需舍近求远，multiprocessing即是如此。multiprocessing.Queue 实际上是一个非常复杂的类，可以创建多个线程用于串行、发送和接收对象，它们也会出现前面提到的问题。如果你发现自己遇到了这种情况，尝试使用 multiprocessing.queues.SimpleQueue，它不会使用额外的线程。 我们在尽最大努力为你化繁为简，确保不会发生死锁的情况，但有时也会出现失控的情况。如果你遇到任何暂时无法解决的问题，可以在论坛上求助，我们将会研究是否可以修复。 通过 Queue 传递重用缓存 记住每次将一个 Tensor 放进一个 multiprocessing.Queue 时，它就会被移动到共享内存中。如果它已经被共享，那将不会有操作，否则将会触发一次额外的内存拷贝，而这将会拖慢整个进程。即使你有一个进程池把数据发送到一个进程，并把缓存送回来——这近乎于无操作，在发送下一个批次的数据时避免拷贝。 异步多进程训练（如Hogwild） 使用 torch.multiprocessing，可以异步训练一个模型，参数要么一直共享，要么周期性同步。在第一个情况下，我们建议传递整个模型的对象，而对于后一种情况，我们将以仅传递 state_dict()。 我们建议使用 multiprocessing.Queue在进程间传递 PyTorch 对象。当使用fork命令时，可以进行诸如继承共享内存中的张量和存储的操作，然而这个操作容易产生问题，应该小心使用，仅建议高级用户使用。Queue，尽管有时不是一个那么优雅的解决方案，但在所有的情况下都可以合理使用。 警告 你应该注意那些不在if __name__ == '__main__'中的全局声明。如果使用了一个不是fork的系统调用，它们将会在所有子进程中执行。 Hogwild 在示例仓库中可以找到一个具体的Hogwild实现，但除了完整的代码结构之外，下面也有一个简化的例子： import torch.multiprocessing as mp from model import MyModel def train(model): # Construct data_loader, optimizer, etc. for data, labels in data_loader: optimizer.zero_grad() loss_fn(model(data), labels).backward() optimizer.step() # This will update the shared parameters if __name__ == '__main__': num_processes = 4 model = MyModel() # NOTE: this is required for the ``fork`` method to work model.share_memory() processes = [] for rank in range(num_processes): p = mp.Process(target=train, args=(model,)) p.start() processes.append(p) for p in processes: p.join() 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes/randomness.html":{"url":"notes/randomness.html","title":"重复性","keywords":"","body":"再生性 译者：ApacheCN PyTorch版本，单个提交或不同平台无法保证完全可重现的结果。此外，即使使用相同的种子，也不需要在CPU和GPU执行之间重现结果。 但是，为了在一个特定平台和PyTorch版本上对您的特定问题进行计算确定，需要采取几个步骤。 PyTorch中涉及两个伪随机数生成器，您需要手动播种以使运行可重现。此外，您应该确保您的代码依赖于使用随机数的所有其他库也使用固定种子。 PyTorch 您可以使用为所有设备（CPU和CUDA）播种RNG： import torch torch.manual_seed(0) 有一些PyTorch函数使用CUDA函数，这些函数可能是非确定性的来源。一类这样的CUDA函数是原子操作，特别是atomicAdd，其中对于相同值的并行加法的顺序是未确定的，并且对于浮点变量，是结果中的变化源。在前向中使用atomicAdd的PyTorch函数包括，。 许多操作具有向后使用atomicAdd，特别是许多形式的池，填充和采样。目前没有简单的方法来避免这些功能中的非确定性。 CuDNN 在CuDNN后端运行时，必须设置另外两个选项： torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False 警告 确定性模式可能会对性能产生影响，具体取决于您的型号。 NumPy 如果您或您使用的任何库依赖于Numpy，您也应该为Numpy RNG播种。这可以通过以下方式完成： import numpy as np np.random.seed(0) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes/serialization.html":{"url":"notes/serialization.html","title":"序列化语义","keywords":"","body":"序列化的相关语义 译者：yuange250 最佳方案 保存模型的推荐方法 Pytorch主要有两种方法可用于序列化和保存一个模型。 第一种只存取模型的参数（更为推荐）： 保存参数： torch.save(the_model.state_dict(), PATH) 读取参数： the_model = TheModelClass(*args, **kwargs) the_model.load_state_dict(torch.load(PATH)) 第二种方法则将整个模型都保存下来： torch.save(the_model, PATH) 读取的时候也是读取整个模型： the_model = torch.load(PATH) 在第二种方法中, 由于特定的序列化的数据与其特定的类别(class)相绑定，并且在序列化的时候使用了固定的目录结构，所以在很多情况下，如在其他的一些项目中使用，或者代码进行了较大的重构的时候，很容易出现问题。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes/windows.html":{"url":"notes/windows.html","title":"Windows 常见问题","keywords":"","body":"Windows FAQ 译者：冯宝宝 从源码中构建 包含可选组件 Windows PyTorch有两个受支持的组件：MKL和MAGMA。 以下是使用它们构建的步骤。 REM Make sure you have 7z and curl installed. REM Download MKL files curl https://s3.amazonaws.com/ossci-windows/mkl_2018.2.185.7z -k -O 7z x -aoa mkl_2018.2.185.7z -omkl REM Download MAGMA files REM cuda90/cuda92/cuda100 is also available in the following line. set CUDA_PREFIX=cuda80 curl -k https://s3.amazonaws.com/ossci-windows/magma_2.4.0_%CUDA_PREFIX%_release.7z -o magma.7z 7z x -aoa magma.7z -omagma REM Setting essential environment variables set \"CMAKE_INCLUDE_PATH=%cd%\\\\mkl\\\\include\" set \"LIB=%cd%\\\\mkl\\\\lib;%LIB%\" set \"MAGMA_HOME=%cd%\\\\magma\" 为Windows构建加速CUDA Visual Studio当前不支持并行自定义任务。 作为替代方案，我们可以使用Ninja来并行化CUDA构建任务。 只需键入几行代码即可使用它。 REM Let's install ninja first. pip install ninja REM Set it as the cmake generator set CMAKE_GENERATOR=Ninja 脚本一键安装 你可以参考这些脚本。它会给你指导方向。 扩展 CFEI扩展 对CFFI扩展的支持是非常试验性的。在Windows下启用它通常有两个步骤。 首先，在Extension对象中指定其他库以使其在Windows上构建。 ffi = create_extension( '_ext.my_lib', headers=headers, sources=sources, define_macros=defines, relative_to=__file__, with_cuda=with_cuda, extra_compile_args=[\"-std=c99\"], libraries=['ATen', '_C'] # Append cuda libaries when necessary, like cudart ) 其次，这是“由extern THCState *state状态引起的未解决的外部符号状态”的工作场所; 将源代码从C更改为C ++。 下面列出了一个例子。 #include #include THCState *state = at::globalContext().thc_state; extern \"C\" int my_lib_add_forward_cuda(THCudaTensor *input1, THCudaTensor *input2, THCudaTensor *output) { if (!THCudaTensor_isSameSizeAs(state, input1, input2)) return 0; THCudaTensor_resizeAs(state, output, input1); THCudaTensor_cadd(state, output, input1, 1.0, input2); return 1; } extern \"C\" int my_lib_add_backward_cuda(THCudaTensor *grad_output, THCudaTensor *grad_input) { THCudaTensor_resizeAs(state, grad_input, grad_output); THCudaTensor_fill(state, grad_input, 1); return 1; } C++扩展 与前一种类型相比，这种类型的扩展具有更好的支持。不过它仍然需要一些手动配置。首先，打开VS 2017的x86_x64交叉工具命令提示符。然后，在其中打开Git-Bash。它通常位于C：\\Program Files\\Git\\git-bash.exe中。最后，您可以开始编译过程。 安装 在Win32 找不到安装包 Solving environment: failed PackagesNotFoundError: The following packages are not available from current channels: - pytorch Current channels: - https://conda.anaconda.org/pytorch/win-32 - https://conda.anaconda.org/pytorch/noarch - https://repo.continuum.io/pkgs/main/win-32 - https://repo.continuum.io/pkgs/main/noarch - https://repo.continuum.io/pkgs/free/win-32 - https://repo.continuum.io/pkgs/free/noarch - https://repo.continuum.io/pkgs/r/win-32 - https://repo.continuum.io/pkgs/r/noarch - https://repo.continuum.io/pkgs/pro/win-32 - https://repo.continuum.io/pkgs/pro/noarch - https://repo.continuum.io/pkgs/msys2/win-32 - https://repo.continuum.io/pkgs/msys2/noarch Pytorch不能在32位系统中工作运行。请安装使用64位的Windows和Python。 导入错误 from torch._C import * ImportError: DLL load failed: The specified module could not be found. 问题是由基本文件丢失导致的。实际上，除了VC2017可再发行组件和一些mkl库之外，我们几乎包含了PyTorch对conda包所需的所有基本文件。您可以通过键入以下命令来解决此问题。 conda install -c peterjc123 vc vs2017_runtime conda install mkl_fft intel_openmp numpy mkl 至于wheel包(轮子)，由于我们没有包含一些库和VS2017可再发行文件，请手动安装它们。可以下载VS 2017可再发行安装程序)。你还应该注意你的Numpy的安装。 确保它使用MKL而不是OpenBLAS版本的。您可以输入以下命令。 pip install numpy mkl intel-openmp mkl_fft 另外一种可能是你安装了GPU版本的Pytorch但是电脑中并没有NVIDIA的显卡。碰到这种情况，就把GPU版本的Pytorch换成CPU版本的就好了。 from torch._C import * ImportError: DLL load failed: The operating system cannot run %1. 这实际上是Anaconda的上游问题。使用conda-forge通道初始化环境时,将出现此问题。您可以通过此命令修复intel-openmp库。 使用（多处理） 无if语句保护的多进程处理错误 RuntimeError: An attempt has been made to start a new process before the current process has finished its bootstrapping phase. This probably means that you are not using fork to start your child processes and you have forgotten to use the proper idiom in the main module: if __name__ == '__main__': freeze_support() ... The \"freeze_support()\" line can be omitted if the program is not going to be frozen to produce an executable. 在Windows上实现多进程处理是不同的，它使用的是spawn而不是fork。 因此，我们必须使用if子句包装代码，以防止代码执行多次。将您的代码重构为以下结构。 import torch def main() for i, data in enumerate(dataloader): # do something here if __name__ == '__main__': main() 多进程处理错误“坏道” ForkingPickler(file, protocol).dump(obj) BrokenPipeError: [Errno 32] Broken pipe 当在父进程完成发送数据之前子进程结束时，会发生此问题。您的代码可能有问题。您可以通过将DataLoader的num_worker减少为零来调试代码，并查看问题是否仍然存在。 多进程处理错误“驱动程序关闭” Couldn’t open shared file mapping: , error code: at torch\\lib\\TH\\THAllocator.c:154 [windows] driver shut down 请更新您的显卡驱动程序。如果这种情况持续存在，则可能是您的显卡太旧或所需要的计算能力对您的显卡负担太重。请根据这篇文章.)更新TDR设置。 CUDA IPC操作 THCudaCheck FAIL file=torch\\csrc\\generic\\StorageSharing.cpp line=252 error=63 : OS call failed or operation not supported on this OS Windows不支持它们。在CUDA张量上进行多处理这样的事情无法成功，有两种选择: 1.不要使用多处理。将Data Loader的num_worker设置为零。 2.采用共享CPU张量方法。确保您的自定义DataSet返回CPU张量。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"community/contribution_guide.html":{"url":"community/contribution_guide.html","title":"PyTorch贡献说明书","keywords":"","body":"PyTorch贡献指南 PyTorch是GPU加速的Python张量计算包大楼建成基于磁带的系统autograd深层神经网络。 所述PyTorch贡献过程 该PyTorch组织由[ PyTorch治理管辖HTG1。 该PyTorch开发过程中涉及的核心开发团队和社区之间公开讨论的一个健康的量。 PyTorch操作类似于GitHub上大多数开源项目。但是，如果你从来没有促成一个开源项目，下面是基本的过程。 找出你要什么去努力。 大多数的开源贡献来自让人摸不着自己的痒处。但是，如果你不知道你想要什么工作，或者只是希望获得与该项目有更多的了解，这里有一些提示，了解如何找到适当的任务： 通览问题跟踪，看看是否有您知道如何解决任何问题。由其他贡献者确认的问题往往是更好地进行调查。我们也维持其很可能是好的，为新人们，例如问题的一些标签， 集训 和 1小时 ，尽管这些标签不太良好的维护。 加入我们的时差，让我们知道你有兴趣去了解PyTorch。我们很高兴提供帮助研究人员和合作伙伴获得了代码库，以加快。 找出你的变化范围，在GitHub的问题达成了设计的意见，如果是大的。 大多数引入请求的小;在这种情况下，没有必要让我们知道你想要做什么，只是让开裂。但是，如果改变将是大的，它通常是一个好主意，先了解一下它的一些设计意见。 如果你不知道变化有多大将是，我们可以帮你看着办吧！只是张贴关于它的问题或懈怠。 一些新增功能是非常标准化的;例如，很多人添加新的运营商或优化，以PyTorch。在这些情况下设计的讨论主要是归结，“难道我们希望这个运营商/优化？”给了其效用，例如，使用在同行评审的论文，或存在其他框架的证据，使这种情况下，当有助于一点。 - 加法运算符/从最近发布的研究算法 > 一般是不会接受的，除非有大量证据表明，这个新出版的作品有突破性的成果，并最终成为该领域的标准。如果你不知道从哪里你的方法跌倒，首先实施PR之前打开的问题。 * 核心变化和refactors可以说是相当难以协调，为发展对PyTorch主步伐也相当快。当然伸手根本性或跨领域的变化;我们经常可以提供有关如何上演这样的变化成更容易审查的作品的指导。 代码吧！ 请参阅建议的技术导则技术形态与PyTorch工作。 打开拉入请求。 如果你还没有准备好拉入请求进行审查，以[WIP]标记它。当这样做评审通过，我们将忽略它。如果你是在一个复杂的变化工作，这是很好的开始做事了作为WIP，因为你将需要花时间看CI结果看，如果事情成功的与否。 找到适合您的变化适当的评审。我们有一些人谁经常通过公关排队去尝试，审查一切，但是如果你碰巧知道受你的补丁给定子系统的维护者是谁，随时直接包括他们拉入请求。您可以了解更多有关此结构在PyTorch子系统所有权。 直到它接受了拉入请求 迭代！ 我们会尽最大努力减少审核往返，只有当有重大事项块永久居民的人数。对于引入请求的最常见的问题，看看常见误区[HTG1。 一旦拉请求被接受和CI在流逝，没有什么别的你需要做的;我们将合并PR为您服务。 入门 提出新的功能 新功能的想法是对具体问题的最佳讨论。请包括你可以尽可能多的信息，相关的数据，和您的建议的解决方案。该PyTorch团队和社区经常回顾了新的问题和意见，他们认为他们可以提供帮助。如果您在您的解决方案有信心，继续前进，实现它。 报告问题 如果您已经通过在回购现有的问题中的列表中标识的问题，第一个搜索。如果你无法找到一个类似的问题，然后创建一个新的。供应尽可能多的信息，你可以重现问题的行为。此外，包括像你期望的行为的任何额外的见解。 实施特色或修复错误 如果你想解决一个具体问题，最好与你的意图个别问题发表评论。但是，我们不锁或分配，除了在我们与开发商合作过案件的问题。这是最好的搭讪上的问题，并讨论你提出的解决方案。该PyTorch团队可以提供为您节省时间的指导。 被标记的第一新问题，低或中等优先问题提供最佳的介入点是伟大的地方开始。 添加教程 教程对 pytorch.org了大量来自社区本身，我们欢迎更多的捐款。要了解更多关于如何贡献新的教程，你可以在这里了解更多：在Github上PyTorch.org教程贡献指南 提高文件&安培;教程 我们的目标是生产出高品质的文档和教程。在极少数情况下的内容包括错别字或错误。如果你发现了一些可以修复，给我们考虑拉入请求。 看看在文档部分，以了解我们的系统是如何工作的。 参与网上讨论 你可以找到积极的讨论发生在PyTorch讨论[论坛HTG1。 提交引入请求解决悬而未决的问题 您可以查看所有打开的问题此处列表。在谈到一个问题，是一个伟大的方式来获得球队的关注。从这里你可以分享你的想法和你打算如何解决这个问题。 更具挑战性的问题，该小组将提供如何最好地解决这一问题的反馈和方向。 如果你不能够解决的问题本身，评论和分享您是否可以重现该问题可以帮助球队找出问题所在有用。 回顾开放引入请求 我们感谢您的帮助审查和评论引入请求。我们的团队努力保持开放引入请求的数量在可管理的范围，我们迅速作出反应的详细信息，如果我们需要它，我们合并，我们认为是有用的永久居民。然而，由于高度的兴趣，对引入请求额外的眼睛是赞赏。 改进代码可读性 提高代码的可读性可以帮助大家。它往往是更好地提交的少数几个触摸与文件触及许多文件大拉的请求拉请求。开始在PyTorch论坛此处或与您的改进问题的讨论是开始的最好方式。 添加测试用例，使代码库更加健壮 附加的测试覆盖率理解。 促进PyTorch 你在你的项目中，研究论文，使用PyTorch的写起坐，博客或一般性讨论在互联网有助于提高意识，为PyTorch和我们成长的社区。请联络[ pytorch- marketing@fb.com HTG1对于营销支持。 检伤分类问题 如果你觉得一个问题可以从有关这一问题的特定标签或复杂的注释水平中受益，分享你的意见。如果你觉得一个问题是未分类的正确意见，并让球队知道。 关于开源开发 如果这是您第一次贡献一个开源项目，开发过程中的某些方面可能看起来不寻常的给你。 有没有办法“要求”的问题。 人们经常想“要求”时，他们决定进行这项工作，以确保当别人结束了它的工作有没有浪费工作的问题。这并没有真正的开源工作也很好，因为有人可以决定的东西的工作，并最终没有时间去做。可以随意的信息咨询的方式，但在这一天结束时，我们将采取运行的代码，并大体一致。 [HTG0存在对于被添加新功能的高杆。 [HTG1不像在企业环境中，谁写代码的人含蓄地“拥有”，并可以预计到，立即照顾它在其生命周期的开始，一次拉请求被合并成一个开源项目，它成为该项目的所有维护人员的集体责任。当我们合并代码中，我们说我们的维护人员，都能够审查的后续变化，并作出修正错误的代码。这自然导致了更高的标准贡献。 常见的错误，以避免 你添加的测试？ （或者如果改变是很难测试，你描述你如何测试你的改变？） 我们有我们为什么要求测试的几个动机： 帮助我们告诉我们，如果以后打破它 帮助我们告诉我们，如果补丁是在第一时间正确的（是的，我们没有审查，但克努特说，“当心下面的代码，因为我还没有运行它，只是证明了它正确”） 什么时候确定不添加测试？有时变化不能方便地进行测试，或者改变是如此明显正确的（且不太可能被打破），它是确定不进行测试。相反，如果一个变化很可能（或者被称为是有可能的）被意外打破，它把在制定测试策略的时间是非常重要的。 是你的PR过长？ 这是我们更容易查看和合并小的PR。回顾公关的难度与它的大小尺度非线性。 什么时候确定，提交了大量公关？它有很大帮助，如果有中的问题相应的设计讨论，与谁是要检查您的DIFF人签署。我们还可以帮助提供有关如何拆分大的变化成单独可交付的部分建议。同样，它帮助，如果还有的公关内容的完整描述：它更容易检查的代码，如果我们知道里面是什么！ 评论对微妙的东西？ [HTG1在情况下，你的代码的行为是细致入微，请包括额外的注释和文档，使我们能够更好地理解你的代码的意图。 你添加一个黑客？ 有时，一个黑客是正确的答案。但通常，我们将要讨论它。 你要摸一个非常核心的组成部分？ [HTG1为了防止大回归，拉那一抹核心组件的请求获得额外的审查。请确保你在进行重大变化之前已经讨论过的团队所做的更改。 要添加新的功能？ [HTG1如果要添加新的功能，对相关问题发表评论你的意图。我们的团队试图发表评论，并提供反馈给社会。这是更好地之前建立新的功能与团队和社会的其他开放式讨论。这有助于我们保持知道你的工作是什么对和增加了它会被合并的机会。 你触摸无关的代码的公关？ 为了在代码审查帮助，请只在您的拉请求直接相关的更改文件。 经常问的问题 如何作为一个评论家贡献？ 有很多的价值，如果小区的开发商重现问题，尝试新的功能，或以其他方式帮助我们确定或解决问题。在谈到与环境信息的任务或引入请求是有帮助和赞赏。 CI测试失败了，这是什么意思？ 也许你需要与主合并或与最新的变化变基。推你的变化应该重新触发CI测试。如果测试持续下去，你会希望通过错误信息来跟踪和解决相关问题。 什么是最高危的变化？ 凡是触摸构建配置是一个有风险的区域。请避免改变这些，除非你已经与球队讨论事前。 嘿，提交我的分支出现了，那是什么回事？ 有时其他社区成员会提供修补程序或补丁，以您的拉请求或分公司。这通常需要获得CI测试通过。 在文档 Python文档 PyTorch文档从蟒源使用斯芬克斯生成。生成的HTML被复制到文档文件夹中的 pytorch.github.io 主分支，并且经由GitHub的页供应。 网站：HTG0] http://pytorch.org/docs GitHub的： https://github.com/pytorch/pytorch/tree/master/docs 从供应： https://github.com/pytorch/pytorch.github.io/tree/master/doc C ++文档 对于C ++代码，我们使用的Doxygen生成内容的文件。 C ++的文档都建有专门的服务器上生成的文件复制到 https://github.com/pytorch/cppdocs 回购，并从GitHub页面服务。 网站：HTG0] http://pytorch.org/cppdocs GitHub的： https://github.com/pytorch/pytorch/tree/master/docs/cpp 从供应： https://github.com/pytorch/cppdocs 教程 PyTorch教程是用于帮助了解使用PyTorch完成特定任务或要了解更全面的概念文件。教程使用狮身人面像，画廊从Python可执行文件的来源，或重组文本（RST）文件建立。 网站：HTG0] http://pytorch.org/tutorials GitHub的： http://github.com/pytorch/tutorials 教程构建概述HTG0] 对于教程拉请求触发使用CircleCI测试变化的影响，重建整个网站。此版本是分片到9个工作建立和总花费约40分钟。与此同时，我们做了Netlify建立使用 让HTML的noplot ，它建立了网站，而无需渲染笔记本输出到快速审核页面。 一个PR被接受后，该网站是重建和CircleCI部署。 贡献新的教程 PyTorch.org教程贡献指南 Next Previous ©版权所有2019年，Torch 贡献者。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"community/governance.html":{"url":"community/governance.html","title":"PyTorch治理","keywords":"","body":"PyTorch治理 治理哲学指导信条 PyTorch采用的治理结构与一小维护者驾驶朝向PyTorch的设计理念有很大成见，其中设计和代码贡献的价值在整个项目的方向。除了核心的维护者，也有一个稍微更广泛的具有直接合并引入请求和核心代码库的自己各部分的功能核心开发人员。 除了维护和核心开发者，社区鼓励贡献，文件的问题，提出建议，检讨引入请求和存在于社会。鉴于捐助和投资意愿，任何人都可以提供写访问或代码库的部分所有权。 在此基础上的治理结构，该项目由作出决定和整体文化起源的以下核心工作原则： 比企业赞助和独立软件开发商 多大关系更多的代码贡献的高度重视。 项目影响 通过捐款获得了（无论是永久居民，论坛答案，代码审查或其他方式） 关键人及其功能 项目维护者 项目维护者为PyTorch项目提供领导和指导。具体包括： 阐明该项目的凝聚力长远眼光 具备PyTorch代码库的深刻理解 协商和生产方式接受有关各方解决争议问题 PyTorch维护者： 亚当Paszke（ apaszke ） Soumith Chintala（ soumith ） 杨德昌（ ezyang ） 格雷格察南（ gchanan ） 德米特罗Dzhulgakov（ dzhulgakov ） （弃用）萨姆总值（ colesbury ） 核心开发 该PyTorch项目是由一个团队的核心开发人员开发的。您可以在 PyTorch治理发现的核心开发者名单|兴趣的人。 虽然成员由在“PyTorch核心”球队存在的“PyTorch” 组织在GitHub上确定的，贡献有多种形式： 提交更改到存储库; 审查通过别人拉的请求; 在这个问题上跟踪检伤分类错误报告; 在讨论关于官方PyTorch通信信道的话题。 版主 有这样一群人，其中一些是不是核心开发人员，负责确保在正式沟通渠道讨论遵守行为守则。他们采取鉴于侵犯行动，并帮助支持一个健康的社区。你可以找到版主这里的列表。 决策 争议的变化 主要工作情况通过bug跟踪系统问题，并在GitHub上引入请求。核心开发人员应避免直接推动其更改PyTorch库，而是依靠引入请求。批准由核心开发者拉请求允许它不经进一步处理合并。核心开发人员和项目维护者最终批准了这些变化。 通报有关专家约一个bug跟踪系统问题或拉的要求是很重要的。从给定的利率方面的专家评测强烈首选，尤其是在拉动请求批准。如果不这样做可能最终改变由相关专家回复。 有争议的决策过程 在给定的感兴趣的领域实质性的改变需要一个GitHub的问题的讨论被打开。这包括： 任何语义句法或改变的框架。 向后兼容改变了Python或CPP API。 增加的核心框架，其中包括现有的库中的大量新功能。 移除核心功能 项目维护者最终批准了这些变化。 常见问题 问：我想如果自己（或部分拥有）项目的一部分，如域API（即Torch 宣）？ 这是绝对有可能的。第一步是启动有助于现有项目区域和促进其健康和成功。除此之外，你可以通过GitHub的问题，新的功能或修改，以改善项目区的建议。 问：如果我公司希望使用PyTorch内部进行开发，可我被授予或购买一个董事会席位，以推动项目的方向是？ 没有，PyTorch项目严格维护者驱动的项目理念驱动，没有一个板或车辆采取与获得对技术发展方向的影响力捐款。 问：请问PyTorch项目的支持或补助的方式支持独立开发者使用或参与该项目？ 不，不是在这一点上。然而，我们正在寻找方法，以更好地支持各地PyTorch独立开发者社区。如果您有任何建议或输入，请在PyTorch论坛伸手讨论。 问：我如何贡献代码的项目？ [HTG1如果变化相对较小，在GitHub上拉的请求可以立即进行审核打开了，并通过该项目的提交合并。对于较大的变化，请打开一个问题提出建议之前进行讨论。也请参阅[ PyTorch投稿指南HTG3对于贡献的指导方针。 问：我能成为该项目的提交者？ [HTG1不幸的是，目前提交过程PyTorch涉及与只能通过Facebook的员工被触发的Facebook基础架构的交互。然而，我们正在寻找方法，以扩大基地的提交个人的Facebook的外面，当工具的存在是为了让这将提供更新。 问：我想如果在一次会议上或以其他方式提供一个PyTorch教程？我需要成为“正式”的提交做到这一点？ 没有，我们鼓励社区成员时，他们可以向人们展示他们的作品在任何地方和。请联络[ pytorch-marketing@fb.com HTG3对于营销支持。 Next Previous ©版权所有2019年，Torch 贡献者。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"community/persons_of_interest.html":{"url":"community/persons_of_interest.html","title":"PyTorch治理感兴趣的人","keywords":"","body":"PyTorch治理感兴趣的人 一般维护者 Adam Paszke (apaszke) Soumith Chintala (soumith) Edward Yang (ezyang) Greg Chanan (gchanan) Dmytro Dzhulgakov (dzhulgakov) (sunsetting) Sam Gross (colesbury) 模块级维护者 JIT Zach Devito (zdevito) Michael Suo (suo) Distributed Pieter Noordhuis (pietern) Shen Li (mrshenli) (sunsetting) Teng Li (teng-li) Autograd Engine Alban Desmaison (alband) Adam Paszke (apaszke) Multiprocessing and DataLoaders Simon Wang (SsnL) Adam Paszke (apaszke) (proposed) Vitaly Fedyunin (VitalyFedyunin) CUDA Edward Yang (ezyang) Natalia Gimelshein (ngimel) C++ Will Feng (yf225) (sunsetting) Peter Goldsborough (goldsborough) Build + CI Will Feng (yf225) Edward Yang (ezyang) Jesse Hellemn (pjh5) Soumith Chintala (soumith) (sunsetting) Orion Reblitz-Richardson (orionr) Distributions & RNG Fritz Obermeyer (fritzo) Neeraj Pradhan (neerajprad) Alican Bozkurt (alicanb) Vishwak Srinivasan (vishwakftw) C10 Sebastian Messmer (smessmer) Edward Yang (ezyang) ONNX PyTorch Lu Fang (houseroad) Lara Haidar (lara-hdr) Spandan Tiwari (spandantiwari) Bowen Bao (BowenBao) torch.nn Thomas Viehmann (t-vi) Adam Paszke (apaszke) Greg Chanan (gchanan) Soumith Chintala (soumith) Sam Gross (colesbury) CPU Performance / SIMD Christian Puhrsch (cpuhrsch) Sam Gross (colesbury) Richard Zou (zou3519) AMD/ROCm/HIP Junjie Bai (bddppq) Johannes M. Dieterich (iotamudelta) Windows Peter Johnson (peterjc123) MKLDNN Yinghai Lu (yinghai) XLA Ailing Zhang (ailzhang) Gregory Chanan (gchanan) Davide Libenzi (dlibenzi) Alex Suhan (asuhan) PPC Alfredo Mendoza (avmgithub) torchvision Francisco Massa (fmassa) torchtext Guanheng George Zhang (zhangguanheng66) torchaudio Vincent Quenneville-Belair (vincentqb) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"torch.html":{"url":"torch.html","title":"torch","keywords":"","body":"torch 译者：@那伊抹微笑、@yudong、@小瑶、@片刻、@李雨龙、@K @devin、@张假飞、@rickllyxu 校对者：@张假飞、@飞龙 torch package 包含了多维张量的数据结构, 以及基于其上的多种数学操作. 此外, 它还提供了许多用于高效序列化 Tensor 和任意类型的实用工具包, 以及一起其它有用的实用工具包. 它有一个 CUDA 的对应实现, 它使您能够在计算能力 >=0.3 的 NVIDIA GPU 上进行张量运算. Tensors (张量) torch.is_tensor(obj) 如果 obj 是一个 pytorch tensor, 则返回True. 参数：obj (Object) – 用于测试的对象 torch.is_storage(obj) 如果 obj 是一个 pytorch storage object, 则返回True. 参数：obj (Object) – 用于测试的对象 torch.set_default_tensor_type(t) torch.numel(input) → int 返回 input Tensor 中的元素总数. 参数：input (Tensor) – 输入的 Tensor 示例： >>> a = torch.randn(1,2,3,4,5) >>> torch.numel(a) 120 >>> a = torch.zeros(4,4) >>> torch.numel(a) 16 torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None) 设置打印选项. 从 Numpy 中采集数据 参数： precision – 浮点输出精度的位数 (默认值为 8). threshold – 触发汇总显示而不是完全显示(repr)的数组元素的总数 (默认值为 1000). edgeitems – 每个维度开始和结束时总结的数组项数 (默认值为 3). linewidth – 插入换行符的每行字符数 (默认值为 80). Thresholded matricies(阈值矩阵) 将忽略这个参数. profile – 用于漂亮格式的打印. 可以用以下任何选项来进行覆盖 (default, short, full) Creation Ops (创建操作) torch.eye(n, m=None, out=None) 返回对角线位置全为1, 其它位置全为0的二维 tensor. 参数： n (int) – 行数 m (int, 可选) – 列数. 如果为 None,则默认为 n out (Tensor, 可选) – 输出 tensor 返回值：一个对角线位置全为1, 其它位置全为0的二维 tensor. 返回类型：Tensor 示例： >>> torch.eye(3) 1 0 0 0 1 0 0 0 1 [torch.FloatTensor of size 3x3] torch.from_numpy(ndarray) → Tensor 从 numpy.ndarray 类 创建一个 Tensor 类. 返回 tensor 和 ndarray 共享相同的内存. 对 tensor 的修改将反映在 ndarray 中, 反之亦然. 返回 tensor 不可调整大小. 示例： >>> a = numpy.array([1, 2, 3]) >>> t = torch.from_numpy(a) >>> t torch.LongTensor([1, 2, 3]) >>> t[0] = -1 >>> a array([-1, 2, 3]) torch.linspace(start, end, steps=100, out=None) → Tensor 返回 start 和 end 之间等间隔 steps 点的一维 Tensor. 输出 是尺寸 steps 为一维 tensor 参数： start (float) – 点集合的起始值 end (float) – 点集合的结束值 steps (int) – 在 start 和 end 之间的样本数 out (Tensor, 可选) – 输出结果的 Tensor 示例： >>> torch.linspace(3, 10, steps=5) 3.0000 4.7500 6.5000 8.2500 10.0000 [torch.FloatTensor of size 5] >>> torch.linspace(-10, 10, steps=5) -10 -5 0 5 10 [torch.FloatTensor of size 5] >>> torch.linspace(start=-10, end=10, steps=5) -10 -5 0 5 10 [torch.FloatTensor of size 5] torch.logspace(start, end, steps=100, out=None) → Tensor 返回一个在 和 之间的对数间隔 steps 点的一维 Tensor 输出是长度为 steps 的一维 tensor 参数： start (float) – 点集合的起始值 end (float) – 点集合的结束值 steps (int) – 在 start 和 end 之间的样本数 out (Tensor, 可选) – 输出结果Tensor 示例： >>> torch.logspace(start=-10, end=10, steps=5) 1.0000e-10 1.0000e-05 1.0000e+00 1.0000e+05 1.0000e+10 [torch.FloatTensor of size 5] >>> torch.logspace(start=0.1, end=1.0, steps=5) 1.2589 2.1135 3.5481 5.9566 10.0000 [torch.FloatTensor of size 5] torch.ones(*sizes, out=None) → Tensor 返回填充了标量值 1 的 Tensor, 其形状由可变参数 sizes 定义. 参数： sizes (int...) – 一组定义输出 Tensor 形状的整数 out (Tensor, 可选) – 输出结果 Tensor 示例： >>> torch.ones(2, 3) 1 1 1 1 1 1 [torch.FloatTensor of size 2x3] >>> torch.ones(5) 1 1 1 1 1 [torch.FloatTensor of size 5] torch.ones_like(input, out=None) → Tensor 返回一个用标量值 1 填充的张量, 大小与 input 相同. 参数： input (Tensor) – 输入的大小将决定输出的大小. out (Tensor, 可选) – 输出结果 Tensor 示例： >>> input = torch.FloatTensor(2, 3) >>> torch.ones_like(input) 1 1 1 1 1 1 [torch.FloatTensor of size 2x3] torch.arange(start=0, end, step=1, out=None) → Tensor 从 start 用步长为 step 开始, 间隔在 [start, end) 中的值返回大小层次为 的一维 Tensor. 参数： start (float) – 点集合的起始值 end (float) – 点集合的结束值 step (float) – 每对相邻点之间的间隔 out (Tensor, 可选) – 输出结果 Tensor 示例： >>> torch.arange(5) 0 1 2 3 4 [torch.FloatTensor of size 5] >>> torch.arange(1, 4) 1 2 3 [torch.FloatTensor of size 3] >>> torch.arange(1, 2.5, 0.5) 1.0000 1.5000 2.0000 [torch.FloatTensor of size 3] torch.range(start, end, step=1, out=None) → Tensor 返回一个在 start 到 end 并且步长为 step 的区间内, 大小为 为一维 Tensor. step 是 tensor 中两个值之间的差距. 警告： 此功能已被弃用, 以支持 torch.arange(). 参数： start (float) – 点集合的起始值 end (float) – 点集合的结束值 step (float) – 每对相邻点之间的间隔 out (Tensor, 可选) – 输出结果 Tensor 示例： >>> torch.range(1, 4) 1 2 3 4 [torch.FloatTensor of size 4] >>> torch.range(1, 4, 0.5) 1.0000 1.5000 2.0000 2.5000 3.0000 3.5000 4.0000 [torch.FloatTensor of size 7] torch.zeros(*sizes, out=None) → Tensor 返回填充了标量值为 0 的 Tensor, 其形状由可变参量 sizes 定义. 参数： sizes (int...) – 定义输出 Tensor 形状的一组整数. out (Tensor, 可选) – 输出结果 Tensor 示例： >>> torch.zeros(2, 3) 0 0 0 0 0 0 [torch.FloatTensor of size 2x3] >>> torch.zeros(5) 0 0 0 0 0 [torch.FloatTensor of size 5] torch.zeros_like(input, out=None) → Tensor 返回一个用标量值 0 填充的 Tensor, 其大小与 input 相同. 参数： input (Tensor) – 输入的大小将决定输出的大小. out (Tensor, 可选) – 输出结果 Tensor 示例： >>> input = torch.FloatTensor(2, 3) >>> torch.zeros_like(input) 0 0 0 0 0 0 [torch.FloatTensor of size 2x3] Indexing, Slicing, Joining, Mutating Ops (索引, 切片, 连接, 换位) 操作 torch.cat(seq, dim=0, out=None) → Tensor 在给定维度上对输入的张量序列 seq 进行连接操作. 所有张量必须具有相同的形状(在 cat 维度中除外) 或为空. torch.cat() 可以看做是 torch.split() 和 torch.chunk() 的逆操作. cat() 可以通过下面的例子更好地理解. 参数： seq (_sequence of Tensors_) – 可以是任何相同类型的 Tensor 的 Python 序列. dim (int, 可选) – tensors 级联的维数 out (Tensor, 可选) – 输出参数 示例： >>> x = torch.randn(2, 3) >>> x 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 2x3] >>> torch.cat((x, x, x), 0) 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 6x3] >>> torch.cat((x, x, x), 1) 0.5983 -0.0341 2.4918 0.5983 -0.0341 2.4918 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 1.5981 -0.5265 -0.8735 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 2x9] torch.chunk(tensor, chunks, dim=0) 在给定维度(轴)上将输入张量进行分块处理. 参数： tensor (Tensor) – 待分块的输入张量. chunks (int) – 要返回的分块的个数. dim (int) – 切分张量所需要沿着的维度. torch.gather(input, dim, index, out=None) → Tensor 沿给定轴 dim ,将输入索引张量 index 指定位置的值进行聚合. 对一个 3 维张量,输出可以定义为: out[i][j][k] = input[index[i][j][k]][j][k] # if dim == 0 out[i][j][k] = input[i][index[i][j][k]][k] # if dim == 1 out[i][j][k] = input[i][j][index[i][j][k]] # if dim == 2 如果 input 是 size 为 且 dim = i 的 n 维张量,则 index 必须是具有 size 为 的 n 维张量,其中 y >= 1 ,并且 out 将与 index 的 size 相同. 参数： input (Tensor) – 源张量 dim (int) – 索引的轴 index (LongTensor) – 聚合元素的下标 out (Tensor, 可选) – 目标张量 示例： >>> t = torch.Tensor([[1,2],[3,4]]) >>> torch.gather(t, 1, torch.LongTensor([[0,0],[1,0]])) 1 1 4 3 [torch.FloatTensor of size 2x2] torch.index_select(input, dim, index, out=None) → Tensor 沿着指定维度 dim 对输入进行切片,取 index 中指定的相应项 ( index 为一个 LongTensor ),然后返回到一个新的张量. 返回的张量与原始张量 Tensor 有相同的维度(在指定轴上). 注解： 返回的张量不与原始张量共享内存空间. 参数： input (Tensor) – 输入张量 dim (int) – 索引的轴 index (LongTensor) – 包含索引下标的一维张量 out (Tensor, 可选) – 输出参数/目标张量 示例： >>> x = torch.randn(3, 4) >>> x 1.2045 2.4084 0.4001 1.1372 0.5596 1.5677 0.6219 -0.7954 1.3635 -1.2313 -0.5414 -1.8478 [torch.FloatTensor of size 3x4] >>> indices = torch.LongTensor([0, 2]) >>> torch.index_select(x, 0, indices) 1.2045 2.4084 0.4001 1.1372 1.3635 -1.2313 -0.5414 -1.8478 [torch.FloatTensor of size 2x4] >>> torch.index_select(x, 1, indices) 1.2045 0.4001 0.5596 0.6219 1.3635 -0.5414 [torch.FloatTensor of size 3x2] torch.masked_select(input, mask, out=None) → Tensor 根据掩码张量 mask 中的二元值,取输入张量中的指定项 ( mask 为一个 ByteTensor ),将取值返回到一个新的一维张量. 张量 mask 与 input 的 shape 或维度不需要相同,但是他们必须是 broadcastable . 注解： 返回的张量不与原始张量共享内存空间. 参数： input (Tensor) – 输入张量 mask (ByteTensor) – 掩码张量,包含了二元索引值 out (Tensor, 可选) – 输出参数/目标张量 示例： >>> x = torch.randn(3, 4) >>> x 1.2045 2.4084 0.4001 1.1372 0.5596 1.5677 0.6219 -0.7954 1.3635 -1.2313 -0.5414 -1.8478 [torch.FloatTensor of size 3x4] >>> mask = x.ge(0.5) >>> mask 1 1 0 1 1 1 1 0 1 0 0 0 [torch.ByteTensor of size 3x4] >>> torch.masked_select(x, mask) 1.2045 2.4084 1.1372 0.5596 1.5677 0.6219 1.3635 [torch.FloatTensor of size 7] torch.nonzero(input, out=None) → LongTensor 返回一个包含输入 input 中非零元素索引的张量. 输出张量中的每行包含 input 中非零元素的索引. 如果输入张量 input 有 n 维,则输出的索引张量 out 的 size 为 z x n , 这里 z 是输入张量 input 中所有非零元素的个数. 参数： input (Tensor) – 输入张量/源张量 out (LongTensor, 可选) – 包含索引值的输出张量 示例： >>> torch.nonzero(torch.Tensor([1, 1, 1, 0, 1])) 0 1 2 4 [torch.LongTensor of size 4x1] >>> torch.nonzero(torch.Tensor([[0.6, 0.0, 0.0, 0.0], ... [0.0, 0.4, 0.0, 0.0], ... [0.0, 0.0, 1.2, 0.0], ... [0.0, 0.0, 0.0,-0.4]])) 0 0 1 1 2 2 3 3 [torch.LongTensor of size 4x2] torch.split(tensor, split_size, dim=0) 将输入张量分割成相等 size 的 chunks (如果可分). 如果沿指定维的张量形状大小不能被 split_size 整分, 则最后一个分块会小于其它分块. 参数： tensor (Tensor) – 待分割张量. split_size (int) – 单个分块的 size 大小. dim (int) – 沿着此维进行分割. torch.squeeze(input, dim=None, out=None) 将 input 张量 size 中的 1 去除并返回. 如果 input 的 shape 如 ,那么输出 shape 就为: 当给定 dim 时,那么挤压操作只在给定维度上.例如, input 的 shape 为: , squeeze(input, 0) 将会保持张量不变,只有用 squeeze(input, 1) , shape 会变成 . 注解： 作为上述的一个例外,size 为 1 的一维张量不会改变维度. 注解： 返回张量与输入张量共享内存,所以改变其中一个的内容会改变另一个. 参数： input (Tensor) – 输入张量 dim (int, 可选) – 如果给定 dim 时,则 input 只会在给定维度执行挤压 out (Tensor, 可选) – 结果张量 示例： >>> x = torch.zeros(2,1,2,1,2) >>> x.size() (2L, 1L, 2L, 1L, 2L) >>> y = torch.squeeze(x) >>> y.size() (2L, 2L, 2L) >>> y = torch.squeeze(x, 0) >>> y.size() (2L, 1L, 2L, 1L, 2L) >>> y = torch.squeeze(x, 1) >>> y.size() (2L, 2L, 1L, 2L) torch.stack(sequence, dim=0, out=None) 沿着一个新维度对输入张量序列进行连接. 序列中所有的张量都应该为相同 size . 参数： sequence (_Sequence_) – 待连接的张量序列. dim (int) – 插入的维度.必须介于 0 与待连接的张量序列数（包含）之间. torch.t(input, out=None) → Tensor 预期 input 为一个矩阵 (2 维张量), 并转置 0, 1 维. 可以被视为函数 transpose(input, 0, 1) 的简写函数. 参数： input (Tensor) – 输入张量 out (Tensor, 可选) – 结果张量 示例： >>> x = torch.randn(2, 3) >>> x 0.4834 0.6907 1.3417 -0.1300 0.5295 0.2321 [torch.FloatTensor of size 2x3] >>> torch.t(x) 0.4834 -0.1300 0.6907 0.5295 1.3417 0.2321 [torch.FloatTensor of size 3x2] torch.take(input, indices) → Tensor 在给定的索引处返回一个新的 Tensor ,其元素为 input . 输入张量被看作是一维张量.结果与索引具有相同的 shape . 参数： input (Tensor) – 输入张量 indices (LongTensor) – 进入 Tensor 的索引 示例： >>> src = torch.Tensor([[4, 3, 5], ... [6, 7, 8]]) >>> torch.take(src, torch.LongTensor([0, 2, 5])) 4 5 8 [torch.FloatTensor of size 3] torch.transpose(input, dim0, dim1, out=None) → Tensor 返回输入矩阵 input 的转置.交换给定维度 dim0 和 dim1 . out 张量与 input 张量共享内存,所以改变其中一个会导致另外一个也被修改. 参数： input (Tensor) – 输入张量 dim0 (int) – 转置的第一个维度 dim1 (int) – 转置的第二个维度 示例： >>> x = torch.randn(2, 3) >>> x 0.5983 -0.0341 2.4918 1.5981 -0.5265 -0.8735 [torch.FloatTensor of size 2x3] >>> torch.transpose(x, 0, 1) 0.5983 1.5981 -0.0341 -0.5265 2.4918 -0.8735 [torch.FloatTensor of size 3x2] torch.unbind(tensor, dim=0) 移除一个张量的维度. 移除指定维后,返回一个元组,包含了沿着指定维切片后的各个切片 (已经没有了移除的维度). 参数： tensor (Tensor) – 要执行 unbind 的张量/输入张量. dim (int) – 要移除的维度. torch.unsqueeze(input, dim, out=None) 返回在指定位置插入维度 size 为 1 的新张量. 返回张量与输入张量共享内存,所以改变其中一个的内容会改变另一个. 如果 dim 为负,则将会被转化 . 参数： input (Tensor) – 输入张量 dim (int) – 插入维度的索引 out (Tensor, 可选) – 结果张量 示例： >>> x = torch.Tensor([1, 2, 3, 4]) >>> torch.unsqueeze(x, 0) 1 2 3 4 [torch.FloatTensor of size 1x4] >>> torch.unsqueeze(x, 1) 1 2 3 4 [torch.FloatTensor of size 4x1] Random sampling (随机采样) torch.manual_seed(seed) 设置生成随机数的种子,并返回一个 torch._C.Generator 对象. 参数：seed (int 或 long) – 种子. torch.initial_seed() 返回用于生成随机数字的初始种子 (python long) . torch.get_rng_state() 以ByteTensor的形式返回随机数发生器的状态. torch.set_rng_state(new_state) 设置随机数发生器的参数. 参数：new_state (torch.ByteTensor) – 理想状态 torch.default_generator = torch.bernoulli(input, out=None) → Tensor 从伯努利分布中抽取二进制随机数 (0 或 1). The input 张量包含用于抽取二进制随机数的概率. 因此, input 中的所有值必须在这个范围内: 根据 input 张量第 i 个概率值, 输出张量的第 i 个元素将取值为1. 返回的 out 张量的值只有 0 或者 1 并且大小与 input 张量相同. 参数： input (Tensor) – 伯努利分布的概率值 out (Tensor, 可选) – 输出张量 示例： >>> a = torch.Tensor(3, 3).uniform_(0, 1) # generate a uniform random matrix with range [0, 1] >>> a 0.7544 0.8140 0.9842 0.5282 0.0595 0.6445 0.1925 0.9553 0.9732 [torch.FloatTensor of size 3x3] >>> torch.bernoulli(a) 1 1 1 0 0 1 0 1 1 [torch.FloatTensor of size 3x3] >>> a = torch.ones(3, 3) # probability of drawing \"1\" is 1 >>> torch.bernoulli(a) 1 1 1 1 1 1 1 1 1 [torch.FloatTensor of size 3x3] >>> a = torch.zeros(3, 3) # probability of drawing \"1\" is 0 >>> torch.bernoulli(a) 0 0 0 0 0 0 0 0 0 [torch.FloatTensor of size 3x3] torch.multinomial(input, num_samples, replacement=False, out=None) → LongTensor 返回一个张量, 其中每一行包含在 input 张量对应行中多项式分布取样的 num_samples 索引. 注解： input 的每行值不需要总和为 1 (我们只使用这些值作为权重), 但必须是非负且非零和的. 取样时从左向右排列(第一个样本在第一列). 如果 input 是一个向量, 则 out 是一个大小为 num_samples 的向量. 如果 input 是一个 m 行的矩阵, 则 out 是一个 m × n 的矩阵. 如果参数 replacement 是 True, 则可重复取样. 否则, 样本在每行不能被重复取样. 参数 num_samples 必须小于 input 长度 (如果是一个矩阵, 则是 input 的列数). 参数： input (Tensor) – 包含概率值的张量 num_samples (int) – 抽取的样本数 replacement (bool, 可选) – 是否重复抽取样本 out (Tensor, 可选) – 输出 Tensor 示例： >>> weights = torch.Tensor([0, 10, 3, 0]) # create a Tensor of weights >>> torch.multinomial(weights, 4) 1 2 0 0 [torch.LongTensor of size 4] >>> torch.multinomial(weights, 4, replacement=True) 1 2 1 2 [torch.LongTensor of size 4] torch.normal() torch.normal(means, std, out=None) 返回一个随机数张量, 随机数从给定平均值和标准差的离散正态分布中抽取. 参数 means 是一个包含每个输出元素的正态分布均值的张量. 参数 std 是一个包含每个输出元素的正态分布标准差的张量. 其中 means 和 std 的形状不需要匹配, 但是每个张量中的元素总数需要相同. 注解： 当形状不匹配时, means 的形状将作为返回输出张量的形状. 参数： means (Tensor) – 均值 std (Tensor) – 标准差 out (Tensor, 可选) – 输出张量 示例： torch.normal(means=torch.arange(1, 11), std=torch.arange(1, 0, -0.1)) 1.5104 1.6955 2.4895 4.9185 4.9895 6.9155 7.3683 8.1836 8.7164 9.8916 [torch.FloatTensor of size 10] torch.normal(mean=0.0, std, out=None) 功能与上面函数类似, 但所有被抽取的元素共享均值. 参数： means (float, 可选) – 所有分布的均值 std (Tensor) – 每个元素标准差的张量 out (Tensor, 可选) – 输出张量 示例： >>> torch.normal(mean=0.5, std=torch.arange(1, 6)) 0.5723 0.0871 -0.3783 -2.5689 10.7893 [torch.FloatTensor of size 5] torch.normal(means, std=1.0, out=None) 功能与上面函数类似, 但所有被抽取的元素共享标准差. 参数： means (Tensor) – 每个元素均值的张量 std (float, 可选) – 所有分布的标准差 out (Tensor, 可选) – 输出张量 示例： >>> torch.normal(means=torch.arange(1, 6)) 1.1681 2.8884 3.7718 2.5616 4.2500 [torch.FloatTensor of size 5] torch.rand(*sizes, out=None) → Tensor 在区间 ![0, 1) 中, 返回一个填充了均匀分布的随机数的张量. 这个张量的形状由可变参数 sizes 来定义. 参数： sizes (int...) – 定义输出张量形状的整数集. out (Tensor, 可选) – 结果张量 示例： >>> torch.rand(4) 0.9193 0.3347 0.3232 0.7715 [torch.FloatTensor of size 4] >>> torch.rand(2, 3) 0.5010 0.5140 0.0719 0.1435 0.5636 0.0538 [torch.FloatTensor of size 2x3] torch.randn(*sizes, out=None) → Tensor 返回一个从正态分布中填充随机数的张量, 其均值为 0 , 方差为 1 . 这个张量的形状被可变参数 sizes 定义. 参数： sizes (int...) – 定义输出张量形状的整数集. out (Tensor, 可选) – 结果张量 示例： >>> torch.randn(4) -0.1145 0.0094 -1.1717 0.9846 [torch.FloatTensor of size 4] >>> torch.randn(2, 3) 1.4339 0.3351 -1.0999 1.5458 -0.9643 -0.3558 [torch.FloatTensor of size 2x3] torch.randperm(n, out=None) → LongTensor 返回一个从 0 to n - 1 的整数的随机排列. 参数：n (int) – 上限 (唯一的) 示例： >>> torch.randperm(4) 2 1 3 0 [torch.LongTensor of size 4] In-place random sampling (直接随机采样) 在Tensors模块上还定义了许多 in-place 随机采样函数,可以点击参考它们的文档: torch.Tensor.bernoulli_()](tensors.html#torch.Tensor.bernoulli \"torch.Tensor.bernoulli\") - 是 [torch.bernoulli() 的 in-place 版本 torch.Tensor.cauchy_() - 从柯西分布中抽取数字 torch.Tensor.exponential_() - 从指数分布中抽取数字 torch.Tensor.geometric_() - 从几何分布中抽取元素 torch.Tensor.log_normal_() - 对数正态分布中的样本 torch.Tensor.normal_()](tensors.html#torch.Tensor.normal \"torch.Tensor.normal\") - 是 [torch.normal() 的 in-place 版本 torch.Tensor.random_() - 离散均匀分布中采样的数字 torch.Tensor.uniform_() - 正态分布中采样的数字 Serialization (序列化) torch.save(obj, f, pickle_module=, pickle_protocol=2) 将一个对象保存到一个磁盘文件中. 另见: 保存模型的推荐方法 参数: obj: 要保存的对象 f: 类文件对象 (必须实现返回文件描述符的 fileno 方法) 或包含文件名的字符串 pickle_module: 用于 pickling 元数据和对象的模块 pickle_protocol: 可以指定来覆盖默认协议 torch.load(f, map_location=None, pickle_module=) 从磁盘文件中加载一个用 torch.save() 保存的对象. Func: torch.load 使用 Python 的解封 (unpickling) 设施, 但特殊对待张量下的存储 (storages). 它们首先在 CPU 上反序列化, 然后移动到所保存的设备上. 如果这个过程失败了 (例如, 因为运行时的系统没有确定的设备), 将会抛出异常. 然而, 使用 map_location 参数, 存储可以被动态地重新映射到另一组设备上. 如果 map_location 是可调用对象, 则对于每个序列化存储, 它都将以两个参数调用一次: storage 和 location. 参数 storage 是驻留在 CPU 上的存储的初始反序列化. 每个序列化后的存储都有一个与之关联的位置标签, 它标识了保存它的设备, 而此标签是传递给 map_location 的第二个参数. 对于 CPU 张量, 内建的位置标签是 ‘cpu’, 对于 CUDA 张量, 内建的位置标签是 ‘cuda:device_id’ (例如 ‘cuda:2’). map_location 要么返回 None , 要么返回一个存储. 如果 map_location 返回存储, 它将用作已移动到正确设备上的, 最终反序列化的对象. 否则, 如果没有指明 map_location, 即返回 None, torch.load 会回落到默认的行为. 如果 map_location 是一个字典, 它用于将出现在文件 (键) 中的位置标签, 重新映射到另一个位置标签, 它出现在值中并指明在哪里存放存储. 用户扩展可以使用 register_package 来注册他们自己的位置标签, 以及标记和反序列化方法. 参数: f: 一个类文件对象 (必须实现返回文件描述符的 fileno, 以及 seek 方法), 或者包含文件名的字符串. map_location: 一个函数或者一个指明如何重新映射存储位置的字典 pickle_module: 用于解封 (unpickling) 元数据和对象的模块 (必须匹配用于序列化文件的 pickle_module) 示例: >>> torch.load('tensors.pt') # Load all tensors onto the CPU >>> torch.load('tensors.pt', map_location=lambda storage, loc: storage) # Load all tensors onto GPU 1 >>> torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1)) # Map tensors from GPU 1 to GPU 0 >>> torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'}) Parallelism (并行化) torch.get_num_threads() → int 获得 OpenMP 并行化操作的线程数目 torch.set_num_threads(int) 设置 OpenMP 并行化操作的线程数目 Math operations (数学操作) Pointwise Ops (逐点操作) torch.abs(input, out=None) → Tensor 计算给定 input 张量的元素的绝对值. 示例： >>> torch.abs(torch.FloatTensor([-1, -2, 3])) FloatTensor([1, 2, 3]) torch.acos(input, out=None) → Tensor 用 input 元素的反余弦返回一个新的张量. 参数： input (Tensor) – the input Tensor out (Tensor, 可选) – The result Tensor 示例： >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.acos(a) 2.2608 1.2956 1.1075 nan [torch.FloatTensor of size 4] torch.add() torch.add(input, value, out=None) 将标量值 value 添加到输入张量 attr:input 的每个元素并返回一个新的结果张量. 如果输入张量 input 是 FloatTensor 或者 DoubleTensor 类型, 则 value 必须为实数, 否则为整数. 参数： input (Tensor) – 输入 Tensor value (Number) – 要添加到 input 每个元素的数 out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a 0.4050 -1.2227 1.8688 -0.4185 [torch.FloatTensor of size 4] >>> torch.add(a, 20) 20.4050 18.7773 21.8688 19.5815 [torch.FloatTensor of size 4] torch.add(input, value=1, other, out=None) 张量 other 的每个元素乘以标量值 value 并加到张量 input 上, 返回生成的张量 out . 张量 input 的形状与张量 other 的形状必须 broadcastable. 如果张量 other 是 FloatTensor 或者 DoubleTensor 类型, 则 value 必须为实数, 否则为整数. 参数： input (Tensor) – 第一个输入 Tensor value (Number) – 张量 other 的标量乘数 other (Tensor) – 第二个输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> import torch >>> a = torch.randn(4) >>> a -0.9310 2.0330 0.0852 -0.2941 [torch.FloatTensor of size 4] >>> b = torch.randn(2, 2) >>> b 1.0663 0.2544 -0.1513 0.0749 [torch.FloatTensor of size 2x2] >>> torch.add(a, 10, b) 9.7322 4.5770 -1.4279 0.4552 [torch.FloatTensor of size 4] torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None) → Tensor 将张量 tensor1 逐元素除以张量 tensor2, 然后乘以标量值 value 并加到张量 tensor 上. 张量 tensor, 张量 tensor1, 张量 tensor2 的形状必须 broadcastable. 对于类型为 FloatTensor 或者 DoubleTensor 的张量输入, value 必须为实数, 否则为整数. 参数： tensor (Tensor) – 张量, 对 tensor1 ./ tensor2 进行相加 value (Number, 可选) – 标量, 对 tensor1 ./ tensor2 进行相乘 tensor1 (Tensor) – 分子张量, 即作为被除数 tensor2 (Tensor) – 分母张量, 即作为除数 out (Tensor, 可选) – 输出张量 示例： >>> t = torch.randn(2, 3) >>> t1 = torch.randn(1, 6) >>> t2 = torch.randn(6, 1) >>> torch.addcdiv(t, 0.1, t1, t2) 0.0122 -0.0188 -0.2354 0.7396 -1.5721 1.2878 [torch.FloatTensor of size 2x3] torch.addcmul(tensor, value=1, tensor1, tensor2, out=None) → Tensor 将张量 tensor1 逐元素与张量 tensor2 相乘, 然后乘以标量值 value 并加到张量 tensor 上. 张量 tensor, 张量 tensor1, 张量 tensor2 的形状必须 broadcastable. 对于类型为 FloatTensor 或者 DoubleTensor 的张量输入, value 必须为实数, 否则为整数. :param tensor: 张量, 对 tensor1 .* tensor2 进行相加 :type tensor: Tensor :param value: 标量, 对 tensor1 .* tensor2 进行相乘 :type value: Number, 可选 :param tensor1: 张量, 作为乘子1 :type tensor1: Tensor :param tensor2: 张量, 作为乘子2 :type tensor2: Tensor :param out: 输出张量 :type out: Tensor, 可选 示例： >>> t = torch.randn(2, 3) >>> t1 = torch.randn(1, 6) >>> t2 = torch.randn(6, 1) >>> torch.addcmul(t, 0.1, t1, t2) 0.0122 -0.0188 -0.2354 0.7396 -1.5721 1.2878 [torch.FloatTensor of size 2x3] torch.asin(input, out=None) → Tensor 返回一个新的 Tensor , 其元素为张量 input 的每个元素的反正弦. 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.asin(a) -0.6900 0.2752 0.4633 nan [torch.FloatTensor of size 4] torch.atan(input, out=None) → Tensor 返回一个新的 Tensor , 其元素为张量 input 的每个元素的反正切. 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.atan(a) -0.5669 0.2653 0.4203 0.9196 [torch.FloatTensor of size 4] torch.atan2(input1, input2, out=None) → Tensor 返回一个新的张量 Tensor , 其元素是输入张量 input1 和输入张量 input2 元素的反正切. 输入张量 input1 的形状和输入张量 input2 的形状必须可 broadcastable. 参数： input1 (Tensor) – 第一个输入 Tensor input2 (Tensor) – 第二个输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.atan2(a, torch.randn(4)) -2.4167 2.9755 0.9363 1.6613 [torch.FloatTensor of size 4] torch.ceil(input, out=None) → Tensor 返回一个新的张量 Tensor , 其元素是张量 input 的元素向上取整(取不小于每个元素的最小整数). 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> torch.ceil(a) 2 1 -0 -0 [torch.FloatTensor of size 4] torch.clamp(input, min, max, out=None) → Tensor 将输入张量 input 所有元素限制在区间 [min, max] 中并返回一个结果张量. | min, if x_i max 如果输入张量 input 的类型 FloatTensor 或者 DoubleTensor, 那么参数 min 和 max 必须为实数, 否则为整数. 参数： input (Tensor) – 输入 Tensor min (Number) – 限制范围下限 max (Number) – 限制范围上限 out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> torch.clamp(a, min=-0.5, max=0.5) 0.5000 0.3912 -0.5000 -0.5000 [torch.FloatTensor of size 4] torch.clamp(input, *, min, out=None) → Tensor 张量 input 的所有元素值大于或者等于 min. 如果张量 input 的类型是 FloatTensor 或者 DoubleTensor, 则 value 必须是实数, 否则应该是整数. 参数： input (Tensor) – 输入 Tensor value (Number) – 输出中每个元素的最小值 out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> torch.clamp(a, min=0.5) 1.3869 0.5000 0.5000 0.5000 [torch.FloatTensor of size 4] torch.clamp(input, *, max, out=None) → Tensor 张量 input 的所有元素值小于或者等于 max. 如果张量 input 的类型是 FloatTensor 或者 DoubleTensor, 则 value 必须是实数, 否则应该是整数. 参数： input (Tensor) – 输入 Tensor value (Number) – 输出中每个元素的最大值 out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> torch.clamp(a, max=0.5) 0.5000 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] torch.cos(input, out=None) → Tensor 返回一个新的张量 Tensor , 其元素是张量 input 每个元素的余弦. 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.cos(a) 0.8041 0.9633 0.9018 0.2557 [torch.FloatTensor of size 4] torch.cosh(input, out=None) → Tensor 返回一个新的张量 Tensor , 其元素是张量 input 每个元素的双曲余弦. 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.cosh(a) 1.2095 1.0372 1.1015 1.9917 [torch.FloatTensor of size 4] torch.div() torch.div(input, value, out=None) 将张量 input 的元素逐一除以标量值 value , 其结果作为一个新的张量返回. 如果张量 input 的类型是 FloatTensor 或者 DoubleTensor, 则标量值 value 必须是实数, 否则应该是整数. 参数： input (Tensor) – 输入 Tensor value (Number) – 除数, 被张量 input 的元素除 out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(5) >>> a -0.6147 -1.1237 -0.1604 -0.6853 0.1063 [torch.FloatTensor of size 5] >>> torch.div(a, 0.5) -1.2294 -2.2474 -0.3208 -1.3706 0.2126 [torch.FloatTensor of size 5] torch.div(input, other, out=None) 张量 input 的元素与张量 other 的元素逐一相除. 返回一个新的结果张量 out . 张量 input 与张量 other 的形状必须可 broadcastable. 参数： input (Tensor) – 分子 Tensor (被除数) other (Tensor) – 分母 Tensor (除数) out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4,4) >>> a -0.1810 0.4017 0.2863 -0.1013 0.6183 2.0696 0.9012 -1.5933 0.5679 0.4743 -0.0117 -0.1266 -0.1213 0.9629 0.2682 1.5968 [torch.FloatTensor of size 4x4] >>> b = torch.randn(8, 2) >>> b 0.8774 0.7650 0.8866 1.4805 -0.6490 1.1172 1.4259 -0.8146 1.4633 -0.1228 0.4643 -0.6029 0.3492 1.5270 1.6103 -0.6291 [torch.FloatTensor of size 8x2] >>> torch.div(a, b) -0.2062 0.5251 0.3229 -0.0684 -0.9528 1.8525 0.6320 1.9559 0.3881 -3.8625 -0.0253 0.2099 -0.3473 0.6306 0.1666 -2.5381 [torch.FloatTensor of size 4x4] torch.erf(tensor, out=None) → Tensor 计算每个元素的误差函数. 示例： >>> torch.erf(torch.Tensor([0, -1., 10.])) torch.FloatTensor([0., -0.8427, 1.]) torch.erfinv(tensor, out=None) → Tensor 计算每个元素的反向误差函数. 示例： >>> torch.erfinv(torch.Tensor([0, 0.5., -1.])) torch.FloatTensor([0., 0.4769, -inf]) torch.exp(tensor, out=None) → Tensor 计算每个元素的指数. 示例： >>> torch.exp(torch.Tensor([0, math.log(2)])) torch.FloatTensor([1, 2]) torch.floor(input, out=None) → Tensor 返回一个新的张量 Tensor , 其元素是张量 input 的元素向下取整(取不大于每个元素的最大整数). 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> torch.floor(a) 1 0 -1 -1 [torch.FloatTensor of size 4] torch.fmod(input, divisor, out=None) → Tensor 计算除法余数. 被除数和除数可能同时含有整数和浮点数. 这时余数的正负与被除数 tensor 相同. 当除数 divisor 是一个张量时r, 张量 input 和张量 divisor 的形状必须可 broadcastable. 参数： input (Tensor) – 被除数 divisor (Tensor 或 float) – 除数. 可能是一个数或者是一个与被除数相同形状的张量. out (Tensor, 可选) – 输出张量 示例： >>> torch.fmod(torch.Tensor([-3, -2, -1, 1, 2, 3]), 2) torch.FloatTensor([-1, -0, -1, 1, 0, 1]) >>> torch.fmod(torch.Tensor([1, 2, 3, 4, 5]), 1.5) torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5]) See also torch.remainder(), 其计算等价于 Python’s % 操作符的元素余数 torch.frac(tensor, out=None) → Tensor 计算张量 tensor 每个元素的分数部分. 示例： >>> torch.frac(torch.Tensor([1, 2.5, -3.2]) torch.FloatTensor([0, 0.5, -0.2]) torch.lerp(start, end, weight, out=None) 基于标量值 weight: , 在张量 start 与张量 end 之间做线性插值 并返回结果张量 out . 张量 start 和张量 end 的形状必须可 broadcastable. 参数： start (Tensor) – 起始点 Tensor end (Tensor) – 终点 Tensor weight (float) – 插值公式的权重 out (Tensor, 可选) – 结果 Tensor 示例： >>> start = torch.arange(1, 5) >>> end = torch.Tensor(4).fill_(10) >>> start 1 2 3 4 [torch.FloatTensor of size 4] >>> end 10 10 10 10 [torch.FloatTensor of size 4] >>> torch.lerp(start, end, 0.5) 5.5000 6.0000 6.5000 7.0000 [torch.FloatTensor of size 4] torch.log(input, out=None) → Tensor 返回一个新的张量 Tensor , 其元素是张量 input 所有元素的自然对数. 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(5) >>> a -0.4183 0.3722 -0.3091 0.4149 0.5857 [torch.FloatTensor of size 5] >>> torch.log(a) nan -0.9883 nan -0.8797 -0.5349 [torch.FloatTensor of size 5] torch.log1p(input, out=None) → Tensor 返回一个新的张量 Tensor , 其元素是(1 + input) 的自然对数. 注解： 对于较小的张量 input 的值, 此函数比 torch.log() 更精确. 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(5) >>> a -0.4183 0.3722 -0.3091 0.4149 0.5857 [torch.FloatTensor of size 5] >>> torch.log1p(a) -0.5418 0.3164 -0.3697 0.3471 0.4611 [torch.FloatTensor of size 5] torch.mul() torch.mul(input, value, out=None) 将输入张量 input 的每个元素与标量值 value 相乘并返回一个新的结果张量. 如果张量 input 的类型为 FloatTensor or DoubleTensor, 则 value 应该是实数, 否则为整数. 参数： input (Tensor) – 输入 Tensor value (Number) – 与张量 input 每个元素相乘的数 out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(3) >>> a -0.9374 -0.5254 -0.6069 [torch.FloatTensor of size 3] >>> torch.mul(a, 100) -93.7411 -52.5374 -60.6908 [torch.FloatTensor of size 3] torch.mul(input, other, out=None) 张量 input 的元素与张量 other 的元素逐一相乘. 其结果作为一个新的张量返回. 张量 input 和张量 other 的形状必须可 broadcastable. 参数： input (Tensor) – 第一个乘数 Tensor other (Tensor) – 第二个乘数 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4,4) >>> a -0.7280 0.0598 -1.4327 -0.5825 -0.1427 -0.0690 0.0821 -0.3270 -0.9241 0.5110 0.4070 -1.1188 -0.8308 0.7426 -0.6240 -1.1582 [torch.FloatTensor of size 4x4] >>> b = torch.randn(2, 8) >>> b 0.0430 -1.0775 0.6015 1.1647 -0.6549 0.0308 -0.1670 1.0742 -1.2593 0.0292 -0.0849 0.4530 1.2404 -0.4659 -0.1840 0.5974 [torch.FloatTensor of size 2x8] >>> torch.mul(a, b) -0.0313 -0.0645 -0.8618 -0.6784 0.0934 -0.0021 -0.0137 -0.3513 1.1638 0.0149 -0.0346 -0.5068 -1.0304 -0.3460 0.1148 -0.6919 [torch.FloatTensor of size 4x4] torch.neg(input, out=None) → Tensor 返回一个新的张量 Tensor , 其元素是张量 input 的元素的负值. 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(5) >>> a -0.4430 1.1690 -0.8836 -0.4565 0.2968 [torch.FloatTensor of size 5] >>> torch.neg(a) 0.4430 -1.1690 0.8836 0.4565 -0.2968 [torch.FloatTensor of size 5] torch.pow() torch.pow(input, exponent, out=None) 对输入张量 input 按元素求 exponent 次幂值并返回结果张量(其值作为结果张量的元素). 幂值 exponent 可以是一个单一的浮点数 float 或者是一个与张量 input 有相同元素数的张量 Tensor . 当指数 exponent 是一个标量时, 执行操作: 当指数 exponent 是一个张量, 执行操作: 当幂值 exponent 是一个张量, 张量 input 和张量 exponent 的形状必须可 broadcastable. 参数： input (Tensor) – 输入 Tensor exponent (float 或 Tensor) – 指数 out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a -0.5274 -0.8232 -2.1128 1.7558 [torch.FloatTensor of size 4] >>> torch.pow(a, 2) 0.2781 0.6776 4.4640 3.0829 [torch.FloatTensor of size 4] >>> exp = torch.arange(1, 5) >>> a = torch.arange(1, 5) >>> a 1 2 3 4 [torch.FloatTensor of size 4] >>> exp 1 2 3 4 [torch.FloatTensor of size 4] >>> torch.pow(a, exp) 1 4 27 256 [torch.FloatTensor of size 4] torch.pow(base, input, out=None) base 是一个标量浮点值, input 是一个张量. 返回的张量 out 的形状与张量 input 的形状相同. 执行操作: 参数： base (float) – 幂运算的底数 input (Tensor) – 指数 out (Tensor, 可选) – 结果 Tensor 示例： >>> exp = torch.arange(1, 5) >>> base = 2 >>> torch.pow(base, exp) 2 4 8 16 [torch.FloatTensor of size 4] torch.reciprocal(input, out=None) → Tensor 返回一个新的 Tensor , 其元素是张量 input 元素的倒数, i.e. 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> torch.reciprocal(a) 0.7210 2.5565 -1.1583 -1.8289 [torch.FloatTensor of size 4] torch.remainder(input, divisor, out=None) → Tensor 计算元素的除法的余数. 除数与被除数可能同时包含整数或浮点数. 余数与除数有相同的符号. 当除数 divisor 是一个张量, 张量 input 的形状和张量 divisor 得形状必须可 broadcastable. 参数： input (Tensor) – 被除数 divisor (Tensor 或 float) – 除数. 可能是一个数或者可能是一个与被除数大小相同的张量 out (Tensor, 可选) – 输出张量 示例： >>> torch.remainder(torch.Tensor([-3, -2, -1, 1, 2, 3]), 2) torch.FloatTensor([1, 0, 1, 1, 0, 1]) >>> torch.remainder(torch.Tensor([1, 2, 3, 4, 5]), 1.5) torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5]) See also torch.fmod() 同样计算除法余数, 等效于C库函数中的 fmod() torch.round(input, out=None) → Tensor 返回一个新的张量 Tensor , 其元素是输入张量的元素四舍五入到最近的整数. 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a 1.2290 1.3409 -0.5662 -0.0899 [torch.FloatTensor of size 4] >>> torch.round(a) 1 1 -1 -0 [torch.FloatTensor of size 4] torch.rsqrt(input, out=None) → Tensor 返回一个新的张量 Tensor , 其元素是张量 input 元素的平方根的倒数. 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a 1.2290 1.3409 -0.5662 -0.0899 [torch.FloatTensor of size 4] >>> torch.rsqrt(a) 0.9020 0.8636 nan nan [torch.FloatTensor of size 4] torch.sigmoid(input, out=None) → Tensor 返回一个新的张量 Tensor , 其元素是张量 input 元素的sigmoid值. 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a -0.4972 1.3512 0.1056 -0.2650 [torch.FloatTensor of size 4] >>> torch.sigmoid(a) 0.3782 0.7943 0.5264 0.4341 [torch.FloatTensor of size 4] torch.sign(input, out=None) → Tensor 返回一个新的张量 Tensor , 其元素是张量 input 元素的符号. 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.sign(a) -1 1 1 1 [torch.FloatTensor of size 4] torch.sin(input, out=None) → Tensor 返回一个新的张量 Tensor , 其元素是张量 input 元素的正弦. 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.sin(a) -0.5944 0.2684 0.4322 0.9667 [torch.FloatTensor of size 4] torch.sinh(input, out=None) → Tensor 返回一个新的张量 Tensor , 其元素是张量 input 元素的双曲正弦. 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.sinh(a) -0.6804 0.2751 0.4619 1.7225 [torch.FloatTensor of size 4] torch.sqrt(input, out=None) → Tensor 返回一个新的张量 Tensor , 其元素是张量 input 元素的平方根. 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a 1.2290 1.3409 -0.5662 -0.0899 [torch.FloatTensor of size 4] >>> torch.sqrt(a) 1.1086 1.1580 nan nan [torch.FloatTensor of size 4] torch.tan(input, out=None) → Tensor 返回一个新的张量 Tensor , 其元素是张量 input 元素的正切. 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.tan(a) -0.7392 0.2786 0.4792 3.7801 [torch.FloatTensor of size 4] torch.tanh(input, out=None) → Tensor 返回一个新的张量 Tensor , 其元素是张量 input 元素的双曲正切. 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4) >>> a -0.6366 0.2718 0.4469 1.3122 [torch.FloatTensor of size 4] >>> torch.tanh(a) -0.5625 0.2653 0.4193 0.8648 [torch.FloatTensor of size 4] torch.trunc(input, out=None) → Tensor 返回一个新的张量 Tensor , 其元素是张量 input 元素的截断整数值 (直接去除小数部分) . 参数： input (Tensor) – 输入 Tensor out (Tensor, 可选) – 输出 Tensor 示例： >>> a = torch.randn(4) >>> a -0.4972 1.3512 0.1056 -0.2650 [torch.FloatTensor of size 4] >>> torch.trunc(a) -0 1 0 -0 [torch.FloatTensor of size 4] Reduction Ops (归约操作) torch.cumprod(input, dim, out=None) → Tensor 返回元素 input 在给定维度 dim 下的累积积. 例如, 如果 input 是一个N元张量, 结果也是一个N元张量, 元素为: 参数： input (Tensor) – 输入 Tensor dim (int) – 进行操作的维度 out (Tensor, 可选) – 输出 Tensor 示例： >>> a = torch.randn(10) >>> a 1.1148 1.8423 1.4143 -0.4403 1.2859 -1.2514 -0.4748 1.1735 -1.6332 -0.4272 [torch.FloatTensor of size 10] >>> torch.cumprod(a, dim=0) 1.1148 2.0537 2.9045 -1.2788 -1.6444 2.0578 -0.9770 -1.1466 1.8726 -0.8000 [torch.FloatTensor of size 10] >>> a[5] = 0.0 >>> torch.cumprod(a, dim=0) 1.1148 2.0537 2.9045 -1.2788 -1.6444 -0.0000 0.0000 0.0000 -0.0000 0.0000 [torch.FloatTensor of size 10] torch.cumsum(input, dim, out=None) → Tensor 返回元素 input 在给定维度 dim 下的累积和. 例如, 如果 input 是一个N元张量, 结果将也是一个N元张量, 元素为: 参数： input (Tensor) – 输入 Tensor dim (int) – 进行操作的维度 out (Tensor, 可选) – 输出 Tensor 示例： >>> a = torch.randn(10) >>> a -0.6039 -0.2214 -0.3705 -0.0169 1.3415 -0.1230 0.9719 0.6081 -0.1286 1.0947 [torch.FloatTensor of size 10] >>> torch.cumsum(a, dim=0) -0.6039 -0.8253 -1.1958 -1.2127 0.1288 0.0058 0.9777 1.5858 1.4572 2.5519 [torch.FloatTensor of size 10] torch.dist(input, other, p=2) → float 返回(input - other)的p-范数 input 和 other 的形状必须满足 broadcastable. 参数： input (Tensor) – 输入 Tensor other (Tensor) – 右侧输入 Tensor p (float, 可选) – 所计算的范数. 示例： >>> x = torch.randn(4) >>> x 0.2505 -0.4571 -0.3733 0.7807 [torch.FloatTensor of size 4] >>> y = torch.randn(4) >>> y 0.7782 -0.5185 1.4106 -2.4063 [torch.FloatTensor of size 4] >>> torch.dist(x, y, 3.5) 3.302832063224223 >>> torch.dist(x, y, 3) 3.3677282206393286 >>> torch.dist(x, y, 0) inf >>> torch.dist(x, y, 1) 5.560028076171875 torch.mean() torch.mean(input) → float 返回张量 input 所有元素的均值. 参数：input (Tensor) – 输入 Tensor 示例： >>> a = torch.randn(1, 3) >>> a -0.2946 -0.9143 2.1809 [torch.FloatTensor of size 1x3] >>> torch.mean(a) 0.32398951053619385 torch.mean(input, dim, keepdim=False, out=None) → Tensor 返回张量 input 在给定维度 dim 上每行的均值. 如果 keepdim 是 True, 输出张量的大小与输入张量 input 相同, 除了维度 dim 是1. 另外, dim 被挤压 (参看 torch.squeeze() ), 导致输出张量减少一维. 参数： input (Tensor) – 输入 Tensor dim (int) – 要减少的维度 keepdim (bool, 可选) – 输出张量的维度 dim 保持与否 out (Tensor) – 输出张量 示例： >>> a = torch.randn(4, 4) >>> a -1.2738 -0.3058 0.1230 -1.9615 0.8771 -0.5430 -0.9233 0.9879 1.4107 0.0317 -0.6823 0.2255 -1.3854 0.4953 -0.2160 0.2435 [torch.FloatTensor of size 4x4] >>> torch.mean(a, 1) -0.8545 0.0997 0.2464 -0.2157 [torch.FloatTensor of size 4] >>> torch.mean(a, 1, True) -0.8545 0.0997 0.2464 -0.2157 [torch.FloatTensor of size 4x1] torch.median() torch.median(input) → float 返回输出张量 input 所有元素的中位数. 参数：input (Tensor) – the input Tensor 示例： >>> a = torch.randn(1, 3) >>> a 0.4729 -0.2266 -0.2085 [torch.FloatTensor of size 1x3] >>> torch.median(a) -0.2085 torch.median(input, dim=-1, keepdim=False, values=None, indices=None) -> (Tensor, LongTensor) 返回输出张量 input 在给定维度 dim 下每行的中位数. 同时返回一个包含中位数的索引 LongTensor. dim 的缺省值为输入张量 input 的最后一维. 如果 keepdim 是 True, 输出张量与输入张量 input 形状相同, 除了维数 dim 是1. 另外, dim 被挤压 (参看 torch.squeeze() ), 导致输出张量比输入张量 input 少一维. 参数： input (Tensor) – 输入张量 Tensor dim (int) – 要减少的维度 keepdim (bool) – 输出张量的维度 dim 保留与否 values (Tensor, 可选) – 结果张量 indices (Tensor, 可选) – 结果张量索引 示例： >>> a -0.6891 -0.6662 0.2697 0.7412 0.5254 -0.7402 0.5528 -0.2399 [torch.FloatTensor of size 4x2] >>> a = torch.randn(4, 5) >>> a 0.4056 -0.3372 1.0973 -2.4884 0.4334 2.1336 0.3841 0.1404 -0.1821 -0.7646 -0.2403 1.3975 -2.0068 0.1298 0.0212 -1.5371 -0.7257 -0.4871 -0.2359 -1.1724 [torch.FloatTensor of size 4x5] >>> torch.median(a, 1) ( 0.4056 0.1404 0.0212 -0.7257 [torch.FloatTensor of size 4] , 0 2 4 1 [torch.LongTensor of size 4] ) torch.mode(input, dim=-1, keepdim=False, values=None, indices=None) -> (Tensor, LongTensor) 返回输入张量 input 在给定维数 dim 下每行元素的众数值. 同时也返回众数值的索引 LongTensor. 维度 dim 的缺省值是输入张量 input 的最后一维. . 如果 keepdim 是 True, 输出张量的大小与输入张量 input 相同, 除了维度 dim 是1. 另外, dim 被挤压 (参看 torch.squeeze() ), 导致输出张量减少一维. 注解： 这个函数至今没有为 torch.cuda.Tensor 定义. 参数： input (Tensor) – 输入张量 Tensor dim (int) – 要减少的维度 keepdim (bool) – 输出张量的维度 dim 保持与否 values (Tensor, 可选) – 结果张量 indices (Tensor, 可选) – 结果索引张量 示例： >>> a -0.6891 -0.6662 0.2697 0.7412 0.5254 -0.7402 0.5528 -0.2399 [torch.FloatTensor of size 4x2] >>> a = torch.randn(4, 5) >>> a 0.4056 -0.3372 1.0973 -2.4884 0.4334 2.1336 0.3841 0.1404 -0.1821 -0.7646 -0.2403 1.3975 -2.0068 0.1298 0.0212 -1.5371 -0.7257 -0.4871 -0.2359 -1.1724 [torch.FloatTensor of size 4x5] >>> torch.mode(a, 1) ( -2.4884 -0.7646 -2.0068 -1.5371 [torch.FloatTensor of size 4] , 3 4 2 0 [torch.LongTensor of size 4] ) torch.norm() torch.norm(input, p=2) → float 返回输入张量 input 的p-范数 参数： input (Tensor) – 输入张量 Tensor p (float, 可选) – 范数计算中的幂指数值 示例： >>> a = torch.randn(1, 3) >>> a -0.4376 -0.5328 0.9547 [torch.FloatTensor of size 1x3] >>> torch.norm(a, 3) 1.0338925067372466 torch.norm(input, p, dim, keepdim=False, out=None) → Tensor 返回输入张量 input 在给定维度 dim 下每行元素的p-范数. 如果 keepdim 是 True, 输出张量的大小与输入张量 input 相同, 除非维度 dim 是1. 另外, dim 被挤压 (参看 torch.squeeze() ), 导致输出张量减少一维. 参数： input (Tensor) – 输入张量 Tensor p (float) – 范数计算中的幂指数值 dim (int) – 要减少的维度 keepdim (bool) – 输出张量的维度 dim 保持与否 out (Tensor, 可选) – 结果张量 示例： >>> a = torch.randn(4, 2) >>> a -0.6891 -0.6662 0.2697 0.7412 0.5254 -0.7402 0.5528 -0.2399 [torch.FloatTensor of size 4x2] >>> torch.norm(a, 2, 1) 0.9585 0.7888 0.9077 0.6026 [torch.FloatTensor of size 4] >>> torch.norm(a, 0, 1, True) 2 2 2 2 [torch.FloatTensor of size 4x1] torch.prod() torch.prod(input) → float 返回输入张量 input 所有元素的乘积. 参数：input (Tensor) – 输入张量 Tensor 示例： >>> a = torch.randn(1, 3) >>> a 0.6170 0.3546 0.0253 [torch.FloatTensor of size 1x3] >>> torch.prod(a) 0.005537458061418483 torch.prod(input, dim, keepdim=False, out=None) → Tensor 返回输入张量 input 在给定维度 dim 下每行元素的积. 如果 keepdim 是 True, 输出张量的大小与输入张量 input 相同, 除了维度 dim 是1. 另外, dim 被挤压 (参看 torch.squeeze() ), 导致输出张量减少一维. 参数： input (Tensor) – 输入张量 Tensor dim (int) – 要减少的维度 keepdim (bool) – 输出张量的维度 dim 保持与否 out (Tensor, 可选) – 结果张量 示例： >>> a = torch.randn(4, 2) >>> a 0.1598 -0.6884 -0.1831 -0.4412 -0.9925 -0.6244 -0.2416 -0.8080 [torch.FloatTensor of size 4x2] >>> torch.prod(a, 1) -0.1100 0.0808 0.6197 0.1952 [torch.FloatTensor of size 4] torch.std() torch.std(input, unbiased=True) → float 返回输入张量 input 所有元素的标准差. 如果 unbiased 是 False , 那么标准差将通过有偏估计计算.否则, Bessel’s correction 将被使用. 参数： input (Tensor) – 输入 Tensor unbiased (bool) – 是否使用无偏估计 示例： >>> a = torch.randn(1, 3) >>> a -1.3063 1.4182 -0.3061 [torch.FloatTensor of size 1x3] >>> torch.std(a) 1.3782334731508061 torch.std(input, dim, keepdim=False, unbiased=True, out=None) → Tensor 返回输入张量 input 在给定维度 dim 下每行元素的标准差. 如果 keepdim 是 True, 输出张量的大小与输入张量 input 相同, 除了维度 dim 是 1. 另外, dim 被挤压 (参看 torch.squeeze() ), 导致输出张量减少一维. 如果 unbiased 是 False , 那么标准差将通过有偏估计来计算. 否则, Bessel’s correction 将被使用. 参数： input (Tensor) – 输入 Tensor dim (int) – 要减少的维度 keepdim (bool) – 输出张量的维度 dim 保持与否 unbiased (bool) – 是否使用无偏估计 out (Tensor, 可选) – 结果张量 示例： >>> a = torch.randn(4, 4) >>> a 0.1889 -2.4856 0.0043 1.8169 -0.7701 -0.4682 -2.2410 0.4098 0.1919 -1.1856 -1.0361 0.9085 0.0173 1.0662 0.2143 -0.5576 [torch.FloatTensor of size 4x4] >>> torch.std(a, dim=1) 1.7756 1.1025 1.0045 0.6725 [torch.FloatTensor of size 4] torch.sum() torch.sum(input) → float 返回输入张量 input 所有元素的和. 参数：input (Tensor) – 输入张量 Tensor 示例： >>> a = torch.randn(1, 3) >>> a 0.6170 0.3546 0.0253 [torch.FloatTensor of size 1x3] >>> torch.sum(a) 0.9969287421554327 torch.sum(input, dim, keepdim=False, out=None) → Tensor 返回输入张量 input 在给定维度 dim 下每行元素的和. 如果 keepdim 是 True, 输出张量的大小与输入张量 input 相同, 除了维度 dim 是 1. 另外, dim 被挤压 (参看 torch.squeeze() ), 导致输出张量减少一维. 参数： input (Tensor) – 输入张量 Tensor dim (int) – 要减少的维度 keepdim (bool) – 输出张量的维度 dim 保持与否 out (Tensor, 可选) – 结果张量 示例： >>> a = torch.randn(4, 4) >>> a -0.4640 0.0609 0.1122 0.4784 -1.3063 1.6443 0.4714 -0.7396 -1.3561 -0.1959 1.0609 -1.9855 2.6833 0.5746 -0.5709 -0.4430 [torch.FloatTensor of size 4x4] >>> torch.sum(a, 1) 0.1874 0.0698 -2.4767 2.2440 [torch.FloatTensor of size 4] torch.var() torch.var(input, unbiased=True) → float 返回输入张量 input 的方差. 如果 unbiased 是 False , 方差的计算将通过有偏估计计算. 否则, Bessel’s correction 将会被使用. 参数： input (Tensor) – 输入张量 Tensor unbiased (bool) – 是否使用无偏估计 示例： >>> a = torch.randn(1, 3) >>> a -1.3063 1.4182 -0.3061 [torch.FloatTensor of size 1x3] >>> torch.var(a) 1.899527506513334 torch.var(input, dim, keepdim=False, unbiased=True, out=None) → Tensor 返回输入张量 input 在给定维度 dim 下每行的方差. 如果 keepdim 是 True, 输出张量的大小与输入张量 input 相同, 除了维度 dim 是 1. 另外, dim 被挤压 (参看 torch.squeeze()), 导致输出张量减少一维. 如果 unbiased 是False, 方差的计算将通过有偏估计计算. 否则, Bessel’s correction 将会被使用. 参数： input (Tensor) – 输入张量 Tensor dim (int) – 要减少的维度 keepdim (bool) – 输出张量的维度 dim 保留与否 unbiased (bool) – 是否使用无偏估计 out (Tensor, 可选) – 结果张量 示例： >>> a = torch.randn(4, 4) >>> a -1.2738 -0.3058 0.1230 -1.9615 0.8771 -0.5430 -0.9233 0.9879 1.4107 0.0317 -0.6823 0.2255 -1.3854 0.4953 -0.2160 0.2435 [torch.FloatTensor of size 4x4] >>> torch.var(a, 1) 0.8859 0.9509 0.7548 0.6949 [torch.FloatTensor of size 4] Comparison Ops (比较操作) torch.eq(input, other, out=None) → Tensor 比较元素是否相等 第二个元素可以是一个数字或 broadcastable 为与第一个参数形状相同的张量. 参数： input (Tensor) – 待比较张量 other (Tensor 或 float) – 比较张量或数 out (Tensor, 可选) – 输出张量, 须为 ByteTensor 类型或与 input (Tensor) 同类型 返回值：一个 torch.ByteTensor 张量, 待比较和要比较张量逐位置比较, 相等为 1 , 不等为 0 示例： >>> torch.eq(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 1 0 0 1 [torch.ByteTensor of size 2x2] torch.equal(tensor1, tensor2) → bool 如果两个张量有相同的形状和元素值, 则返回 True , 否则 False . 示例： >>> torch.equal(torch.Tensor([1, 2]), torch.Tensor([1, 2])) True torch.ge(input, other, out=None) → Tensor 逐元素比较 input 和 other , 即是否 input>=other . 第二个参数可以为一个数或形状可 broadcastable 为和第一个参数相同类型的张量. 参数： input (Tensor) – 待对比的张量 other (Tensor 或 float) – 对比的张量或 float 值 out (Tensor, 可选) – 输出张量. 必须为 ByteTensor 或者与第一个参数 tensor 相同类型. 返回值：一个 torch.ByteTensor 张量, 包含了每个位置的比较结果(是否 input >= other ). 返回类型：Tensor 示例： >>> torch.ge(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 1 1 0 1 [torch.ByteTensor of size 2x2] torch.gt(input, other, out=None) → Tensor 逐元素比较 input 和 other , 即是否 input>other 如果两个张量有相同的形状和元素值, 则返回 True ,否则 False. 第二个参数可以为一个数或形状可 broadcastable 为和第一个参数相同类型的张量. 参数： input (Tensor) – 待对比的张量 other (Tensor 或 float) – 对比的张量或 float 值 out (Tensor, 可选) – 输出张量. 必须为 ByteTensor 或者与第一个参数 tensor 相同类型. 返回值：一个 torch.ByteTensor 张量, 包含了每个位置的比较结果(是否 input > other ). 返回类型：Tensor 示例： >>> torch.gt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 0 1 0 0 [torch.ByteTensor of size 2x2] torch.kthvalue(input, k, dim=None, keepdim=False, out=None) -> (Tensor, LongTensor) 取输入张量 input 指定维上第 k 个最小值. 如果不指定 dim , 则默认为 input 的最后一维. 返回一个元组 (values,indices) ,其中 indices 是原始输入张量 input 中沿 dim 维的第 k 个最小值下标. 如果 keepdim 为 True , values 和 indices 张量都和 input 大小相同, 除了在所有值都为1的 dim 维度上. 如果 keepdim 为 False , dim 被压缩. (参见 torch.squeeze() ), 使 values 和 indices 两个张量比 input 张量小一个的维度. 参数： input (Tensor) – 输入 Tensor k (int) – 第 k 个最小值 dim (int, 可选) – 沿着此维进行排序 keepdim (bool) – 输出张量是否保持维度 dim 不变 out (tuple, 可选) – 输出元组 ( Tensor, LongTensor ) 可选参数(作为输出 buffers ) 示例： >>> x = torch.arange(1, 6) >>> x 1 2 3 4 5 [torch.FloatTensor of size 5] >>> torch.kthvalue(x, 4) ( 4 [torch.FloatTensor of size 1] , 3 [torch.LongTensor of size 1] ) >>> x=torch.arange(1,7).resize_(2,3) >>> x 1 2 3 4 5 6 [torch.FloatTensor of size 2x3] >>> torch.kthvalue(x,2,0,True) ( 4 5 6 [torch.FloatTensor of size 1x3] , 1 1 1 [torch.LongTensor of size 1x3] ) torch.le(input, other, out=None) → Tensor 逐元素比较 input 和 other , 即是否 input 如果两个张量有相同的形状和元素值, 则返回 True ,否则 False . 第二个参数可以为一个数或形状可 broadcastable 为和第一个参数相同类型的张量. 参数： input (Tensor) – 待对比的张量 other (Tensor 或 float) – 对比的张量或 float 值 out (Tensor, 可选) – 输出张量. 必须为 ByteTensor 或者与第一个参数 tensor 相同类型. 返回值：一个 torch.ByteTensor 张量, 包含了每个位置的比较结果(是否 input 返回类型：Tensor 示例： >>> torch.le(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 1 0 1 1 [torch.ByteTensor of size 2x2] torch.lt(input, other, out=None) → Tensor 逐元素比较 input 和 other , 即是否 input 如果两个张量有相同的形状和元素值, 则返回 True ,否则 False . 第二个参数可以为一个数或形状可 broadcastable 为和第一个参数相同类型的张量. 参数： input (Tensor) – 待对比的张量 other (Tensor 或 float) – 对比的张量或 float 值 out (Tensor, 可选) – 输出张量. 必须为 ByteTensor 或者与第一个参数 tensor 相同类型. 返回值：一个 torch.ByteTensor 张量, 包含了每个位置的比较结果(是否 input 返回类型：Tensor 示例： >>> torch.lt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 0 0 1 0 [torch.ByteTensor of size 2x2] torch.max() torch.max(input) → float 返回输入 input 张量所有元素的最大值. 参数：input (Tensor) – 输入 Tensor 示例： >>> a = torch.randn(1, 3) >>> a 0.4729 -0.2266 -0.2085 [torch.FloatTensor of size 1x3] >>> torch.max(a) 0.4729 torch.max(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor) 返回输入张量 input 在给定维度 dim 上每行的最大值, 并同时返回每个最大值的位置索引. 如果 keepdim 为 True , values 和 indices 张量都和 input 尺寸相同, 除了在所有值都为 1 的 dim 维度上. 如果 keepdim 为 False , dim 被压缩. (参见 torch.squeeze() ), 使 values 和 indices 两个张量比 input 张量小一个的维度. 参数： input (Tensor) – 输入 Tensor k (int) – 第 k 个最小值 dim (int, 可选) – 沿着此维进行排序 keepdim (bool) – 输出张量是否保持维度 dim 不变 out (tuple, 可选) – 输出元组 (max, max_indices) 示例： >> a = torch.randn(4, 4) >> a 0.0692 0.3142 1.2513 -0.5428 0.9288 0.8552 -0.2073 0.6409 1.0695 -0.0101 -2.4507 -1.2230 0.7426 -0.7666 0.4862 -0.6628 torch.FloatTensor of size 4x4] >>> torch.max(a, 1) ( 1.2513 0.9288 1.0695 0.7426 [torch.FloatTensor of size 4] , 2 0 0 0 [torch.LongTensor of size 4] ) torch.max(input, other, out=None) → Tensor 输入 input 每一个元素和对应的比较张量 other 进行比较, 留下较大的元素 max. 要比较的张量 input 与比较张量 other 不必大小一致, 但它们一定要能 broadcastable . 参数： input (Tensor) – 要比较张量 Tensor other (Tensor) – 比较张量 Tensor out (Tensor, 可选) – 输出张量 Tensor 示例： >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> b = torch.randn(4) >>> b 1.0067 -0.8010 0.6258 0.3627 [torch.FloatTensor of size 4] >>> torch.max(a, b) 1.3869 0.3912 0.6258 0.3627 [torch.FloatTensor of size 4] torch.min() torch.min(input) → float 返回输入张量 input 所有元素的最小值. 参数：input (Tensor) – 输入 Tensor 示例： >>> a = torch.randn(1, 3) >>> a 0.4729 -0.2266 -0.2085 [torch.FloatTensor of size 1x3] >>> torch.min(a) -0.22663167119026184 torch.min(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor) 返回输入张量 input 在给定维度 dim 下每行元素的最小值. 其中第二个返回值是每个被找出的最小值的索引位置 ( argmin ) . 如果 keepdim 是 True, 输出张量的大小与输入张量 input 相同, 除了维数 dim 是 1 . 另外, dim 被挤压 (参看 torch.squeeze() ), 导致输出张量比输入张量 input 少一维. 参数： input (Tensor) – 输入张量 Tensor dim (int) – 要减少的维度 keepdim (bool) – 输出张量的维度 dim 保持与否 out (tuple, 可选) – 两个输出张量的结果元组 (min, min_indices) 示例： >> a = torch.randn(4, 4) >> a 0.0692 0.3142 1.2513 -0.5428 0.9288 0.8552 -0.2073 0.6409 1.0695 -0.0101 -2.4507 -1.2230 0.7426 -0.7666 0.4862 -0.6628 torch.FloatTensor of size 4x4] >> torch.min(a, 1) 0.5428 0.2073 2.4507 0.7666 torch.FloatTensor of size 4] 3 2 2 1 torch.LongTensor of size 4] torch.min(input, other, out=None) → Tensor 输入 input 每一个元素和对应的比较张量 other 进行比较, 留下较小的元素 min . 要比较的张量 input 与比较张量 other 不必尺寸一致, 但它们一定要能广播 broadcastable . 参数： input (Tensor) – 第一个张量 Tensor other (Tensor) – 第二个张量 Tensor out (Tensor, 可选) – 输出的张量 Tensor 示例： >>> a = torch.randn(4) >>> a 1.3869 0.3912 -0.8634 -0.5468 [torch.FloatTensor of size 4] >>> b = torch.randn(4) >>> b 1.0067 -0.8010 0.6258 0.3627 [torch.FloatTensor of size 4] >>> torch.min(a, b) 1.0067 -0.8010 -0.8634 -0.5468 [torch.FloatTensor of size 4] torch.ne(input, other, out=None) → Tensor 逐元素比较 input 和 other , 即是否 tensor != other 如果两个张量有相同的形状和元素值, 则返回 True , 否则 False . 第二个参数可以为一个数或形状广播 broadcastable 为和第一个参数相同类型的张量. 参数： input (Tensor) – 待对比的张量 other (Tensor 或 float) – 对比的张量或 float 值 out (Tensor, 可选) – 输出张量. 必须为 ByteTensor 或者与第一个参数 tensor 相同类型. 返回值：一个 torch.ByteTensor 张量, 包含了每个位置的比较结果 (是否 input != other ) . 返回类型：Tensor 示例： >>> torch.ne(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) 0 1 1 0 [torch.ByteTensor of size 2x2] torch.sort(input, dim=None, descending=False, out=None) -> (Tensor, LongTensor) 对输入张量 input 沿着指定维按升序排序. 如果不给定 dim ,则默认为输入的最后一维. 如果指定参数 descending 为 True , 则按降序排序. 返回元组 (sorted_tensor, sorted_indices) , sorted_indices 为原始输入中的下标. 参数： input (Tensor) – 要对比的张量 dim (int, 可选) – 沿着此维排序 descending (bool, 可选) – 布尔值, 控制升降排序 out (tuple, 可选) – 输出张量. 必须为 ByteTensor 或者与第一个参数 tensor 相同类型. 示例： >>> x = torch.randn(3, 4) >>> sorted, indices = torch.sort(x) >>> sorted -1.6747 0.0610 0.1190 1.4137 -1.4782 0.7159 1.0341 1.3678 -0.3324 -0.0782 0.3518 0.4763 [torch.FloatTensor of size 3x4] >>> indices 0 1 3 2 2 1 0 3 3 1 0 2 [torch.LongTensor of size 3x4] >>> sorted, indices = torch.sort(x, 0) >>> sorted -1.6747 -0.0782 -1.4782 -0.3324 0.3518 0.0610 0.4763 0.1190 1.0341 0.7159 1.4137 1.3678 [torch.FloatTensor of size 3x4] >>> indices 0 2 1 2 2 0 2 0 1 1 0 1 [torch.LongTensor of size 3x4] torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -> (Tensor, LongTensor) 沿给定 dim 维度返回输入张量 input 中 k 个最大值. 如果不指定 dim , 则默认为 input 的最后一维. 如果为 largest 为 False ,则返回最小的 k 个值. 返回一个元组 (values, indices) , 其中 indices 是原始输入张量 input 中测元素下标. 如果设定布尔值 sorted 为 True , 将会确保返回的 k 个值被排序. 参数： input (Tensor) – 输入张量 k (int) – “top-k” 中的 k dim (int, 可选) – 排序的维 largest (bool, 可选) – 布尔值, 控制返回最大或最小值 sorted (bool, 可选) – 布尔值, 控制返回值是否排序 out (tuple, 可选) – 可选输出张量 (Tensor, LongTensor) output buffers 示例： >>> x = torch.arange(1, 6) >>> x 1 2 3 4 5 [torch.FloatTensor of size 5] >>> torch.topk(x, 3) ( 5 4 3 [torch.FloatTensor of size 3] , 4 3 2 [torch.LongTensor of size 3] ) >>> torch.topk(x, 3, 0, largest=False) ( 1 2 3 [torch.FloatTensor of size 3] , 0 1 2 [torch.LongTensor of size 3] ) Other Operations (其它操作) torch.cross(input, other, dim=-1, out=None) → Tensor 返回沿着维度 dim 上, 两个张量 input 和 other 的向量积 (叉积), input 和 other 必须有相同的形状, 且指定的 dim 维上 size 必须为 3. 如果不指定 dim, 则默认为第一个尺度为 3 的维. 参数： input (Tensor) – 输入 Tensor other (Tensor) – 第二个输入 Tensor dim (int, 可选) – 沿着此维进行叉积操作. out (Tensor, 可选) – 结果 Tensor 示例： >>> a = torch.randn(4, 3) >>> a -0.6652 -1.0116 -0.6857 0.2286 0.4446 -0.5272 0.0476 0.2321 1.9991 0.6199 1.1924 -0.9397 [torch.FloatTensor of size 4x3] >>> b = torch.randn(4, 3) >>> b -0.1042 -1.1156 0.1947 0.9947 0.1149 0.4701 -1.0108 0.8319 -0.0750 0.9045 -1.3754 1.0976 [torch.FloatTensor of size 4x3] >>> torch.cross(a, b, dim=1) -0.9619 0.2009 0.6367 0.2696 -0.6318 -0.4160 -1.6805 -2.0171 0.2741 0.0163 -1.5304 -1.9311 [torch.FloatTensor of size 4x3] >>> torch.cross(a, b) -0.9619 0.2009 0.6367 0.2696 -0.6318 -0.4160 -1.6805 -2.0171 0.2741 0.0163 -1.5304 -1.9311 [torch.FloatTensor of size 4x3] torch.diag(input, diagonal=0, out=None) → Tensor 如果输入是一个向量( 1D 张量), 则返回一个以 input 为对角线元素的 2D 方阵. 如果输入是一个矩阵( 2D 张量), 则返回一个包含 input 对角线元素的1D张量. 参数 diagonal 指定对角线: diagonal = 0, 主对角线. diagonal > 0, 主对角线之上. diagonal 参数： input (Tensor) – 输入 Tensor diagonal (int, 可选) – 指定对角线 out (Tensor, 可选) – 输出 Tensor 示例： 获得以 input 为对角线的方阵: >>> a = torch.randn(3) >>> a 1.0480 -2.3405 -1.1138 [torch.FloatTensor of size 3] >>> torch.diag(a) 1.0480 0.0000 0.0000 0.0000 -2.3405 0.0000 0.0000 0.0000 -1.1138 [torch.FloatTensor of size 3x3] >>> torch.diag(a, 1) 0.0000 1.0480 0.0000 0.0000 0.0000 0.0000 -2.3405 0.0000 0.0000 0.0000 0.0000 -1.1138 0.0000 0.0000 0.0000 0.0000 [torch.FloatTensor of size 4x4] 获得给定矩阵的第k条对角线: >>> a = torch.randn(3, 3) >>> a -1.5328 -1.3210 -1.5204 0.8596 0.0471 -0.2239 -0.6617 0.0146 -1.0817 [torch.FloatTensor of size 3x3] >>> torch.diag(a, 0) -1.5328 0.0471 -1.0817 [torch.FloatTensor of size 3] >>> torch.diag(a, 1) -1.3210 -0.2239 [torch.FloatTensor of size 2] torch.histc(input, bins=100, min=0, max=0, out=None) → Tensor 计算输入张量的直方图. 以 min 和 max 为 range 边界, 将其均分成 bins 个直条, 然后将排序好的数据划分到各个直条 (bins) 中. 如果 min 和 max 都为 0, 则利用数据中的最大最小值作为边界. 参数： input (Tensor) – 输入张量 bins (int) – 直方图 bins (直条)的个数(默认100个) min (int) – range 的下边界(包含) max (int) – range 的上边界(包含) out (Tensor, 可选) – 结果张量 返回值：直方图 返回类型：Tensor 示例： >>> torch.histc(torch.FloatTensor([1, 2, 1]), bins=4, min=0, max=3) FloatTensor([0, 2, 1, 0]) torch.renorm(input, p, dim, maxnorm, out=None) → Tensor 返回一个张量, 包含规范化后的各个子张量, 使得沿着 dim 维划分的各子张量的 p 范数小于 maxnorm 注解： 如果 p 范数的值小于 maxnorm, 则当前子张量不需要修改. 参数： input (Tensor) – 输入 Tensor p (float) – 范数的 p dim (int) – 沿着此维切片, 得到张量子集 maxnorm (float) – 每个子张量的范数的最大值 out (Tensor, 可选) – 结果张量 示例： >>> x = torch.ones(3, 3) >>> x[1].fill_(2) >>> x[2].fill_(3) >>> x 1 1 1 2 2 2 3 3 3 [torch.FloatTensor of size 3x3] >>> torch.renorm(x, 1, 0, 5) 1.0000 1.0000 1.0000 1.6667 1.6667 1.6667 1.6667 1.6667 1.6667 [torch.FloatTensor of size 3x3] torch.trace(input) → float 返回输入 2 维矩阵对角线元素的和(迹). 示例： >>> x = torch.arange(1, 10).view(3, 3) >>> x 1 2 3 4 5 6 7 8 9 [torch.FloatTensor of size 3x3] >>> torch.trace(x) 15.0 torch.tril(input, diagonal=0, out=None) → Tensor 返回一个张量, 包含输入矩阵 ( 2D 张量)的下三角部分, 其余部分被设为 0. 这里所说的下三角部分为矩阵指定对角线 diagonal 在线里的和下面的元素. 参数 diagonal 控制对角线. diagonal = 0, 主对角线. diagonal > 0, 主对角线之上. diagonal 参数： input (Tensor) – 输入 Tensor diagonal (int, 可选) – 指定对角线 out (Tensor, 可选) – 输出 Tensor 示例： >>> a = torch.randn(3,3) >>> a 1.3225 1.7304 1.4573 -0.3052 -0.3111 -0.1809 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] >>> torch.tril(a) 1.3225 0.0000 0.0000 -0.3052 -0.3111 0.0000 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] >>> torch.tril(a, diagonal=1) 1.3225 1.7304 0.0000 -0.3052 -0.3111 -0.1809 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] >>> torch.tril(a, diagonal=-1) 0.0000 0.0000 0.0000 -0.3052 0.0000 0.0000 1.2469 0.0064 0.0000 [torch.FloatTensor of size 3x3] torch.triu(input, diagonal=0, out=None) → Tensor 返回一个张量, 包含输入矩阵 ( 2D 张量)的上三角部分, 其余部分被设为 0. 这里所说的下三角部分为矩阵指定对角线 diagonal 在线里的和上面的元素. 参数 diagonal 控制对角线. diagonal = 0, 主对角线. diagonal > 0, 主对角线之上. diagonal 参数： input (Tensor) – 输入 Tensor diagonal (int, 可选) – 指定对角线 out (Tensor, 可选) – 输出 Tensor 示例： >>> a = torch.randn(3,3) >>> a 1.3225 1.7304 1.4573 -0.3052 -0.3111 -0.1809 1.2469 0.0064 -1.6250 [torch.FloatTensor of size 3x3] >>> torch.triu(a) 1.3225 1.7304 1.4573 0.0000 -0.3111 -0.1809 0.0000 0.0000 -1.6250 [torch.FloatTensor of size 3x3] >>> torch.triu(a, diagonal=1) 0.0000 1.7304 1.4573 0.0000 0.0000 -0.1809 0.0000 0.0000 0.0000 [torch.FloatTensor of size 3x3] >>> torch.triu(a, diagonal=-1) 1.3225 1.7304 1.4573 -0.3052 -0.3111 -0.1809 0.0000 0.0064 -1.6250 [torch.FloatTensor of size 3x3] BLAS and LAPACK Operations (BLAS和LAPACK操作) torch.addbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) → Tensor 执行保存在 batch1 和 batch2 中的矩阵的批量点乘, 伴随着一个减少的相加步骤 (所有的矩阵乘法沿第一维累加). mat 被相加到最终的结果中. batch1 和 batch2 必须是三维的张量, 且每个包含相同数量的矩阵. 如果 batch1 是一个 b x n x m 的张量, batch2 是一个 b x m x p的张量, 那么 mat 必须是 broadcastable 且是一个 n x p 的张量, 同时 attr:out 将是一个 n x p 的张量. 换句话说, 对于 FloatTensor 或者 DoubleTensor 类型的输入, 参数 beta 和 alpha 必须是实数, 否则他们应该是整数. 参数： beta (Number, 可选) – 作用于 mat 的乘子 (系数) mat (Tensor) – 要被相加的矩阵 alpha (Number, 可选) – 作用于 batch1 @ batch2 的乘子 batch1 (Tensor) – 要相乘的第一批矩阵 batch2 (Tensor) – 要相乘的第二批矩阵 out (Tensor, 可选) – 输出的张量结果 示例： >>> M = torch.randn(3, 5) >>> batch1 = torch.randn(10, 3, 4) >>> batch2 = torch.randn(10, 4, 5) >>> torch.addbmm(M, batch1, batch2) -3.1162 11.0071 7.3102 0.1824 -7.6892 1.8265 6.0739 0.4589 -0.5641 -5.4283 -9.3387 -0.1794 -1.2318 -6.8841 -4.7239 [torch.FloatTensor of size 3x5] torch.addmm(beta=1, mat, alpha=1, mat1, mat2, out=None) → Tensor 执行矩阵 mat1 和 mat2 的相乘. 矩阵 mat 将与相乘的最终计算结果相加. 如果 mat1 是一个 n x m 的张量, mat2 是一个 m x p的张量, 那么 mat 必须是 broadcastable 且是一个 n x p 的张量, 同时 attr:out 将是一个 n x p 的张量. 换句话说, 对于 FloatTensor 或者 DoubleTensor 类型的输入, 参数 beta 和 alpha 必须是实数, 否则他们应该是整数. 参数： beta (Number, 可选) – 作用于mat的乘子 mat (Tensor) – 要被相加的矩阵 alpha (Number, 可选) – 作用于mat1 @ mat2的乘子 mat1 (Tensor) – 要相乘的第一个矩阵 mat2 (Tensor) – 要相乘的第二个矩阵 out (Tensor, 可选) – 输出结果 示例： >>> M = torch.randn(2, 3) >>> mat1 = torch.randn(2, 3) >>> mat2 = torch.randn(3, 3) >>> torch.addmm(M, mat1, mat2) -0.4095 -1.9703 1.3561 5.7674 -4.9760 2.7378 [torch.FloatTensor of size 2x3] torch.addmv(beta=1, tensor, alpha=1, mat, vec, out=None) → Tensor 执行矩阵 mat 和向量 vec 的相乘. 矩阵 tensor 将与相乘的最终计算结果相加. 如果 mat 是一个 n x m 的张量, vec 是一个长度为 m 的一维张量, 那么 :tensor 必须是 broadcastable 且是一个长度为 n 的一维张量, 同时 attr:out 将是一个长度为 n 的一维张量. alpha 和 beta 分别是 mat * vec 和 tensor 的缩放因子. 换句话说, 对于 FloatTensor 或者 DoubleTensor 类型的输入, 参数 beta 和 alpha 必须是实数, 否则他们应该是整数. 参数： beta (Number, 可选) – 作用于 tensor 的乘子 tensor (Tensor) – 要被相加的向量 alpha (Number, 可选) – 作用于 mat @ vec 的乘子 mat (Tensor) – 要被相乘的矩阵 vec (Tensor) – 要被要乘的向量 out (Tensor, 可选) – 输出结果 示例： >>> M = torch.randn(2) >>> mat = torch.randn(2, 3) >>> vec = torch.randn(3) >>> torch.addmv(M, mat, vec) -2.0939 -2.2950 [torch.FloatTensor of size 2] torch.addr(beta=1, mat, alpha=1, vec1, vec2, out=None) → Tensor 执行向量 vec1 和 vec2 的外积, 并把外积计算结果与矩阵 mat 相加. 可选值 beta 和 alpha 是标量, 分别与 mat 和 相乘. 换句话说, 如果 vec1 是一个长度为 n 的向量, vec2 是一个长度为 m 的向量, 那么 mat 必须是 broadcastable 且是一个大小为 n x m 的矩阵, 同时 out 将是一个大小为 n x m 的矩阵. 对于 FloatTensor 或者 DoubleTensor 类型的输入, 参数 beta 和 alpha 必须是实数, 否则他们应该是整数. 参数： beta (Number, 可选) – 作用于 mat 的乘子 mat (Tensor) – 要被相加的矩阵 alpha (Number, 可选) – 作用于 vec1 和 vec2 外积计算结果的乘子 vec1 (Tensor) – 外积计算的第一个向量 vec2 (Tensor) – 外积计算的第二个向量 out (Tensor, 可选) – 输出结果 示例： >>> vec1 = torch.arange(1, 4) >>> vec2 = torch.arange(1, 3) >>> M = torch.zeros(3, 2) >>> torch.addr(M, vec1, vec2) 1 2 2 4 3 6 [torch.FloatTensor of size 3x2] torch.baddbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) → Tensor 执行保存在 batch1 和 batch2 中的矩阵的批量点乘. mat 被相加到最终的结果中. batch1 和 batch2 必须是三维的张量, 且每个包含相同数量的矩阵. 如果 batch1 是一个 b x n x m 的张量, batch2 是一个 b x m x p的张量, 那么 mat 必须是 broadcastable 且是一个 b x n x p 的张量, 同时 attr:out 将是一个 b x n x p 的张量. 换句话说, 对于 FloatTensor 或者 DoubleTensor 类型的输入, 参数 beta 和 alpha 必须是实数, 否则他们应该是整数. 参数： beta (Number, 可选) – 作用于 mat 的乘子 (系数) mat (Tensor) – 要被相加的张量 alpha (Number, 可选) – 作用于 batch1 @ batch2 的乘子 batch1 (Tensor) – 要相乘的第一批矩阵 batch2 (Tensor) – 要相乘的第二批矩阵 out (Tensor, 可选) – 输出的张量结果 示例： >>> M = torch.randn(10, 3, 5) >>> batch1 = torch.randn(10, 3, 4) >>> batch2 = torch.randn(10, 4, 5) >>> torch.baddbmm(M, batch1, batch2).size() torch.Size([10, 3, 5]) torch.bmm(batch1, batch2, out=None) → Tensor 执行保存在 batch1 和 batch2 中的矩阵的批量点乘. batch1 和 batch2 必须是三维的张量, 且每个包含相同数量的矩阵. 如果 batch1 是一个 b x n x m 的张量, batch2 是一个 b x m x p 的张量, out 将是一个 b x n x p 的张量. 注解： 这个函数不能参考 broadcast](notes/broadcasting.html#broadcasting-semantics). 对于广播矩阵相乘, 参见 [torch.matmul(). 参数： batch1 (Tensor) – 要相乘的第一批矩阵 batch2 (Tensor) – 要相乘的第二批矩阵 out (Tensor, 可选) – 输出结果 示例： >>> batch1 = torch.randn(10, 3, 4) >>> batch2 = torch.randn(10, 4, 5) >>> res = torch.bmm(batch1, batch2) >>> res.size() torch.Size([10, 3, 5]) torch.btrifact(A, info=None, pivot=True) → Tensor, IntTensor 批量 LU 分解. 返回一个包含 LU 分解和枢轴的元组. 对于每个 minibatch 示例, 如果分解成功, 可选参数 info 将提供分解信息. info 的值来自 dgetrf, 若是非零值, 则表示有错误发生. 如果 cuda 被使用的话, 具体的值来自 cublas, 否则来自 LAPACK. 如果设置了 pivot, 那么旋转操作将被执行. 参数：A (Tensor) – 要分解的张量. 示例： >>> A = torch.randn(2, 3, 3) >>> A_LU = A.btrifact() torch.btrisolve(b, LU_data, LU_pivots) → Tensor 批量 LU 解. 返回线性系统 Ax = b 的 LU 解. 参数： b (Tensor) – RHS tensor. LU_data (Tensor) – Pivoted LU factorization of A from btrifact. LU_pivots (IntTensor) – Pivots of the LU factorization. 示例： >>> A = torch.randn(2, 3, 3) >>> b = torch.randn(2, 3) >>> A_LU = torch.btrifact(A) >>> x = b.btrisolve(*A_LU) >>> torch.norm(A.bmm(x.unsqueeze(2)) - b) 6.664001874625056e-08 torch.dot(tensor1, tensor2) → float 计算两个张量的点乘 (内积). 注解： 这个函数不支持 broadcast. 示例： >>> torch.dot(torch.Tensor([2, 3]), torch.Tensor([2, 1])) 7.0 torch.eig(a, eigenvectors=False, out=None) -> (Tensor, Tensor) 计算实数方阵的特征值和特征向量. 参数： a (Tensor) – 一个要被计算特征值与特征向量的方阵 eigenvectors (bool) – 若为 True, 表示特征值与特征向量都被计算. 否则, 仅计算特征值. out (tuple, 可选) – 输出张量 返回值：包含以下的元组： e (Tensor): a 的左特征值 v (Tensor): 如果 eigenvectors 为 True, 表示 a 的特征向量; 否则是一个空的张量 返回类型：返回一个元组, (Tensor, Tensor) torch.gels(B, A, out=None) → Tensor 计算秩为 的， 大小为 m x n 的矩阵 最小二乘和最小范数问题的解 如果 !m >= n](img/tex-67ab86856a95fdd869cf2a0fff67d8be.gif), [gels() 求解最小二乘问题: 如果 !m gels() 求解最小范数问题: 返回的矩阵 的头 行包含解信息. 其余行包含剩余信息: 从第 行开始的每列的 euclidean 范数, 是对应列的剩余. 参数： B (Tensor) – The matrix A (Tensor) – The by matrix out (tuple, 可选) – Optional destination tensor 返回值：包含以下的元组： X (Tensor): 最小二乘解 qr (Tensor): QR 分解的详细信息 返回类型：(Tensor, Tensor) 注解： 不管输入矩阵的步长如何, 返回来的矩阵将总是被转置. 也就是, 他们的步长是 (1, m) 而不是 (m, 1). 示例： >>> A = torch.Tensor([[1, 1, 1], ... [2, 3, 4], ... [3, 5, 2], ... [4, 2, 5], ... [5, 4, 3]]) >>> B = torch.Tensor([[-10, -3], [ 12, 14], [ 14, 12], [ 16, 16], [ 18, 16]]) >>> X, _ = torch.gels(B, A) >>> X 2.0000 1.0000 1.0000 1.0000 1.0000 2.0000 [torch.FloatTensor of size 3x2] torch.geqrf(input, out=None) -> (Tensor, Tensor) 这是直接调用 LAPACK 的低层函数. 通常您应该使用 torch.qr() 来代替之. 计算 input 的 QR 分解, 但不构造 Q 和 R 作为显示分开的矩阵. 然而, 这样直接调用 LAPACK 的底层函数 ?geqrf, 会产生一连串的 ‘elementary reflectors’. 更多信息请参见 LAPACK documentation . 参数： input (Tensor) – the input matrix out (tuple, 可选) – The result tuple of (Tensor, Tensor) torch.ger(vec1, vec2, out=None) → Tensor 计算 vec1 和 vec2 的外积. 如果 vec1 是一个长度为 n 的向量, vec2 是一个长度为 m 的向量, 那么 out 必须是一个 n x m 的矩阵. 注解： 这个函数不支持 broadcast. 参数： vec1 (Tensor) – 1D input vector vec2 (Tensor) – 1D input vector out (Tensor, 可选) – optional output matrix 示例： >>> v1 = torch.arange(1, 5) >>> v2 = torch.arange(1, 4) >>> torch.ger(v1, v2) 1 2 3 2 4 6 3 6 9 4 8 12 [torch.FloatTensor of size 4x3] torch.gesv(B, A, out=None) -> (Tensor, Tensor) X, LU = torch.gesv(B, A) , 该函数返回线性系统 的解. LU 包含 A 的 LU 分解因子 L 和 U. A 必须是方阵, 且是非奇异的 (2维可逆张量). 如果 A 是一个 m x m 矩阵, B 是一个 m x k 的矩阵, 那么结果 LU 的大小为 m x m, X 的大小为 m x k . 注解： Irrespective of the original strides, the returned matrices X and LU will be transposed, i.e. with strides (1, m) instead of (m, 1). 参数： B (Tensor) – input matrix of m x k dimensions A (Tensor) – input square matrix of m x m dimensions out (Tensor, 可选) – optional output matrix 示例： >>> A = torch.Tensor([[6.80, -2.11, 5.66, 5.97, 8.23], ... [-6.05, -3.30, 5.36, -4.44, 1.08], ... [-0.45, 2.58, -2.70, 0.27, 9.04], ... [8.32, 2.71, 4.35, -7.17, 2.14], ... [-9.67, -5.14, -7.26, 6.08, -6.87]]).t() >>> B = torch.Tensor([[4.02, 6.19, -8.22, -7.57, -3.03], ... [-1.56, 4.00, -8.67, 1.75, 2.86], ... [9.81, -4.09, -4.57, -8.61, 8.99]]).t() >>> X, LU = torch.gesv(B, A) >>> torch.dist(B, torch.mm(A, X)) 9.250057093890353e-06 torch.inverse(input, out=None) → Tensor 计算方阵 input 的逆. 注解： Irrespective of the original strides, the returned matrix will be transposed, i.e. with strides (1, m) instead of (m, 1) 参数： input (Tensor) – the input 2D square Tensor out (Tensor, 可选) – the optional output Tensor 示例： >>> x = torch.rand(10, 10) >>> x 0.7800 0.2267 0.7855 0.9479 0.5914 0.7119 0.4437 0.9131 0.1289 0.1982 0.0045 0.0425 0.2229 0.4626 0.6210 0.0207 0.6338 0.7067 0.6381 0.8196 0.8350 0.7810 0.8526 0.9364 0.7504 0.2737 0.0694 0.5899 0.8516 0.3883 0.6280 0.6016 0.5357 0.2936 0.7827 0.2772 0.0744 0.2627 0.6326 0.9153 0.7897 0.0226 0.3102 0.0198 0.9415 0.9896 0.3528 0.9397 0.2074 0.6980 0.5235 0.6119 0.6522 0.3399 0.3205 0.5555 0.8454 0.3792 0.4927 0.6086 0.1048 0.0328 0.5734 0.6318 0.9802 0.4458 0.0979 0.3320 0.3701 0.0909 0.2616 0.3485 0.4370 0.5620 0.5291 0.8295 0.7693 0.1807 0.0650 0.8497 0.1655 0.2192 0.6913 0.0093 0.0178 0.3064 0.6715 0.5101 0.2561 0.3396 0.4370 0.4695 0.8333 0.1180 0.4266 0.4161 0.0699 0.4263 0.8865 0.2578 [torch.FloatTensor of size 10x10] >>> x = torch.rand(10, 10) >>> y = torch.inverse(x) >>> z = torch.mm(x, y) >>> z 1.0000 0.0000 0.0000 -0.0000 0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 1.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 -0.0000 -0.0000 0.0000 0.0000 1.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 1.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 1.0000 0.0000 0.0000 -0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 0.0000 1.0000 -0.0000 -0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 0.0000 0.0000 1.0000 0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 -0.0000 1.0000 -0.0000 0.0000 -0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 -0.0000 -0.0000 1.0000 -0.0000 -0.0000 0.0000 -0.0000 -0.0000 -0.0000 0.0000 -0.0000 -0.0000 0.0000 1.0000 [torch.FloatTensor of size 10x10] >>> torch.max(torch.abs(z - torch.eye(10))) # Max nonzero 5.096662789583206e-07 torch.matmul(tensor1, tensor2, out=None) Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), then a batched matrix multiply is returned. If the first argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after. If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. The non-matrix (i.e. batch) dimensions are broadcasted (and thus must be broadcastable). For example, if tensor1 is a j x 1 x n x m Tensor and tensor2 is a k x m x p Tensor, out will be an j x k x n x p Tensor. 注解： The 1-dimensional dot product version of this function does not support an out parameter. 参数： tensor1 (Tensor) – First tensor to be multiplied tensor2 (Tensor) – Second tensor to be multiplied out (Tensor, 可选) – Output tensor torch.mm(mat1, mat2, out=None) → Tensor 执行 mat1 和 mat2 的矩阵乘法. 如果 mat1 是一个 n x m 张量, mat2 是一个 m x p 张量, out 将是一个 n x p 张量. 注解： 这个函数不支持 broadcast](notes/broadcasting.html#broadcasting-semantics). 要使用支持广播矩阵乘法, 参见 [torch.matmul(). 参数： mat1 (Tensor) – First matrix to be multiplied mat2 (Tensor) – Second matrix to be multiplied out (Tensor, 可选) – Output tensor 示例： >>> mat1 = torch.randn(2, 3) >>> mat2 = torch.randn(3, 3) >>> torch.mm(mat1, mat2) 0.0519 -0.3304 1.2232 4.3910 -5.1498 2.7571 [torch.FloatTensor of size 2x3] torch.mv(mat, vec, out=None) → Tensor 执行矩阵 mat 与向量 vec 的乘法操作. 如果 mat 是一个 n x m 张量, vec 是一个大小为 m 的一维张量, out 将是一个大小为 n 的张量. 注解： 这个函数不支持 broadcast. 参数： mat (Tensor) – matrix to be multiplied vec (Tensor) – vector to be multiplied out (Tensor, 可选) – Output tensor 示例： >>> mat = torch.randn(2, 3) >>> vec = torch.randn(3) >>> torch.mv(mat, vec) -2.0939 -2.2950 [torch.FloatTensor of size 2] torch.orgqr() torch.ormqr() torch.potrf(a, out=None) potrf(a, upper, out=None) 计算半正定矩阵 a: 的 Cholesky 分解. 返回结果 u, 若 upper 设为 True 或未提供时, u 是一个上三角矩阵, 使得 成立; 若 upper 设为 False, u 是一个下三角矩阵, 使得 成立. 参数： a (Tensor) – the input 2D Tensor, a symmetric positive semidefinite matrix upper (bool, 可选) – Return upper (default) or lower triangular matrix out (Tensor, 可选) – A Tensor for u 示例： >>> a = torch.randn(3,3) >>> a = torch.mm(a, a.t()) # make symmetric positive definite >>> u = torch.potrf(a) >>> a 2.3563 3.2318 -0.9406 3.2318 4.9557 -2.1618 -0.9406 -2.1618 2.2443 [torch.FloatTensor of size 3x3] >>> u 1.5350 2.1054 -0.6127 0.0000 0.7233 -1.2053 0.0000 0.0000 0.6451 [torch.FloatTensor of size 3x3] >>> torch.mm(u.t(),u) 2.3563 3.2318 -0.9406 3.2318 4.9557 -2.1618 -0.9406 -2.1618 2.2443 [torch.FloatTensor of size 3x3] torch.potri(u, out=None) potri(u, upper, out=None) 给定一个半正定矩阵的 Cholesky 分解因子 u, 计算该半正定矩阵的逆. 返回矩阵 inv, 若 upper 设为 True 或为提供, u 是一个上三角矩阵, 使得 成立; 若 upper 设为 False, u 是一个下三角矩阵, 使得 成立. 参数： u (Tensor) – the input 2D Tensor, a upper or lower triangular Cholesky factor upper (bool, 可选) – Flag if upper (default) or lower triangular matrix out (Tensor, 可选) – A Tensor for inv 示例： >>> a = torch.randn(3,3) >>> a = torch.mm(a, a.t()) # make symmetric positive definite >>> u = torch.potrf(a) >>> a 2.3563 3.2318 -0.9406 3.2318 4.9557 -2.1618 -0.9406 -2.1618 2.2443 [torch.FloatTensor of size 3x3] >>> torch.potri(u) 12.5724 -10.1765 -4.5333 -10.1765 8.5852 4.0047 -4.5333 4.0047 2.4031 [torch.FloatTensor of size 3x3] >>> a.inverse() 12.5723 -10.1765 -4.5333 -10.1765 8.5852 4.0047 -4.5333 4.0047 2.4031 [torch.FloatTensor of size 3x3] torch.potrs(b, u, out=None) potrs(b, u, upper, out=None) Solves a linear system of equations with a positive semidefinite matrix to be inverted given its given a Cholesky factor matrix u: returns matrix c If upper is True or not provided, u is and upper triangular such that . If upper is False, u is and lower triangular such that . 注解： b is always a 2D Tensor, use b.unsqueeze(1) to convert a vector. 参数： b (Tensor) – the right hand side 2D Tensor u (Tensor) – the input 2D Tensor, a upper or lower triangular Cholesky factor upper (bool, 可选) – Return upper (default) or lower triangular matrix out (Tensor, 可选) – A Tensor for c 示例： >>> a = torch.randn(3,3) >>> a = torch.mm(a, a.t()) # make symmetric positive definite >>> u = torch.potrf(a) >>> a 2.3563 3.2318 -0.9406 3.2318 4.9557 -2.1618 -0.9406 -2.1618 2.2443 [torch.FloatTensor of size 3x3] >>> b = torch.randn(3,2) >>> b -0.3119 -1.8224 -0.2798 0.1789 -0.3735 1.7451 [torch.FloatTensor of size 3x2] >>> torch.potrs(b,u) 0.6187 -32.6438 -0.7234 27.0703 -0.6039 13.1717 [torch.FloatTensor of size 3x2] >>> torch.mm(a.inverse(),b) 0.6187 -32.6436 -0.7234 27.0702 -0.6039 13.1717 [torch.FloatTensor of size 3x2] torch.pstrf(a, out=None) pstrf(a, upper, out=None) Computes the pivoted Cholesky decomposition of a positive semidefinite matrix a: returns matrices u and piv. If upper is True or not provided, u is and upper triangular such that , with p the permutation given by piv. If upper is False, u is and lower triangular such that . 参数： a (Tensor) – the input 2D Tensor upper (bool, 可选) – Return upper (default) or lower triangular matrix out (tuple, 可选) – A tuple of u and piv Tensors 示例： >>> a = torch.randn(3,3) >>> a = torch.mm(a, a.t()) # make symmetric positive definite >>> a 5.4417 -2.5280 1.3643 -2.5280 2.9689 -2.1368 1.3643 -2.1368 4.6116 [torch.FloatTensor of size 3x3] >>> u,piv = torch.pstrf(a) >>> u 2.3328 0.5848 -1.0837 0.0000 2.0663 -0.7274 0.0000 0.0000 1.1249 [torch.FloatTensor of size 3x3] >>> piv 0 2 1 [torch.IntTensor of size 3] >>> p = torch.eye(3).index_select(0,piv.long()).index_select(0,piv.long()).t() # make pivot permutation >>> torch.mm(torch.mm(p.t(),torch.mm(u.t(),u)),p) # reconstruct 5.4417 1.3643 -2.5280 1.3643 4.6116 -2.1368 -2.5280 -2.1368 2.9689 [torch.FloatTensor of size 3x3] torch.qr(input, out=None) -> (Tensor, Tensor) 计算矩阵 input 的 QR 分解. 返回矩阵 q 和 r 使得 , 且 q 是一个 正交矩阵, r 是一个上三角矩阵. This returns the thin (reduced) QR factorization. 注解： 如果矩阵 input 中的元素太大, 那么精度可能会丢失. 注解： 尽管该函数总是能给您一个有效的分解, 但在不同平台上结果可能不同 - 取决于该平台上 LAPACK 的实现. 注解： Irrespective of the original strides, the returned matrix q will be transposed, i.e. with strides (1, m) instead of (m, 1). 参数： input (Tensor) – the input 2D Tensor out (tuple, 可选) – A tuple of Q and R Tensors 示例： >>> a = torch.Tensor([[12, -51, 4], [6, 167, -68], [-4, 24, -41]]) >>> q, r = torch.qr(a) >>> q -0.8571 0.3943 0.3314 -0.4286 -0.9029 -0.0343 0.2857 -0.1714 0.9429 [torch.FloatTensor of size 3x3] >>> r -14.0000 -21.0000 14.0000 0.0000 -175.0000 70.0000 0.0000 0.0000 -35.0000 [torch.FloatTensor of size 3x3] >>> torch.mm(q, r).round() 12 -51 4 6 167 -68 -4 24 -41 [torch.FloatTensor of size 3x3] >>> torch.mm(q.t(), q).round() 1 -0 0 -0 1 0 0 0 1 [torch.FloatTensor of size 3x3] torch.svd(input, some=True, out=None) -> (Tensor, Tensor, Tensor) U, S, V = torch.svd(A) 返回大小为 (n x m) 的实矩阵 A 的奇异值分解, 使得 . U 的大小为 n x n S 的大小为n x m V 的大小为 m x m. some 表示将被计算的奇异值的总数. 如果 some=True, 它将计算指定的 some 数量个奇异值, 如果 some=False, 则计算所有奇异值. 注解： Irrespective of the original strides, the returned matrix U will be transposed, i.e. with strides (1, n) instead of (n, 1). 参数： input (Tensor) – the input 2D Tensor some (bool, 可选) – controls the number of singular values to be computed out (tuple, 可选) – the result tuple 示例： >>> a = torch.Tensor([[8.79, 6.11, -9.15, 9.57, -3.49, 9.84], ... [9.93, 6.91, -7.93, 1.64, 4.02, 0.15], ... [9.83, 5.04, 4.86, 8.83, 9.80, -8.99], ... [5.45, -0.27, 4.85, 0.74, 10.00, -6.02], ... [3.16, 7.98, 3.01, 5.80, 4.27, -5.31]]).t() >>> a 8.7900 9.9300 9.8300 5.4500 3.1600 6.1100 6.9100 5.0400 -0.2700 7.9800 -9.1500 -7.9300 4.8600 4.8500 3.0100 9.5700 1.6400 8.8300 0.7400 5.8000 -3.4900 4.0200 9.8000 10.0000 4.2700 9.8400 0.1500 -8.9900 -6.0200 -5.3100 [torch.FloatTensor of size 6x5] >>> u, s, v = torch.svd(a) >>> u -0.5911 0.2632 0.3554 0.3143 0.2299 -0.3976 0.2438 -0.2224 -0.7535 -0.3636 -0.0335 -0.6003 -0.4508 0.2334 -0.3055 -0.4297 0.2362 -0.6859 0.3319 0.1649 -0.4697 -0.3509 0.3874 0.1587 -0.5183 0.2934 0.5763 -0.0209 0.3791 -0.6526 [torch.FloatTensor of size 6x5] >>> s 27.4687 22.6432 8.5584 5.9857 2.0149 [torch.FloatTensor of size 5] >>> v -0.2514 0.8148 -0.2606 0.3967 -0.2180 -0.3968 0.3587 0.7008 -0.4507 0.1402 -0.6922 -0.2489 -0.2208 0.2513 0.5891 -0.3662 -0.3686 0.3859 0.4342 -0.6265 -0.4076 -0.0980 -0.4932 -0.6227 -0.4396 [torch.FloatTensor of size 5x5] >>> torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t())) 8.934150226306685e-06 torch.symeig(input, eigenvectors=False, upper=True, out=None) -> (Tensor, Tensor) e, V = torch.symeig(input) 返回实对称矩阵 input 的特征值和特征向量. input 和 V 是 m x m 矩阵, e 是一个 m 维的向量. 这个函数计算矩阵 input 的所有特征值 (和向量), 使得 input = V diag(e) V’. 布尔参数 eigenvectors 定义了是否计算特征向量. 如果它为 False, 那么只有特征值会被计算. 如果它为 True, 特征值和特征向量都会被计算. 由于输入矩阵 input 被假定是对称的, 因此默认地只有它的上三角部分会被使用. 如果 upper 是 False, 那么它的下三角部分会被使用. Note: Irrespective of the original strides, the returned matrix V will be transposed, i.e. with strides (1, m) instead of (m, 1). 参数： input (Tensor) – the input symmetric matrix eigenvectors (boolean, 可选) – controls whether eigenvectors have to be computed upper (boolean, 可选) – controls whether to consider upper-triangular or lower-triangular region out (tuple, 可选) – The result tuple of (Tensor, Tensor) Examples: >>> a = torch.Tensor([[ 1.96, 0.00, 0.00, 0.00, 0.00], ... [-6.49, 3.80, 0.00, 0.00, 0.00], ... [-0.47, -6.39, 4.17, 0.00, 0.00], ... [-7.20, 1.50, -1.51, 5.70, 0.00], ... [-0.65, -6.34, 2.67, 1.80, -7.10]]).t() >>> e, v = torch.symeig(a, eigenvectors=True) >>> e -11.0656 -6.2287 0.8640 8.8655 16.0948 [torch.FloatTensor of size 5] >>> v -0.2981 -0.6075 0.4026 -0.3745 0.4896 -0.5078 -0.2880 -0.4066 -0.3572 -0.6053 -0.0816 -0.3843 -0.6600 0.5008 0.3991 -0.0036 -0.4467 0.4553 0.6204 -0.4564 -0.8041 0.4480 0.1725 0.3108 0.1622 [torch.FloatTensor of size 5x5] torch.trtrs() 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"tensors.html":{"url":"tensors.html","title":"torch.Tensor","keywords":"","body":"torch.Tensor 译者：hijkzzz torch.Tensor 是一种包含单一数据类型元素的多维矩阵. Torch定义了八种CPU张量类型和八种GPU张量类型： Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor torch.Tensor 是默认的tensor类型 (torch.FloatTensor) 的简称. Tensor 可以用torch.tensor()转换Python的 list 或序列​​生成： >>> torch.tensor([[1., -1.], [1., -1.]]) tensor([[ 1.0000, -1.0000], [ 1.0000, -1.0000]]) >>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]])) tensor([[ 1, 2, 3], [ 4, 5, 6]]) 警告 torch.tensor() 总是拷贝 data. 如果你有一个 Tensor data 并且仅仅想改变它的 requires_grad 属性, 可用 requires_grad_() or detach() 来避免拷贝. 如果你有一个 numpy 数组并且想避免拷贝, 请使用 torch.as_tensor(). 指定数据类型的Tensor可以通过传递参数 torch.dtype 和/或者 torch.device 到构造函数生成： >>> torch.zeros([2, 4], dtype=torch.int32) tensor([[ 0, 0, 0, 0], [ 0, 0, 0, 0]], dtype=torch.int32) >>> cuda0 = torch.device('cuda:0') >>> torch.ones([2, 4], dtype=torch.float64, device=cuda0) tensor([[ 1.0000, 1.0000, 1.0000, 1.0000], [ 1.0000, 1.0000, 1.0000, 1.0000]], dtype=torch.float64, device='cuda:0') Tensor的内容可以通过Python索引或者切片访问以及修改： >>> x = torch.tensor([[1, 2, 3], [4, 5, 6]]) >>> print(x[1][2]) tensor(6) >>> x[0][1] = 8 >>> print(x) tensor([[ 1, 8, 3], [ 4, 5, 6]]) 使用 torch.Tensor.item() 从只有一个值的Tensor中获取Python Number： >>> x = torch.tensor([[1]]) >>> x tensor([[ 1]]) >>> x.item() 1 >>> x = torch.tensor(2.5) >>> x tensor(2.5000) >>> x.item() 2.5 Tensor可以通过参数 requires_grad=True 创建, 这样 torch.autograd 会记录相关的运算实现自动求导. >>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True) >>> out = x.pow(2).sum() >>> out.backward() >>> x.grad tensor([[ 2.0000, -2.0000], [ 2.0000, 2.0000]]) 每一个tensor都有一个相应的 torch.Storage 保存其数据. tensor 类提供了一个多维的、strided视图, 并定义了数值操作. 注意 更多关于 torch.dtype, torch.device, 和 torch.layout 等 torch.Tensor的属性, 见 Tensor Attributes. 注意 注意：修改tensor的方法可以用一个下划线后缀来标示.比如, torch.FloatTensor.abs_() 会在原地计算绝对值并返回修改的张量, 而 torch.FloatTensor.abs() 将会在新张量中计算结果. 注意 为了改变已有的 tensor 的 torch.device 和/或者 torch.dtype, 考虑使用 to() 方法. class torch.Tensor 这里有少数几种生成Tensor的方法, 取决于你的实际情况. 从已经存在的数据生成, 用 torch.tensor(). 生成特殊尺寸的Tensor, 用 torch.* creation ops (见 Creation Ops). 生成与其它Tensor尺寸相同的Tensor (并且数据类型相同), 用 torch.*_like creation ops (见 Creation Ops). 生成与其它Tesor数据类型相同但是尺寸不同的Tensor, 用 tensor.new_* creation ops. new_tensor(data, dtype=None, device=None, requires_grad=False) → Tensor 返回一个新的Tensor用 data 作为tensor data.默认情况下, 返回的Tensor有相同的 torch.dtype 和 torch.device . 警告 new_tensor() 总是拷贝 data. 如果 你有一个 Tensor data 并且想避免拷贝, 使用 torch.Tensor.requires_grad_() 或者 torch.Tensor.detach(). 如果你有一个 numpy 数组并且想避免拷贝, 使用 torch.from_numpy(). 警告 当 data 是一个 tensor x, new_tensor() 读取 x 的 ‘data’ 并且创建一个叶子变量. 因此 tensor.new_tensor(x) 等价于 x.clone().detach() 并且 tensor.new_tensor(x, requires_grad=True) 等价于 x.clone().detach().requires_grad_(True). 推荐使用 clone() 和 detach(). 参数: data (array_like) – 返回的 Tensor 拷贝 data. dtype (torch.dtype, 可选) – 期望返回的Tensor的数据类型. 默认值: 如果是 None, 等于 torch.dtype. device (torch.device, 可选) – 期望返回的Tesor所在设备. 默认值: 如果是 None, 等于 torch.device. requires_grad (bool, 可选) – 是否为自动求导记录相关的运算. 默认值: False. 例子: >>> tensor = torch.ones((2,), dtype=torch.int8) >>> data = [[0, 1], [2, 3]] >>> tensor.new_tensor(data) tensor([[ 0, 1], [ 2, 3]], dtype=torch.int8) new_full(size, fill_value, dtype=None, device=None, requires_grad=False) → Tensor 返回一个Tesnor的尺寸等于 size 用 fill_value填充. 默认情况下, 返回的 Tensor 具有与此Tensor相同的 torch.dtype 和 torch.device. 参数: fill_value (scalar) – 用于填充的数值. dtype (torch.dtype, 可选) – 期望返回的Tensor的数据类型. 默认值: 如果是 None, 等于 torch.dtype. device (torch.device, 可选) – 期望返回的Tesor所在设备. 默认值: 如果是 None, 等于 torch.device. requires_grad (bool, 可选) – 是否为自动求导记录相关的运算. 默认值: False. 例子: >>> tensor = torch.ones((2,), dtype=torch.float64) >>> tensor.new_full((3, 4), 3.141592) tensor([[ 3.1416, 3.1416, 3.1416, 3.1416], [ 3.1416, 3.1416, 3.1416, 3.1416], [ 3.1416, 3.1416, 3.1416, 3.1416]], dtype=torch.float64) new_empty(size, dtype=None, device=None, requires_grad=False) → Tensor 返回一个Tesnor的尺寸等于 size 用 未初始化的值填充. 默认情况下, 返回的 Tensor 具有与此Tensor相同的 torch.dtype 和 torch.device. Parameters: dtype (torch.dtype, 可选) – 期望返回的Tensor的数据类型. 默认值: 如果是 None, 等于 torch.dtype. device (torch.device, 可选) – 期望返回的Tesor所在设备. 默认值: 如果是 None, 等于 torch.device. requires_grad (bool, 可选) – 是否为自动求导记录相关的运算. 默认值: False. Example: >>> tensor = torch.ones(()) >>> tensor.new_empty((2, 3)) tensor([[ 5.8182e-18, 4.5765e-41, -1.0545e+30], [ 3.0949e-41, 4.4842e-44, 0.0000e+00]]) new_ones(size, dtype=None, device=None, requires_grad=False) → Tensor 返回一个Tesnor的尺寸等于 size 用 1填充. 默认情况下, 返回的 Tensor 具有与此Tensor相同的 torch.dtype 和 torch.device. Parameters: size (int...) – list, tuple, 或者 torch.Size 定义了输出Tensor的形状. dtype (torch.dtype, 可选) – 期望返回的Tensor的数据类型. 默认值: 如果是 None, 等于 torch.dtype. device (torch.device, 可选) – 期望返回的Tesor所在设备. 默认值: 如果是 None, 等于 torch.device. requires_grad (bool, 可选) – 是否为自动求导记录相关的运算. 默认值: False. 例子: >>> tensor = torch.tensor((), dtype=torch.int32) >>> tensor.new_ones((2, 3)) tensor([[ 1, 1, 1], [ 1, 1, 1]], dtype=torch.int32) new_zeros(size, dtype=None, device=None, requires_grad=False) → Tensor 返回一个Tesnor的尺寸等于 size 用 0填充. 默认情况下, 返回的 Tensor 具有与此Tensor相同的 torch.dtype 和 torch.device. 参数: size (int...) – list, tuple, 或者 torch.Size 定义了输出Tensor的形状. dtype (torch.dtype, 可选) – 期望返回的Tensor的数据类型. 默认值: 如果是 None, 等于 torch.dtype. device (torch.device, 可选) – 期望返回的Tesor所在设备. 默认值: 如果是 None, 等于 torch.device. requires_grad (bool, 可选) – 是否为自动求导记录相关的运算. 默认值: False. 例子: >>> tensor = torch.tensor((), dtype=torch.float64) >>> tensor.new_zeros((2, 3)) tensor([[ 0., 0., 0.], [ 0., 0., 0.]], dtype=torch.float64) is_cuda True 如果 Tensor 在 GPU 上, 否则 False. device torch.device Tensor 所在的设备. abs() → Tensor 见 torch.abs() abs_() → Tensor 原地版本的 abs() acos() → Tensor 见 torch.acos() acos_() → Tensor 原地版本的 acos() add(value) → Tensor add(value=1, other) -> Tensor 见 torch.add() add_(value) → Tensor add_(value=1, other) -> Tensor 原地版本的 add() addbmm(beta=1, mat, alpha=1, batch1, batch2) → Tensor 见 torch.addbmm() addbmm_(beta=1, mat, alpha=1, batch1, batch2) → Tensor 原地版本的 addbmm() addcdiv(value=1, tensor1, tensor2) → Tensor 见 torch.addcdiv() addcdiv_(value=1, tensor1, tensor2) → Tensor 原地版本的 addcdiv() addcmul(value=1, tensor1, tensor2) → Tensor 见 torch.addcmul() addcmul_(value=1, tensor1, tensor2) → Tensor 原地版本的 addcmul() addmm(beta=1, mat, alpha=1, mat1, mat2) → Tensor 见 torch.addmm() addmm_(beta=1, mat, alpha=1, mat1, mat2) → Tensor 原地版本的 addmm() addmv(beta=1, tensor, alpha=1, mat, vec) → Tensor 见 torch.addmv() addmv_(beta=1, tensor, alpha=1, mat, vec) → Tensor 原地版本的 addmv() addr(beta=1, alpha=1, vec1, vec2) → Tensor 见 torch.addr() addr_(beta=1, alpha=1, vec1, vec2) → Tensor 原地版本的 addr() allclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) → Tensor 见 torch.allclose() apply_(callable) → Tensor 应用函数 callable 到Tensor中的每一个元素, 用 callable的返回值替换每一个元素. 注意 这个函数仅仅能在CPU上工作, 并且不要用于需要高性能的代码区域. argmax(dim=None, keepdim=False) 见 torch.argmax() argmin(dim=None, keepdim=False) 见 torch.argmin() asin() → Tensor 见 torch.asin() asin_() → Tensor 原地版本的 asin() atan() → Tensor 见 torch.atan() atan2(other) → Tensor 见 torch.atan2() atan2_(other) → Tensor 原地版本的 atan2() atan_() → Tensor 原地版本的 atan() baddbmm(beta=1, alpha=1, batch1, batch2) → Tensor 见 torch.baddbmm() baddbmm_(beta=1, alpha=1, batch1, batch2) → Tensor 原地版本的 baddbmm() bernoulli(*, generator=None) → Tensor 返回一个Tensor, 每一个 都是独立采样于 . self 必须是浮点型 dtype, 并且返回值有相同的 dtype. 见 torch.bernoulli() bernoulli_() bernoulli_(p=0.5, *, generator=None) → Tensor 从 独立采样填充 self 的每一个位置.self 可以是整型 dtype. bernoulli_(p_tensor, *, generator=None) → Tensor p_tensor 必须是一个包含概率的 Tensor 用于取得二元随机数. self tensor 的 元素将会被设置为采样于 的值. self 可以有整型 dtype, 但是 :attrp_tensor 必须有浮点型 dtype. 可参考 bernoulli() and torch.bernoulli() bmm(batch2) → Tensor 见 torch.bmm() byte() → Tensor self.byte() is equivalent to self.to(torch.uint8). See to(). btrifact(info=None, pivot=True) 见 torch.btrifact() btrifact_with_info(pivot=True) -> (Tensor, Tensor, Tensor) 见 torch.btrifact_with_info() btrisolve(LU_data, LU_pivots) → Tensor 见 torch.btrisolve() cauchy_(median=0, sigma=1, *, generator=None) → Tensor 用取自 Cauchy 分布得值填充Tensor: ceil() → Tensor 见 torch.ceil() ceil_() → Tensor 原地版本的 ceil() char() → Tensor self.char() 等价于 self.to(torch.int8). 见 to(). cholesky(upper=False) → Tensor 见 torch.cholesky() chunk(chunks, dim=0) → List of Tensors 见 torch.chunk() clamp(min, max) → Tensor 见 torch.clamp() clamp_(min, max) → Tensor 原地版本的 clamp() clone() → Tensor 返回一份拷贝的 self tensor. 这份拷贝有 self 相同的数据和类型. 注意 与copy_()不同, 此函数会被记录在计算图中. 传给克隆tensor的梯度将传播到原始tensor. contiguous() → Tensor 返回一个连续的得Tensor, 其data与 self 相同. 如果 self tensor 是连续的, 此函数返回 self tensor 自身. copy_(src, non_blocking=False) → Tensor 从 src 拷贝元素到 self tensor 然后返回 self. src tensor 必须与 self tensor 是 broadcastable. 但数据类型可以不同, 所在的设备也可以不同. 参数: src (Tensor) – 源 tensor non_blocking (bool) – 如果是 True 并且这次复制在 CPU 和 GPU 之间进行, 这次复制将会是异步的. 其他情况则没有影响. cos() → Tensor 见 torch.cos() cos_() → Tensor 原地版本的 cos() cosh() → Tensor 见 torch.cosh() cosh_() → Tensor 原地版本的 cosh() cpu() → Tensor 返回一个拷贝对象于 CPU 内存中. 如果这个对象已经在 CPU 内存中, 并且在者正确的设备上, 那么将会返回其本身. cross(other, dim=-1) → Tensor 见 torch.cross() cuda(device=None, non_blocking=False) → Tensor 返回一个拷贝对象于 CUDA 内存中. 如果这个对象已经在 CUDA 内存中, 并且在者正确的设备上, 那么将会返回其本身. 参数: device (torch.device) –目标GPU设备. 默认值是当前GPU. non_blocking (bool) – 如果是 True 并且源在pinned memory中, 这次拷贝将是异步的.否则此参数没有影响. 默认值: False. cumprod(dim, dtype=None) → Tensor 见 torch.cumprod() cumsum(dim, dtype=None) → Tensor 见 torch.cumsum() data_ptr() → int 返回 self tensor 的第一个元素的指针. det() → Tensor 见 torch.det() diag(diagonal=0) → Tensor 见 torch.diag() diag_embed(offset=0, dim1=-2, dim2=-1) → Tensor 见 torch.diag_embed() dim() → int 返回 self tensor 的维度. dist(other, p=2) → Tensor 见 torch.dist() div(value) → Tensor 见 torch.div() div_(value) → Tensor 原地版本的 div() dot(tensor2) → Tensor 见 torch.dot() double() → Tensor self.double() 等价于 self.to(torch.float64). 见 to(). eig(eigenvectors=False) -> (Tensor, Tensor) 见 torch.eig() element_size() → int 返回每个元素占用的字节数 Example: >>> torch.tensor([]).element_size() 4 >>> torch.tensor([], dtype=torch.uint8).element_size() 1 eq(other) → Tensor 见 torch.eq() eq_(other) → Tensor 原地版本的 eq() equal(other) → bool 见 torch.equal() erf() → Tensor 见 torch.erf() erf_() → Tensor 原地版本的 erf() erfc() → Tensor 见 torch.erfc() erfc_() → Tensor 原地版本的 erfc() erfinv() → Tensor 见 torch.erfinv() erfinv_() → Tensor 原地版本的 erfinv() exp() → Tensor 见 torch.exp() exp_() → Tensor 原地版本的 exp() expm1() → Tensor 见 torch.expm1() expm1_() → Tensor 原地版本的 expm1() expand(*sizes) → Tensor 返回一个新的 self tensor 的视图, 其中单一维度扩展到更大的尺寸. 传递-1意味着不改变该维度的大小. tensor 也可以扩展到更大的维度, 新的维度将会附加在前面.对于新维度, 其大小不能设置为- 1. 扩展张量不会分配新的内存, 但只会在现有张量上创建一个新的视图, 其中通过将stride设置为0, 第一个尺寸的维度会扩展到更大的尺寸.大小为1的任何维度都可以扩展到任意值, 而无需分配新内存. 参数: *sizes (torch.Size or int...) – 期望扩展的尺寸 例子: >>> x = torch.tensor([[1], [2], [3]]) >>> x.size() torch.Size([3, 1]) >>> x.expand(3, 4) tensor([[ 1, 1, 1, 1], [ 2, 2, 2, 2], [ 3, 3, 3, 3]]) >>> x.expand(-1, 4) # -1 意味着不会改变该维度 tensor([[ 1, 1, 1, 1], [ 2, 2, 2, 2], [ 3, 3, 3, 3]]) expand_as(other) → Tensor 扩展这个 tensor 使得其尺寸和 other 相同. self.expand_as(other) 等价于 self.expand(other.size()). 请看 expand() 获得更多关于 expand 的信息. 参数: other (torch.Tensor) – 返回的 tensor 的尺寸和 other. 相同 exponential_(lambd=1, *, generator=None) → Tensor 用取自 exponential 分布 的元素填充 self tensor : fill_(value) → Tensor 用指定的值填充 self. flatten(input, start_dim=0, end_dim=-1) → Tensor 见 torch.flatten() flip(dims) → Tensor 见 torch.flip() float() → Tensor self.float() 等价于 self.to(torch.float32). See to(). floor() → Tensor 见 torch.floor() floor_() → Tensor 原地版本的 floor() fmod(divisor) → Tensor 见 torch.fmod() fmod_(divisor) → Tensor 原地版本的 fmod() frac() → Tensor 见 torch.frac() frac_() → Tensor 原地版本的 frac() gather(dim, index) → Tensor 见 torch.gather() ge(other) → Tensor 见 torch.ge() ge_(other) → Tensor 原地版本的 ge() gels(A) → Tensor 见 torch.gels() geometric_(p, *, generator=None) → Tensor 用取自geometric 分布的值填充 self : geqrf() -> (Tensor, Tensor) 见 torch.geqrf() ger(vec2) → Tensor 见 torch.ger() gesv(A) → Tensor, Tensor 见 torch.gesv() get_device() -> Device ordinal (Integer) 对于 CUDA tensors, 这个函数返回一个 GPU 序号, 对应 tensor 所在的设备. 对于 CPU tensors, 抛出一个错误. Example: >>> x = torch.randn(3, 4, 5, device='cuda:0') >>> x.get_device() 0 >>> x.cpu().get_device() # 运行时错误: get_device 没有在 torch.FloatTensor 上实现 gt(other) → Tensor 见 torch.gt() gt_(other) → Tensor 原地版本的 gt() half() → Tensor self.half() 等价于 self.to(torch.float16). 见 to(). histc(bins=100, min=0, max=0) → Tensor 见 torch.histc() index_add_(dim, index, tensor) → Tensor 根据参数index 中的索引的顺序, 累加 tensor 中的元素到 self tensor, 例如, 如果 dim == 0 并且 index[i] == j, 则第 i 行 tensor 会被加到第 j行. tensor 第 dim 维度 必须和 index(必须是一个向量) 的长度相同, 并且其它维度必须和 self 匹配, 否则将会抛出一个错误. 注意 当使用 CUDA 作为后端, 这个操作可能导致不确定性行为, 且不容易关闭. 请看 Reproducibility. Parameters: dim (int) – 要索引的维度 index (LongTensor) – 从 tensor 中选择的索引 tensor (Tensor) – 用于相加的tensor 例子: >>> x = torch.ones(5, 3) >>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) >>> index = torch.tensor([0, 4, 2]) >>> x.index_add_(0, index, t) tensor([[ 2., 3., 4.], [ 1., 1., 1.], [ 8., 9., 10.], [ 1., 1., 1.], [ 5., 6., 7.]]) index_copy_(dim, index, tensor) → Tensor 根据参数index 中的选择的索引, 复制 tensor 中的元素到 self tensor, 例如, 如果 dim == 0 并且 index[i] == j, 则第 i 行 tensor 会被加到第 j行. tensor 第 dim 维度 必须和 index(必须是一个向量) 的长度相同, 并且其它维度必须和 self 匹配, 否则将会抛出一个错误. Parameters: dim (int) – 要索引的维度 index (LongTensor) – 从 tensor 中选择的索引 tensor (Tensor) – 用于复制的tensor 例子: >>> x = torch.zeros(5, 3) >>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) >>> index = torch.tensor([0, 4, 2]) >>> x.index_copy_(0, index, t) tensor([[ 1., 2., 3.], [ 0., 0., 0.], [ 7., 8., 9.], [ 0., 0., 0.], [ 4., 5., 6.]]) index_fill_(dim, index, val) → Tensor 根据 index 中指定的顺序索引, 用值 val填充 self tensor 中的元素. 参数: dim (int) – 指定索引对应的维度 index (LongTensor) – self tensor 中将被填充的索引值 val (float) – 用于填充的值 例子: >>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) >>> index = torch.tensor([0, 2]) >>> x.index_fill_(1, index, -1) tensor([[-1., 2., -1.], [-1., 5., -1.], [-1., 8., -1.]]) index_put_(indices, value, accumulate=False) → Tensor 根据 indices (是一个 Tensors 的tuple)中指定的索引, 取出 tensor value 中的值放入 tensor self . 表达式 tensor.index_put_(indices, value) 等价于 tensor[indices] = value. 返回 self. 如果 accumulate 等于 True, tensor 中的元素会被加到 self. 如果是 False, 且 indices 中含有重复的元素, 则行为是未定义的. 参数: indices (tuple of LongTensor) – tensors 用于索引 self. value (Tensor) – 与 self 有相同数据类型的 tensor. accumulate (bool) – 是否累加到自身 index_select(dim, index) → Tensor 见 torch.index_select() int() → Tensor self.int() is equivalent to self.to(torch.int32). See to(). inverse() → Tensor 见 torch.inverse() is_contiguous() → bool 返回 True 如果 self tensor 在内存中是连续存储的. is_pinned() 返回 true 如果 tensor 储存在pinned memory is_set_to(tensor) → bool 返回 True 如果此对象在 Torch C API 中引用的 THTensor 对象和给定 tensor 是相同的. is_signed() item() → number 返回 tensor 中的值作为一个标准的 Python number. 仅在只有一个元素的时候有效. 对于其他情况, 见 tolist(). 这个操作是不可微分的. 例子: >>> x = torch.tensor([1.0]) >>> x.item() 1.0 kthvalue(k, dim=None, keepdim=False) -> (Tensor, LongTensor) 见 torch.kthvalue() le(other) → Tensor 见 torch.le() le_(other) → Tensor 原地版本的 le() lerp(start, end, weight) → Tensor 见 torch.lerp() lerp_(start, end, weight) → Tensor 原地版本的 lerp() log() → Tensor 见 torch.log() log_() → Tensor 原地版本的 log() logdet() → Tensor 见 torch.logdet() log10() → Tensor 见 torch.log10() log10_() → Tensor 原地版本的 log10() log1p() → Tensor 见 torch.log1p() log1p_() → Tensor 原地版本的 log1p() log2() → Tensor 见 torch.log2() log2_() → Tensor 原地版本的 log2() log_normal_(mean=1, std=2, *, generator=None) 用 mean 和std 初始化的 log-normal 分布 中取出的值填充 self. 注意 mean 和 std 是下面的 normal 分布的平均值和标准差, 而不是返回的分布: logsumexp(dim, keepdim=False) → Tensor 见 torch.logsumexp() long() → Tensor self.long() is equivalent to self.to(torch.int64). See to(). lt(other) → Tensor 见 torch.lt() lt_(other) → Tensor 原地版本的 lt() map_(tensor, callable) 对 self tensor 和 给定的 tensor 中的每一个元素应用 callable 然后把结果存于 self tensor. self tensor 和给定的 tensor 必须可广播 broadcastable. callable 应该有下面的函数签名: def callable(a, b) -> number masked_scatter_(mask, source) 从 source 复制元素到 self tensor 当对应 mask 对应的值是 1. mask 的形状必须和底层 tensor 可广播 broadcastable. source 的元素数量至少和 mask里面的1一样多 Parameters: mask (ByteTensor) – 二值掩码 source (Tensor) – 源 tensor 注意 mask 操作于 self tensor, 而不是给定的 source tensor. masked_fill_(mask, value) 用value填充 self tensor 中的元素, 当对应位置的 mask 是1. mask 的形状必须和底层 tensor broadcastable. 参数: mask (ByteTensor) – 二值掩码 value (float) – 用于填充的值 masked_select(mask) → Tensor 见 torch.masked_select() matmul(tensor2) → Tensor 见 torch.matmul() matrix_power(n) → Tensor 见 torch.matrix_power() max(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor) 见 torch.max() mean(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor) 见 torch.mean() median(dim=None, keepdim=False) -> (Tensor, LongTensor) 见 torch.median() min(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor) 见 torch.min() mm(mat2) → Tensor 见 torch.mm() mode(dim=None, keepdim=False) -> (Tensor, LongTensor) 见 torch.mode() mul(value) → Tensor 见 torch.mul() mul_(value) 原地版本的 mul() multinomial(num_samples, replacement=False, *, generator=None) → Tensor 见 torch.multinomial() mv(vec) → Tensor 见 torch.mv() mvlgamma(p) → Tensor 见 torch.mvlgamma() mvlgamma_(p) → Tensor 原地版本的 mvlgamma() narrow(dimension, start, length) → Tensor 见 torch.narrow() Example: >>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) >>> x.narrow(0, 0, 2) tensor([[ 1, 2, 3], [ 4, 5, 6]]) >>> x.narrow(1, 1, 2) tensor([[ 2, 3], [ 5, 6], [ 8, 9]]) ndimension() → int Alias for dim() ne(other) → Tensor 见 torch.ne() ne_(other) → Tensor 原地版本的 ne() neg() → Tensor 见 torch.neg() neg_() → Tensor 原地版本的 neg() nelement() → int 别名 numel() nonzero() → LongTensor 见 torch.nonzero() norm(p='fro', dim=None, keepdim=False) 见 :func: torch.norm normal_(mean=0, std=1, *, generator=None) → Tensor 用采样于 normal 分布的元素填充 self tensor, normal 分布使用参数 mean and std初始化. numel() → int 见 torch.numel() numpy() → numpy.ndarray 返回 self tensor 作为一个 NumPy ndarray. 此 tensor 和返回的 ndarray 共享同一个底层存储. 改变self tensor 将会同时改变 ndarray . orgqr(input2) → Tensor 见 torch.orgqr() ormqr(input2, input3, left=True, transpose=False) → Tensor 见 torch.ormqr() permute(*dims) → Tensor 排列 tensor 的维度. 参数: *dims (int...) – 维度的排列顺序 Example >>> x = torch.randn(2, 3, 5) >>> x.size() torch.Size([2, 3, 5]) >>> x.permute(2, 0, 1).size() torch.Size([5, 2, 3]) pin_memory() pinverse() → Tensor 见 torch.pinverse() potrf(upper=True) 见 torch.cholesky() potri(upper=True) → Tensor 见 torch.potri() potrs(input2, upper=True) → Tensor 见 torch.potrs() pow(exponent) → Tensor 见 torch.pow() pow_(exponent) → Tensor 原地版本的 pow() prod(dim=None, keepdim=False, dtype=None) → Tensor 见 torch.prod() pstrf(upper=True, tol=-1) -> (Tensor, IntTensor) 见 torch.pstrf() put_(indices, tensor, accumulate=False) → Tensor 从 tensor 中复制元素到 indices 指定的位置. 对于目的索引, self tensor 被当作一个 1-D tensor. 如果 accumulate 是 True, tensor 中的元素被被加到 self. 如果 accumulate 是 False, 当 indices 中有重复索引时行为未定义. Parameters: indices (LongTensor) – self 的索引位置 tensor (Tensor) – 包含待复制元素的 tensor accumulate (bool) – 是否累加到 self 例子: >>> src = torch.tensor([[4, 3, 5], [6, 7, 8]]) >>> src.put_(torch.tensor([1, 3]), torch.tensor([9, 10])) tensor([[ 4, 9, 5], [ 10, 7, 8]]) qr() -> (Tensor, Tensor) 见 torch.qr() random_(from=0, to=None, *, generator=None) → Tensor 用离散均匀分布介于 [from, to - 1] 采样的数字填充 self tensor. 如果没有特别指定, 这些采样的数值被 self tensor’s 数据类型界定. 然而, 对于浮点型, 如果没有特别指定, 范围将是 [0, 2^mantissa] 来确保每一个值是可表示的. 例如, torch.tensor(1, dtype=torch.double).random_() 将会被设为 [0, 2^53]. reciprocal() → Tensor 见 torch.reciprocal() reciprocal_() → Tensor 原地版本的 reciprocal() remainder(divisor) → Tensor 见 torch.remainder() remainder_(divisor) → Tensor 原地版本的 remainder() renorm(p, dim, maxnorm) → Tensor 见 torch.renorm() renorm_(p, dim, maxnorm) → Tensor 原地版本的 renorm() repeat(*sizes) → Tensor 在指定的维度重复这个 tensor. 不像 expand(), 这个函数会拷贝底层数据. 警告 torch.repeat() 的行为和 numpy.repeat 不一样, 更类似于 numpy.tile. 参数: sizes (torch.Size or int...) – 每个维度重复的次数 例子: >>> x = torch.tensor([1, 2, 3]) >>> x.repeat(4, 2) tensor([[ 1, 2, 3, 1, 2, 3], [ 1, 2, 3, 1, 2, 3], [ 1, 2, 3, 1, 2, 3], [ 1, 2, 3, 1, 2, 3]]) >>> x.repeat(4, 2, 1).size() torch.Size([4, 2, 3]) requires_grad_(requires_grad=True) → Tensor 设置是否应该自动求导: 原地设置这个 tensor 的 requires_grad 属性.返回这个 tensor. require_grad_() 的主要使用情况是告诉自动求导开始记录Tensor tensor上的操作. 如果 tensor 的 requires_grad=False (因为它是通过 DataLoader 获得或者需要预处理或初始化), tensor.requires_grad_() 将会使得自动求导开始生效. 参数: requires_grad (bool) – 是否自动求导应该记录相关操作. Default: True. 例子: >>> # Let's say we want to preprocess some saved weights and use >>> # the result as new weights. >>> saved_weights = [0.1, 0.2, 0.3, 0.25] >>> loaded_weights = torch.tensor(saved_weights) >>> weights = preprocess(loaded_weights) # some function >>> weights tensor([-0.5503, 0.4926, -2.1158, -0.8303]) >>> # Now, start to record operations done to weights >>> weights.requires_grad_() >>> out = weights.pow(2).sum() >>> out.backward() >>> weights.grad tensor([-1.1007, 0.9853, -4.2316, -1.6606]) reshape(*shape) → Tensor 返回一个 tensor, 其data和元素数量与 self 一样, 但是改变成指定的形状. 这个方法返回一个tensor的试图 如果 shape 和当前的形状是兼容的. 见 torch.Tensor.view() 关于是什么时候返回一个 view. 见 torch.reshape() 参数: shape (tuple of python:ints or int...) – 期望变成的形状 reshape_as(other) → Tensor 返回一个tensor形状与 other 相同. self.reshape_as(other) 等价于 self.reshape(other.sizes()). 这个方法返回一个tensor的试图 如果 self.reshape(other.sizes()) 和当前的形状是兼容的. 见 torch.Tensor.view() 关于是什么时候返回一个 view. 请参考 reshape() 获得更多关于 reshape 的信息. 参数: other (torch.Tensor) – 返回的tensor形状与 other 一致. resize_(*sizes) → Tensor 缩放 self tensor到指定的大小. 如果指定的元素数量比当前的要大, 底层的存储结构会缩放到合适的大小. 如果数量更小, 底层存储不变. 当前的元素都会被保留, 没有任何的新的初始化. 警告 这是一个底层的操作. 存储被重新解释为C-contiguous, 忽略当前stride（除非目标大小等于当前大小, 在这种情况下tensor保持不变）.在大多数情况下, 您将要使用 view(), 它会检查连续性, 或者 reshape(), 在必要的时候会拷贝数据. 如果想要改变大小并且自定义stride, 见 set_(). 参数: sizes (torch.Size or int...) – 期望的大小 例子: >>> x = torch.tensor([[1, 2], [3, 4], [5, 6]]) >>> x.resize_(2, 2) tensor([[ 1, 2], [ 3, 4]]) resize_as_(tensor) → Tensor 缩放 self tensor 的大小与参数 tensor 相同. 等价于 self.resize_(tensor.size()). round() → Tensor 见 torch.round() round_() → Tensor 原地版本的 round() rsqrt() → Tensor 见 torch.rsqrt() rsqrt_() → Tensor 原地版本的 rsqrt() scatter_(dim, index, src) → Tensor 根据 index tensor 中指定的索引, 将所有 tensor src 中的值写入self . 对于 src 中的每一个值, 当 dimension != dim, 它的输出的索引由 src 中的索引指定, 当 dimension = dim, 由 index 中对应的值指定. 对于一个 3-D tensor, self 的更新规则如下: self[index[i][j][k]][j][k] = src[i][j][k] # if dim == 0 self[i][index[i][j][k]][k] = src[i][j][k] # if dim == 1 self[i][j][index[i][j][k]] = src[i][j][k] # if dim == 2 这是 gather() 中描述的方式的逆向操作. self, index and src (if it is a Tensor) 应该有相同数量的维度. 同时也要求 index.size(d) 对于每一个维度 d, 而且 index.size(d) 对于每一个维度 d != dim. 此外, 关于 gather(), index 的值必须介于 0 和 self.size(dim) - 1 (包括), 并且沿着指定维度dim的行中的所有值必须是唯一的. 参数: dim (int) – 要索引的轴 index (LongTensor) – 需要 scatter 的元素的索引, 可以是空的，也可以与src大小相同。当为空时，操作返回恒等 src (Tensor or float) – scatter 源 例子: >>> x = torch.rand(2, 5) >>> x tensor([[ 0.3992, 0.2908, 0.9044, 0.4850, 0.6004], [ 0.5735, 0.9006, 0.6797, 0.4152, 0.1732]]) >>> torch.zeros(3, 5).scatter_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x) tensor([[ 0.3992, 0.9006, 0.6797, 0.4850, 0.6004], [ 0.0000, 0.2908, 0.0000, 0.4152, 0.0000], [ 0.5735, 0.0000, 0.9044, 0.0000, 0.1732]]) >>> z = torch.zeros(2, 4).scatter_(1, torch.tensor([[2], [3]]), 1.23) >>> z tensor([[ 0.0000, 0.0000, 1.2300, 0.0000], [ 0.0000, 0.0000, 0.0000, 1.2300]]) scatter_add_(dim, index, other) → Tensor 根据 index tensor 中指定的索引(方式和scatter_()类似), 将所有 tensor other 中的值加到self . 对于 other 中的每一个值, 当 dimension != dim, 它的输出的索引由 other 中的索引指定, 当 dimension = dim, 由 index 中对应的值指定. 对于一个 3-D tensor, self 的更新规则如下: self[index[i][j][k]][j][k] += other[i][j][k] # if dim == 0 self[i][index[i][j][k]][k] += other[i][j][k] # if dim == 1 self[i][j][index[i][j][k]] += other[i][j][k] # if dim == 2 self, index and other 应该有相同数量的维度. 也要求 index.size(d) 对于所有的维度 d, 并且 index.size(d) 对于所有的维度 d != dim. 此外, 关于 gather(), index 的值必须介于 0 和 self.size(dim) - 1 (包括), 并且沿着指定维度dim的行中的所有值必须是唯一的. 注意 当使用 CUDA 作为后端, 这个操作将导致不确定性行为, 并且难以停止. 请参考 Reproducibility 获得相关背景. 参数: dim (int) – 要索引的轴 index (LongTensor) – 需要 scatter add 的元素的索引, 可以是空的，也可以与src大小相同。当为空时，操作返回恒等 src (Tensor or float) – scatter 源 例子: >>> x = torch.rand(2, 5) >>> x tensor([[0.7404, 0.0427, 0.6480, 0.3806, 0.8328], [0.7953, 0.2009, 0.9154, 0.6782, 0.9620]]) >>> torch.ones(3, 5).scatter_add_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x) tensor([[1.7404, 1.2009, 1.9154, 1.3806, 1.8328], [1.0000, 1.0427, 1.0000, 1.6782, 1.0000], [1.7953, 1.0000, 1.6480, 1.0000, 1.9620]]) select(dim, index) → Tensor 沿着选择的维度在给定的索引处切取 self tensor.这个函数返回的 tensor 指定的维度被移除了. 参数: dim (int) – 要切片的维度 index (int) – 选择的索引 注意 select() 等价于切片. 例如, tensor.select(0, index) 等价于 tensor[index] and tensor.select(2, index) 等价于 tensor[:,:,index]. set_(source=None, storage_offset=0, size=None, stride=None) → Tensor 设置底层存储, 大小, 和 strides. 如果 source 是一个 tensor, self tensor 将会和 source 共享底层存储, 并有用一样的大小和 strides. 在一个 tensor 中改变元素将会反应到另一个tensor. 如果 source 是一个 Storage, 此方法设置底层存储, offset, 大小, 和 stride. 参数: source (Tensor or Storage) – 要设置的 tensor 或者 storage storage_offset (int, optional) – storage 的 offset size (torch.Size__, optional) – 期望的大小.默认是 source 的大小. stride (tuple, optional) – 期望的 stride.默认值是 C-contiguous strides. share_memory_() 移动底层存储到共享内存. 这是一个空操作如果底层存储已经在共享内存中或者是 CUDA tensors. 共享内存中的 tensor 不能 resize. short() → Tensor self.short() 等价于 self.to(torch.int16). 见 to(). sigmoid() → Tensor 见 torch.sigmoid() sigmoid_() → Tensor 原地版本的 sigmoid() sign() → Tensor 见 torch.sign() sign_() → Tensor 原地版本的 sign() sin() → Tensor 见 torch.sin() sin_() → Tensor 原地版本的 sin() sinh() → Tensor 见 torch.sinh() sinh_() → Tensor 原地版本的 sinh() size() → torch.Size 返回 self tensor 的尺寸. 返回值是 [tuple] 的子类(https://docs.python.org/3/library/stdtypes.html#tuple \"(in Python v3.7)\"). 例如: >>> torch.empty(3, 4, 5).size() torch.Size([3, 4, 5]) slogdet() -> (Tensor, Tensor) 见 torch.slogdet() sort(dim=None, descending=False) -> (Tensor, LongTensor) 见 torch.sort() split(split_size, dim=0) 见 torch.split() sparse_mask(input, mask) → Tensor 用 mask 的索引过滤 Tensor input, 返回一个新的 SparseTensor. input 和 mask 必须有相同的形状. 参数: input (Tensor) – 输入 Tensor mask (SparseTensor) – SparseTensor 用其索引过滤 input 例子: >>> nnz = 5 >>> dims = [5, 5, 2, 2] >>> I = torch.cat([torch.randint(0, dims[0], size=(nnz,)), torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz) >>> V = torch.randn(nnz, dims[2], dims[3]) >>> size = torch.Size(dims) >>> S = torch.sparse_coo_tensor(I, V, size).coalesce() >>> D = torch.randn(dims) >>> D.sparse_mask(S) tensor(indices=tensor([[0, 0, 0, 2], [0, 1, 4, 3]]), values=tensor([[[ 1.6550, 0.2397], [-0.1611, -0.0779]], [[ 0.2326, -1.0558], [ 1.4711, 1.9678]], [[-0.5138, -0.0411], [ 1.9417, 0.5158]], [[ 0.0793, 0.0036], [-0.2569, -0.1055]]]), size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo) sqrt() → Tensor 见 torch.sqrt() sqrt_() → Tensor 原地版本的 sqrt() squeeze(dim=None) → Tensor 见 torch.squeeze() squeeze_(dim=None) → Tensor 原地版本的 squeeze() std(dim=None, unbiased=True, keepdim=False) → Tensor 见 torch.std() storage() → torch.Storage 返回底层的 storage storage_offset() → int 根据存储元素的数量(而不是字节)，返回底层存储中的tesor偏移量(offset)。 例子: >>> x = torch.tensor([1, 2, 3, 4, 5]) >>> x.storage_offset() 0 >>> x[3:].storage_offset() 3 storage_type() stride(dim) → tuple or int 返回 self tensor 的 stride. stride 是必要的用于在指定的维度 dim 找到下一个元素. 如果传入空, 则返回一个 tuple 包含所有维度的 stride. 否则, 将会返回一个 int 表示指定维度 dim 的 stride. 参数: dim (int, optional) – 需要返回 stride 的维度 例子: >>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) >>> x.stride() (5, 1) >>>x.stride(0) 5 >>> x.stride(-1) 1 sub(value, other) → Tensor self tensor 减去一个 scalar 或者 tensor. 如果 value 和 other 都被指定, 在相减之前, other 的每个元素将会用 value 缩放. 当 other 是一个 tensor, other 的形状必须和底层存储是可广播的 broadcastable . sub_(x) → Tensor 原地版本的 sub() sum(dim=None, keepdim=False, dtype=None) → Tensor 见 torch.sum() svd(some=True, compute_uv=True) -> (Tensor, Tensor, Tensor) 见 torch.svd() symeig(eigenvectors=False, upper=True) -> (Tensor, Tensor) 见 torch.symeig() t() → Tensor 见 torch.t() t_() → Tensor 原地版本的 t() to(*args, **kwargs) → Tensor 执行 tensor 类型或者设备转换. torch.dtype 和 torch.device 是从参数中推断的 self.to(*args, **kwargs). 注意 如果 self Tensor 已经有正确的 torch.dtype 和 torch.device, 则 self 被返回. 否则, 将返回复制的 self 期望的 torch.dtype 和 torch.device. 下面是调用的方法 to: to(dtype, non_blocking=False, copy=False) → Tensor 返回一个 Tensor 指定类型 dtype to(device=None, dtype=None, non_blocking=False, copy=False) → Tensor 返回一个 Tensor 并指定 device 和 (可选的) dtype. 如果 dtype 是 None 则推断为 self.dtype . 当启用 non_blocking, 试图在主机上执行异步转换, 例如, 转换一个 pinned memory 的 CPU Tensor 到 CUDA Tensor. 当 copy 被设置, 一个新的 tensor 被创建. to(other, non_blocking=False, copy=False) → Tensor 返回一个 Tensor 并有和 Tensor other 相同的 torch.dtype 和 torch.device. 当启用 non_blocking, 试图在主机上执行异步转换, 例如, 转换一个 pinned memory 的 CPU Tensor 到 CUDA Tensor. 当 copy 被设置, 一个新的 tensor 被创建. 例子: >>> tensor = torch.randn(2, 2) # Initially dtype=float32, device=cpu >>> tensor.to(torch.float64) tensor([[-0.5044, 0.0005], [ 0.3310, -0.0584]], dtype=torch.float64) >>> cuda0 = torch.device('cuda:0') >>> tensor.to(cuda0) tensor([[-0.5044, 0.0005], [ 0.3310, -0.0584]], device='cuda:0') >>> tensor.to(cuda0, dtype=torch.float64) tensor([[-0.5044, 0.0005], [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0') >>> other = torch.randn((), dtype=torch.float64, device=cuda0) >>> tensor.to(other, non_blocking=True) tensor([[-0.5044, 0.0005], [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0') take(indices) → Tensor 见 torch.take() tan() tan_() → Tensor 原地版本的 tan() tanh() → Tensor 见 torch.tanh() tanh_() → Tensor 原地版本的 tanh() tolist() ” tolist() -> list or number 返回tensor 作为(嵌套的) list. 对于 scalars,一个标准的 Python number 被返回, 就像 item() 一样. Tensors 会自动移动到 CPU 上如果有必要. 这个操作是不可微分的. 例子: >>> a = torch.randn(2, 2) >>> a.tolist() [[0.012766935862600803, 0.5415473580360413], [-0.08909505605697632, 0.7729271650314331]] >>> a[0,0].tolist() 0.012766935862600803 topk(k, dim=None, largest=True, sorted=True) -> (Tensor, LongTensor) 见 torch.topk() to_sparse(sparseDims) → Tensor 返回一个稀疏复制的 tensor. PyTorch 支持 coordinate 格式 的稀疏 tensors. :param sparseDims: 要包含在新稀疏tensor中的稀疏维数 :type sparseDims: int, 可选的 例子:: >>> d = torch.tensor([[0, 0, 0], [9, 0, 10], [0, 0, 0]]) >>> d tensor([[ 0, 0, 0], [ 9, 0, 10], [ 0, 0, 0]]) >>> d.to_sparse() tensor(indices=tensor([[1, 1], [0, 2]]), values=tensor([ 9, 10]), size=(3, 3), nnz=2, layout=torch.sparse_coo) >>> d.to_sparse(1) tensor(indices=tensor([[1]]), values=tensor([[ 9, 0, 10]]), size=(3, 3), nnz=1, layout=torch.sparse_coo) trace() → Tensor 见 torch.trace() transpose(dim0, dim1) → Tensor 见 torch.transpose() transpose_(dim0, dim1) → Tensor 原地版本的 transpose() tril(k=0) → Tensor 见 torch.tril() tril_(k=0) → Tensor 原地版本的 tril() triu(k=0) → Tensor 见 torch.triu() triu_(k=0) → Tensor 原地版本的 triu() trtrs(A, upper=True, transpose=False, unitriangular=False) -> (Tensor, Tensor) 见 torch.trtrs() trunc() → Tensor 见 torch.trunc() trunc_() → Tensor 原地版本的 trunc() type(dtype=None, non_blocking=False, **kwargs) → str or Tensor 返回 type 如果 dtype 没有被设置, 否则将会强制转换成 dtype 类型. 如果这已经是正确的类型，则不执行复制，并返回原始对象. 参数: dtype (type or string) – 期望类型 non_blocking (bool) – 如果 True，并且源在pinned memory中，目的地在GPU上，则拷贝相对于主机异步执行。否则，这个参数没有任何作用。 **kwargs – 为了兼容性, 可能包含 async 用来置换 non_blocking 参数. async 参数被废弃了. type_as(tensor) → Tensor 返回 tensor 强制转换为 tensor 的数据类型. 如果这已经是正确的类型，则是空操作. 等价于: self.type(tensor.type()) Params: tensor (Tensor): 拥有目标数据类型的 tensor unfold(dim, size, step) → Tensor 返回一个 tensor 包含 self tensor 在维度 dim 上的所有切片, 每一个的大小为 size. step 指定每一个切片的间距. 如果 sizedim 是 self dim 维度的大小, 返回的 tensor 的维度 dim 大小是 (sizedim - size) / step + 1. 一个附加的size size的维度追加于返回的 tensor. 参数: dim (int) – 指定 unfold 的维度 size (int) – 指定每个slice的大小 step (int) – 指定步长 例子: >>> x = torch.arange(1., 8) >>> x tensor([ 1., 2., 3., 4., 5., 6., 7.]) >>> x.unfold(0, 2, 1) tensor([[ 1., 2.], [ 2., 3.], [ 3., 4.], [ 4., 5.], [ 5., 6.], [ 6., 7.]]) >>> x.unfold(0, 2, 2) tensor([[ 1., 2.], [ 3., 4.], [ 5., 6.]]) uniform_(from=0, to=1) → Tensor 用连续均匀分布的采样值填充 self tensor: unique(sorted=False, return_inverse=False, dim=None) 返回 tensor 中唯一的标量作为 1-D tensor. 见 torch.unique() unsqueeze(dim) → Tensor 见 torch.unsqueeze() unsqueeze_(dim) → Tensor 原地版本的 unsqueeze() var(dim=None, unbiased=True, keepdim=False) → Tensor 见 torch.var() view(*shape) → Tensor 返回一个新的 tersor, 和 self 有相同的数据, 但是有不同的 shape. 返回的 tensor 共享相同的数据，并且具有相同数量的元素，但是可能有不同的大小。要 view() 一个tensor，新视图大小必须与其原始大小和 stride 兼容, 例如, 每个新视图维度必须是原始维度的子空间，或者仅跨越原始维度 满足以下连续性条件 , 否则在 view() 之前, contiguous() 需要被调用. 可参考: reshape(), 返回一个view 当形状是兼容的, 否则复制 (等价于调用 contiguous()). 参数: shape (torch.Size or int...) – the desired size 例子: >>> x = torch.randn(4, 4) >>> x.size() torch.Size([4, 4]) >>> y = x.view(16) >>> y.size() torch.Size([16]) >>> z = x.view(-1, 8) # the size -1 is inferred from other dimensions >>> z.size() torch.Size([2, 8]) view_as(other) → Tensor 使用 other 的大小 View tensor . self.view_as(other) 等价于 self.view(other.size()). 请参考 view() 获得更多信息关于 view. 参数: other (torch.Tensor) – 返回的tensor 和 other 大小相同. zero_() → Tensor 用 0 填充 self tensor. class torch.ByteTensor 下面的方法是 torch.ByteTensor 独占. all() all() → bool 返回 True 如果所有的元素非零, 否则 False. 例子: >>> a = torch.randn(1, 3).byte() % 2 >>> a tensor([[1, 0, 0]], dtype=torch.uint8) >>> a.all() tensor(0, dtype=torch.uint8) all(dim, keepdim=False, out=None) → Tensor 返回 True 如果 tensor 在指定维度dim每一行的所有的元素非零, 否则 False. 如果 keepdim 是 True, 则输出 tensor 的大小与 input相同, 但尺寸为1的维度dim除外. 否则, dim 会被压缩 (见 torch.squeeze()), 导致输出张量比input少1维. Parameters: dim (int) – 要reduce的维度 keepdim (bool) – output tensor 是否保留 dim out (Tensor, 可选的) – output tensor 例子: >>> a = torch.randn(4, 2).byte() % 2 >>> a tensor([[0, 0], [0, 0], [0, 1], [1, 1]], dtype=torch.uint8) >>> a.all(dim=1) tensor([0, 0, 0, 1], dtype=torch.uint8) any() any() → bool 返回 True 如果任意元素非零, 否则 False. 例子: >>> a = torch.randn(1, 3).byte() % 2 >>> a tensor([[0, 0, 1]], dtype=torch.uint8) >>> a.any() tensor(1, dtype=torch.uint8) any(dim, keepdim=False, out=None) → Tensor 返回 True 如果 tensor 在指定维度dim每一行的任意的元素非零, 否则 False. 如果 keepdim 是 True, 则输出 tensor 的大小与 input相同, 但尺寸为1的维度dim除外. 否则, dim 会被压缩 (见 torch.squeeze()), 导致输出张量比input少1维. 参数: dim (int) – 要减少的维度 keepdim (bool) – output tensor 是否保留 dim out (Tensor, 可选的) – output tensor Example: >>> a = torch.randn(4, 2).byte() % 2 >>> a tensor([[1, 0], [0, 0], [0, 1], [0, 0]], dtype=torch.uint8) >>> a.any(dim=1) tensor([1, 0, 1, 0], dtype=torch.uint8) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"tensor_attributes.html":{"url":"tensor_attributes.html","title":"Tensor Attributes","keywords":"","body":"Tensor（张量）的属性 译者：阿远 每个 torch.Tensor 对象都有以下几个属性： torch.dtype, torch.device， 和 torch.layout。 torch.dtype class torch.dtype torch.dtype 属性标识了 torch.Tensor的数据类型。PyTorch 有八种不同的数据类型： Data type dtype Tensor types 32-bit floating point torch.float32 or torch.float torch.*.FloatTensor 64-bit floating point torch.float64 or torch.double torch.*.DoubleTensor 16-bit floating point torch.float16 or torch.half torch.*.HalfTensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.*.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.*.LongTensor torch.device class torch.device torch.device 属性标识了torch.Tensor对象在创建之后所存储在的设备名称，而在对象创建之前此属性标识了即将为此对象申请存储空间的设备名称。 torch.device 包含了两种设备类型 ('cpu' 或者 'cuda') ，分别标识将Tensor对象储存于cpu内存或者gpu内存中，同时支持指定设备编号，比如多张gpu，可以通过gpu编号指定某一块gpu。 如果没有指定设备编号，则默认将对象存储于current_device()当前设备中； 举个例子， 一个torch.Tensor 对象构造函数中的设备字段如果填写'cuda'，那等价于填写了'cuda:X'，其中X是函数 torch.cuda.current_device()的返回值。 在torch.Tensor对象创建之后，可以通过访问Tensor.device属性实时访问当前对象所存储在的设备名称。 torch.device 对象支持使用字符串或者字符串加设备编号这两种方式来创建： 通过字符串创建： >>> torch.device('cuda:0') device(type='cuda', index=0) # 编号为0的cuda设备 >>> torch.device('cpu') # cpu内存 device(type='cpu') >>> torch.device('cuda') # 当前cuda设备 device(type='cuda') 通过字符串加设备编号创建： >>> torch.device('cuda', 0) device(type='cuda', index=0) >>> torch.device('cpu', 0) device(type='cpu', index=0) Note 当torch.device作为函数的参数的时候， 可以直接用字符串替换。 这样有助于加快代码创建原型的速度。 >>> # 一个接受torch.device对象为参数的函数例子 >>> cuda1 = torch.device('cuda:1') >>> torch.randn((2,3), device=cuda1) >>> # 可以用一个字符串替换掉torch.device对象，一样的效果 >>> torch.randn((2,3), 'cuda:1') Note 由于一些历史遗留问题, device对象还可以仅通过一个设备编号来创建，这些设备编号对应的都是相应的cuda设备。 这正好对应了 Tensor.get_device()函数, 这个仅支持cuda Tensor的函数返回的就是当前tensor所在的cuda设备编号，cpu Tensor不支持这个函数。 >>> torch.device(1) device(type='cuda', index=1) Note 接受device参数的函数同时也可以接受一个正确格式的字符串或者正确代表设备编号的数字（数字这个是历史遗留问题）作为参数，以下的操作是等价的： >>> torch.randn((2,3), device=torch.device('cuda:1')) >>> torch.randn((2,3), device='cuda:1') >>> torch.randn((2,3), device=1) # 历史遗留做法 torch.layout class torch.layout torch.layout 属性标识了torch.Tensor 在内存中的布局模式。 现在， 我们支持了两种内存布局模式 torch.strided (dense Tensors) 和尚处试验阶段的torch.sparse_coo (sparse COO Tensors， 一种经典的稀疏矩阵存储方式). torch.strided 跨步存储代表了密集张量的存储布局方式，当然也是最常用最经典的一种布局方式。 每一个strided tensor都有一个与之相连的torch.Storage对象, 这个对象存储着tensor的数据. 这些Storage对象为tensor提供了一种多维的， 跨步的(strided)数据视图. 这一视图中的strides是一个interger整形列表：这个列表的主要作用是给出当前张量的各个维度的所占内存大小，严格的定义就是，strides中的第k个元素代表了在第k维度下，从一个元素跳转到下一个元素所需要跨越的内存大小。 跨步这个概念有助于提高多种张量运算的效率。 例子: >>> x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) >>> x.stride() (5, 1) # 此时在这个二维张量中，在第0维度下，从一个元素到下一个元素需要跨越的内存大小是5，比如x[0] 到x[1]需要跨越x[0]这5个元素, 在第1维度下，是1，如x[0, 0]到x[0, 1]需要跨越1个元素 >>> x.t().stride() (1, 5) 更多关于 torch.sparse_coo tensors的信息, 请看torch.sparse. 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"type_info.html":{"url":"type_info.html","title":"Type Info","keywords":"","body":"数据类型信息 译者：冯宝宝 可以通过torch.finfo 或 torch.iinfo访问torch.dtype的数字属性。 torch.finfo class torch.finfo torch.finfo 是一个用来表示浮点torch.dtype的数字属性的对象（即torch.float32，torch.float64和torch.float16）。 这类似于 numpy.finfo。 torch.finfo 提供以下属性: 名称 类型 描述 bits 整型　int 数据类型占用的位数 eps 浮点型float 可表示的最小数字，使得1.0 + eps！= 1.0 max 浮点型float 可表示的最大数字 tiny 浮点型float 可表示的最小正数 注意 在使用pytorch默认dtype创建类（由torch.get_default_dtype（）返回）的情况下，构造的 torch.finfo 函数可以不带参数被调用。 torch.iinfo class torch.iinfo torch.iinfo是一个用来表示整数torch.dtype 的数字属性的对象，（即torch.uint8，torch.int8，torch.int16，torch.int32和torch.int64）。 这与numpy.iinfo类似。 torch.iinfo 提供以下属性： 名称 类型 描述 bits 整型 数据类型占用的位数 max 整型 可表示的最大数字 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"sparse.html":{"url":"sparse.html","title":"torch.sparse","keywords":"","body":"torch.sparse 译者：hijkzzz 警告 这个API目前还处于试验阶段, 可能在不久的将来会发生变化. Torch支持COO(rdinate )格式的稀疏张量, 这可以有效地存储和处理大多数元素为零的张量. 稀疏张量表示为一对稠密张量:一个值张量和一个二维指标张量. 一个稀疏张量可以通过提供这两个张量, 以及稀疏张量的大小来构造(从这些张量是无法推导出来的!)假设我们要定义一个稀疏张量, 它的分量3在(0,2)处, 分量4在(1,0)处, 分量5在(1,2)处, 然后我们可以这样写 >>> i = torch.LongTensor([[0, 1, 1], [2, 0, 2]]) >>> v = torch.FloatTensor([3, 4, 5]) >>> torch.sparse.FloatTensor(i, v, torch.Size([2,3])).to_dense() 0 0 3 4 0 5 [torch.FloatTensor of size 2x3] 注意, LongTensor的输入不是索引元组的列表. 如果你想这样写你的指标, 你应该在把它们传递给稀疏构造函数之前进行转置: >>> i = torch.LongTensor([[0, 2], [1, 0], [1, 2]]) >>> v = torch.FloatTensor([3, 4, 5 ]) >>> torch.sparse.FloatTensor(i.t(), v, torch.Size([2,3])).to_dense() 0 0 3 4 0 5 [torch.FloatTensor of size 2x3] 也可以构造混合稀疏张量, 其中只有前n个维度是稀疏的, 其余维度是密集的. >>> i = torch.LongTensor([[2, 4]]) >>> v = torch.FloatTensor([[1, 3], [5, 7]]) >>> torch.sparse.FloatTensor(i, v).to_dense() 0 0 0 0 1 3 0 0 5 7 [torch.FloatTensor of size 5x2] 可以通过指定其大小来构造空的稀疏张量： >>> torch.sparse.FloatTensor(2, 3) SparseFloatTensor of size 2x3 with indices: [torch.LongTensor with no dimension] and values: [torch.FloatTensor with no dimension] SparseTensor 具有以下不变量: sparse_dim + dense_dim = len(SparseTensor.shape) SparseTensor._indices().shape = (sparse_dim, nnz) SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:]) 因为SparseTensor._indices()总是一个二维张量, 最小的sparse_dim = 1. 因此, sparse_dim = 0的稀疏张量的表示就是一个稠密张量. 注意 我们的稀疏张量格式允许uncoalesced(未合并) 的稀疏张量, 其中索引中可能有重复的坐标;在这种情况下, 解释是索引处的值是所有重复值项的和. uncoalesced 张量允许我们更有效地实现某些运算符. 在大多数情况下, 你不需要关心一个稀疏张量是否coalesced(合并), 因为大多数操作在给出一个coalesced或uncoalesced稀疏张量的情况下都是一样的. 然而, 有两种情况您可能需要注意. 第一, 如果您重复执行可以产生重复项的操作 (例如, torch.sparse.FloatTensor.add()), 你应该偶尔将稀疏张量coalesced一起, 以防止它们变得太大. 第二, 一些运算符将根据它们是否coalesced产生不同的值 (例如, torch.sparse.FloatTensor._values() and torch.sparse.FloatTensor._indices(), 以及 torch.Tensor.sparse_mask()). 这些操作符以下划线作为前缀, 表示它们揭示了内部实现细节, 应该小心使用, 因为使用合并稀疏张量的代码可能无法使用未合并稀疏张量;一般来说, 在使用这些操作符之前显式地合并是最安全的. 例如, 假设我们想通过直接操作torch.sparse.FloatTensor._values().来实现一个操作符.标量乘法可以用很明显的方法实现, 因为乘法分布于加法之上;但是, 平方根不能直接实现, 因为sqrt(a + b) != sqrt(a) + sqrt(b)(如果给定一个uncoalesced的张量, 就会计算出这个结果). class torch.sparse.FloatTensor add() add_() clone() dim() div() div_() get_device() hspmm() mm() mul() mul_() narrow_copy() resizeAs_() size() spadd() spmm() sspaddmm() sspmm() sub() sub_() t_() toDense() transpose() transpose_() zero_() coalesce() is_coalesced() _indices() _values() _nnz() 函数 torch.sparse.addmm(mat, mat1, mat2, beta=1, alpha=1) 这个函数和 torch.addmm() 在forward中做同样的事情, 除了它支持稀疏矩阵mat1 的 backward. mat1应具有 sparse_dim = 2. 请注意, mat1的梯度是一个合并的稀疏张量. 参数: mat (Tensor) – 被相加的稠密矩阵 mat1 (SparseTensor) – 被相乘的稀疏矩阵 mat2 (Tensor) – 被相乘的稠密矩阵 beta (Number__, optional) – 乘数 mat () alpha (Number__, optional) – 乘数 () torch.sparse.mm(mat1, mat2) 执行稀疏矩阵mat1 和 稠密矩阵 mat2的矩阵乘法. 类似于 torch.mm(), 如果 mat1 是一个 tensor, mat2 是一个 tensor, 输出将会是 稠密的 tensor. mat1 应具有 sparse_dim = 2. 此函数也支持两个矩阵的向后. 请注意, mat1的梯度是一个合并的稀疏张量 参数: mat1 (SparseTensor) – 第一个要相乘的稀疏矩阵 mat2 (Tensor) – 第二个要相乘的稠密矩阵 例子: >>> a = torch.randn(2, 3).to_sparse().requires_grad_(True) >>> a tensor(indices=tensor([[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]]), values=tensor([ 1.5901, 0.0183, -0.6146, 1.8061, -0.0112, 0.6302]), size=(2, 3), nnz=6, layout=torch.sparse_coo, requires_grad=True) >>> b = torch.randn(3, 2, requires_grad=True) >>> b tensor([[-0.6479, 0.7874], [-1.2056, 0.5641], [-1.1716, -0.9923]], requires_grad=True) >>> y = torch.sparse.mm(a, b) >>> y tensor([[-0.3323, 1.8723], [-1.8951, 0.7904]], grad_fn=) >>> y.sum().backward() >>> a.grad tensor(indices=tensor([[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]]), values=tensor([ 0.1394, -0.6415, -2.1639, 0.1394, -0.6415, -2.1639]), size=(2, 3), nnz=6, layout=torch.sparse_coo) torch.sparse.sum(input, dim=None, dtype=None) 返回给定维度dim中每行SparseTensor input的总和. 如果 :attr::dim 是一个维度的list, reduce将在全部给定维度进行.如果包括全部的 sparse_dim, 此方法将返回 Tensor 代替 SparseTensor. 所有被求和的 dim 将被 squeezed (see torch.squeeze()),导致速出 tensor 的 :attr::dim 小于 input. backward 过程中, 仅仅 input 的 nnz 位置被反向传播. 请注意, input的梯度是合并的. 参数: input (Tensor) – t输入 SparseTensor dim (int or tuple of python:ints) – 维度或者维度列表. Default: 所有维度. dtype (torch.dtype, optional) – 返回 Tensor 的数据类型. 默认值: dtype 和 input 一致. 例子: >>> nnz = 3 >>> dims = [5, 5, 2, 3] >>> I = torch.cat([torch.randint(0, dims[0], size=(nnz,)), torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz) >>> V = torch.randn(nnz, dims[2], dims[3]) >>> size = torch.Size(dims) >>> S = torch.sparse_coo_tensor(I, V, size) >>> S tensor(indices=tensor([[2, 0, 3], [2, 4, 1]]), values=tensor([[[-0.6438, -1.6467, 1.4004], [ 0.3411, 0.0918, -0.2312]], [[ 0.5348, 0.0634, -2.0494], [-0.7125, -1.0646, 2.1844]], [[ 0.1276, 0.1874, -0.6334], [-1.9682, -0.5340, 0.7483]]]), size=(5, 5, 2, 3), nnz=3, layout=torch.sparse_coo) # when sum over only part of sparse_dims, return a SparseTensor >>> torch.sparse.sum(S, [1, 3]) tensor(indices=tensor([[0, 2, 3]]), values=tensor([[-1.4512, 0.4073], [-0.8901, 0.2017], [-0.3183, -1.7539]]), size=(5, 2), nnz=3, layout=torch.sparse_coo) # when sum over all sparse dim, return a dense Tensor # with summed dims squeezed >>> torch.sparse.sum(S, [0, 1, 3]) tensor([-2.6596, -1.1450]) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"cuda.html":{"url":"cuda.html","title":"torch.cuda","keywords":"","body":"torch.cuda 译者：bdqfork 这个包添加了对CUDA张量类型的支持，它实现了与CPU张量同样的功能，但是它使用GPU进计算。 它是懒加载的，所以你可以随时导入它，并使用 is_available() 来决定是否让你的系统支持CUDA。 CUDA semantics 有关于使用CUDA更详细的信息。 torch.cuda.current_blas_handle() 返回一个cublasHandle_t指针给当前的cuBLAS处理。 torch.cuda.current_device() 返回当前选择地设备索引。 torch.cuda.current_stream() 返回当前选择地 Stream。 class torch.cuda.device(device) Context-manager 用来改变选择的设备。 参数: device (torch.device 或者 int) – 要选择的设备索引。如果这个参数是负数或者是 None，那么它不会起任何作用。 torch.cuda.device_count() 返回可用的GPU数量。 torch.cuda.device_ctx_manager torch.cuda.device 的别名。 class torch.cuda.device_of(obj) Context-manager 将当前的设备改变成传入的对象。. 你可以使用张量或者存储作为参数。如果传入的对象没有分配在GPU上，这个操作是无效的。 参数: obj (Tensor 或者 Storage) – 分配在已选择的设备上的对象。 torch.cuda.empty_cache() 释放缓存分配器当前持有的所有未占用的缓存显存，使其可以用在其他GPU应用且可以在 nvidia-smi可视化。 注意 empty_cache() 并不会增加PyTorch可以使用的GPU显存的大小。 查看 显存管理 来获取更多的GPU显存管理的信息。 torch.cuda.get_device_capability(device) 获取一个设备的cuda容量。 参数: device (torch.device 或者 int, 可选的) – 需要返回容量的设备。如果这个参数传入的是负数，那么这个方法不会起任何作用。如果device是None（默认值），会通过 current_device()传入当前设备。 返回: 设备的最大和最小的cuda容量。 --- --- 返回 类型: tuple(int, int) --- --- torch.cuda.get_device_name(device) 获取设备名称。 参数: device (torch.device 或者 int, 可选的) – 需要返回名称的设备。如果参数是负数，那么将不起作用。如果device是None（默认值），会通过 current_device()传入当前设备。 torch.cuda.init() 初始化PyTorch的CUDA状态。如果你通过C API与PyTorch进行交互，你可能需要显式调用这个方法。只有CUDA的初始化完成，CUDA的功能才会绑定到Python。用户一般不应该需要这个，因为所有PyTorch的CUDA方法都会自动在需要的时候初始化CUDA。 如果CUDA的状态已经初始化了，将不起任何作用。 torch.cuda.is_available() 返回一个bool值，表示当前CUDA是否可用。 torch.cuda.max_memory_allocated(device=None) 返回给定设备的张量的最大GPU显存使用量（以字节为单位）。 参数: device (torch.device or int, optional) – 选择的设备。如果 device 是None（默认的），将返回 current_device()返回的当前设备的数据。 注意 查看 显存管理 部分了解更多关于GPU显存管理部分的详细信息。 torch.cuda.max_memory_cached(device=None) 返回给定设备的缓存分配器管理的最大GPU显存（以字节为单位）。 参数: device (torch.device 或者 int, 可选的) – 选择的设备。如果 device 是None（默认的），将返回 current_device()返回的当前设备的数据。 注意 查看 显存管理 部分了解更多关于GPU显存管理部分的详细信息。 torch.cuda.memory_allocated(device=None) 返回给定设备的当前GPU显存使用量（以字节为单位）。 参数: device (torch.device 或者 int, 可选的) – 选择的设备。如果 device 是None（默认的），将返回 current_device()返回的当前设备的数据。 注意 这可能比 nvidia-smi 显示的数量少，因为一些没有使用的显存会被缓存分配器持有，且一些上下文需要在GPU中创建。查看 显存管理 部分了解更多关于GPU显存管理部分的详细信息。 torch.cuda.memory_cached(device=None) 返回由缓存分配器管理的当前GPU显存（以字节为单位）。 参数: device (torch.device 或者 int, 可选的) – 选择的设备。如果 device 是None（默认的），将返回 current_device()返回的当前设备的数据。 注意 查看 显存管理 部分了解更多关于GPU显存管理部分的详细信息。 torch.cuda.set_device(device) 设置当前设备。 不鼓励使用此功能以支持 device.。在多数情况下，最好使用CUDA_VISIBLE_DEVICES环境变量。 参数: device (torch.device 或者 int) – 选择的设备。如果参数是负数，将不会起任何作用。 torch.cuda.stream(stream) 给定流的上下文管理器。 所有CUDA在上下文中排队的内核将会被添加到选择的流中。 参数: stream (Stream) – 选择的流。如果为None，这个管理器将不起任何作用。 注意 流是针对每个设备的，这个方法只更改当前选择设备的“当前流”。选择一个不同的设备流是不允许的。 torch.cuda.synchronize() 等待所有当前设备的所有流完成。 随机数生成器 torch.cuda.get_rng_state(device=-1) 以ByteTensor的形式返回当前GPU的随机数生成器的状态。 参数: device (int, 可选的) – 需要返回RNG状态的目标设备。默认：-1 (例如，使用当前设备)。 警告 此函数会立即初始化CUDA。 torch.cuda.set_rng_state(new_state, device=-1) 设置当前GPU的随机数生成器状态。 参数: new_state (torch.ByteTensor) – 目标状态 torch.cuda.manual_seed(seed) 设置为当前GPU生成随机数的种子。如果CUDA不可用，可以安全地调用此函数；在这种情况下，它将被静默地忽略。 参数: seed (int) – 目标种子。 警告 如果您使用的是多GPU模型，那么这个函数不具有确定性。设置用于在所有GPU上生成随机数的种子，使用 manual_seed_all(). torch.cuda.manual_seed_all(seed) 设置用于在所有GPU上生成随机数的种子。 如果CUDA不可用，可以安全地调用此函数；在这种情况下，它将被静默地忽略。 参数: seed (int) – 目标种子。 torch.cuda.seed() 将用于生成随机数的种子设置为当前GPU的随机数。 如果CUDA不可用，可以安全地调用此函数；在这种情况下，它将被静默地忽略。 警告 如果您使用的是多GPU模型，此函数将只初始化一个GPU上的种子。在所有GPU上将用于生成随机数的种子设置为随机数， 使用 seed_all(). torch.cuda.seed_all() 在所有GPU上将用于生成随机数的种子设置为随机数。 如果CUDA不可用，可以安全地调用此函数；在这种情况下，它将被静默地忽略。 torch.cuda.initial_seed() 返回当前GPU的当前随机种子。 警告 此函数会立即初始化CUDA。 通信集合 torch.cuda.comm.broadcast(tensor, devices) 将张量广播到多个GPU。 | 参数: | tensor (Tensor) – 需要广播的张量。 devices (Iterable) – 一个要被广播的可迭代的张量集合。注意，它应该是这样的形式 (src, dst1, dst2, …)，其中第一个元素是广播的源设备。 返回: 一个包含tensor副本的元组，放置在与设备索引相对应的设备上。 torch.cuda.comm.broadcast_coalesced(tensors, devices, buffer_size=10485760) 将序列张量广播到指定的GPU。 首先将小型张量合并到缓冲区中以减少同步次数。 | 参数: | tensors (sequence) – 要被广播的张量。 devices (Iterable) – 一个要被广播的可迭代的张量集合。注意，它应该是这样的形式 (src, dst1, dst2, …)，其中第一个元素是广播的源设备。 buffer_size (int) – 用于合并的缓冲区的最大大小 返回: 一个包含tensor副本的元组，放置在与设备索引相对应的设备上。 torch.cuda.comm.reduce_add(inputs, destination=None) 从多个GPU上对张量进行求和。 所有输入必须有相同的形状。 | 参数: | inputs (Iterable__[Tensor]) – 一个可迭代的要添加的张量集合。 destination (int, 可选的) – 输出所在的设备。(默认值: 当前设备)。 返回: 一个包含按元素相加的所有输入的和的张量，在 destination 设备上。 torch.cuda.comm.scatter(tensor, devices, chunk_sizes=None, dim=0, streams=None) 将张量分散在多个GPU上。 | 参数: | tensor (Tensor) – 要分散的张量. devices (Iterable__[int]) – 可迭代的数字集合，指明在哪个设备上的张量要被分散。 chunk_sizes (Iterable__[int]__, 可选的) – 每个设备上放置的块的大小。它应该和devices的长度相等，并相加等于tensor.size(dim)。如果没有指定，张量将会被分散成相同的块。 dim (int, 可选的) – 分块张量所在的维度。 返回: 一个包含tensor块的元组，分散在给定的devices上。 torch.cuda.comm.gather(tensors, dim=0, destination=None) 从多个GPU收集张量。 在所有维度中与dim不同的张量尺寸必须匹配。 | 参数: | tensors (Iterable__[Tensor]) – 可迭代的张量集合。 dim (int) – 纬度，张量将会在这个维度上被连接。 destination (int, 可选的) – 输出设备(-1 表示 CPU, 默认值: 当前设备) 返回: 在destination 设备上的张量，这是沿着dim连接张量的结果。 流和事件 class torch.cuda.Stream 围绕CUDA流的包装器。 CUDA流是属于特定设备的线性执行序列，独立于其他流。 查看 CUDA semantics 获取更详细的信息。 参数: device (torch.device 或者 int, 可选的) – 要在其上分配流的设备。 如果 device 为None（默认值）或负整数，则将使用当前设备。 priority (int, 可选的) – 流的优先级。数字越小，优先级越高。 query() 检查提交的所有工作是否已完成。 返回: 一个布尔值，表示此流中的所有内核是否都已完成。 record_event(event=None) 记录一个事件。 参数: event (Event, 可选的) – 需要记录的事件。如果没有给出，将分配一个新的。 返回: 记录的事件。 --- --- synchronize() 等待此流中的所有内核完成。 注意 这是一个围绕 cudaStreamSynchronize()的包装： 查看 CUDA 文档 获取更详细的信息。 wait_event(event) 使提交给流的所有未来工作等待事件。 参数: event (Event) – 需要等待的事件。 注意 这是一个围绕 cudaStreamWaitEvent()的包装： 查看 CUDA 文档 获取更详细的信息。 此函数返回时无需等待event： 只有未来的操作受到影响。 wait_stream(stream) 与另一个流同步。 提交给此流的所有未来工作将等到所有内核在呼叫完成时提交给给定流。 参数: stream (Stream) – 要同步的流。 注意 此函数返回时不等待stream中当前排队的内核 ： 只有未来的操作受到影响。 class torch.cuda.Event(enable_timing=False, blocking=False, interprocess=False, _handle=None) 围绕CUDA事件的包装。 参数: enable_timing (bool) – 表示事件是否应该测量时间（默认值：False） blocking (bool) – 如果是True， wait() 将会阻塞 (默认值: False) interprocess (bool) – 如果是 True，事件将会在进程中共享 (默认值: False) elapsed_time(end_event) 返回记录事件之前经过的时间。 ipc_handle() 返回此事件的IPC句柄。 query() 检测事件是否被记录。 返回: 一个布尔值，表示事件是否被记录。 record(stream=None) 记录给定流的一个事件。 synchronize() 和一个事件同步。 wait(stream=None) 使给定的流等待一个事件。 显存管理 torch.cuda.empty_cache() 释放当前由缓存分配器保存的所有未占用的缓存显存，以便可以在其他GPU应用程序中使用这些缓存并在nvidia-smi中可见。 注意 empty_cache() 不会增加PyTorch可用的GPU显存量。 查看 显存管理 以了解更多GPU显存管理的详细信息。 torch.cuda.memory_allocated(device=None) 返回给定设备的当前GPU显存使用量（以字节为单位）。 参数: device (torch.device 或者 int, 可选的) – 选定的设备。如果 device 是None（默认的），将返回 current_device()返回的当前设备的数据。 注意 这可能比 nvidia-smi 显示的数量少，因为一些没有使用的显存会被缓存分配器持有，且一些上下文需要在GPU中创建。查看 显存管理 部分了解更多关于GPU显存管理部分的详细信息。 torch.cuda.max_memory_allocated(device=None) 返回给定设备的张量的最大GPU显存使用量（以字节为单位）。 参数: device (torch.device 或者 int, 可选的) – 选择的设备。如果 device 是None（默认的），将返回 current_device()返回的当前设备的数据。 注意 查看 显存管理 部分了解更多关于GPU显存管理部分的详细信息。 torch.cuda.memory_cached(device=None) 返回由缓存分配器管理的当前GPU显存（以字节为单位）。 参数: device (torch.device or int, 可选的) – 选择的设备。如果 device 是None（默认的），将返回 current_device()返回的当前设备的数据。 注意 查看 显存管理 部分了解更多关于GPU显存管理部分的详细信息。 torch.cuda.max_memory_cached(device=None) 返回给定设备的缓存分配器管理的最大GPU显存（以字节为单位）。 参数: device (torch.device 或者 int, 可选的) – 选择的设备。如果 device 是None（默认的），将返回 current_device()返回的当前设备的数据。 注意 查看 显存管理 部分了解更多关于GPU显存管理部分的详细信息。 NVIDIA Tools Extension (NVTX) torch.cuda.nvtx.mark(msg) 描述某个时刻发生的瞬时事件。 参数: msg (string) – 与时间相关的ASCII信息。 torch.cuda.nvtx.range_push(msg) 将范围推到嵌套范围跨度的堆栈上。 返回启动范围的从零开始的深度。 参数: msg (string) – 与时间相关的ASCII信息。 torch.cuda.nvtx.range_pop() 从一堆嵌套范围跨度中弹出一个范围。 返回结束范围的从零开始的深度。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"storage.html":{"url":"storage.html","title":"torch.Storage","keywords":"","body":"torch.Storage 译者：yuange250 torch.Storage 跟绝大部分基于连续存储的数据结构类似，本质上是一个单一数据类型的一维连续数组(array)。 每一个 torch.Tensor 都有一个与之相对应的torch.Storage对象，两者存储数据的数据类型(data type)保持一致。 下面以数据类型为float的torch.FloatStorage 为例介绍一下torch.Storage的成员函数。 class torch.FloatStorage byte() byte()函数可以将此storage对象的数据类型转换为byte char() char()函数可以将此storage对象的数据类型转换为char clone() clone()函数可以返回一个此storage对象的复制 copy_() cpu() 如果此storage对象一开始不在cpu设备上，调用cpu()函数返回此storage对象的一个cpu上的复制 cuda(device=None, non_blocking=False, **kwargs) cuda()函数返回一个存储在CUDA内存中的复制，其中device可以指定cuda设备。 但如果此storage对象早已在CUDA内存中存储，并且其所在的设备编号与cuda()函数传入的device参数一致，则不会发生复制操作，返回原对象。 cuda()函数的参数信息: device (int) – 指定的GPU设备id. 默认为当前设备，即 torch.cuda.current_device()的返回值。 non_blocking (bool) – 如果此参数被设置为True, 并且此对象的资源存储在固定内存上(pinned memory)，那么此cuda()函数产生的复制将与host端的原storage对象保持同步。否则此参数不起作用。 **kwargs – 为了保证兼容性，也支持async参数，此参数的作用与no_blocking参数的作用完全相同，旧版本的遗留问题之一。 data_ptr() double() double()函数可以将此storage对象的数据类型转换为double element_size() fill_() float() float()函数可以将此storage对象的数据类型转换为float static from_buffer() static from_file(filename, shared=False, size=0) → Storage 对于from_file()函数，如果shared参数被设置为True， 那么此部分内存可以在进程间共享，任何对storage对象的更改都会被写入存储文件。 如果 shared 被置为 False, 那么在内存中对storage对象的更改则不会影响到储存文件中的数据。 size 参数是此storage对象中的元素个数。 如果shared被置为False, 那么此存储文件必须要包含size * sizeof(Type)字节大小的数据 (Type是此storage对象的数据类型)。 如果 shared 被置为 True，那么此存储文件只有在需要的时候才会被创建。 from_file()函数的参数： filename (str) – 对应的存储文件名 shared (bool) – 是否共享内存 size (int) – 此storage对象中的元素个数 half() half()函数可以将此storage对象的数据类型转换为half int() int()函数可以将此storage对象的数据类型转换为int is_cuda = False is_pinned() is_shared() is_sparse = False long() long()函数可以将此storage对象的数据类型转换为long new() pin_memory() 如果此storage对象还没有被存储在固定内存中，则pin_memory()函数可以将此storage对象存储到固定内存中 resize_() share_memory_() sharememory()函数可以将此storage对象转移到共享内存中。 对于早已在共享内存中的storage对象，这个操作无效；对于存储在CUDA设备上的storage对象，无需移动即可实现此类对象在进程间的共享，所以此操作对于它们来说也无效。 在共享内存中存储的storage对象无法被更改大小。 sharememory()函数返回值: self short() short()函数可以将此storage对象的数据类型转换为short size() tolist() tolist()函数可以返回一个包含此storage对象所有元素的列表 type(dtype=None, non_blocking=False, **kwargs) 如果函数调用时没有提供dtype参数，则type()函数的调用结果是返回此storage对象的数据类型。如果提供了此参数，则将此storage对象转化为此参数指定的数据类型。如果所提供参数所指定的数据类型与当前storage对象的数据类型一致，则不会进行复制操作，将原对象返回。 type()函数的参数信息: dtype (type or string) – 想要转化为的数据类型 non_blocking (bool) – 如果此参数被设置为True, 并且此对象的资源存储在固定内存上(pinned memory)，那么此cuda()函数产生的复制将与host端的原storage对象保持同步。否则此参数不起作用。 **kwargs – 为了保证兼容性，也支持async参数，此参数的作用与no_blocking参数的作用完全相同，旧版本的遗留问题之一 (已经被deprecated)。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"nn.html":{"url":"nn.html","title":"torch.nn","keywords":"","body":"torch.nn Parameters（参数） class torch.nn.Parameter Parameters对象是一种会被视为模块参数（module parameter）的Tensor张量。 Parameters类是Tensor 的子类, 不过相对于它的父类，Parameters类有一个很重要的特性就是当其在 Module类中被使用并被当做这个Module类的模块属性的时候，那么这个Parameters对象会被自动地添加到这个Module类的参数列表(list of parameters)之中，同时也就会被添加入此Module类的 parameters()方法所返回的参数迭代器中。而Parameters类的父类Tensor类也可以被用为构建模块的属性，但不会被加入参数列表。这样主要是因为，有时可能需要在模型中存储一些非模型参数的临时状态，比如RNN中的最后一个隐状态。而通过使用非Parameter的Tensor类，可以将这些临时变量注册(register)为模型的属性的同时使其不被加入参数列表。 Parameters: data (Tensor) – 参数张量(parameter tensor). requires_grad (bool, optional) – 参数是否需要梯度， 默认为 True。更多细节请看 如何将子图踢出反向传播过程。 Containers（容器） Module（模块） class torch.nn.Module 模块（Module）是所有神经网络模型的基类。 你创建模型的时候也应该继承这个类哦。 模块(Module)中还可以包含其他的模块，你可以将一个模块赋值成为另一个模块的属性，从而成为这个模块的一个子模块。而通过不断的赋值，你可以将不同的模块组织成一个树结构: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) # 当前的nn.Conv2d模块就被赋值成为Model模块的一个子模块，成为“树结构”的叶子 self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) 通过赋值这种方式添加的子模块将会被模型注册(register)，而后当调用模块的一些参数转换函数（to()）的时候，子模块的参数也会一并转换。 add_module(name, module) 向当前模块添加一个子模块。 此子模块可以作为当前模块的属性被访问到，而属性名就是add_module()函数中的name参数。 add_module()函数参数: name (string) – 子模块的名字. 函数调用完成后，可以通过访问当前模块的此字段来访问该子模块。 parameter (Module) – 要添加到当前模块的子模块。 apply(fn) apply()函数的主要作用是将 fn 递归地应用于模块的所有子模块（.children()函数的返回值）以及模块自身。此函数的一个经典应用就是初始化模型的所有参数这一过程(同样参见于 torch-nn-init)。 Parameters: fn (Module -> None) – 要应用于所有子模型的函数 Returns: self --- --- Return type: Module --- --- 例子: >>> def init_weights(m): print(m) if type(m) == nn.Linear: m.weight.data.fill_(1.0) print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) # 将init_weights()函数应用于模块的所有子模块 Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) buffers(recurse=True) 返回模块的缓冲区的迭代器 Parameters: recurse (bool) – 如果设置为True，产生的缓冲区迭代器会遍历模块自己与所有子模块，否则只会遍历模块的直连的成员。 Yields: torch.Tensor – 模型缓冲区 --- --- 举例: >>> for buf in model.buffers(): >>> print(type(buf.data), buf.size()) (20L,) (20L, 1L, 5L, 5L) children() 返回一个当前所有子模块的迭代器 Returns an iterator over immediate children modules. Yields: Module – 子模块 cpu() 将模型的所有参数(parameter)和缓冲区(buffer)都转移到CPU内存中。 Returns: self Return type: Module --- --- cuda(device=None) 将模型的所有参数和缓冲区都转移到CUDA设备内存中。 因为cuda()函数同时会将处理模块中的所有参数并缓存这些参数的对象。所以如果想让模块在GPU上进行优化操作，一定要在构建优化器之前调用模块的cuda()函数。 Parameters: device (int, optional) – 如果设备编号被指定，所有的参数都会被拷贝到编号指定设备上 Returns: self --- --- Return type: Module --- --- double() 将所有的浮点数类型的参数(parameters)和缓冲区(buffers)转换为double数据类型。 Returns: self Return type: Module --- --- dump_patches = False 这个字段可以为load_state_dict()提供 BC 支持（BC support实在不懂是什么意思-.-）。 在 state_dict()函数返回的状态字典（state dict）中， 有一个名为_metadata的属性中存储了这个state_dict的版本号。_metadata是一个遵从了状态字典（state dict）的命名规范的关键字字典， 要想了解这个_metadata在加载状态（loading state dict）的时候是怎么用的，可以看一下 _load_from_state_dict部分的文档。 如果新的参数/缓冲区被添加于/移除自这个模块之中时，这个版本号数字会随之发生变化。同时模块的_load_from_state_dict方法会比较版本号的信息并依据此状态词典（state dict）的变化做出一些适当的调整。 eval() 将模块转换为测试模式。 这个函数只对特定的模块类型有效，如 Dropout和BatchNorm等等。如果想了解这些特定模块在训练/测试模式下各自的运作细节，可以看一下这些特殊模块的文档部分。 extra_repr() 为模块设置额外的展示信息(extra representation)。 如果想要打印展示(print)你的模块的一些定制的额外信息，那你应该在你的模块中复现这个函数。单行和多行的字符串都可以被接受。 float() 将所有浮点数类型的参数(parameters)和缓冲区(buffers)转换为float数据类型。 Returns: self Return type: Module --- --- forward(*input) 定义了每次模块被调用之后所进行的计算过程。 应该被Module类的所有子类重写。 Note 尽管模块的前向操作都被定义在这个函数里面，但是当你要进行模块的前向操作的时候，还是要直接调用模块Module 的实例函数，而不是直接调用这个forward()函数。这主要是因为前者会照顾到注册在此模块之上的钩子函数（the registered hooks）的运行，而后者则不会。 half() 将所有的浮点数类型的参数(parameters)和缓冲区(buffers)转换为half数据类型。 Returns: self Return type: Module --- --- load_state_dict(state_dict, strict=True) 将state_dict中的参数（parameters）和缓冲区（buffers）拷贝到模块和其子模块之中。如果strict被设置为True，那么state_dict中的键值（keys）必须与模型的[state_dict()]函数所返回的键值（keys）信息保持完全的一致。 load_state_dict()函数参数： state_dict (dict) – 一个包含了参数和持久缓冲区的字典。 strict (bool, optional) – 是否严格要求 state_dict 中的键值（keys）与模型 state_dict() 函数返回的键值（keys）信息保持完全一致。 默认： True modules() 返回一个当前模块内所有模块（包括自身）的迭代器。 Yields: Module – a module in the network Note 注意重复的模块只会被返回一次。比在下面这个例子中，l就只会被返回一次。 例子: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential ( (0): Linear (2 -> 2) (1): Linear (2 -> 2) ) 1 -> Linear (2 -> 2) named_buffers(prefix='', recurse=True) 返回一个模块缓冲区的迭代器，每次返回的元素是由缓冲区的名字和缓冲区自身组成的元组。 named_buffers()函数的参数: prefix (str) – 要添加在所有缓冲区名字之前的前缀。 recurse (bool) – 如果设置为True，那样迭代器中不光会返回这个模块自身直连成员的缓冲区，同时也会递归返回其子模块的缓冲区。否则，只返回这个模块直连成员的缓冲区。 Yields: (string, torch.Tensor) – 包含了缓冲区的名字和缓冲区自身的元组 例子: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) named_children() 返回一个当前模型直连的子模块的迭代器，每次返回的元素是由子模块的名字和子模块自身组成的元组。 Yields: (string, Module) – 包含了子模块的名字和子模块自身的元组 例子： >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) named_modules(memo=None, prefix='') 返回一个当前模块内所有模块（包括自身）的迭代器，每次返回的元素是由模块的名字和模块自身组成的元组。 Yields: (string, Module) – 模块名字和模块自身组成的元组 Note 重复的模块只会被返回一次。在下面的例子中，l只被返回了一次。 例子： >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential ( (0): Linear (2 -> 2) (1): Linear (2 -> 2) )) 1 -> ('0', Linear (2 -> 2)) named_parameters(prefix='', recurse=True) 返回一个当前模块内所有参数的迭代器，每次返回的元素是由参数的名字和参数自身组成的元组。 named_parameters()函数参数： prefix (str) – 要在所有参数名字前面添加的前缀。 recurse (bool) – 如果设置为True，那样迭代器中不光会返回这个模块自身直连成员的参数，同时也会返回其子模块的参数。否则，只返回这个模块直连成员的参数。 Yields: (string, Parameter) – 参数名字和参数自身组成的元组 例子: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) parameters(recurse=True) 返回一个遍历模块所有参数的迭代器。 parameters()函数一个经典的应用就是实践中经常将此函数的返回值传入优化器。 Parameters: recurse (bool) – 如果设置为True，那样迭代器中不光会返回这个模块自身直连成员的参数，同时也会递归返回其子模块的参数。否则，只返回这个模块直连成员的参数。 Yields: Parameter – 模块参数 --- --- 例子: >>> for param in model.parameters(): >>> print(type(param.data), param.size()) (20L,) (20L, 1L, 5L, 5L) register_backward_hook(hook) 在模块上注册一个挂载在反向操作之后的钩子函数。（挂载在backward之后这个点上的钩子函数） 对于每次输入，当模块关于此次输入的反向梯度的计算过程完成，该钩子函数都会被调用一次。此钩子函数需要遵从以下函数签名： hook(module, grad_input, grad_output) -> Tensor or None 如果模块的输入或输出是多重的（multiple inputs or outputs），那 grad_input 和 grad_output 应当是元组数据。 钩子函数不能对输入的参数grad_input 和 grad_output进行任何更改，但是可以选择性地根据输入的参数返回一个新的梯度回去，而这个新的梯度在后续的计算中会替换掉grad_input。 Returns: 一个句柄（handle），这个handle的特点就是通过调用handle.remove()函数就可以将这个添加于模块之上的钩子移除掉。 Return type: torch.utils.hooks.RemovableHandle --- --- Warning 对于一些具有很多复杂操作的Module，当前的hook实现版本还不能达到完全理想的效果。举个例子，有些错误的情况下，函数的输入参数grad_input 和 grad_output中可能只是真正的输入和输出变量的一个子集。对于此类的Module，你应该使用[torch.Tensor.register_hook()]直接将钩子挂载到某个特定的输入输出的变量上，而不是当前的模块。 register_buffer(name, tensor) 往模块上添加一个持久缓冲区。 这个函数的经常会被用于向模块添加不会被认为是模块参数（model parameter）的缓冲区。举个栗子，BatchNorm的running_mean就不是一个参数，但却属于持久状态。 所添加的缓冲区可以通过给定的名字(name参数)以访问模块的属性的方式进行访问。 register_buffer()函数的参数: name (string) – 要添加的缓冲区的名字。所添加的缓冲区可以通过此名字以访问模块的属性的方式进行访问。 tensor (Tensor) – 需要注册到模块上的缓冲区。 例子: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook(hook) 在模块上注册一个挂载在前向操作之后的钩子函数。（挂载在forward操作结束之后这个点） 此钩子函数在每次模块的 forward()函数运行结束产生output之后就会被触发。此钩子函数需要遵从以下函数签名： hook(module, input, output) -> None 此钩子函数不能进行会修改 input 和 output 这两个参数的操作。 Returns: 一个句柄（handle），这个handle的特点就是通过调用handle.remove()函数就可以将这个添加于模块之上的钩子移除掉。 Return type: torch.utils.hooks.RemovableHandle --- --- register_forward_pre_hook(hook) 在模块上注册一个挂载在前向操作之前的钩子函数。（挂载在forward操作开始之前这个点） 此钩子函数在每次模块的 forward()函数运行开始之前会被触发。此钩子函数需要遵从以下函数签名： The hook will be called every time before forward() is invoked. It should have the following signature: hook(module, input) -> None 此钩子函数不能进行会修改 input 这个参数的操作。 Returns: 一个句柄（handle），这个handle的特点就是通过调用handle.remove()函数就可以将这个添加于模块之上的钩子移除掉。 Return type: torch.utils.hooks.RemovableHandle --- --- register_parameter(name, param) 向模块添加一个参数（parameter）。 所添加的参数（parameter）可以通过给定的名字(name参数)以访问模块的属性的方式进行访问。 register_parameter()函数的参数： name (string) – 所添加的参数的名字. 所添加的参数（parameter）可以通过此名字以访问模块的属性的方式进行访问 parameter (Parameter) – 要添加到模块之上的参数。 state_dict(destination=None, prefix='', keep_vars=False) 返回一个包含了模块当前所有状态(state)的字典(dictionary)。 所有的参数和持久缓冲区都被囊括在其中。字典的键值就是响应的参数和缓冲区的名字(name)。 Returns: 一个包含了模块当前所有状态的字典 Return type: dict --- --- 例子: >>> module.state_dict().keys() ['bias', 'weight'] to(*args, **kwargs) 移动 并且/或者（and/or）转换所有的参数和缓冲区。 这个函数可以这样调用： to(device=None, dtype=None, non_blocking=False) to(dtype, non_blocking=False) to(tensor, non_blocking=False) 此函数的函数签名跟torch.Tensor.to()函数的函数签名很相似，只不过这个函数dtype参数只接受浮点数类型的dtype，如float， double， half（ floating point desired dtype s）。同时，这个方法只会将浮点数类型的参数和缓冲区（the floating point parameters and buffers）转化为dtype（如果输入参数中给定的话）的数据类型。而对于整数类型的参数和缓冲区（the integral parameters and buffers），即便输入参数中给定了dtype，也不会进行转换操作，而如果给定了 device参数，移动操作则会正常进行。当non_blocking参数被设置为True之后，此函数会尽可能地相对于 host 进行异步的 转换/移动 操作，比如，将存储在固定内存（pinned memory）上的CPU Tensors移动到CUDA设备上这一过程既是如此。 例子在下面。 Note 这个方法对模块的修改都是in-place操作。 to()函数的参数: device (torch.device) – 想要将这个模块中的参数和缓冲区转移到的设备。 dtype (torch.dtype) – 想要将这个模块中浮点数的参数和缓冲区转化为的浮点数数据类型。 tensor (torch.Tensor) – 一个Tensor，如果被指定，其dtype和device信息，将分别起到上面两个参数的作用，也就是说，这个模块的浮点数的参数和缓冲区的数据类型将会被转化为这个Tensor的dtype类型，同时被转移到此Tensor所处的设备device上去。 Returns: self Return type: Module --- --- 例子: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) train(mode=True) 将模块转换成训练模式。 这个函数只对特定的模块类型有效，如 Dropout和BatchNorm等等。如果想了解这些特定模块在训练/测试模式下各自的运作细节，可以看一下这些特殊模块的文档部分。 Returns: self Return type: Module --- --- type(dst_type) 将所有的参数和缓冲区转化为 dst_type的数据类型。 Parameters: dst_type (type or string) – 要转化的数据类型 Returns: self --- --- Return type: Module --- --- zero_grad() 讲模块所有参数的梯度设置为0。 Sequential class torch.nn.Sequential(*args) 一种顺序容器。传入Sequential构造器中的模块会被按照他们传入的顺序依次添加到Sequential之上。相应的，一个由模块组成的顺序词典也可以被传入到Sequential的构造器中。 为了方便大家理解，举个简单的例子： # 构建Sequential的例子 model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # 利用OrderedDict构建Sequential的例子 model = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ])) ModuleList (模块列表) class torch.nn.ModuleList(modules=None) ModuleList的作用是将一堆模块（module）存储在一个列表之中。 ModuleList 可以按一般的python列表的索引方式进行索引，但ModuleList中的模块都已被正确注册，并且对所有的Module method可见。 Parameters: modules (iterable__, optional) – 一个要添加到ModuleList中的由模块组成的可迭代结构(an iterable of modules) 例子: class MyModule(nn.Module): def __init__(self): super(MyModule, self).__init__() self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)]) def forward(self, x): # ModuleList可以被当作一个迭代器，同时也可以使用index索引 for i, l in enumerate(self.linears): x = self.linears[i // 2](x) + l(x) return x append(module) 将一个模块添加到ModuleList的末尾，与python list的append()一致。 Parameters: module (nn.Module) – 要添加的模块 extend(modules) 将一个由模块组成的可迭代结构添加到ModuleList的末尾，与python list的extend()一致。 Parameters: modules (iterable) – 要添加到ModuleList末尾的由模块组成的可迭代结构 insert(index, module) 将给定的module插入到ModuleList的index位置。 insert()函数的参数: index (int) – 要插入的位置 module (nn.Module) – 要插入的模块 ModuleDict (模块词典) class torch.nn.ModuleDict(modules=None) ModuleDict的作用是将一堆模块（module）存储在一个词典之中。 ModuleDict 可以按一般的python词典的索引方式进行索引，但ModuleDict中的模块都已被正确注册，并且对所有的Module method可见。 Parameters: modules (iterable__, optional) – 一个由(string: module)映射组成的映射集合（词典）或者 一个由(string, module)键/值对组成的可迭代结构 Example: class MyModule(nn.Module): def __init__(self): super(MyModule, self).__init__() self.choices = nn.ModuleDict({ 'conv': nn.Conv2d(10, 10, 3), 'pool': nn.MaxPool2d(3) }) self.activations = nn.ModuleDict([ ['lrelu', nn.LeakyReLU()], ['prelu', nn.PReLU()] ]) def forward(self, x, choice, act): x = self.choices[choice](x) x = self.activations[act](x) return x clear() 移除ModuleDict中所有的元素。 items() 返回一个由ModuleDict中的键/值对组成的可迭代结构。 keys() 返回一个由ModuleDict中的键组成的可迭代结构。 pop(key) 将key这个键从ModuleDict中删除，并将其对应的模块返回。 Parameters: key (string) – 要从ModuleDict中弹出的键 update(modules) 通过传入的映射或者由键/值对组成的可迭代结构对当前的ModuleDict进行更新，如果传入对象与当前ModuleDict中存在键重复，当前ModuleDict中这些重复的键所对应的值将被覆盖。 Parameters: modules (iterable) – 一个由(string: Module)映射组成的映射集合（词典）或者 一个由(string: Module)键/值对组成的可迭代结构 values() 返回一个由ModuleDict中的值组成的可迭代结构。 ParameterList (参数列表) class torch.nn.ParameterList(parameters=None) ParameterList的作用是将一堆参数（parameter）存储到一个列表中。 ParameterList 可以按一般的python列表的索引方式进行索引，但ParameterList中的参数（parameter）都已被正确注册，并且对所有的Module method可见。 Parameters: parameters (iterable__, optional) – 要添加到ParameterList之上的由parameter组成的可迭代结构 例子: class MyModule(nn.Module): def __init__(self): super(MyModule, self).__init__() self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)]) def forward(self, x): # ParameterList可以被当作一个迭代器，同时也可以使用index索引 for i, p in enumerate(self.params): x = self.params[i // 2].mm(x) + p.mm(x) return x append(parameter) 将一个parameter添加到ParameterList的末尾。 Parameters: parameter (nn.Parameter) – 要添加的参数 extend(parameters) 将一个由parameter组成的Python可迭代结构添加到ParameterList的末尾。 Parameters: parameters (iterable) – 要添加到ParameterList的末尾的由parameter组成的Python可迭代结构 ParameterDict (参数词典) class torch.nn.ParameterDict(parameters=None) ParameterDict的作用是将一堆参数（Parameter）存储在一个词典之中。 ParameterDict 可以按一般的python词典的索引方式进行索引，但ParameterDictt中的参数都已被正确注册，并且对所有的Module method可见。 Parameters: parameters (iterable__, optional) – 一个由(string:Parameter)映射组成的映射集合（词典）或者 一个由(string, Parameter)键/值对组成的可迭代结构 例子: class MyModule(nn.Module): def __init__(self): super(MyModule, self).__init__() self.params = nn.ParameterDict({ 'left': nn.Parameter(torch.randn(5, 10)), 'right': nn.Parameter(torch.randn(5, 10)) }) def forward(self, x, choice): x = self.params[choice].mm(x) return x clear() 移除ParameterDict中所有的元素。 items() 返回一个由ParameterDict中的键/值对组成的可迭代结构。 keys() 返回一个由 ParameterDict中的键组成的可迭代结构。 pop(key) 将key这个键从ParameterDict中删除，并将其对应的模块返回。 Parameters: key (string) – 要从ParameterDict中弹出的键 update(parameters) 通过传入的映射或者由键/值对组成的可迭代结构对当前的ParameterDict进行更新，如果传入对象与当前ParameterDict中存在键重复，当前ParameterDict中这些重复的键所对应的值将被覆盖。 Parameters: parameters (iterable) – modules (iterable) – 一个由(string: Parameter)映射组成的映射集合（词典）或者 一个由(string: Parameter)键/值对组成的可迭代结构 values() 返回一个由ParameterDict中的值组成的可迭代结构。 Convolution layers (卷积层) Conv1d class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) 利用指定大小的一维卷积核对输入的多通道一维输入信号进行一维卷积操作的卷积层。 在最简单的情况下，对于输入大小为，输出大小为的一维卷积层，其卷积计算过程可以如下表述： 这里的符号实际上是一个互相关（cross-correlation） 操作符（大家可以自己查一下互相关和真卷积的区别，互相关因为实现起来很简单，所以一般的深度学习框架都是用互相关操作取代真卷积）, is a batch size, 代表通道的数量, 代表信号序列的长度。 stride 参数控制了互相关操作（伪卷积）的步长，参数的数据类型一般是单个数字或者一个只有一个元素的元组。 padding 参数控制了要在一维卷积核的输入信号的各维度各边上要补齐0的层数。 dilation 参数控制了卷积核中各元素之间的距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 groups 控制了输入输出之间的连接（connections）的数量。in_channels 和 out_channels 必须能被 groups 整除。举个栗子， > * 当 groups=1, 此Conv1d层会使用一个卷积层进行所有输入到输出的卷积操作。 > * 当 groups=2, 此时Conv1d层会产生两个并列的卷积层。同时，输入通道被分为两半，两个卷积层分别处理一半的输入通道，同时各自产生一半的输出通道。最后这两个卷积层的输出会被concatenated一起，作为此Conv1d层的输出。 > * 当 groups= in_channels, 每个输入通道都会被单独的一组卷积层处理，这个组的大小是 Note 取决于你卷积核的大小，有些时候输入数据中某些列（最后几列）可能不会参与计算（比如列数整除卷积核大小有余数，而又没有padding，那最后的余数列一般不会参与卷积计算），这主要是因为pytorch中的互相关操作cross-correlation是保证计算正确的操作(valid operation)， 而不是满操作(full operation)。所以实际操作中，还是要亲尽量选择好合适的padding参数哦。 Note 当groups == in_channels 并且 out_channels == K * in_channels（其中K是正整数）的时候，这个操作也被称为深度卷积。 举个创建深度卷积层的例子，对于一个大小为 的输入，要构建一个深度乘数为K的深度卷积层，可以通过以下参数来创建：。 Note 当程序的运行环境是使用了CuDNN的CUDA环境的时候，一些非确定性的算法（nondeterministic algorithm）可能会被采用以提高整个计算的性能。如果不想使用这些非确定性的算法，你可以通过设置torch.backends.cudnn.deterministic = True来让整个计算过程保持确定性（可能会损失一定的计算性能）。对于后端(background)，你可以看一下这一部分Reproducibility了解其相关信息。 Conv1d的参数: in_channels (int) – 输入通道个数 out_channels (int) – 输出通道个数 kernel_size (int or tuple) – 卷积核大小 stride (int or tuple, optional) – 卷积操作的步长。 默认： 1 padding (int or tuple, optional) – 输入数据各维度各边上要补齐0的层数。 默认： 0 dilation (int or tuple, optional) – 卷积核各元素之间的距离。 默认： 1 groups (int, optional) – 输入通道与输出通道之间相互隔离的连接的个数。 默认：1 bias (bool, optional) – 如果被置为 True，向输出增加一个偏差量，此偏差是可学习参数。 默认：True Shape: 输入: 输出: 其中 | 内部Variables： | weight (Tensor) – Conv1d模块中的一个大小为(out_channels, in_channels, kernel_size)的权重张量，这些权重可训练学习(learnable)。这些权重的初始值的采样空间是， 其中。 bias (Tensor) – 模块的偏差项，大小为(out_channels)，可训练学习。如果构造Conv1d时构造函数中的bias 被置为 True，那么这些权重的初始值的采样空间是， 其中 。 例子: >>> m = nn.Conv1d(16, 33, 3, stride=2) >>> input = torch.randn(20, 16, 50) >>> output = m(input) Conv2d class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) 利用指定大小的二维卷积核对输入的多通道二维输入信号进行二维卷积操作的卷积层。 在最简单的情况下，对于输入大小为，输出大小为的二维维卷积层，其卷积计算过程可以如下表述： 这里的符号实际上是一个二维互相关（cross-correlation） 操作符（大家可以自己查一下互相关和真卷积的区别，互相关因为实现起来很简单，所以一般的深度学习框架都是用互相关操作取代真卷积）, is a batch size, 代表通道的数量, 是输入的二维数据的像素高度， 是输入的二维数据的像素宽度。 stride 参数控制了互相关操作（伪卷积）的步长，参数的数据类型一般是单个数字或者一个只有一个元素的元组。 padding 参数控制了要在二维卷积核的输入信号的各维度各边上要补齐0的层数。 dilation 参数控制了卷积核中各元素之间的距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 groups 控制了输入输出之间的连接（connections）的数量。in_channels 和 out_channels 必须能被 groups 整除。举个栗子， > * 当 groups=1, 此Conv1d层会使用一个卷积层进行所有输入到输出的卷积操作。 > * 当 groups=2, 此时Conv1d层会产生两个并列的卷积层。同时，输入通道被分为两半，两个卷积层分别处理一半的输入通道，同时各自产生一半的输出通道。最后这两个卷积层的输出会被concatenated一起，作为此Conv1d层的输出。 > * 当 groups= in_channels, 每个输入通道都会被单独的一组卷积层处理，这个组的大小是 kernel_size, stride, padding, dilation这几个参数均支持一下输入形式： 一个 int 数字 – 二维数据的高和宽这两个维度都会采用这一个数字。 一个由两个int数字组成的tuple– 这种情况下，二维数据的高这一维度会采用元组中的第一个int数字，宽这一维度会采用第二个int数字。 Note 取决于你卷积核的大小，有些时候输入数据中某些列（最后几列）可能不会参与计算（比如列数整除卷积核大小有余数，而又没有padding，那最后的余数列一般不会参与卷积计算），这主要是因为pytorch中的互相关操作cross-correlation是保证计算正确的操作(valid operation)， 而不是满操作(full operation)。所以实际操作中，还是要亲尽量选择好合适的padding参数哦。 Note 当groups == in_channels 并且 out_channels == K * in_channels（其中K是正整数）的时候，这个操作也被称为深度卷积。 换句话说，对于一个大小为的输入，要构建一个深度乘数为K的深度卷积层，可以通过以下参数来创建：。 Note 当程序的运行环境是使用了CuDNN的CUDA环境的时候，一些非确定性的算法（nondeterministic algorithm）可能会被采用以提高整个计算的性能。如果不想使用这些非确定性的算法，你可以通过设置torch.backends.cudnn.deterministic = True来让整个计算过程保持确定性（可能会损失一定的计算性能）。对于后端(background)，你可以看一下这一部分Reproducibility了解其相关信息。 Conv2d的参数: in_channels (int) – 输入通道个数 out_channels (int) – 输出通道个数 kernel_size (int or tuple) – 卷积核大小 stride (int or tuple, optional) –卷积操作的步长。 默认： 1 padding (int or tuple, optional) – 输入数据各维度各边上要补齐0的层数。 默认： 0 dilation (int or tuple, optional) –卷积核各元素之间的距离。 默认： 1 groups (int, optional) – 输入通道与输出通道之间相互隔离的连接的个数。 默认：1 bias (bool, optional) – 如果被置为 True，向输出增加一个偏差量，此偏差是可学习参数。 默认：True Shape: 输入: 输出: 其中 | 内部Variables: | weight (Tensor) – Conv2d模块中的一个大小为 (out_channels, in_channels, kernel_size[0], kernel_size[1])的权重张量，这些权重可训练学习(learnable)。这些权重的初始值的采样空间是 ， 其中。 bias (Tensor) – 块的偏差项，大小为(out_channels)，可训练学习。如果构造Conv2d时构造函数中的bias 被置为 True，那么这些权重的初始值的采样空间是，其中。 例子: >>> # With square kernels and equal stride >>> m = nn.Conv2d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) >>> # non-square kernels and unequal stride and with padding and dilation >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)) >>> input = torch.randn(20, 16, 50, 100) >>> output = m(input) Conv3d class torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) 利用指定大小的三维卷积核对输入的多通道三维输入信号进行三维卷积操作的卷积层。 最简单的情况下，对于输入大小为，输出大小为 的三维卷积层，其卷积计算过程可以如下表述： 这里的 符号实际上是一个三维互相关 cross-correlation 操作符。 stride 数控制了互相关操作（伪卷积）的步长。 padding 参数控制了要在三维卷积核的输入信号的各维度各边上要补齐0的层数。 dilation 参数控制了卷积核中各元素之间的距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 groups 控制了输入输出之间的连接（connections）的数量。in_channels 和 out_channels 必须能被 groups 整除。举个栗子， > * 当 groups=1, 此Conv3d层会使用一个卷积层进行对所有输入到输出的卷积操作。 > * 当 groups=2, 此时Conv3d层会产生两个并列的卷积层。同时，输入通道被分为两半，两个卷积层分别处理一半的输入通道，同时各自产生一半的输出通道。最后这两个卷积层的输出会被concatenated一起，作为此Conv3d层的输出。 > * 当 groups= in_channels, 每个输入通道都会被单独的一组卷积层处理，这个组的大小是 . kernel_size, stride, padding, dilation这几个参数均支持一下输入形式： 一个 int 数字 – 三维维数据的深度，高和宽这三个维度都会采用这一个数字。 一个由三个int数字组成的tuple– 这种情况下，三维数据的深度这一维度会采用元组中的第一个int数字，高这一维度会采用元组中的第二个int数字，宽这一维度会采用第三个int数字。 Note 取决于你卷积核的大小，有些时候输入数据中某些列（最后几列）可能不会参与计算（比如列数整除卷积核大小有余数，而又没有padding，那最后的余数列一般不会参与卷积计算），这主要是因为pytorch中的互相关操作cross-correlation是保证计算正确的操作(valid operation)， 而不是满操作(full operation)。所以实际操作中，还是要亲尽量选择好合适的padding参数哦。 Note 当groups == in_channels 并且 out_channels == K * in_channels（其中K是正整数）的时候，这个操作也被称为深度卷积。 换句话说，对于一个大小为 的输入，要构建一个深度乘数为K的深度卷积层，可以通过以下参数来创建：。 Note 当程序的运行环境是使用了CuDNN的CUDA环境的时候，一些非确定性的算法（nondeterministic algorithm）可能会被采用以提高整个计算的性能。如果不想使用这些非确定性的算法，你可以通过设置torch.backends.cudnn.deterministic = True来让整个计算过程保持确定性（可能会损失一定的计算性能）。对于后端(background)，你可以看一下这一部分Reproducibility了解其相关信息。 Parameters: in_channels (int) – 输入通道的个数 out_channels (int) – 卷积操作输出通道的个数 kernel_size (int or tuple) – 卷积核大小 stride (int or tuple, optional) – 卷积操作的步长。 默认： 1 padding (int or tuple, optional) – 输入数据各维度各边上要补齐0的层数。 默认： 0 dilation (int or tuple, optional) – 卷积核各元素之间的距离。 默认： 1 groups (int, optional) – 输入通道与输出通道之间相互隔离的连接的个数。 默认：1 bias (bool, optional) – 如果被置为 True，向输出增加一个偏差量，此偏差是可学习参数。 默认：True Shape: 输入: 输出: where | 内部Variables: | weight (Tensor) – Conv3d模块中的一个大小为 (out_channels, in_channels, kernel_size[0], kernel_size[1], kernel_size[2]) 的权重张量，这些权重可训练学习(learnable)。这些权重的初始值的采样空间是，其中。 bias (Tensor) – 模块的偏差项，大小为(out_channels)，可训练学习。如果构造Conv1d时构造函数中的bias 被置为 True，那么这些权重的初始值的采样空间是 ，其中 。 例子: >>> # With square kernels and equal stride >>> m = nn.Conv3d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0)) >>> input = torch.randn(20, 16, 10, 50, 100) >>> output = m(input) ConvTranspose1d class torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1) 利用指定大小的一维转置卷积核对输入的多通道一维输入信号进行转置卷积（当然此卷积也是互相关操作，cross-correlation）操作的模块。 该模块可以看作是Conv1d相对于其输入的梯度(the gradient of Conv1d with respect to its input， 直译)， 转置卷积又被称为小数步长卷积或是反卷积（尽管这不是一个真正意义上的反卷积）。 stride 控制了转置卷积操作的步长 padding 控制了要在输入的各维度的各边上补齐0的层数，与Conv1d不同的地方，此padding参数与实际补齐0的层数的关系为层数 = kernel_size - 1 - padding，详情请见下面的note。 output_padding 控制了转置卷积操作输出的各维度的长度增量，但注意这个参数不是说要往转置卷积的输出上pad 0，而是直接控制转置卷积的输出大小为根据此参数pad后的大小。更多的详情请见下面的note。 dilation 控制了卷积核中各点之间的空间距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 groups 控制了输入输出之间的连接（connections）的数量。in_channels 和 out_channels 必须能被 groups 整除。举个栗子， > * 当 groups=1, 此Conv1d层会使用一个卷积层进行所有输入到输出的卷积操作。 > * 当 groups=2, 此时Conv1d层会产生两个并列的卷积层。同时，输入通道被分为两半，两个卷积层分别处理一半的输入通道，同时各自产生一半的输出通道。最后这两个卷积层的输出会被concatenated一起，作为此Conv1d层的输出。 > * 当 groups= in_channels, 每个输入通道都会被单独的一组卷积层处理，这个组的大小是。 Note 取决于你卷积核的大小，有些时候输入数据中某些列（最后几列）可能不会参与计算（比如列数整除卷积核大小有余数，而又没有padding，那最后的余数列一般不会参与卷积计算），这主要是因为pytorch中的互相关操作cross-correlation是保证计算正确的操作(valid operation)， 而不是满操作(full operation)。所以实际操作中，还是要亲尽量选择好合适的padding参数哦。 Note padding 参数控制了要在输入的各维度各边上补齐0的层数，与在Conv1d中不同的是，在转置卷积操作过程中，此padding参数与实际补齐0的层数的关系为层数 = kernel_size - 1 - padding， 这样设置的主要原因是当使用相同的参数构建Conv1d 和ConvTranspose1d模块的时候，这种设置能够实现两个模块有正好相反的输入输出的大小，即Conv1d的输出大小是其对应的ConvTranspose1d模块的输入大小，而ConvTranspose1d的输出大小又恰好是其对应的Conv1d模块的输入大小。然而，当stride > 1的时候，Conv1d 的一个输出大小可能会对应多个输入大小，上一个note中就详细的介绍了这种情况，这样的情况下要保持前面提到两种模块的输入输出保持反向一致，那就要用到 output_padding参数了，这个参数可以增加转置卷积输出的某一维度的大小，以此来达到前面提到的同参数构建的Conv1d 和ConvTranspose1d模块的输入输出方向一致。 但注意这个参数不是说要往转置卷积的输出上pad 0，而是直接控制转置卷积的输出各维度的大小为根据此参数pad后的大小。 Note 当程序的运行环境是使用了CuDNN的CUDA环境的时候，一些非确定性的算法（nondeterministic algorithm）可能会被采用以提高整个计算的性能。如果不想使用这些非确定性的算法，你可以通过设置torch.backends.cudnn.deterministic = True来让整个计算过程保持确定性（可能会损失一定的计算性能）。对于后端(background)，你可以看一下这一部分Reproducibility了解其相关信息。 Parameters: in_channels (int) – 输入通道的个数 out_channels (int) – 卷积操作输出通道的个数 kernel_size (int or tuple) – 卷积核大小 stride (int or tuple, optional) – 卷积操作的步长。 默认： 1 padding (int or tuple, optional) – kernel_size - 1 - padding 层 0 会被补齐到输入数据的各边上。 默认： 0 output_padding (int or tuple, optional) – 输出的各维度要增加的大小。默认：0 groups (int, optional) – 输入通道与输出通道之间相互隔离的连接的个数。 默认：1 bias (bool, optional) – 如果被置为 True，向输出增加一个偏差量，此偏差是可学习参数。 默认：True dilation (int or tuple, optional) – 卷积核各元素之间的距离。 默认： 1 Shape: 输入: 输出: 其中， | Variables: | weight (Tensor) – 模块中的一个大小为 (in_channels, out_channels, kernel_size[0])的权重张量，这些权重可训练学习(learnable)。这些权重的初始值的采样空间是，其中 。 bias (Tensor) – 模块的偏差项，大小为 (out_channels)， 如果构造函数中的 bias 被置为 True，那么这些权重的初始值的采样空间是 ，其中 。 ConvTranspose2d class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1) 利用指定大小的二维转置卷积核对输入的多通道二维输入信号进行转置卷积（当然此卷积也是互相关操作，cross-correlation）操作的模块。 该模块可以看作是Conv2d相对于其输入的梯度(the gradient of Conv2d with respect to its input， 直译)， 转置卷积又被称为小数步长卷积或是反卷积（尽管这不是一个真正意义上的反卷积）。 stride 控制了转置卷积操作的步长 padding 控制了要在输入的各维度的各边上补齐0的层数，与Conv1d不同的地方，此padding参数与实际补齐0的层数的关系为层数 = kernel_size - 1 - padding，详情请见下面的note。 output_padding 控制了转置卷积操作输出的各维度的长度增量，但注意这个参数不是说要往转置卷积的输出上pad 0，而是直接控制转置卷积的输出大小为根据此参数pad后的大小。更多的详情请见下面的note。 dilation 控制了卷积核中各点之间的空间距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 groups 控制了输入输出之间的连接（connections）的数量。in_channels 和 out_channels 必须能被 groups 整除。举个栗子， > * 当 groups=1, 此Conv1d层会使用一个卷积层进行所有输入到输出的卷积操作。 > * 当 groups=2, 此时Conv1d层会产生两个并列的卷积层。同时，输入通道被分为两半，两个卷积层分别处理一半的输入通道，同时各自产生一半的输出通道。最后这两个卷积层的输出会被concatenated一起，作为此Conv1d层的输出。 > * 当 groups= in_channels, 每个输入通道都会被单独的一组卷积层处理，这个组的大小是。 kernel_size, stride, padding, output_padding 这几个参数均支持一下输入形式： 一个 int 数字 – 二维维数据的高和宽这两个维度都会采用这一个数字。 一个由两个int数字组成的tuple– 这种情况下，二维数据的高这一维度会采用元组中的第一个int数字，宽这一维度会采用第二个int数字。 Note 取决于你卷积核的大小，有些时候输入数据中某些列（最后几列）可能不会参与计算（比如列数整除卷积核大小有余数，而又没有padding，那最后的余数列一般不会参与卷积计算），这主要是因为pytorch中的互相关操作cross-correlation是保证计算正确的操作(valid operation)， 而不是满操作(full operation)。所以实际操作中，还是要亲尽量选择好合适的padding参数哦。 Note padding 参数控制了要在输入的各维度各边上补齐0的层数，与在Conv1d中不同的是，在转置卷积操作过程中，此padding参数与实际补齐0的层数的关系为层数 = kernel_size - 1 - padding， 这样设置的主要原因是当使用相同的参数构建Conv2d 和ConvTranspose2d模块的时候，这种设置能够实现两个模块有正好相反的输入输出的大小，即Conv2d的输出大小是其对应的ConvTranspose2d模块的输入大小，而ConvTranspose2d的输出大小又恰好是其对应的Conv2d模块的输入大小。然而，当stride > 1的时候，Conv2d 的一个输出大小可能会对应多个输入大小，上一个note中就详细的介绍了这种情况，这样的情况下要保持前面提到两种模块的输入输出保持反向一致，那就要用到 output_padding参数了，这个参数可以增加转置卷积输出的某一维度的大小，以此来达到前面提到的同参数构建的Conv2d 和ConvTranspose2d模块的输入输出方向一致。 但注意这个参数不是说要往转置卷积的输出上pad 0，而是直接控制转置卷积的输出各维度的大小为根据此参数pad后的大小。 Note 当程序的运行环境是使用了CuDNN的CUDA环境的时候，一些非确定性的算法（nondeterministic algorithm）可能会被采用以提高整个计算的性能。如果不想使用这些非确定性的算法，你可以通过设置torch.backends.cudnn.deterministic = True来让整个计算过程保持确定性（可能会损失一定的计算性能）。对于后端(background)，你可以看一下这一部分Reproducibility了解其相关信息。 Parameters: in_channels (int) – 输入通道的个数 out_channels (int) – 卷积操作输出通道的个数 kernel_size (int or tuple) – 卷积核大小 stride (int or tuple, optional) – 卷积操作的步长。 默认： 1 padding (int or tuple, optional) – kernel_size - 1 - padding 层 0 会被补齐到输入数据的各边上。 默认： 0 output_padding (int or tuple, optional) – 输出的各维度要增加的大小。默认：0 groups (int, optional) – 输入通道与输出通道之间相互隔离的连接的个数。 默认：1 bias (bool, optional) – 如果被置为 True，向输出增加一个偏差量，此偏差是可学习参数。 默认：True dilation (int or tuple, optional) – 卷积核各元素之间的距离。 默认： 1 Shape: 输入: 输出: 其中 | Variables: | weight (Tensor) – 模块中的一个大小为 (in_channels, out_channels, kernel_size[0], kernel_size[1])的权重张量，这些权重可训练学习(learnable)。这些权重的初始值的采样空间是，其中 。 bias (Tensor) – 模块的偏差项，大小为 (out_channels)， 如果构造函数中的 bias 被置为 True，那么这些权重的初始值的采样空间是 ，其中 。 例子: >>> # With square kernels and equal stride >>> m = nn.ConvTranspose2d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) >>> input = torch.randn(20, 16, 50, 100) >>> output = m(input) >>> # exact output size can be also specified as an argument >>> input = torch.randn(1, 16, 12, 12) >>> downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1) >>> upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1) >>> h = downsample(input) >>> h.size() torch.Size([1, 16, 6, 6]) >>> output = upsample(h, output_size=input.size()) >>> output.size() torch.Size([1, 16, 12, 12]) ConvTranspose3d class torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1) 利用指定大小的三维转置卷积核对输入的多通道三维输入信号进行转置卷积（当然此卷积也是互相关操作，cross-correlation）操作的模块。转置卷积的操作本质是将各通道输入与卷积核做乘法，然后返回各通道与此卷积核乘积结果之和（卷积的定义）。 该模块可以看作是Conv3d相对于其输入的梯度(the gradient of Conv3d with respect to its input， 直译)， 转置卷积又被称为小数步长卷积或是反卷积（尽管这不是一个真正意义上的反卷积）。 stride 控制了转置卷积操作的步长 padding 控制了要在输入的各维度的各边上补齐0的层数，与Conv1d不同的地方，此padding参数与实际补齐0的层数的关系为层数 = kernel_size - 1 - padding，详情请见下面的note。 output_padding 控制了转置卷积操作输出的各维度的长度增量，但注意这个参数不是说要往转置卷积的输出上pad 0，而是直接控制转置卷积的输出大小为根据此参数pad后的大小。更多的详情请见下面的note。 dilation 控制了卷积核中各点之间的空间距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 groups 控制了输入输出之间的连接（connections）的数量。in_channels 和 out_channels 必须能被 groups 整除。举个栗子， > * 当 groups=1, 此Conv1d层会使用一个卷积层进行所有输入到输出的卷积操作。 > * 当 groups=2, 此时Conv1d层会产生两个并列的卷积层。同时，输入通道被分为两半，两个卷积层分别处理一半的输入通道，同时各自产生一半的输出通道。最后这两个卷积层的输出会被concatenated一起，作为此Conv1d层的输出。 > * 当 groups= in_channels, 每个输入通道都会被单独的一组卷积层处理，这个组的大小是。 kernel_size, stride, padding, output_padding 这几个参数均支持一下输入形式： 一个 int 数字 – 三维维数据的深度，高和宽这两个维度都会采用这一个数字。 一个由三个int数字组成的tuple– 这种情况下，三维数据的深度这一维度会采用元组中的第一个int数字，高这一维度会采用元组中的第二个int数字，宽这一维度会采用第三个int数字。 Note 取决于你卷积核的大小，有些时候输入数据中某些列（最后几列）可能不会参与计算（比如列数整除卷积核大小有余数，而又没有padding，那最后的余数列一般不会参与卷积计算），这主要是因为pytorch中的互相关操作cross-correlation是保证计算正确的操作(valid operation)， 而不是满操作(full operation)。所以实际操作中，还是要亲尽量选择好合适的padding参数哦。 Note padding 参数控制了要在输入的各维度各边上补齐0的层数，与在Conv3d中不同的是，在转置卷积操作过程中，此padding参数与实际补齐0的层数的关系为层数 = kernel_size - 1 - padding， 这样设置的主要原因是当使用相同的参数构建Conv3d 和ConvTranspose3d模块的时候，这种设置能够实现两个模块有正好相反的输入输出的大小，即Conv3d的输出大小是其对应的ConvTranspose3d模块的输入大小，而ConvTranspose3d的输出大小又恰好是其对应的Conv3d模块的输入大小。然而，当stride > 1的时候，Conv3d 的一个输出大小可能会对应多个输入大小，上一个note中就详细的介绍了这种情况，这样的情况下要保持前面提到两种模块的输入输出保持反向一致，那就要用到 output_padding参数了，这个参数可以增加转置卷积输出的某一维度的大小，以此来达到前面提到的同参数构建的Conv3d 和ConvTranspose3d模块的输入输出方向一致。 但注意这个参数不是说要往转置卷积的输出上pad 0，而是直接控制转置卷积的输出各维度的大小为根据此参数pad后的大小。 Note 当程序的运行环境是使用了CuDNN的CUDA环境的时候，一些非确定性的算法（nondeterministic algorithm）可能会被采用以提高整个计算的性能。如果不想使用这些非确定性的算法，你可以通过设置torch.backends.cudnn.deterministic = True来让整个计算过程保持确定性（可能会损失一定的计算性能）。对于后端(background)，你可以看一下这一部分Reproducibility了解其相关信息。 Parameters: in_channels (int) – 输入通道的个数 out_channels (int) – 卷积操作输出通道的个数 kernel_size (int or tuple) – 卷积核大小 stride (int or tuple, optional) – 卷积操作的步长。 默认： 1 padding (int or tuple, optional) – kernel_size - 1 - padding 层 0 会被补齐到输入数据的各边上。 默认： 0 output_padding (int or tuple, optional) – 输出的各维度要增加的大小。默认：0 groups (int, optional) – 输入通道与输出通道之间相互隔离的连接的个数。 默认：1 bias (bool, optional) – 如果被置为 True，向输出增加一个偏差量，此偏差是可学习参数。 默认：True dilation (int or tuple, optional) – 卷积核各元素之间的距离。 默认： 1 Shape: 输入: 输出: 其中 | Variables: | weight (Tensor) – 模块中的一个大小为 (in_channels, out_channels, kernel_size[0], kernel_size[1], kernel_size[2])的权重张量，这些权重可训练学习(learnable)。这些权重的初始值的采样空间是，其中 。 bias (Tensor) – 模块的偏差项，大小为 (out_channels)， 如果构造函数中的 bias 被置为 True，那么这些权重的初始值的采样空间是 ，其中 。 例子: >>> # With square kernels and equal stride >>> m = nn.ConvTranspose3d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2)) >>> input = torch.randn(20, 16, 10, 50, 100) >>> output = m(input) Unfold class torch.nn.Unfold(kernel_size, dilation=1, padding=0, stride=1) 将一个batch的输入张量展开成由多个滑动局部块组成的形式。（im2col的扩展模块，起到基本类似im2col的作用） 以一个大小为的批次化(batched)输入张量为例，其中是batch的大小，是通道数量，代表了任意空间维度。那Unfold这个操作在此张量上的操作就是，将这个张量展开成由多个kernel_size大小的滑动块组成的大小为的三维张量，其中是每个块中数的个数（每个块有个空间位置，每个空间位置存储一个通道大小为的向量），是块的个数： （这张图有问题啊，编辑整理的时候注意修正一下） 其中 是由上面例子中的input各空间维度组成的，遍历了各个空间维度。 因此，索引Fold操作的output的最后一个维度等价于索引某一个block，而索引操作的返回值是这个索引到的block中的所有值。 padding, stride 和 dilation 参数指明了滑动块的相关性质。 stride 控制了滑动块的步长。 padding 控制了在变形之前要向input的各维度各边上补齐的0的层数。 dilation 控制了卷积核中各点之间的空间距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 Parameters: kernel_size (int or tuple) – 滑动块的大小 stride (int or tuple, optional) – 滑动块在输入各维度上的步长。默认: 1 padding (int or tuple, optional) – 在输入各维度各边上补齐0的层数。 dilation (int or tuple, optional) – 控制了各元素之间的距离（没有指明元素具体指的是谁的元素，猜测是输出的）。默认：1 如果 kernel_size, dilation, padding 或者 stride的值是一个int，或是一个长度为1的int元组，在相关操作的时候各个空间维度上都会使用这同一个值。 如果输出向量有两个空间维度，那么此Fold操作有时又被称为im2col。 Note Fold在执行类col2im的操作的时候，主要是是通过集成此im（输出张量）分裂出所有对应位置的col（输入的滑动块）来复原原im。而Unfold则是通过从输入张量中不断拷贝数值到相应的block中来生成由滑动块组成的输出张量。所以，如果滑动块之间如果有数值重叠，那这些滑动块之间并不是互逆的。 Warning 目前，只有四维张量（比如批次化的图像张量）支持这个操作。 Shape: 输入: 输出: Examples: >>> unfold = nn.Unfold(kernel_size=(2, 3)) >>> input = torch.randn(2, 5, 3, 4) >>> output = unfold(input) >>> # each patch contains 30 values (2x3=6 vectors, each of 5 channels) >>> # 4 blocks (2x3 kernels) in total in the 3x4 input >>> output.size() torch.Size([2, 30, 4]) >>> # Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape) >>> inp = torch.randn(1, 3, 10, 12) >>> w = torch.randn(2, 3, 4, 5) >>> inp_unf = torch.nn.functional.unfold(inp, (4, 5)) >>> out_unf = inp_unf.transpose(1, 2).matmul(w.view(w.size(0), -1).t()).transpose(1, 2) >>> out = torch.nn.functional.fold(out_unf, (7, 8), (1, 1)) >>> # or equivalently (and avoiding a copy), >>> # out = out_unf.view(1, 2, 7, 8) >>> (torch.nn.functional.conv2d(inp, w) - out).abs().max() tensor(1.9073e-06) Fold class torch.nn.Fold(output_size, kernel_size, dilation=1, padding=0, stride=1) 将由滑动局部块组成的数组集合为一个大张量。(类col2im) 考虑一个包含了很多个滑动局部块的输入张量，比如，一批图像分割块(patches of images)的集合，大小为，其中是batch大小， 是一个块中的数值个数（每个块有个空间位置，每个空间位置存储一个通道大小为的向量），是滑动块的个数。（这些大小参数严格遵循了Unfold操作的输出向量的大小规定。）Fold操作通过求和重叠值的方式来将这些局部块集合为一个大小为的output张量。与 Unfold类似，这些参数必须满足： 其中遍历了各个空间维度。 output_size 描述了要生成的output的各空间维度的大小。有时，同样数量的滑动块，可能会产生多种input的形状，比如，当stride > 0的时候，这时候，设置output_size参数就会显得极为重要。 padding, stride 和 dilation 参数指明了滑动块的相关性质。 stride 控制了滑动块的步长。 padding 控制了在变形之前要向input的各维度各边上补齐的0的层数。 dilation 控制了卷积核中各点之间的空间距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 Parameters: output_size (int or tuple) – 输出向量的各空间维度的大小 (i.e., input.sizes()[2:]) kernel_size (int or tuple) – 滑动块的大小 stride (int or tuple, optional) – 滑动块在输入各维度上的步长。默认: 1 padding (int or tuple, optional) – 在输入各维度各边上补齐0的层数。 dilation (int or tuple, optional) – 控制了各元素之间的距离（没有指明元素具体指的是谁的元素，猜测是输出的）。默认：1 如果output_size， kernel_size, dilation, padding 或者 stride是一个int或者长度为1的int元组，在相关操作的时候各个空间维度上都会使用这同一个值。 如果此输出向量的空间维度数为2，那么此Fold操作有时又被称为col2im。 Note Fold在执行类col2im的操作的时候，主要是是通过集成此im（输出张量）分裂出所有对应位置的col（输入的滑动块）来复原原im。而Unfold则是通过从输入张量中不断拷贝数值到相应的block中来生成由滑动块组成的输出张量。所以，如果滑动块之间如果有数值重叠，那这些滑动块之间并不是互逆的。 Warning 目前，只有四维张量（比如批次化的图像张量）支持这个操作。 Shape: 输入: 输出: 举例: >>> fold = nn.Fold(output_size=(4, 5), kernel_size=(2, 2)) >>> input = torch.randn(1, 3 * 2 * 2, 1) >>> output = fold(input) >>> output.size() 卷积层部分Fold 与 Unfold 是1.0新增的内容，猜测其主要目的是开放col2im和im2col这两个通过矩阵乘法实现卷积操作的前序接口，要好好理解这部分可能要了解一下现在主流框架通过大矩阵乘法来实现卷积操作这一通用做法了，这一篇文章就介绍的很好[Implementing convolution as a matrix multiplication](https://buptldy.github.io/2016/10/01/2016-10-01-im2col/)，这一段如果感觉我的直译晦涩难懂，那我深感抱歉并建议看一下英文原版，虽然我觉得英文原版介绍的也是晦涩难懂 池化层（Pooling layers） MaxPool1d class torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) 对输入的多通道信号执行一维最大池化操作。 最简单的情况下，对于输入大小为 ，输出大小为的池化操作，此池化过程可表述如下： padding 参数控制了要在输入信号的各维度各边上要补齐0的层数。 dilation 参数控制了池化核中各元素之间的距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 Parameters: kernel_size – 最大池化操作的滑动窗大小 stride – 滑动窗的步长，默认值是 kernel_size padding – 要在输入信号的各维度各边上要补齐0的层数 dilation – 滑动窗中各元素之间的距离 return_indices – 如果此参数被设置为True， 那么此池化层在返回输出信号的同时还会返回一连串滑动窗最大值的索引位置，即每个滑动窗的最大值位置信息。这些信息可以在后面的上采样torch.nn.MaxUnpool1d中被用到。 ceil_mode – 如果此参数被设置为True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作 Shape: 输入: 输出: 其中 例子: >>> # pool of size=3, stride=2 >>> m = nn.MaxPool1d(3, stride=2) >>> input = torch.randn(20, 16, 50) >>> output = m(input) MaxPool2d class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) 对输入的多通道信号执行二维最大池化操作。 最简单的情况下，对于输入大小为 ，输出大小为，kernel_size为的池化操作，此池化过程可表述如下： padding 参数控制了要在输入信号的各维度各边上要补齐0的层数。 dilation 参数控制了池化核中各元素之间的距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 kernel_size, stride, padding, dilation 等参数均支持以下类型输入： 一个单独的 int – 此时这个int会同时控制池化滑动窗的宽和高这两个维度的大小 一个由两个int组成的tuple – 这种情况下，高这一维度会采用元组中的第一个int数字，宽这一维度会采用第二个int数字。 Parameters: kernel_size – 最大池化操作的滑动窗大小 stride – 滑动窗的步长，默认值是 kernel_size padding – 要在输入信号的各维度各边上要补齐0的层数 dilation – 滑动窗中各元素之间的距离 return_indices – 如果此参数被设置为True， 那么此池化层在返回输出信号的同时还会返回一连串滑动窗最大值的索引位置，即每个滑动窗的最大值位置信息。这些信息可以在后面的上采样torch.nn.MaxUnpool2d中被用到。 ceil_mode – 如果此参数被设置为True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作 Shape: 输入: 输出: , 其中 例子: >>> # pool of square window of size=3, stride=2 >>> m = nn.MaxPool2d(3, stride=2) >>> # pool of non-square window >>> m = nn.MaxPool2d((3, 2), stride=(2, 1)) >>> input = torch.randn(20, 16, 50, 32) >>> output = m(input) MaxPool3d class torch.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) 对输入的多通道信号执行三维最大池化操作。 最简单的情况下，对于输入大小为 ，输出大小为，kernel_size为 的池化操作，此池化过程可表述如下： padding 参数控制了要在输入信号的各维度各边上要补齐0的层数。 dilation 参数控制了池化核中各元素之间的距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 kernel_size, stride, padding, dilation 等参数均支持以下类型输入： 一个单独的 int – 此时这个int会同时控制池化滑动窗的深度，宽和高这三个维度的大小 一个由三个int组成的tuple – 这种情况下，深度这一维度会采用元组中的第一个int数字，高这一维度会采用元组中的第二个int数字，宽这一维度会采用第三个int数字。 Parameters: kernel_size – 最大池化操作的滑动窗大小 stride – 滑动窗的步长，默认值是 kernel_size padding – 要在输入信号的各维度各边上要补齐0的层数 dilation – 滑动窗中各元素之间的距离 return_indices – 如果此参数被设置为True， 那么此池化层在返回输出信号的同时还会返回一连串滑动窗最大值的索引位置，即每个滑动窗的最大值位置信息。这些信息可以在后面的上采样torch.nn.MaxUnpool3d中被用到。 ceil_mode – 如果此参数被设置为True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作 Shape: 输入: 输出: , 其中 例子: >>> # pool of square window of size=3, stride=2 >>> m = nn.MaxPool3d(3, stride=2) >>> # pool of non-square window >>> m = nn.MaxPool3d((3, 2, 2), stride=(2, 1, 2)) >>> input = torch.randn(20, 16, 50,44, 31) >>> output = m(input) MaxUnpool1d class torch.nn.MaxUnpool1d(kernel_size, stride=None, padding=0) MaxPool1d的逆过程，不过并不是完全的逆过程，因为在MaxPool1d的过程中，池化窗区域内的非最大值都已经丢失。 MaxUnpool1d的输入是MaxPool1d的输出，其中也包括包括滑动窗最大值的索引（即return_indices所控制的输出），逆池化操作的过程就是将MaxPool1d过程中产生的最大值插回到原来的位置，并将非最大值区域置为0。 Note MaxPool1d操作可以将多个大小不同的输入映射到相同的输出大小。因此，池化操作的反过程，MaxUnpool1d的上采样过程的输出大小就不唯一了。为了适应这一点，可以在设置控制上采样输出大小的（output_size）参数。 具体用法，请参阅下面的输入和示例 Parameters: kernel_size (int or tuple) – 最大池化窗的大小 stride (int or tuple) – 最大池化窗的步长。默认kernel_size padding (int or tuple) – 输入信号的各维度各边要补齐0的层数 Inputs: input: 要执行上采样操作的张量 indices: MaxPool1d池化过程中输出的池化窗最大值的位置索引 output_size (选填): 指定的输出大小 Shape: 输入: 输出: , 其中 也可以使用output_size指定输出的大小 例子: >>> pool = nn.MaxPool1d(2, stride=2, return_indices=True) >>> unpool = nn.MaxUnpool1d(2, stride=2) >>> input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]]) >>> output, indices = pool(input) >>> unpool(output, indices) tensor([[[ 0., 2., 0., 4., 0., 6., 0., 8.]]]) >>> # Example showcasing the use of output_size >>> input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8, 9]]]) >>> output, indices = pool(input) >>> unpool(output, indices, output_size=input.size()) tensor([[[ 0., 2., 0., 4., 0., 6., 0., 8., 0.]]]) >>> unpool(output, indices) tensor([[[ 0., 2., 0., 4., 0., 6., 0., 8.]]]) MaxUnpool2d class torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0) MaxPool2d的逆过程，不过并不是完全的逆过程，因为在MaxPool2d的过程中，池化窗区域内的非最大值都已经丢失。 MaxUnpool2d的输入是MaxPool2d的输出，其中也包括包括滑动窗最大值的索引（即return_indices所控制的输出），逆池化操作的过程就是将MaxPool2d过程中产生的最大值插回到原来的位置，并将非最大值区域置为0。 Note MaxPool2d操作可以将多个大小不同的输入映射到相同的输出大小。因此，池化操作的反过程，MaxUnpool2d的上采样过程的输出大小就不唯一了。为了适应这一点，可以在设置控制上采样输出大小的（output_size）参数。 具体用法，请参阅下面的输入和示例 Parameters: kernel_size (int or tuple) – 最大池化窗的大小 stride (int or tuple) – 最大池化窗的步长。默认kernel_size padding (int or tuple) – 输入信号的各维度各边要补齐0的层数 Inputs: input: 要执行上采样操作的张量 indices: MaxPool2d池化过程中输出的池化窗最大值的位置索引 output_size (选填): 指定的输出大小 Shape: 输入: 输出: , 其中 也可以使用output_size指定输出的大小 例子: >>> pool = nn.MaxPool2d(2, stride=2, return_indices=True) >>> unpool = nn.MaxUnpool2d(2, stride=2) >>> input = torch.tensor([[[[ 1., 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12], [13, 14, 15, 16]]]]) >>> output, indices = pool(input) >>> unpool(output, indices) tensor([[[[ 0., 0., 0., 0.], [ 0., 6., 0., 8.], [ 0., 0., 0., 0.], [ 0., 14., 0., 16.]]]]) >>> # specify a different output size than input size >>> unpool(output, indices, output_size=torch.Size([1, 1, 5, 5])) tensor([[[[ 0., 0., 0., 0., 0.], [ 6., 0., 8., 0., 0.], [ 0., 0., 0., 14., 0.], [ 16., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0.]]]]) MaxUnpool3d class torch.nn.MaxUnpool3d(kernel_size, stride=None, padding=0) MaxPool3d的逆过程，不过并不是完全的逆过程，因为在MaxPool3d的过程中，池化窗区域内的非最大值都已经丢失。 MaxUnpool3d的输入是MaxPool3d的输出，其中也包括包括滑动窗最大值的索引（即return_indices所控制的输出），逆池化操作的过程就是将MaxPool3d过程中产生的最大值插回到原来的位置，并将非最大值区域置为0。 Note MaxPool3d操作可以将多个大小不同的输入映射到相同的输出大小。因此，池化操作的反过程，MaxUnpool3d的上采样过程的输出大小就不唯一了。为了适应这一点，可以在设置控制上采样输出大小的（output_size）参数。 具体用法，请参阅下面的输入和示例 Parameters: kernel_size (int or tuple) – 最大池化窗的大小 stride (int or tuple) – 最大池化窗的步长。默认kernel_size padding (int or tuple) – 输入信号的各维度各边要补齐0的层数 Inputs: input: 要执行上采样操作的张量 indices: MaxPool3d池化过程中输出的池化窗最大值的位置索引 output_size (选填): 指定的输出大小 Shape: 输入: 输出: , 其中 也可以使用output_size指定输出的大小 例子: >>> # pool of square window of size=3, stride=2 >>> pool = nn.MaxPool3d(3, stride=2, return_indices=True) >>> unpool = nn.MaxUnpool3d(3, stride=2) >>> output, indices = pool(torch.randn(20, 16, 51, 33, 15)) >>> unpooled_output = unpool(output, indices) >>> unpooled_output.size() torch.Size([20, 16, 51, 33, 15]) AvgPool1d class torch.nn.AvgPool1d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) 对输入的多通道信号执行一维平均池化操作。 最简单的情况下，对于输入大小为 ，输出大小为，kernel_size为的池化操作，此池化过程可表述如下： padding 参数控制了要在输入信号的各维度各边上要补齐0的层数。 kernel_size, stride, padding, dilation 等参数均支持输入一个int或者由一个int组成的tuple。 Parameters: kernel_size – 平均池化操作的滑动窗大小 stride – 滑动窗的步长，默认值是 kernel_size padding – 要在输入信号的各维度各边上要补齐0的层数 ceil_mode – 如果此参数被设置为True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作 count_include_pad – 如果被设置为True, 那么在进行平均运算的时候也会将用于补齐的0加入运算。 Shape: 输入: 输出: , 其中 例子: >>> # pool with window of size=3, stride=2 >>> m = nn.AvgPool1d(3, stride=2) >>> m(torch.tensor([[[1.,2,3,4,5,6,7]]])) tensor([[[ 2., 4., 6.]]]) AvgPool2d class torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) 对输入的多通道信号执行二维平均池化操作。 最简单的情况下，对于输入大小为 ，输出大小为，kernel_size为的池化操作，此池化过程可表述如下： padding 参数控制了要在输入信号的各维度各边上要补齐0的层数。 kernel_size, stride, padding等参数均支持以下类型输入： 一个单独的 int – 此时这个int会同时控制池化滑动窗的宽和高这两个维度的大小 一个由两个int组成的tuple – 这种情况下，高这一维度会采用元组中的第一个int数字，宽这一维度会采用第二个int数字。 Parameters: kernel_size – 平均池化操作的滑动窗大小 stride – 滑动窗的步长，默认值是 kernel_size padding – 要在输入信号的各维度各边上要补齐0的层数 ceil_mode – 如果此参数被设置为True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作 count_include_pad – 如果被设置为True, 那么在进行平均运算的时候也会将用于补齐的0加入运算。 Shape: 输入: 输出: , 其中 例子: >>> # pool of square window of size=3, stride=2 >>> m = nn.AvgPool2d(3, stride=2) >>> # pool of non-square window >>> m = nn.AvgPool2d((3, 2), stride=(2, 1)) >>> input = torch.randn(20, 16, 50, 32) >>> output = m(input) AvgPool3d class torch.nn.AvgPool3d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) 对输入的多通道信号执行三维平均池化操作。 最简单的情况下，对于输入大小为，输出大小为，kernel_size为的池化操作，此池化过程可表述如下： padding 参数控制了要在输入信号的各维度各边上要补齐0的层数。 kernel_size, stride, padding等参数均支持以下类型输入： 一个单独的 int – 此时这个int会同时控制池化滑动窗的深度，宽和高这两个维度的大小 一个由三个int组成的tuple – 这种情况下，深度这一维度会采用元组中的第一个int数字，高这一维度会采用元组中的第二个int数字，宽这一维度会采用第三个int数字。 Parameters: kernel_size – 平均池化操作的滑动窗大小 stride – 滑动窗的步长，默认值是 kernel_size padding – 要在输入信号的各维度各边上要补齐0的层数 ceil_mode – 如果此参数被设置为True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作 count_include_pad – 如果被设置为True, 那么在进行平均运算的时候也会将用于补齐的0加入运算。 Shape: 输入: 输出: , 其中 例子: >>> # pool of square window of size=3, stride=2 >>> m = nn.AvgPool3d(3, stride=2) >>> # pool of non-square window >>> m = nn.AvgPool3d((3, 2, 2), stride=(2, 1, 2)) >>> input = torch.randn(20, 16, 50,44, 31) >>> output = m(input) FractionalMaxPool2d class torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None) 对输入的多通道信号执行小数级二维最大池化操作。小数级指的是此操作的输出大小与输入大小成指定的小数倍数关系。 Ben Graham的这篇文章Fractional MaxPooling中详细地介绍了小数级二维最大池化的基本思想和技术细节。 小数级二维最大池化的基本思想就是将最大池化操作应用于个由随机步长大小采集的区域中，这些步长大小是由输出目标的大小决定的。小数级二维最大池化的输出特征的数量等于输入通道的数量。 Parameters: kernel_size – 执行最大操作的窗口大小。支持的数据类型包括一个单独的数字k(生成一个大小为k x k的正方形kernal)，或者一个元组 (kh x kw) output_size – 池化输出目标大小，具体形式是 oH x oW。支持的数据类型包括一个单独的数字oH，或者一个元组 (oH, oW)，注意此处oH x oW与kernal_size中的kh x ow相呼应，两者成一定的小数级倍数关系 output_ratio – 如果想让输出目标的大小是输入目标大小的ratio倍，可以通过设置此参数来实现。此参数可以是一个小数数字或者小数元组，数字范围是(0, 1) return_indices – 如果此参数设置为True, 那么在池化操作结束后，返回池化输出结果的同时也会返回每个池化区域中，最大值的位置信息。这些信息在nn.MaxUnpool2d()可以被用到。此参数默认为False 例子 >>> # pool of square window of size=3, and target output size 13x12 >>> m = nn.FractionalMaxPool2d(3, output_size=(13, 12)) >>> # pool of square window and target output size being half of input image size >>> m = nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5)) >>> input = torch.randn(20, 16, 50, 32) >>> output = m(input) LPPool1d class torch.nn.LPPool1d(norm_type, kernel_size, stride=None, ceil_mode=False) 对输入的多通道信号执行一维幂平均池化操作。 对于每个池化窗口，此池化操作的计算方式如下： 当p为无穷大的时候时，等价于最大池化操作 当p=1时，等价于求和池化操作（一定程度上等价于平均池化） Note 如果某个特殊的输入导致这个输入关于幂指数p的求和是0，那上述池化函数在这一点是没有意义的。在实际实现过程中，此点的梯度被设置为0。 Parameters: kernel_size: 池化窗口的大小 stride：池化窗口移动的步长。默认值是kernel_size ceil_mode: 当此参数被设置为True时，在计算输出大小的时候将使用向下取整代替向上取整 Shape: 输入: 输出: ，其中 例子: >>> # power-2 pool of window of length 3, with stride 2. >>> m = nn.LPPool1d(2, 3, stride=2) >>> input = torch.randn(20, 16, 50) >>> output = m(input) LPPool2d class torch.nn.LPPool2d(norm_type, kernel_size, stride=None, ceil_mode=False) 对输入的多通道信号执行二维幂平均池化操作。 对于每个池化窗口，此池化操作的计算方式如下： 当p等于时候时，等价于最大池化操作 当p=1时，等价于求和池化操作（一定程度上等价于平均池化） 参数kernel_size, stride支持的数据类型： int，池化窗口的宽和高相等 tuple数组（两个数字的），第一个元素是池化窗口的高，第二个是宽 Note 如果某个特殊的输入导致这个输入关于幂指数p的求和是0，那上述池化函数在这一点是没有意义的。在实际实现过程中，此点的梯度被设置为0。 Parameters: kernel_size: 池化窗口的大小 stride：池化窗口移动的步长。默认值是kernel_size ceil_mode: 当此参数被设置为True时，在计算输出大小的时候将使用向下取整代替向上取整 Shape: 输入: 输出: , 其中 例子: >>> # power-2 pool of square window of size=3, stride=2 >>> m = nn.LPPool2d(2, 3, stride=2) >>> # pool of non-square window of power 1.2 >>> m = nn.LPPool2d(1.2, (3, 2), stride=(2, 1)) >>> input = torch.randn(20, 16, 50, 32) >>> output = m(input) AdaptiveMaxPool1d class torch.nn.AdaptiveMaxPool1d(output_size, return_indices=False) 对输入的多通道信号进行1维的自适应最大池化操作。 此池化层可以通过指定输出大小H，将任意输入大小的输入强行的池化到指定的输出大小。不过输入和输出特征的通道数不会变化。 Parameters: output_size – 指定的输出大小H return_indices – 如果此参数设置为True, 那么在池化操作结束后，返回池化输出结果的同时也会返回每个池化区域中，最大值的位置信息。这些信息在nn.MaxUnpool1d()可以被用到。此参数默认为False 例子 >>> # target output size of 5 >>> m = nn.AdaptiveMaxPool1d(5) >>> input = torch.randn(1, 64, 8) >>> output = m(input) AdaptiveMaxPool2d class torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False) 对输入的多通道信号进行2维的自适应最大池化操作。 此池化层可以通过指定输出大小H x W，将任意输入大小的输入强行的池化到指定的输出大小。不过输入和输出特征的通道数不会变化。 Parameters: output_size – 指定的输出大小H x W。此参数支持的数据类型可以是一个元组(H, W)，又或者是一个单独的int H（等价于H x H）。H 和 W这两个参数支持输入一个int又或者是None, None表示此输出维度的大小等价于输入数据此维度的大小 return_indices – 如果此参数设置为True, 那么在池化操作结束后，返回池化输出结果的同时也会返回每个池化区域中，最大值的位置信息。这些信息在nn.MaxUnpool2d()可以被用到。此参数默认为False 例子 >>> # target output size of 5x7 >>> m = nn.AdaptiveMaxPool2d((5,7)) >>> input = torch.randn(1, 64, 8, 9) >>> output = m(input) >>> # target output size of 7x7 (square) >>> m = nn.AdaptiveMaxPool2d(7) >>> input = torch.randn(1, 64, 10, 9) >>> output = m(input) >>> # target output size of 10x7 >>> m = nn.AdaptiveMaxPool2d((None, 7)) >>> input = torch.randn(1, 64, 10, 9) >>> output = m(input) AdaptiveMaxPool3d class torch.nn.AdaptiveMaxPool3d(output_size, return_indices=False) 对输入的多通道信号进行3维的自适应最大池化操作。 此池化层可以通过指定输出大小D x H x W，将任意输入大小的输入强行的池化到指定的输出大小。不过输入和输出特征的通道数不会变化。 Parameters: output_size – 指定的输出大小D x H x W。此参数支持的数据类型可以是一个元组(D, H, W)，又或者是一个单独的int D（等价于D x D x D)。D, H 和 W这三个参数支持输入一个int又或者是None, None表示此输出维度的大小等价于输入数据此维度的大小 return_indices – 如果此参数设置为True, 那么在池化操作结束后，返回池化输出结果的同时也会返回每个池化区域中，最大值的位置信息。这些信息在nn.MaxUnpool3d()可以被用到。此参数默认为False 例子 >>> # target output size of 5x7x9 >>> m = nn.AdaptiveMaxPool3d((5,7,9)) >>> input = torch.randn(1, 64, 8, 9, 10) >>> output = m(input) >>> # target output size of 7x7x7 (cube) >>> m = nn.AdaptiveMaxPool3d(7) >>> input = torch.randn(1, 64, 10, 9, 8) >>> output = m(input) >>> # target output size of 7x9x8 >>> m = nn.AdaptiveMaxPool3d((7, None, None)) >>> input = torch.randn(1, 64, 10, 9, 8) >>> output = m(input) AdaptiveAvgPool1d class torch.nn.AdaptiveAvgPool1d(output_size) 对输入的多通道信号进行1维的自适应平均池化操作。 此池化层可以通过指定输出大小H，将任意输入大小的输入强行的池化到指定的输出大小。不过输入和输出特征的通道数不会变化。 Parameters: output_size – 指定的输出大小H 例子 >>> # target output size of 5 >>> m = nn.AdaptiveAvgPool1d(5) >>> input = torch.randn(1, 64, 8) >>> output = m(input) AdaptiveAvgPool2d class torch.nn.AdaptiveAvgPool2d(output_size) 对输入的多通道信号进行2维的自适应平均池化操作。 此池化层可以通过指定输出大小H x W，将任意输入大小的输入强行的池化到指定的输出大小。不过输入和输出特征的通道数不会变化。 Parameters: output_size – 指定的输出大小H x W。此参数支持的数据类型可以是一个元组(H, W)，又或者是一个单独的int H（等价于H x H）。H 和 W这两个参数支持输入一个int又或者是None, None表示此输出维度的大小等价于输入数据此维度的大小 例子 >>> # target output size of 5x7 >>> m = nn.AdaptiveAvgPool2d((5,7)) >>> input = torch.randn(1, 64, 8, 9) >>> output = m(input) >>> # target output size of 7x7 (square) >>> m = nn.AdaptiveAvgPool2d(7) >>> input = torch.randn(1, 64, 10, 9) >>> output = m(input) >>> # target output size of 10x7 >>> m = nn.AdaptiveMaxPool2d((None, 7)) >>> input = torch.randn(1, 64, 10, 9) >>> output = m(input) AdaptiveAvgPool3d class torch.nn.AdaptiveAvgPool3d(output_size) 对输入的多通道信号进行3维的自适应平均池化操作。 此池化层可以通过指定输出大小D x H x W，将任意输入大小的输入强行的池化到指定的输出大小。不过输入和输出特征的通道数不会变化。 Parameters: output_size – 指定的输出大小D x H x W。此参数支持的数据类型可以是一个元组(D, H, W)，又或者是一个单独的int D（等价于D x D x D)。D, H 和 W这三个参数支持输入一个int又或者是None, None表示此输出维度的大小等价于输入数据此维度的大小 例子 >>> # target output size of 5x7x9 >>> m = nn.AdaptiveAvgPool3d((5,7,9)) >>> input = torch.randn(1, 64, 8, 9, 10) >>> output = m(input) >>> # target output size of 7x7x7 (cube) >>> m = nn.AdaptiveAvgPool3d(7) >>> input = torch.randn(1, 64, 10, 9, 8) >>> output = m(input) >>> # target output size of 7x9x8 >>> m = nn.AdaptiveMaxPool3d((7, None, None)) >>> input = torch.randn(1, 64, 10, 9, 8) >>> output = m(input) 填充层（Padding layers） ReflectionPad1d class torch.nn.ReflectionPad1d(padding) 以输入张量的各边界为轴，通过对输入张量数据的进行镜像复制的方式来对输入张量进行填充操作。 对于N维的填充操作，调用torch.nn.functional.pad()。 Parameters: padding (int, tuple) – 要填充的范围大小。如果输入数据是一个int, 那各个边界上都会填充同样大小的数据。如果是一个两个元素的元组，那么按照 (, )的大小设定来在各边上填充。 Shape: 输入: 输出: 其中 示例: >>> m = nn.ReflectionPad1d(2) >>> input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4) >>> input tensor([[[0., 1., 2., 3.], [4., 5., 6., 7.]]]) >>> m(input) tensor([[[2., 1., 0., 1., 2., 3., 2., 1.], [6., 5., 4., 5., 6., 7., 6., 5.]]]) >>> m(input) tensor([[[2., 1., 0., 1., 2., 3., 2., 1.], [6., 5., 4., 5., 6., 7., 6., 5.]]]) >>> # using different paddings for different sides >>> m = nn.ReflectionPad1d((3, 1)) >>> m(input) tensor([[[3., 2., 1., 0., 1., 2., 3., 2.], [7., 6., 5., 4., 5., 6., 7., 6.]]]) ReflectionPad2d class torch.nn.ReflectionPad2d(padding) 以输入张量的各边界为轴，通过对输入张量数据的进行镜像复制的方式来对输入张量进行填充操作。 对于N维的填充操作，调用torch.nn.functional.pad()。 Parameters: padding (int, tuple) – 要填充的范围大小。如果输入数据是一个int, 那各个边界上都会填充同样大小的数据。如果是一个四个元素的元组，那么按照(, , , )的大小设定来在各边上填充。 Shape: 输入: 输出: 其中 示例: >>> m = nn.ReflectionPad2d(2) >>> input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3) >>> input tensor([[[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]]]) >>> m(input) tensor([[[[8., 7., 6., 7., 8., 7., 6.], [5., 4., 3., 4., 5., 4., 3.], [2., 1., 0., 1., 2., 1., 0.], [5., 4., 3., 4., 5., 4., 3.], [8., 7., 6., 7., 8., 7., 6.], [5., 4., 3., 4., 5., 4., 3.], [2., 1., 0., 1., 2., 1., 0.]]]]) >>> # using different paddings for different sides >>> m = nn.ReflectionPad2d((1, 1, 2, 0)) >>> m(input) tensor([[[[7., 6., 7., 8., 7.], [4., 3., 4., 5., 4.], [1., 0., 1., 2., 1.], [4., 3., 4., 5., 4.], [7., 6., 7., 8., 7.]]]]) ReplicationPad1d class torch.nn.ReplicationPad1d(padding) 通过复制输入张量边界元素的方式对输入张量进行填充操作。 对于N维的填充操作，调用torch.nn.functional.pad()。 Parameters: padding (int, tuple) – 要填充的范围大小。如果输入数据是一个int, 那各个边界上都会填充同样大小的数据。如果是一个两个元素的元组，那么按照 (, )的大小设定来在各边上填充。 Shape: 输入: 输出: 其中 示例: >>> m = nn.ReplicationPad1d(2) >>> input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4) >>> input tensor([[[0., 1., 2., 3.], [4., 5., 6., 7.]]]) >>> m(input) tensor([[[0., 0., 0., 1., 2., 3., 3., 3.], [4., 4., 4., 5., 6., 7., 7., 7.]]]) >>> # using different paddings for different sides >>> m = nn.ReplicationPad1d((3, 1)) >>> m(input) tensor([[[0., 0., 0., 0., 1., 2., 3., 3.], [4., 4., 4., 4., 5., 6., 7., 7.]]]) ReplicationPad2d class torch.nn.ReplicationPad2d(padding) 通过复制输入张量边界元素的方式对输入张量进行填充操作。 对于N维的填充操作，调用torch.nn.functional.pad()。 Parameters: padding (int, tuple) – 要填充的范围大小。如果输入数据是一个int, 那各个边界上都会填充同样大小的数据。如果是一个四个元素的元组，那么按照(, , , )的大小设定来在各边上填充。 Shape: 输入: 输出: 其中 示例: >>> m = nn.ReplicationPad2d(2) >>> input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3) >>> input tensor([[[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]]]) >>> m(input) tensor([[[[0., 0., 0., 1., 2., 2., 2.], [0., 0., 0., 1., 2., 2., 2.], [0., 0., 0., 1., 2., 2., 2.], [3., 3., 3., 4., 5., 5., 5.], [6., 6., 6., 7., 8., 8., 8.], [6., 6., 6., 7., 8., 8., 8.], [6., 6., 6., 7., 8., 8., 8.]]]]) >>> # using different paddings for different sides >>> m = nn.ReplicationPad2d((1, 1, 2, 0)) >>> m(input) tensor([[[[0., 0., 1., 2., 2.], [0., 0., 1., 2., 2.], [0., 0., 1., 2., 2.], [3., 3., 4., 5., 5.], [6., 6., 7., 8., 8.]]]]) ReplicationPad3d class torch.nn.ReplicationPad3d(padding) 通过复制输入张量边界元素的方式对输入张量进行填充操作。 对于N维的填充操作，调用torch.nn.functional.pad()。 Parameters: padding (int, tuple) – 要填充的范围大小。如果输入数据是一个int, 那各个边界上都会填充同样大小的数据。如果是一个六个元素的元组，那么按照(, , , , , )的大小设定来在各边上填充。 Shape: 输入: 输出: 其中 示例: >>> m = nn.ReplicationPad3d(3) >>> input = torch.randn(16, 3, 8, 320, 480) >>> output = m(input) >>> # using different paddings for different sides >>> m = nn.ReplicationPad3d((3, 3, 6, 6, 1, 1)) >>> output = m(input) ZeroPad2d class torch.nn.ZeroPad2d(padding) 通过在各边上填充0的方式对输入张量进行填充操作。 对于N维的填充操作，调用torch.nn.functional.pad()。 Parameters: padding (int, tuple) – 要填充的范围大小。如果输入数据是一个int, 那各个边界上都会填充同样大小的数据。如果是一个四个元素的元组，那么按照(, , , )的大小设定来在各边上填充。 Shape: 输入: 输出: 其中 示例: >>> m = nn.ZeroPad2d(2) >>> input = torch.randn(1, 1, 3, 3) >>> input tensor([[[[-0.1678, -0.4418, 1.9466], [ 0.9604, -0.4219, -0.5241], [-0.9162, -0.5436, -0.6446]]]]) >>> m(input) tensor([[[[ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, -0.1678, -0.4418, 1.9466, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.9604, -0.4219, -0.5241, 0.0000, 0.0000], [ 0.0000, 0.0000, -0.9162, -0.5436, -0.6446, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]]) >>> # using different paddings for different sides >>> m = nn.ZeroPad2d((1, 1, 2, 0)) >>> m(input) tensor([[[[ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, -0.1678, -0.4418, 1.9466, 0.0000], [ 0.0000, 0.9604, -0.4219, -0.5241, 0.0000], [ 0.0000, -0.9162, -0.5436, -0.6446, 0.0000]]]]) ConstantPad1d class torch.nn.ConstantPad1d(padding, value) 通过在各边上填充固定数字的方式对输入张量进行填充操作 对于N维的填充操作，调用torch.nn.functional.pad()。 Parameters: padding (int, tuple) – 要填充的范围大小。如果输入数据是一个int, 那各个边界上都会填充同样大小的数据。如果是一个两个元素的元组，那么按照 (, )的大小设定来在各边上填充。 Shape: 输入: 输出: 其中 示例: >>> m = nn.ConstantPad1d(2, 3.5) >>> input = torch.randn(1, 2, 4) >>> input tensor([[[-1.0491, -0.7152, -0.0749, 0.8530], [-1.3287, 1.8966, 0.1466, -0.2771]]]) >>> m(input) tensor([[[ 3.5000, 3.5000, -1.0491, -0.7152, -0.0749, 0.8530, 3.5000, 3.5000], [ 3.5000, 3.5000, -1.3287, 1.8966, 0.1466, -0.2771, 3.5000, 3.5000]]]) >>> m = nn.ConstantPad1d(2, 3.5) >>> input = torch.randn(1, 2, 3) >>> input tensor([[[ 1.6616, 1.4523, -1.1255], [-3.6372, 0.1182, -1.8652]]]) >>> m(input) tensor([[[ 3.5000, 3.5000, 1.6616, 1.4523, -1.1255, 3.5000, 3.5000], [ 3.5000, 3.5000, -3.6372, 0.1182, -1.8652, 3.5000, 3.5000]]]) >>> # using different paddings for different sides >>> m = nn.ConstantPad1d((3, 1), 3.5) >>> m(input) tensor([[[ 3.5000, 3.5000, 3.5000, 1.6616, 1.4523, -1.1255, 3.5000], [ 3.5000, 3.5000, 3.5000, -3.6372, 0.1182, -1.8652, 3.5000]]]) ConstantPad2d class torch.nn.ConstantPad2d(padding, value) 通过在各边上填充固定数字的方式对输入张量进行填充操作 对于N维的填充操作，调用torch.nn.functional.pad()。 Parameters: padding (int, tuple) – 要填充的范围大小。如果输入数据是一个int, 那各个边界上都会填充同样大小的数据。如果是一个四个元素的元组，那么按照(, , , )的大小设定来在各边上填充。 Shape: 输入: 输出: 其中 示例: >>> m = nn.ConstantPad2d(2, 3.5) >>> input = torch.randn(1, 2, 2) >>> input tensor([[[ 1.6585, 0.4320], [-0.8701, -0.4649]]]) >>> m(input) tensor([[[ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], [ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], [ 3.5000, 3.5000, 1.6585, 0.4320, 3.5000, 3.5000], [ 3.5000, 3.5000, -0.8701, -0.4649, 3.5000, 3.5000], [ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], [ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]]]) >>> m(input) tensor([[[ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], [ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], [ 3.5000, 3.5000, 1.6585, 0.4320, 3.5000, 3.5000], [ 3.5000, 3.5000, -0.8701, -0.4649, 3.5000, 3.5000], [ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], [ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]]]) >>> # using different paddings for different sides >>> m = nn.ConstantPad2d((3, 0, 2, 1), 3.5) >>> m(input) tensor([[[ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], [ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], [ 3.5000, 3.5000, 3.5000, 1.6585, 0.4320], [ 3.5000, 3.5000, 3.5000, -0.8701, -0.4649], [ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]]]) ConstantPad3d class torch.nn.ConstantPad3d(padding, value) 通过在各边上填充固定数字的方式对输入张量进行填充操作 对于N维的填充操作，调用torch.nn.functional.pad()。 Parameters: padding (int, tuple) – 要填充的范围大小。如果输入数据是一个int, 那各个边界上都会填充同样大小的数据。如果是一个六个元素的元组，那么按照(, , , , , )的大小设定来在各边上填充。 Shape: 输入: 输出: 其中 示例: >>> m = nn.ConstantPad3d(3, 3.5) >>> input = torch.randn(16, 3, 10, 20, 30) >>> output = m(input) >>> # using different paddings for different sides >>> m = nn.ConstantPad3d((3, 3, 6, 6, 0, 1), 3.5) >>> output = m(input) 非线性激活(加权求和，非线性) ( Non-linear activations (weighted sum, nonlinearity) ) ELU class torch.nn.ELU(alpha=1.0, inplace=False) 将下面的元素级函数应用到输入张量上： Parameters: alpha – ELU操作的值。 默认： 1.0 inplace – 是否进行原位操作。 默认： False Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.ELU() >>> input = torch.randn(2) >>> output = m(input) Hardshrink class torch.nn.Hardshrink(lambd=0.5) 将下面的元素级hard shrinkage函数应用到输入张量上： Parameters: lambd – Hardshrink运算中的 值。 默认: 0.5 Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.Hardshrink() >>> input = torch.randn(2) >>> output = m(input) Hardtanh class torch.nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False, min_value=None, max_value=None) 将下面的元素级HardTanh函数应用到输入张量上。 HardTanh函数定义如下: 线性区域 的大小可以通过设置min_val 参数 max_val来进行调整。 参数: min_val – 线性区域的下限. 默认: -1 max_val – 线性区域的上限. 默认: 1 inplace – 是否进行原位操作。 默认： False 之前版本的min_value 和 max_value 参数已经被废弃掉了，改为min_val 和 max_val参数。 Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.Hardtanh(-2, 2) >>> input = torch.randn(2) >>> output = m(input) LeakyReLU class torch.nn.LeakyReLU(negative_slope=0.01, inplace=False) 将下面的元素级函数应用到输入张量上： 或 Parameters: negative_slope – 控制负数范围函数的斜率。 默认: 1e-2 inplace – 是否进行原位操作。 默认： False Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.LeakyReLU(0.1) >>> input = torch.randn(2) >>> output = m(input) LogSigmoid class torch.nn.LogSigmoid 将下面的元素级函数LogSigmoid应用到输入张量上：(此处漏了一张图，后期补一下) Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.LogSigmoid() >>> input = torch.randn(2) >>> output = m(input) PReLU class torch.nn.PReLU(num_parameters=1, init=0.25) 将下面的元素级函数应用到输入张量上： 或 此处 是一个可学习的参数。 如果在调用nn.PReLU()函数的时候没有传入参数，那么会默认在所有的输入通道上应用同一个 参数。 如果以nn.PReLU(nChannels)这种方式调用， 每个输入通道都会有一个单独的 参数。 Note 想要学一个好的参数，最好不要用weight decay。 Note 通道维度是输入张量的第二个维度。当输入张量的维度数 Parameters: num_parameters (int) – 要进行训练学习的 参数的数量。尽管此函数的输入是一个整形，但此函数要求输入的整形只能为两个值，1或者输入张量的通道数。默认：1 init (float) – 的初始值，默认: 0.25 Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 Variables: weight (Tensor) – 大小为num_parameters的可学习参数。The attr:dtype is default to（这句话有点问题， to后面漏掉了） 示例: >>> m = nn.PReLU() >>> input = torch.randn(2) >>> output = m(input) ReLU class torch.nn.ReLU(inplace=False) 将元素级线性整流函数函数应用到输入张量上 Applies the rectified linear unit function element-wise Parameters: inplace – 是否进行原位操作。 默认： False Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.ReLU() >>> input = torch.randn(2) >>> output = m(input) ReLU6 class torch.nn.ReLU6(inplace=False) 将下面的元素级函数应用到输入张量上: Parameters: inplace – 是否进行原位操作。 默认： False Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.ReLU6() >>> input = torch.randn(2) >>> output = m(input) RReLU class torch.nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=False) 将元素级随机线性整流函数函数应用到输入张量上，详情此文章： Empirical Evaluation of Rectified Activations in Convolutional Network. 此函数定义如下: 其中 是从此均匀分布中采样而来：. 详见: https://arxiv.org/pdf/1505.00853.pdf Parameters: lower – 均匀分布下限， 默认: upper – 均匀分布上限，默认: inplace – 是否进行原位操作。 默认： False Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.RReLU(0.1, 0.3) >>> input = torch.randn(2) >>> output = m(input) SELU class torch.nn.SELU(inplace=False) 将下面的元素级函数应用到输入张量上: 其中 而且 . More details can be found in the paper Self-Normalizing Neural Networks . Parameters: inplace (bool, optional) – 是否进行原位操作。 默认： False Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.SELU() >>> input = torch.randn(2) >>> output = m(input) CELU class torch.nn.CELU(alpha=1.0, inplace=False) 将下面的元素级函数应用到输入张量上： 更多细节请见paper: Continuously Differentiable Exponential Linear Units . Parameters: alpha – CELU操作中的 值，默认: 1.0 inplace – 是否进行原位操作。 默认： False Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.CELU() >>> input = torch.randn(2) >>> output = m(input) Sigmoid class torch.nn.Sigmoid 将下面的元素级函数应用到输入张量上： Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.Sigmoid() >>> input = torch.randn(2) >>> output = m(input) Softplus class torch.nn.Softplus(beta=1, threshold=20) 将下面的元素级函数应用到输入张量上: SoftPlus是一个平滑的类ReLU函数，可以用于将输出结果规范到全正。 为了数值稳定性，在实现此函数的过程中，当 x 超过某个特定值之后，我们会将此函数转化为一个线性函数。 Parameters: beta – Softplus操作的 值，默认: 1 threshold – 将函数转化为线性函数的阈值， 默认: 20 Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.Softplus() >>> input = torch.randn(2) >>> output = m(input) Softshrink class torch.nn.Softshrink(lambd=0.5) 将下面的元素级软收缩函数应用到输入张量上: Parameters: lambd – 软收缩运算的值，默认: 0.5 Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.Softshrink() >>> input = torch.randn(2) >>> output = m(input) Softsign class torch.nn.Softsign 将下面的元素级函数应用到输入张量上: Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.Softsign() >>> input = torch.randn(2) >>> output = m(input) Tanh class torch.nn.Tanh 将下面的元素级函数应用到输入张量上: Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.Tanh() >>> input = torch.randn(2) >>> output = m(input) Tanhshrink class torch.nn.Tanhshrink 将下面的元素级函数应用到输入张量上: Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.Tanhshrink() >>> input = torch.randn(2) >>> output = m(input) Threshold class torch.nn.Threshold(threshold, value, inplace=False) 使用阈值过滤输入张量的每个元素 阈值被定义如下： Parameters: threshold – 阈值大小 value – 小于阈值的元素的替换值 inplace – 是否进行原位操作。 默认： False Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.Threshold(0.1, 20) >>> input = torch.randn(2) >>> output = m(input) Non-linear activations (other) Softmin class torch.nn.Softmin(dim=None) Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range (0, 1) and sum to 1 Shape: Input: any shape Output: same as input Parameters: dim (int) – A dimension along which Softmin will be computed (so every slice along dim will sum to 1). Returns: a Tensor of the same dimension and shape as the input, with values in the range [0, 1] --- --- Examples: >>> m = nn.Softmin() >>> input = torch.randn(2, 3) >>> output = m(input) Softmax class torch.nn.Softmax(dim=None) Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range (0,1) and sum to 1 Softmax is defined as: Shape: Input: any shape Output: same as input Returns: a Tensor of the same dimension and shape as the input with values in the range [0, 1] Parameters: dim (int) – A dimension along which Softmax will be computed (so every slice along dim will sum to 1). --- --- Note This module doesn’t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use LogSoftmax instead (it’s faster and has better numerical properties). Examples: >>> m = nn.Softmax() >>> input = torch.randn(2, 3) >>> output = m(input) Softmax2d class torch.nn.Softmax2d Applies SoftMax over features to each spatial location. When given an image of Channels x Height x Width, it will apply Softmax to each location Shape: Input: Output: (same shape as input) Returns: a Tensor of the same dimension and shape as the input with values in the range [0, 1] Examples: >>> m = nn.Softmax2d() >>> # you softmax over the 2nd dimension >>> input = torch.randn(2, 3, 12, 13) >>> output = m(input) LogSoftmax class torch.nn.LogSoftmax(dim=None) Applies the function to an n-dimensional input Tensor. The LogSoftmax formulation can be simplified as: Shape: Input: any shape Output: same as input Parameters: dim (int) – A dimension along which Softmax will be computed (so every slice along dim will sum to 1). Returns: a Tensor of the same dimension and shape as the input with values in the range [-inf, 0) --- --- Examples: >>> m = nn.LogSoftmax() >>> input = torch.randn(2, 3) >>> output = m(input) AdaptiveLogSoftmaxWithLoss class torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, div_value=4.0, head_bias=False) Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Cissé, David Grangier, and Hervé Jégou. Adaptive softmax is an approximate strategy for training models with large output spaces. It is most effective when the label distribution is highly imbalanced, for example in natural language modelling, where the word frequency distribution approximately follows the Zipf’s law. Adaptive softmax partitions the labels into several clusters, according to their frequency. These clusters may contain different number of targets each. Additionally, clusters containing less frequent labels assign lower dimensional embeddings to those labels, which speeds up the computation. For each minibatch, only clusters for which at least one target is present are evaluated. The idea is that the clusters which are accessed frequently (like the first one, containing most frequent labels), should also be cheap to compute – that is, contain a small number of assigned labels. We highly recommend taking a look at the original paper for more details. cutoffs should be an ordered Sequence of integers sorted in the increasing order. It controls number of clusters and the partitioning of targets into clusters. For example setting cutoffs = [10, 100, 1000] means that first 10 targets will be assigned to the ‘head’ of the adaptive softmax, targets 11, 12, …, 100 will be assigned to the first cluster, and targets 101, 102, …, 1000 will be assigned to the second cluster, while targets 1001, 1002, …, n_classes - 1 will be assigned to the last, third cluster div_value is used to compute the size of each additional cluster, which is given as , where is the cluster index (with clusters for less frequent words having larger indices, and indices starting from ). head_bias if set to True, adds a bias term to the ‘head’ of the adaptive softmax. See paper for details. Set to False in the official implementation. Warning Labels passed as inputs to this module should be sorted accoridng to their frequency. This means that the most frequent label should be represented by the index 0, and the least frequent label should be represented by the index n_classes - 1. Note This module returns a NamedTuple with output and loss fields. See further documentation for details. Note To compute log-probabilities for all classes, the log_prob method can be used. Parameters: in_features (int) – Number of features in the input tensor n_classes (int) – Number of classes in the dataset. cutoffs (Sequence) – Cutoffs used to assign targets to their buckets. div_value (float, optional) – value used as an exponent to compute sizes of the clusters. Default: 4.0 | Returns: | output is a Tensor of size N containing computed target log probabilities for each example loss is a Scalar representing the computed negative log likelihood loss Return type: NamedTuple with output and loss fields Shape: input: target: where each value satisfies output: loss: Scalar log_prob(input) Computes log probabilities for all Parameters: input (Tensor) – a minibatch of examples Returns: log-probabilities of for each class in range , where is a parameter passed to AdaptiveLogSoftmaxWithLoss constructor. --- --- Shape: Input: Output: predict(input) This is equivalent to self.log_pob(input).argmax(dim=1), but is more efficient in some cases. Parameters: input (Tensor) – a minibatch of examples Returns: a class with the highest probability for each example --- --- Return type: output (Tensor) --- --- Shape: Input: Output: Normalization layers BatchNorm1d class torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift . The mean and standard-deviation are calculated per-dimension over the mini-batches and and are learnable parameter vectors of size C (where C is the input size). By default, the elements of are sampled from and the elements of are set to 0. Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well. Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is , where is the estimated statistic and is the new observed value. Because the Batch Normalization is done over the C dimension, computing statistics on (N, L) slices, it’s common terminology to call this Temporal Batch Normalization. Parameters: num_features – from an expected input of size or from input of size eps – a value added to the denominator for numerical stability. Default: 1e-5 momentum – the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1 affine – a boolean value that when set to True, this module has learnable affine parameters. Default: True track_running_stats – a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: True Shape: Input: or Output: or (same shape as input) Examples: >>> # With Learnable Parameters >>> m = nn.BatchNorm1d(100) >>> # Without Learnable Parameters >>> m = nn.BatchNorm1d(100, affine=False) >>> input = torch.randn(20, 100) >>> output = m(input) BatchNorm2d class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift . The mean and standard-deviation are calculated per-dimension over the mini-batches and and are learnable parameter vectors of size C (where C is the input size). By default, the elements of are sampled from and the elements of are set to 0. Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well. Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is , where is the estimated statistic and is the new observed value. Because the Batch Normalization is done over the C dimension, computing statistics on (N, H, W) slices, it’s common terminology to call this Spatial Batch Normalization. Parameters: num_features – from an expected input of size eps – a value added to the denominator for numerical stability. Default: 1e-5 momentum – the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1 affine – a boolean value that when set to True, this module has learnable affine parameters. Default: True track_running_stats – a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: True Shape: Input: Output: (same shape as input) Examples: >>> # With Learnable Parameters >>> m = nn.BatchNorm2d(100) >>> # Without Learnable Parameters >>> m = nn.BatchNorm2d(100, affine=False) >>> input = torch.randn(20, 100, 35, 45) >>> output = m(input) BatchNorm3d class torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift . The mean and standard-deviation are calculated per-dimension over the mini-batches and and are learnable parameter vectors of size C (where C is the input size). By default, the elements of are sampled from and the elements of are set to 0. Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well. Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is , where is the estimated statistic and is the new observed value. Because the Batch Normalization is done over the C dimension, computing statistics on (N, D, H, W) slices, it’s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization. Parameters: num_features – from an expected input of size eps – a value added to the denominator for numerical stability. Default: 1e-5 momentum – the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1 affine – a boolean value that when set to True, this module has learnable affine parameters. Default: True track_running_stats – a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: True Shape: Input: Output: (same shape as input) Examples: >>> # With Learnable Parameters >>> m = nn.BatchNorm3d(100) >>> # Without Learnable Parameters >>> m = nn.BatchNorm3d(100, affine=False) >>> input = torch.randn(20, 100, 35, 45, 10) >>> output = m(input) GroupNorm class torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True) Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization . The input channels are separated into num_groups groups, each containing num_channels / num_groups channels. The mean and standard-deviation are calculated separately over the each group. and are learnable per-channel affine transform parameter vectorss of size num_channels if affine is True. This layer uses statistics computed from input data in both training and evaluation modes. Parameters: num_groups (int) – number of groups to separate the channels into num_channels (int) – number of channels expected in input eps – a value added to the denominator for numerical stability. Default: 1e-5 affine – a boolean value that when set to True, this module has learnable per-channel affine parameters initialized to ones (for weights) and zeros (for biases). Default: True. Shape: Input: Output: (same shape as input) Examples: >>> input = torch.randn(20, 6, 10, 10) >>> # Separate 6 channels into 3 groups >>> m = nn.GroupNorm(3, 6) >>> # Separate 6 channels into 6 groups (equivalent with InstanceNorm) >>> m = nn.GroupNorm(6, 6) >>> # Put all 6 channels into a single group (equivalent with LayerNorm) >>> m = nn.GroupNorm(1, 6) >>> # Activating the module >>> output = m(input) InstanceNorm1d class torch.nn.InstanceNorm1d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False) Applies Instance Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization . The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. and are learnable parameter vectors of size C (where C is the input size) if affine is True. By default, this layer uses instance statistics computed from input data in both training and evaluation modes. If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is , where is the estimated statistic and is the new observed value. Note InstanceNorm1d and LayerNorm are very similar, but have some subtle differences. InstanceNorm1d is applied on each channel of channeled data like multidimensional time series, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionaly, LayerNorm applies elementwise affine transform, while InstanceNorm1d usually don’t apply affine transform. Parameters: num_features – from an expected input of size or from input of size eps – a value added to the denominator for numerical stability. Default: 1e-5 momentum – the value used for the running_mean and running_var computation. Default: 0.1 affine – a boolean value that when set to True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: False. track_running_stats – a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: False Shape: Input: Output: (same shape as input) Examples: >>> # Without Learnable Parameters >>> m = nn.InstanceNorm1d(100) >>> # With Learnable Parameters >>> m = nn.InstanceNorm1d(100, affine=True) >>> input = torch.randn(20, 100, 40) >>> output = m(input) InstanceNorm2d class torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False) Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization . The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. and are learnable parameter vectors of size C (where C is the input size) if affine is True. By default, this layer uses instance statistics computed from input data in both training and evaluation modes. If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is , where is the estimated statistic and is the new observed value. Note InstanceNorm2d and LayerNorm are very similar, but have some subtle differences. InstanceNorm2d is applied on each channel of channeled data like RGB images, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionaly, LayerNorm applies elementwise affine transform, while InstanceNorm2d usually don’t apply affine transform. Parameters: num_features – from an expected input of size eps – a value added to the denominator for numerical stability. Default: 1e-5 momentum – the value used for the running_mean and running_var computation. Default: 0.1 affine – a boolean value that when set to True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: False. track_running_stats – a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: False Shape: Input: Output: (same shape as input) Examples: >>> # Without Learnable Parameters >>> m = nn.InstanceNorm2d(100) >>> # With Learnable Parameters >>> m = nn.InstanceNorm2d(100, affine=True) >>> input = torch.randn(20, 100, 35, 45) >>> output = m(input) InstanceNorm3d class torch.nn.InstanceNorm3d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False) Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization . The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. and are learnable parameter vectors of size C (where C is the input size) if affine is True. By default, this layer uses instance statistics computed from input data in both training and evaluation modes. If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is , where is the estimated statistic and is the new observed value. Note InstanceNorm3d and LayerNorm are very similar, but have some subtle differences. InstanceNorm3d is applied on each channel of channeled data like 3D models with RGB color, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionaly, LayerNorm applies elementwise affine transform, while InstanceNorm3d usually don’t apply affine transform. Parameters: num_features – from an expected input of size eps – a value added to the denominator for numerical stability. Default: 1e-5 momentum – the value used for the running_mean and running_var computation. Default: 0.1 affine – a boolean value that when set to True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: False. track_running_stats – a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: False Shape: Input: Output: (same shape as input) Examples: >>> # Without Learnable Parameters >>> m = nn.InstanceNorm3d(100) >>> # With Learnable Parameters >>> m = nn.InstanceNorm3d(100, affine=True) >>> input = torch.randn(20, 100, 35, 45, 10) >>> output = m(input) LayerNorm class torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True) Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization . The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by normalized_shape. and are learnable affine transform parameters of normalized_shape if elementwise_affine is True. Note Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the affine option, Layer Normalization applies per-element scale and bias with elementwise_affine. This layer uses statistics computed from input data in both training and evaluation modes. Parameters: normalized_shape (int or list or torch.Size) – input shape from an expected input of size If a single integer is used, it is treated as a singleton list, and this module will normalize over the last dimension which is expected to be of that specific size. eps – a value added to the denominator for numerical stability. Default: 1e-5 elementwise_affine – a boolean value that when set to True, this module has learnable per-element affine parameters initialized to ones (for weights) and zeros (for biases). Default: True. Shape: Input: Output: (same shape as input) Examples: >>> input = torch.randn(20, 5, 10, 10) >>> # With Learnable Parameters >>> m = nn.LayerNorm(input.size()[1:]) >>> # Without Learnable Parameters >>> m = nn.LayerNorm(input.size()[1:], elementwise_affine=False) >>> # Normalize over last two dimensions >>> m = nn.LayerNorm([10, 10]) >>> # Normalize over last dimension of size 10 >>> m = nn.LayerNorm(10) >>> # Activating the module >>> output = m(input) LocalResponseNorm class torch.nn.LocalResponseNorm(size, alpha=0.0001, beta=0.75, k=1.0) Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels. Parameters: size – amount of neighbouring channels used for normalization alpha – multiplicative factor. Default: 0.0001 beta – exponent. Default: 0.75 k – additive factor. Default: 1 Shape: Input: Output: (same shape as input) Examples: >>> lrn = nn.LocalResponseNorm(2) >>> signal_2d = torch.randn(32, 5, 24, 24) >>> signal_4d = torch.randn(16, 5, 7, 7, 7, 7) >>> output_2d = lrn(signal_2d) >>> output_4d = lrn(signal_4d) Recurrent layers RNN class torch.nn.RNN(*args, **kwargs) Applies a multi-layer Elman RNN with or non-linearity to an input sequence. For each element in the input sequence, each layer computes the following function: where is the hidden state at time t, is the input at time t, and is the hidden state of the previous layer at time t-1 or the initial hidden state at time 0. If nonlinearity is ‘relu’, then ReLU is used instead of tanh. Parameters: input_size – The number of expected features in the input x hidden_size – The number of features in the hidden state h num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1 nonlinearity – The non-linearity to use. Can be either ‘tanh’ or ‘relu’. Default: ‘tanh’ bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True batch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False dropout – If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0 bidirectional – If True, becomes a bidirectional RNN. Default: False Inputs: input, h_0 input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details. h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1. Outputs: output, h_n output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_k) from the last layer of the RNN, for each k. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case. h_n (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for k = seq_len. Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size). | Variables: | weight_ih_l[k] – the learnable input-hidden weights of the k-th layer, of shape (hidden_size * input_size) for k = 0. Otherwise, the shape is (hidden_size * hidden_size) weight_hh_l[k] – the learnable hidden-hidden weights of the k-th layer, of shape (hidden_size * hidden_size) bias_ih_l[k] – the learnable input-hidden bias of the k-th layer, of shape (hidden_size) bias_hh_l[k] – the learnable hidden-hidden bias of the k-th layer, of shape (hidden_size) Note All the weights and biases are initialized from where Note If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance. Examples: >>> rnn = nn.RNN(10, 20, 2) >>> input = torch.randn(5, 3, 10) >>> h0 = torch.randn(2, 3, 20) >>> output, hn = rnn(input, h0) LSTM class torch.nn.LSTM(*args, **kwargs) Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence. For each element in the input sequence, each layer computes the following function: where is the hidden state at time t, is the cell state at time t, is the input at time t, is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and , , , are the input, forget, cell, and output gates, respectively. is the sigmoid function. In a multilayer LSTM, the input of the -th layer () is the hidden state of the previous layer multiplied by dropout where each is a Bernoulli random variable which is with probability dropout. Parameters: input_size – The number of expected features in the input x hidden_size – The number of features in the hidden state h num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1 bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True batch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False dropout – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0 bidirectional – If True, becomes a bidirectional LSTM. Default: False Inputs: input, (h_0, c_0) input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details. h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. If the RNN is bidirectional, num_directions should be 2, else it should be 1. c_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial cell state for each element in the batch. If (h_0, c_0) is not provided, both h_0 and c_0 default to zero. Outputs: output, (h_n, c_n) output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the LSTM, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case. h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len. Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size) and similarly for c_n. c_n (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for t = seq_len | Variables: | weight_ih_l[k] – the learnable input-hidden weights of the layer (W_ii&#124;W_if&#124;W_ig&#124;W_io), of shape (4*hidden_size x input_size) weight_hh_l[k] – the learnable hidden-hidden weights of the layer (W_hi&#124;W_hf&#124;W_hg&#124;W_ho), of shape (4*hidden_size x hidden_size) bias_ih_l[k] – the learnable input-hidden bias of the layer (b_ii&#124;b_if&#124;b_ig&#124;b_io), of shape (4*hidden_size) bias_hh_l[k] – the learnable hidden-hidden bias of the layer (b_hi&#124;b_hf&#124;b_hg&#124;b_ho), of shape (4*hidden_size) Note All the weights and biases are initialized from where Note If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance. Examples: >>> rnn = nn.LSTM(10, 20, 2) >>> input = torch.randn(5, 3, 10) >>> h0 = torch.randn(2, 3, 20) >>> c0 = torch.randn(2, 3, 20) >>> output, (hn, cn) = rnn(input, (h0, c0)) GRU class torch.nn.GRU(*args, **kwargs) Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence. For each element in the input sequence, each layer computes the following function: where is the hidden state at time t, is the input at time t, is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and , , are the reset, update, and new gates, respectively. is the sigmoid function. In a multilayer GRU, the input of the -th layer () is the hidden state of the previous layer multiplied by dropout where each is a Bernoulli random variable which is with probability dropout. Parameters: input_size – The number of expected features in the input x hidden_size – The number of features in the hidden state h num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1 bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True batch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False dropout – If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0 bidirectional – If True, becomes a bidirectional GRU. Default: False Inputs: input, h_0 input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() for details. h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1. Outputs: output, h_n output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case. h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size). | Variables: | weight_ih_l[k] – the learnable input-hidden weights of the layer (W_ir|W_iz|W_in), of shape (3*hidden_size x input_size) weight_hh_l[k] – the learnable hidden-hidden weights of the layer (W_hr|W_hz|W_hn), of shape (3*hidden_size x hidden_size) bias_ih_l[k] – the learnable input-hidden bias of the layer (b_ir|b_iz|b_in), of shape (3*hidden_size) bias_hh_l[k] – the learnable hidden-hidden bias of the layer (b_hr|b_hz|b_hn), of shape (3*hidden_size) Note All the weights and biases are initialized from where Note If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance. Examples: >>> rnn = nn.GRU(10, 20, 2) >>> input = torch.randn(5, 3, 10) >>> h0 = torch.randn(2, 3, 20) >>> output, hn = rnn(input, h0) RNNCell class torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh') An Elman RNN cell with tanh or ReLU non-linearity. If nonlinearity is ‘relu’, then ReLU is used in place of tanh. Parameters: input_size – The number of expected features in the input x hidden_size – The number of features in the hidden state h bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True nonlinearity – The non-linearity to use. Can be either ‘tanh’ or ‘relu’. Default: ‘tanh’ Inputs: input, hidden input of shape (batch, input_size): tensor containing input features hidden of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. Outputs: h’ h’ of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch | Variables: | weight_ih – the learnable input-hidden weights, of shape (hidden_size x input_size) weight_hh – the learnable hidden-hidden weights, of shape (hidden_size x hidden_size) bias_ih – the learnable input-hidden bias, of shape (hidden_size) bias_hh – the learnable hidden-hidden bias, of shape (hidden_size) Note All the weights and biases are initialized from where Examples: >>> rnn = nn.RNNCell(10, 20) >>> input = torch.randn(6, 3, 10) >>> hx = torch.randn(3, 20) >>> output = [] >>> for i in range(6): hx = rnn(input[i], hx) output.append(hx) LSTMCell class torch.nn.LSTMCell(input_size, hidden_size, bias=True) A long short-term memory (LSTM) cell. where is the sigmoid function. Parameters: input_size – The number of expected features in the input x hidden_size – The number of features in the hidden state h bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True Inputs: input, (h_0, c_0) input of shape (batch, input_size): tensor containing input features h_0 of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. c_0 of shape (batch, hidden_size): tensor containing the initial cell state for each element in the batch. If (h_0, c_0) is not provided, both h_0 and c_0 default to zero. Outputs: h_1, c_1 h_1 of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch c_1 of shape (batch, hidden_size): tensor containing the next cell state for each element in the batch | Variables: | weight_ih – the learnable input-hidden weights, of shape (4*hidden_size x input_size) weight_hh – the learnable hidden-hidden weights, of shape (4*hidden_size x hidden_size) bias_ih – the learnable input-hidden bias, of shape (4*hidden_size) bias_hh – the learnable hidden-hidden bias, of shape (4*hidden_size) Note All the weights and biases are initialized from where Examples: >>> rnn = nn.LSTMCell(10, 20) >>> input = torch.randn(6, 3, 10) >>> hx = torch.randn(3, 20) >>> cx = torch.randn(3, 20) >>> output = [] >>> for i in range(6): hx, cx = rnn(input[i], (hx, cx)) output.append(hx) GRUCell class torch.nn.GRUCell(input_size, hidden_size, bias=True) A gated recurrent unit (GRU) cell where is the sigmoid function. Parameters: input_size – The number of expected features in the input x hidden_size – The number of features in the hidden state h bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True Inputs: input, hidden input of shape (batch, input_size): tensor containing input features hidden of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. Outputs: h’ h’ of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch | Variables: | weight_ih – the learnable input-hidden weights, of shape (3*hidden_size x input_size) weight_hh – the learnable hidden-hidden weights, of shape (3*hidden_size x hidden_size) bias_ih – the learnable input-hidden bias, of shape (3*hidden_size) bias_hh – the learnable hidden-hidden bias, of shape (3*hidden_size) Note All the weights and biases are initialized from where Examples: >>> rnn = nn.GRUCell(10, 20) >>> input = torch.randn(6, 3, 10) >>> hx = torch.randn(3, 20) >>> output = [] >>> for i in range(6): hx = rnn(input[i], hx) output.append(hx) Linear layers Linear class torch.nn.Linear(in_features, out_features, bias=True) Applies a linear transformation to the incoming data: Parameters: in_features – size of each input sample out_features – size of each output sample bias – If set to False, the layer will not learn an additive bias. Default: True Shape: Input: where means any number of additional dimensions Output: where all but the last dimension are the same shape as the input. | Variables: | weight – the learnable weights of the module of shape . The values are initialized from , where bias – the learnable bias of the module of shape . If bias is True, the values are initialized from where Examples: >>> m = nn.Linear(20, 30) >>> input = torch.randn(128, 20) >>> output = m(input) >>> print(output.size()) torch.Size([128, 30]) Bilinear class torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True) Applies a bilinear transformation to the incoming data: Parameters: in1_features – size of each first input sample in2_features – size of each second input sample out_features – size of each output sample bias – If set to False, the layer will not learn an additive bias. Default: True Shape: Input: , where means any number of additional dimensions. All but the last dimension of the inputs should be the same. Output: where all but the last dimension are the same shape as the input. | Variables: | weight – the learnable weights of the module of shape . The values are initialized from , where bias – the learnable bias of the module of shape If bias is True, the values are initialized from , where Examples: >>> m = nn.Bilinear(20, 30, 40) >>> input1 = torch.randn(128, 20) >>> input2 = torch.randn(128, 30) >>> output = m(input1, input2) >>> print(output.size()) torch.Size([128, 40]) Dropout layers Dropout class torch.nn.Dropout(p=0.5, inplace=False) During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call. This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper Improving neural networks by preventing co-adaptation of feature detectors . Furthermore, the outputs are scaled by a factor of during training. This means that during evaluation the module simply computes an identity function. Parameters: p – probability of an element to be zeroed. Default: 0.5 inplace – If set to True, will do this operation in-place. Default: False Shape: Input: Any. Input can be of any shape Output: Same. Output is of the same shape as input Examples: >>> m = nn.Dropout(p=0.2) >>> input = torch.randn(20, 16) >>> output = m(input) Dropout2d class torch.nn.Dropout2d(p=0.5, inplace=False) Randomly zero out entire channels (a channel is a 2D feature map, e.g., the -th channel of the -th sample in the batched input is a 2D tensor ) of the input tensor). Each channel will be zeroed out independently on every forward call. with probability p using samples from a Bernoulli distribution. Usually the input comes from nn.Conv2d modules. As described in the paper Efficient Object Localization Using Convolutional Networks , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, nn.Dropout2d() will help promote independence between feature maps and should be used instead. Parameters: p (float, optional) – probability of an element to be zero-ed. inplace (bool, optional) – If set to True, will do this operation in-place Shape: Input: Output: (same shape as input) Examples: >>> m = nn.Dropout2d(p=0.2) >>> input = torch.randn(20, 16, 32, 32) >>> output = m(input) Dropout3d class torch.nn.Dropout3d(p=0.5, inplace=False) Randomly zero out entire channels (a channel is a 3D feature map, e.g., the -th channel of the -th sample in the batched input is a 3D tensor ) of the input tensor). Each channel will be zeroed out independently on every forward call. with probability p using samples from a Bernoulli distribution. Usually the input comes from nn.Conv3d modules. As described in the paper Efficient Object Localization Using Convolutional Networks , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, nn.Dropout3d() will help promote independence between feature maps and should be used instead. Parameters: p (float, optional) – probability of an element to be zeroed. inplace (bool, optional) – If set to True, will do this operation in-place Shape: Input: Output: (same shape as input) Examples: >>> m = nn.Dropout3d(p=0.2) >>> input = torch.randn(20, 16, 4, 32, 32) >>> output = m(input) AlphaDropout class torch.nn.AlphaDropout(p=0.5, inplace=False) Applies Alpha Dropout over the input. Alpha Dropout is a type of Dropout that maintains the self-normalizing property. For an input with zero mean and unit standard deviation, the output of Alpha Dropout maintains the original mean and standard deviation of the input. Alpha Dropout goes hand-in-hand with SELU activation function, which ensures that the outputs have zero mean and unit standard deviation. During training, it randomly masks some of the elements of the input tensor with probability p using samples from a bernoulli distribution. The elements to masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit standard deviation. During evaluation the module simply computes an identity function. More details can be found in the paper Self-Normalizing Neural Networks . Parameters: p (float) – probability of an element to be dropped. Default: 0.5 inplace (bool, optional) – If set to True, will do this operation in-place Shape: Input: Any. Input can be of any shape Output: Same. Output is of the same shape as input Examples: >>> m = nn.AlphaDropout(p=0.2) >>> input = torch.randn(20, 16) >>> output = m(input) Sparse layers Embedding class torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None) A simple lookup table that stores embeddings of a fixed dictionary and size. This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings. Parameters: num_embeddings (int) – size of the dictionary of embeddings embedding_dim (int) – the size of each embedding vector padding_idx (int, optional) – If given, pads the output with the embedding vector at padding_idx (initialized to zeros) whenever it encounters the index. max_norm (float, optional) – If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm. norm_type (float, optional) – The p of the p-norm to compute for the max_norm option. Default 2. scale_grad_by_freq (boolean__, optional) – If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False. sparse (bool, optional) – If True, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients. Variables: weight (Tensor) – the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from Shape: Input: LongTensor of arbitrary shape containing the indices to extract Output: (*, embedding_dim), where * is the input shape Note Keep in mind that only a limited number of optimizers support sparse gradients: currently it’s optim.SGD (CUDA and CPU), optim.SparseAdam (CUDA and CPU) and optim.Adagrad (CPU) Note With padding_idx set, the embedding vector at padding_idx is initialized to all zeros. However, note that this vector can be modified afterwards, e.g., using a customized initialization method, and thus changing the vector used to pad the output. The gradient for this vector from Embedding is always zero. Examples: >>> # an Embedding module containing 10 tensors of size 3 >>> embedding = nn.Embedding(10, 3) >>> # a batch of 2 samples of 4 indices each >>> input = torch.LongTensor([[1,2,4,5],[4,3,2,9]]) >>> embedding(input) tensor([[[-0.0251, -1.6902, 0.7172], [-0.6431, 0.0748, 0.6969], [ 1.4970, 1.3448, -0.9685], [-0.3677, -2.7265, -0.1685]], [[ 1.4970, 1.3448, -0.9685], [ 0.4362, -0.4004, 0.9400], [-0.6431, 0.0748, 0.6969], [ 0.9124, -2.3616, 1.1151]]]) >>> # example with padding_idx >>> embedding = nn.Embedding(10, 3, padding_idx=0) >>> input = torch.LongTensor([[0,2,0,5]]) >>> embedding(input) tensor([[[ 0.0000, 0.0000, 0.0000], [ 0.1535, -2.0309, 0.9315], [ 0.0000, 0.0000, 0.0000], [-0.1655, 0.9897, 0.0635]]]) classmethod from_pretrained(embeddings, freeze=True, sparse=False) Creates Embedding instance from given 2-dimensional FloatTensor. Parameters: embeddings (Tensor) – FloatTensor containing weights for the Embedding. First dimension is being passed to Embedding as ‘num_embeddings’, second as ‘embedding_dim’. freeze (boolean__, optional) – If True, the tensor does not get updated in the learning process. Equivalent to embedding.weight.requires_grad = False. Default: True sparse (bool, optional) – if True, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients. Examples: >>> # FloatTensor containing pretrained weights >>> weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]]) >>> embedding = nn.Embedding.from_pretrained(weight) >>> # Get embeddings for index 1 >>> input = torch.LongTensor([1]) >>> embedding(input) tensor([[ 4.0000, 5.1000, 6.3000]]) EmbeddingBag class torch.nn.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, mode='mean', sparse=False) Computes sums or means of ‘bags’ of embeddings, without instantiating the intermediate embeddings. For bags of constant length, this class with mode=\"sum\" is equivalent to Embedding followed by torch.sum(dim=1), with mode=\"mean\" is equivalent to Embedding followed by torch.mean(dim=1), with mode=\"max\" is equivalent to Embedding followed by torch.max(dim=1). However, EmbeddingBag is much more time and memory efficient than using a chain of these operations. Parameters: num_embeddings (int) – size of the dictionary of embeddings embedding_dim (int) – the size of each embedding vector max_norm (float, optional) – If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm. norm_type (float, optional) – The p of the p-norm to compute for the max_norm option. Default 2. scale_grad_by_freq (boolean__, optional) – if given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False. Note: this option is not supported when mode=\"max\". mode (string__, optional) – \"sum\", \"mean\" or \"max\". Specifies the way to reduce the bag. Default: \"mean\" sparse (bool, optional) – if True, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients. Note: this option is not supported when mode=\"max\". Variables: weight (Tensor) – the learnable weights of the module of shape (num_embeddings x embedding_dim) initialized from . Inputs: input (LongTensor) and offsets (LongTensor, optional) If input is 2D of shape B x N, it will be treated as `B` bags (sequences) each of fixed length `N`, and this will return `B` values aggregated in a way depending on the `mode`. `offsets` is ignored and required to be `None` in this case. If input is 1D of shape N, it will be treated as a concatenation of multiple bags (sequences). `offsets` is required to be a 1D tensor containing the starting index positions of each bag in `input`. Therefore, for `offsets` of shape `B`, `input` will be viewed as having `B` bags. Empty bags (i.e., having 0-length) will have returned vectors filled by zeros. Output shape: B x embedding_dim Examples: >>> # an Embedding module containing 10 tensors of size 3 >>> embedding_sum = nn.EmbeddingBag(10, 3, mode='sum') >>> # a batch of 2 samples of 4 indices each >>> input = torch.LongTensor([1,2,4,5,4,3,2,9]) >>> offsets = torch.LongTensor([0,4]) >>> embedding_sum(input, offsets) tensor([[-0.8861, -5.4350, -0.0523], [ 1.1306, -2.5798, -1.0044]]) Distance functions CosineSimilarity class torch.nn.CosineSimilarity(dim=1, eps=1e-08) Returns cosine similarity between and , computed along dim. Parameters: dim (int, optional) – Dimension where cosine similarity is computed. Default: 1 eps (float, optional) – Small value to avoid division by zero. Default: 1e-8 Shape: Input1: where D is at position dim Input2: , same shape as the Input1 Output: Examples: >>> input1 = torch.randn(100, 128) >>> input2 = torch.randn(100, 128) >>> cos = nn.CosineSimilarity(dim=1, eps=1e-6) >>> output = cos(input1, input2) PairwiseDistance class torch.nn.PairwiseDistance(p=2.0, eps=1e-06, keepdim=False) Computes the batchwise pairwise distance between vectors , using the p-norm: Parameters: p (real) – the norm degree. Default: 2 eps (float, optional) – Small value to avoid division by zero. Default: 1e-6 keepdim (bool, optional) – Determines whether or not to keep the batch dimension. Default: False Shape: Input1: where D = vector dimension Input2: , same shape as the Input1 Output: . If keepdim is False, then . Examples: >>> pdist = nn.PairwiseDistance(p=2) >>> input1 = torch.randn(100, 128) >>> input2 = torch.randn(100, 128) >>> output = pdist(input1, input2) Loss functions L1Loss class torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean') Creates a criterion that measures the mean absolute error (MAE) between each element in the input x and target y. The loss can be described as: where is the batch size. If reduce is True, then: x and y are tensors of arbitrary shapes with a total of n elements each. The sum operation still operates over all the elements, and divides by n. The division by n can be avoided if one sets the constructor argument size_average=False. Parameters: size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: where * means, any number of additional dimensions Target: , same shape as the input Output: scalar. If reduce is False, then , same shape as the input Examples: >>> loss = nn.L1Loss() >>> input = torch.randn(3, 5, requires_grad=True) >>> target = torch.randn(3, 5) >>> output = loss(input, target) >>> output.backward() MSELoss class torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean') Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input x and target y. The loss can be described as: where is the batch size. If reduce is True, then: The sum operation still operates over all the elements, and divides by n. The division by n can be avoided if one sets size_average to False. To get a batch of losses, a loss per batch element, set reduce to False. These losses are not averaged and are not affected by size_average. Parameters: size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: where * means, any number of additional dimensions Target: , same shape as the input Examples: >>> loss = nn.MSELoss() >>> input = torch.randn(3, 5, requires_grad=True) >>> target = torch.randn(3, 5) >>> output = loss(input, target) >>> output.backward() CrossEntropyLoss class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class. It is useful when training a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set. The input is expected to contain scores for each class. input has to be a Tensor of size either or with for the K-dimensional case (described later). This criterion expects a class index (0 to C-1) as the target for each value of a 1D tensor of size minibatch The loss can be described as: or in the case of the weight argument being specified: The losses are averaged across observations for each minibatch. Can also be used for higher dimension inputs, such as 2D images, by providing an input of size with , where is the number of dimensions, and a target of appropriate shape (see below). Parameters: weight (Tensor, optional) – a manual rescaling weight given to each class. If given, has to be a Tensor of size C size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True ignore_index (int, optional) – Specifies a target value that is ignored and does not contribute to the input gradient. When size_average is True, the loss is averaged over non-ignored targets. reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: \\((N, C)\\) where C = number of classes, or with in the case of K-dimensional loss. Target: \\((N)\\) where each value is \\(0 \\leq \\text{targets}[i] \\leq C-1\\), or with in the case of K-dimensional loss. Output: scalar. If reduce is False, then the same size as the target: , or with in the case of K-dimensional loss. Examples: >>> loss = nn.CrossEntropyLoss() >>> input = torch.randn(3, 5, requires_grad=True) >>> target = torch.empty(3, dtype=torch.long).random_(5) >>> output = loss(input, target) >>> output.backward() CTCLoss class torch.nn.CTCLoss(blank=0, reduction='mean') The Connectionist Temporal Classification loss. Parameters: blank (int, optional) – blank label. Default . reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the output losses will be divided by the target lengths and then the mean over the batch is taken. Default: ‘mean’ Inputs: log_probs: Tensor of size \\((T, N, C)\\) where C = number of characters in alphabet including blank, T = input length, and N = batch size. The logarithmized probabilities of the outputs (e.g. obtained with torch.nn.functional.log_softmax()). targets: Tensor of size \\((N, S)\\) or (sum(target_lengths)). Targets (cannot be blank). In the second form, the targets are assumed to be concatenated. input_lengths: Tuple or tensor of size \\((N)\\). Lengths of the inputs (must each be ) target_lengths: Tuple or tensor of size \\((N)\\). Lengths of the targets Example: >>> ctc_loss = nn.CTCLoss() >>> log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_() >>> targets = torch.randint(1, 20, (16, 30), dtype=torch.long) >>> input_lengths = torch.full((16,), 50, dtype=torch.long) >>> target_lengths = torch.randint(10,30,(16,), dtype=torch.long) >>> loss = ctc_loss(log_probs, targets, input_lengths, target_lengths) >>> loss.backward() Reference: A. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks: https://www.cs.toronto.edu/~graves/icml_2006.pdf Note In order to use CuDNN, the following must be satisfied: targets must be in concatenated format, all input_lengths must be T. , target_lengths , the integer arguments must be of dtype torch.int32. The regular implementation uses the (more common in PyTorch) torch.long dtype. Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background. NLLLoss class torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') The negative log likelihood loss. It is useful to train a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set. The input given through a forward call is expected to contain log-probabilities of each class. input has to be a Tensor of size either or with for the K-dimensional case (described later). Obtaining log-probabilities in a neural network is easily achieved by adding a LogSoftmax layer in the last layer of your network. You may use CrossEntropyLoss instead, if you prefer not to add an extra layer. The target that this loss expects is a class index (0 to C-1, where C = number of classes) If reduce is False, the loss can be described as: where is the batch size. If reduce is True (default), then Can also be used for higher dimension inputs, such as 2D images, by providing an input of size with , where is the number of dimensions, and a target of appropriate shape (see below). In the case of images, it computes NLL loss per-pixel. Parameters: weight (Tensor, optional) – a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones. size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True ignore_index (int, optional) – Specifies a target value that is ignored and does not contribute to the input gradient. When size_average is True, the loss is averaged over non-ignored targets. reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: \\((N, C)\\) where C = number of classes, or with in the case of K-dimensional loss. Target: \\((N)\\) where each value is \\(0 \\leq \\text{targets}[i] \\leq C-1\\), or with in the case of K-dimensional loss. Output: scalar. If reduce is False, then the same size as the target: , or with in the case of K-dimensional loss. Examples: >>> m = nn.LogSoftmax() >>> loss = nn.NLLLoss() >>> # input is of size N x C = 3 x 5 >>> input = torch.randn(3, 5, requires_grad=True) >>> # each element in target has to have 0 >> target = torch.tensor([1, 0, 4]) >>> output = loss(m(input), target) >>> output.backward() >>> >>> >>> # 2D loss example (used, for example, with image inputs) >>> N, C = 5, 4 >>> loss = nn.NLLLoss() >>> # input is of size N x C x height x width >>> data = torch.randn(N, 16, 10, 10) >>> conv = nn.Conv2d(16, C, (3, 3)) >>> m = nn.LogSoftmax() >>> # each element in target has to have 0 >> target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C) >>> output = loss(m(conv(data)), target) >>> output.backward() PoissonNLLLoss class torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean') Negative log likelihood loss with Poisson distribution of target. The loss can be described as: The last term can be omitted or approximated with Stirling formula. The approximation is used for target values more than 1. For targets less or equal to 1 zeros are added to the loss. Parameters: log_input (bool, optional) – if True the loss is computed as , if False the loss is . full (bool, optional) – whether to compute full loss, i. e. to add the Stirling approximation term size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True eps (float, optional) – Small value to avoid evaluation of when log_input == False. Default: 1e-8 reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Examples: >>> loss = nn.PoissonNLLLoss() >>> log_input = torch.randn(5, 2, requires_grad=True) >>> target = torch.randn(5, 2) >>> output = loss(log_input, target) >>> output.backward() KLDivLoss class torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean') The Kullback-Leibler divergence Loss KL divergence is a useful distance measure for continuous distributions and is often useful when performing direct regression over the space of (discretely sampled) continuous output distributions. As with NLLLoss, the input given is expected to contain log-probabilities. However, unlike NLLLoss, input is not restricted to a 2D Tensor. The targets are given as probabilities (i.e. without taking the logarithm). This criterion expects a target Tensor of the same size as the input Tensor. The unreduced (i.e. with reduce set to False) loss can be described as: where the index spans all dimensions of input and has the same shape as input. If reduce is True (the default), then: In default reduction mode ‘mean’, the losses are averaged for each minibatch over observations as well as over dimensions. ‘batchmean’ mode gives the correct KL divergence where losses are averaged over batch dimension only. ‘mean’ mode’s behavior will be changed to the same as ‘batchmean’ in the next major release. Parameters: size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘batchmean’ | ‘sum’ | ‘mean’. ‘none’: no reduction will be applied. ‘batchmean’: the sum of the output will be divided by batchsize. ‘sum’: the output will be summed. ‘mean’: the output will be divided by the number of elements in the output. Default: ‘mean’ :param .. note:: size_average and reduce are in the process of being deprecated,: and in the meantime, specifying either of those two args will override reduction. :param .. note:: reduction=’mean’ doesn’t return the true kl divergence value, please use: reduction=’batchmean’ which aligns with KL math definition. In the next major release, ‘mean’ will be changed to be the same as ‘batchmean’. Shape: input: where * means, any number of additional dimensions target: , same shape as the input output: scalar by default. If reduce is False, then \\((N, *)\\), the same shape as the input BCELoss class torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean') Creates a criterion that measures the Binary Cross Entropy between the target and the output: The loss can be described as: where is the batch size. If reduce is True, then This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets y should be numbers between 0 and 1. Parameters: weight (Tensor, optional) – a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size “nbatch”. size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: where * means, any number of additional dimensions Target: , same shape as the input Output: scalar. If reduce is False, then (N, *), same shape as input. Examples: >>> m = nn.Sigmoid() >>> loss = nn.BCELoss() >>> input = torch.randn(3, requires_grad=True) >>> target = torch.empty(3).random_(2) >>> output = loss(m(input), target) >>> output.backward() BCEWithLogitsLoss class torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None) This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability. The loss can be described as: where is the batch size. If reduce is True, then This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets t[i] should be numbers between 0 and 1. It’s possible to trade off recall and precision by adding weights to positive examples. In this case the loss can be described as: where is the positive weight of class . increases the recall, increases the precision. For example, if a dataset contains 100 positive and 300 negative examples of a single class, then pos_weight for the class should be equal to . The loss would act as if the dataset contains positive examples. Parameters: weight (Tensor, optional) – a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size “nbatch”. size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ pos_weight – a weight of positive examples. Must be a vector with length equal to the number of classes. MarginRankingLoss class torch.nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean') Creates a criterion that measures the loss given inputs x1, x2, two 1D mini-batch Tensors, and a label 1D mini-batch tensor y with values (1 or -1). If y == 1 then it assumed the first input should be ranked higher (have a larger value) than the second input, and vice-versa for y == -1. The loss function for each sample in the mini-batch is: Parameters: margin (float, optional) – Has a default value of 0. size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: where N is the batch size and D is the size of a sample. Target: Output: scalar. If reduce is False, then (N). HingeEmbeddingLoss class torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean') Measures the loss given an input tensor x and a labels tensor y containing values (1 or -1). This is usually used for measuring whether two inputs are similar or dissimilar, e.g. using the L1 pairwise distance as x, and is typically used for learning nonlinear embeddings or semi-supervised learning. The loss function for -th sample in the mini-batch is and the total loss functions is where . Parameters: margin (float, optional) – Has a default value of 1. size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: Tensor of arbitrary shape. The sum operation operates over all the elements. Target: Same shape as input. Output: scalar. If reduce is False, then same shape as the input MultiLabelMarginLoss class torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction='mean') Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input x (a 2D mini-batch Tensor) and output y (which is a 2D Tensor of target class indices). For each sample in the mini-batch: where to , to , , and for all and . y and x must have the same size. The criterion only considers a contiguous block of non-negative targets that starts at the front. This allows for different samples to have variable amounts of target classes Parameters: size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: or where N is the batch size and C is the number of classes. Target: or , same shape as the input. Output: scalar. If reduce is False, then (N). SmoothL1Loss class torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean') Creates a criterion that uses a squared term if the absolute element-wise error falls below 1 and an L1 term otherwise. It is less sensitive to outliers than the MSELoss and in some cases prevents exploding gradients (e.g. see “Fast R-CNN” paper by Ross Girshick). Also known as the Huber loss: where is given by: x and y arbitrary shapes with a total of n elements each the sum operation still operates over all the elements, and divides by n. The division by n can be avoided if one sets size_average to False Parameters: size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: where * means, any number of additional dimensions Target: , same shape as the input Output: scalar. If reduce is False, then , same shape as the input SoftMarginLoss class torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction='mean') Creates a criterion that optimizes a two-class classification logistic loss between input tensor x and target tensor y (containing 1 or -1). Parameters: size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: Tensor of arbitrary shape. Target: Same shape as input. Output: scalar. If reduce is False, then same shape as the input MultiLabelSoftMarginLoss class torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=None, reduce=None, reduction='mean') Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input x and target y of size (N, C). For each sample in the minibatch: where i == 0 to x.nElement()-1, y[i] in {0,1}. Parameters: weight (Tensor, optional) – a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones. size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: where N is the batch size and C is the number of classes. Target: , same shape as the input. Output: scalar. If reduce is False, then (N). CosineEmbeddingLoss class torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean') Creates a criterion that measures the loss given input tensors , and a Tensor label y with values 1 or -1. This is used for measuring whether two inputs are similar or dissimilar, using the cosine distance, and is typically used for learning nonlinear embeddings or semi-supervised learning. The loss function for each sample is: Parameters: margin (float, optional) – Should be a number from -1 to 1, 0 to 0.5 is suggested. If margin is missing, the default value is 0. size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ MultiMarginLoss class torch.nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean') Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input x (a 2D mini-batch Tensor) and output y (which is a 1D tensor of target class indices, ): For each mini-batch sample, the loss in terms of the 1D input x and scalar output y is: where i == 0 to x.size(0) and . Optionally, you can give non-equal weighting on the classes by passing a 1D weight tensor into the constructor. The loss function then becomes: Parameters: p (int, optional) – Has a default value of 1. 1 and 2 are the only supported values margin (float, optional) – Has a default value of 1. weight (Tensor, optional) – a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones. size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ TripletMarginLoss class torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean') Creates a criterion that measures the triplet loss given an input tensors x1, x2, x3 and a margin with a value greater than 0. This is used for measuring a relative similarity between samples. A triplet is composed by a, p and n: anchor, positive examples and negative example respectively. The shapes of all input tensors should be . The distance swap is described in detail in the paper Learning shallow convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al. The loss function for each sample in the mini-batch is: where Parameters: margin (float, optional) – Default: 1. p (int, optional) – The norm degree for pairwise distance. Default: 2. swap (float, optional) – The distance swap is described in detail in the paper Learning shallow convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al. Default: False. size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: where D is the vector dimension. Output: scalar. If reduce is False, then (N). >>> triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2) >>> input1 = torch.randn(100, 128, requires_grad=True) >>> input2 = torch.randn(100, 128, requires_grad=True) >>> input3 = torch.randn(100, 128, requires_grad=True) >>> output = triplet_loss(input1, input2, input3) >>> output.backward() Vision layers PixelShuffle class torch.nn.PixelShuffle(upscale_factor) Rearranges elements in a tensor of shape to a tensor of shape . This is useful for implementing efficient sub-pixel convolution with a stride of . Look at the paper: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network by Shi et. al (2016) for more details. Parameters: upscale_factor (int) – factor to increase spatial resolution by Shape: Input: Output: Examples: >>> pixel_shuffle = nn.PixelShuffle(3) >>> input = torch.randn(1, 9, 4, 4) >>> output = pixel_shuffle(input) >>> print(output.size()) torch.Size([1, 1, 12, 12]) Upsample class torch.nn.Upsample(size=None, scale_factor=None, mode='nearest', align_corners=None) Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. The input data is assumed to be of the form minibatch x channels x [optional depth] x [optional height] x width. Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor. The algorithms available for upsampling are nearest neighbor and linear, bilinear and trilinear for 3D, 4D and 5D input Tensor, respectively. One can either give a scale_factor or the target output size to calculate the output size. (You cannot give both, as it is ambiguous) Parameters: size (tuple, optional) – a tuple of ints ([optional D_out], [optional H_out], W_out) output sizes scale_factor (int / tuple of python:ints__, optional) – the multiplier for the image height / width / depth mode (string__, optional) – the upsampling algorithm: one of nearest, linear, bilinear and trilinear. Default: nearest align_corners (bool, optional) – if True, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when mode is linear, bilinear, or trilinear. Default: False Shape: Input: , or Output: , or , where Warning With align_corners = True, the linearly interpolating modes (linear, bilinear, and trilinear) don’t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See below for concrete examples on how this affects the outputs. Note If you want downsampling/general resizing, you should use interpolate(). Examples: >>> input = torch.arange(1, 5).view(1, 1, 2, 2).float() >>> input tensor([[[[ 1., 2.], [ 3., 4.]]]]) >>> m = nn.Upsample(scale_factor=2, mode='nearest') >>> m(input) tensor([[[[ 1., 1., 2., 2.], [ 1., 1., 2., 2.], [ 3., 3., 4., 4.], [ 3., 3., 4., 4.]]]]) >>> m = nn.Upsample(scale_factor=2, mode='bilinear') # align_corners=False >>> m(input) tensor([[[[ 1.0000, 1.2500, 1.7500, 2.0000], [ 1.5000, 1.7500, 2.2500, 2.5000], [ 2.5000, 2.7500, 3.2500, 3.5000], [ 3.0000, 3.2500, 3.7500, 4.0000]]]]) >>> m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) >>> m(input) tensor([[[[ 1.0000, 1.3333, 1.6667, 2.0000], [ 1.6667, 2.0000, 2.3333, 2.6667], [ 2.3333, 2.6667, 3.0000, 3.3333], [ 3.0000, 3.3333, 3.6667, 4.0000]]]]) >>> # Try scaling the same data in a larger tensor >>> >>> input_3x3 = torch.zeros(3, 3).view(1, 1, 3, 3) >>> input_3x3[:, :, :2, :2].copy_(input) tensor([[[[ 1., 2.], [ 3., 4.]]]]) >>> input_3x3 tensor([[[[ 1., 2., 0.], [ 3., 4., 0.], [ 0., 0., 0.]]]]) >>> m = nn.Upsample(scale_factor=2, mode='bilinear') # align_corners=False >>> # Notice that values in top left corner are the same with the small input (except at boundary) >>> m(input_3x3) tensor([[[[ 1.0000, 1.2500, 1.7500, 1.5000, 0.5000, 0.0000], [ 1.5000, 1.7500, 2.2500, 1.8750, 0.6250, 0.0000], [ 2.5000, 2.7500, 3.2500, 2.6250, 0.8750, 0.0000], [ 2.2500, 2.4375, 2.8125, 2.2500, 0.7500, 0.0000], [ 0.7500, 0.8125, 0.9375, 0.7500, 0.2500, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]]) >>> m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) >>> # Notice that values in top left corner are now changed >>> m(input_3x3) tensor([[[[ 1.0000, 1.4000, 1.8000, 1.6000, 0.8000, 0.0000], [ 1.8000, 2.2000, 2.6000, 2.2400, 1.1200, 0.0000], [ 2.6000, 3.0000, 3.4000, 2.8800, 1.4400, 0.0000], [ 2.4000, 2.7200, 3.0400, 2.5600, 1.2800, 0.0000], [ 1.2000, 1.3600, 1.5200, 1.2800, 0.6400, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]]) UpsamplingNearest2d class torch.nn.UpsamplingNearest2d(size=None, scale_factor=None) Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. To specify the scale, it takes either the size or the scale_factor as it’s constructor argument. When size is given, it is the output size of the image (h, w). Parameters: size (tuple, optional) – a tuple of ints (H_out, W_out) output sizes scale_factor (int, optional) – the multiplier for the image height or width Warning This class is deprecated in favor of interpolate(). Shape: Input: Output: where Examples: >>> input = torch.arange(1, 5).view(1, 1, 2, 2) >>> input tensor([[[[ 1., 2.], [ 3., 4.]]]]) >>> m = nn.UpsamplingNearest2d(scale_factor=2) >>> m(input) tensor([[[[ 1., 1., 2., 2.], [ 1., 1., 2., 2.], [ 3., 3., 4., 4.], [ 3., 3., 4., 4.]]]]) UpsamplingBilinear2d class torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None) Applies a 2D bilinear upsampling to an input signal composed of several input channels. To specify the scale, it takes either the size or the scale_factor as it’s constructor argument. When size is given, it is the output size of the image (h, w). Parameters: size (tuple, optional) – a tuple of ints (H_out, W_out) output sizes scale_factor (int, optional) – the multiplier for the image height or width Warning This class is deprecated in favor of interpolate(). It is equivalent to nn.functional.interpolate(..., mode='bilinear', align_corners=True). Shape: Input: Output: where Examples: >>> input = torch.arange(1, 5).view(1, 1, 2, 2) >>> input tensor([[[[ 1., 2.], [ 3., 4.]]]]) >>> m = nn.UpsamplingBilinear2d(scale_factor=2) >>> m(input) tensor([[[[ 1.0000, 1.3333, 1.6667, 2.0000], [ 1.6667, 2.0000, 2.3333, 2.6667], [ 2.3333, 2.6667, 3.0000, 3.3333], [ 3.0000, 3.3333, 3.6667, 4.0000]]]]) DataParallel layers (multi-GPU, distributed) DataParallel class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0) Implements data parallelism at the module level. This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension (other objects will be copied once per device). In the forward pass, the module is replicated on each device, and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module. The batch size should be larger than the number of GPUs used. See also: Use nn.DataParallel instead of multiprocessing Arbitrary positional and keyword inputs are allowed to be passed into DataParallel EXCEPT Tensors. All tensors will be scattered on dim specified (default 0). Primitive types will be broadcasted, but all other types will be a shallow copy and can be corrupted if written to in the model’s forward pass. The parallelized module must have its parameters and buffers on device_ids[0] before running this DataParallel module. Warning In each forward, module is replicated on each device, so any updates to the runing module in forward will be lost. For example, if module has a counter attribute that is incremented in each forward, it will always stay at the initial value becasue the update is done on the replicas which are destroyed after forward. However, DataParallel guarantees that the replica on device[0] will have its parameters and buffers sharing storage with the base parallelized module. So in-place updates to the parameters or buffers on device[0] will be recorded. E.g., BatchNorm2d and spectral_norm() rely on this behavior to update the buffers. Warning Forward and backward hooks defined on module and its submodules will be invoked len(device_ids) times, each with inputs located on a particular device. Particularly, the hooks are only guaranteed to be executed in correct order with respect to operations on corresponding devices. For example, it is not guaranteed that hooks set via register_forward_pre_hook() be executed before all len(device_ids) forward() calls, but that each such hook be executed before the corresponding forward() call of that device. Warning When module returns a scalar (i.e., 0-dimensional tensor) in forward(), this wrapper will return a vector of length equal to number of devices used in data parallelism, containing the result from each device. Note There is a subtlety in using the pack sequence -&gt; recurrent network -&gt; unpack sequence pattern in a Module wrapped in DataParallel. See My recurrent network doesn’t work with data parallelism section in FAQ for details. Parameters: module (Module) – module to be parallelized device_ids (list of python:int or torch.device) – CUDA devices (default: all devices) output_device (int or torch.device) – device location of output (default: device_ids[0]) Variables: module (Module) – the module to be parallelized Example: >>> net = torch.nn.DataParallel(model, device_ids=[0, 1, 2]) >>> output = net(input_var) DistributedDataParallel class torch.nn.parallel.DistributedDataParallel(module, device_ids=None, output_device=None, dim=0, broadcast_buffers=True, process_group=None, bucket_cap_mb=25, check_reduction=False) Implements distributed data parallelism that is based on torch.distributed package at the module level. This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine and each device, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged. The batch size should be larger than the number of GPUs used locally. It should also be an integer multiple of the number of GPUs so that each chunk is the same size (so that each GPU processes the same number of samples). See also: Basics and Use nn.DataParallel instead of multiprocessing. The same constraints on input as in torch.nn.DataParallel apply. Creation of this class requires that torch.distributed to be already initialized, by calling torch.distributed.init_process_group() DistributedDataParallel can be used in the following two ways: Single-Process Multi-GPU In this case, a single process will be spawned on each host/node and each process will operate on all the GPUs of the node where it’s running. To use DistributedDataParallel in this way, you can simply construct the model as the following: >>> torch.distributed.init_process_group(backend=\"nccl\") >>> model = DistributedDataParallel(model) # device_ids will include all GPU devices be default Multi-Process Single-GPU This is the highly recommended way to use DistributedDataParallel, with multiple processes, each of which operates on a single GPU. This is currently the fastest approach to do data parallel training using PyTorch and applies to both single-node(multi-GPU) and multi-node data parallel training. It is proven to be significantly faster than torch.nn.DataParallel for single-node multi-GPU data parallel training. Here is how to use it: on each host with N GPUs, you should spawn up N processes, while ensuring that each process invidually works on a single GPU from 0 to N-1. Therefore, it is your job to ensure that your training script operates on a single given GPU by calling: >>> torch.cuda.set_device(i) where i is from 0 to N-1. In each process, you should refer the following to construct this module: >>> torch.distributed.init_process_group(backend='nccl', world_size=4, init_method='...') >>> model = DistributedDataParallel(model, device_ids=[i], output_device=i) In order to spawn up multiple processes per node, you can use either torch.distributed.launch or torch.multiprocessing.spawn Note nccl backend is currently the fastest and highly recommended backend to be used with Multi-Process Single-GPU distributed training and this applies to both single-node and multi-node distributed training Warning This module works only with the gloo and nccl backends. Warning Constructor, forward method, and differentiation of the output (or a function of the output of this module) is a distributed synchronization point. Take that into account in case different processes might be executing different code. Warning This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Same applies to buffers. Warning This module assumes all parameters are registered in the model of each distributed processes are in the same order. The module itself will conduct gradient all-reduction following the reverse order of the registered parameters of the model. In other wise, it is users’ responsibility to ensure that each distributed process has the exact same model and thus the exact parameter registeration order. Warning This module assumes all buffers and gradients are dense. Warning This module doesn’t work with torch.autograd.grad() (i.e. it will only work if gradients are to be accumulated in .grad attributes of parameters). Warning If you plan on using this module with a nccl backend or a gloo backend (that uses Infiniband), together with a DataLoader that uses multiple workers, please change the multiprocessing start method to forkserver (Python 3 only) or spawn. Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will likely experience deadlocks if you don’t change this setting. Warning Forward and backward hooks defined on module and its submodules won’t be invoked anymore, unless the hooks are initialized in the forward() method. Warning You should never try to change your model’s parameters after wrapping up your model with DistributedDataParallel. In other words, when wrapping up your model with DistributedDataParallel, the constructor of DistributedDataParallel will register the additional gradient reduction functions on all the parameters of the model itself at the time of construction. If you change the model’s parameters after the DistributedDataParallel construction, this is not supported and unexpected behaviors can happen, since some parameters’ gradient reduction functions might not get called. Note Parameters are never broadcast between processes. The module performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all processes in the same way. Buffers (e.g. BatchNorm stats) are broadcast from the module in process of rank 0, to all other replicas in the system in every iteration. Parameters: module (Module) – module to be parallelized device_ids (list of python:int or torch.device) – CUDA devices (default: all devices) output_device (int or torch.device) – device location of output (default: device_ids[0]) broadcast_buffers (bool) – flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function. (default: True) process_group – the process group to be used for distributed data all-reduction. If None, the default process group, which is created by torch.distributed.init_process_group, will be used. (default: None) bucket_cap_mb – DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB) (default: 25) check_reduction – when setting to True, it enables DistributedDataParallel to automatically check if the previous iteration’s backward reductions were successfully issued at the beginning of every iteration’s forward function. You normally don’t need this option enabled unless you are observing weird behaviors such as different ranks are getting different gradients, which should not happen if DistributedDataParallel is corrected used. (default: False) Variables: module (Module) – the module to be parallelized Example:: >>> torch.distributed.init_process_group(backend='nccl', world_size=4, init_method='...') >>> net = torch.nn.DistributedDataParallel(model, pg) DistributedDataParallelCPU class torch.nn.parallel.DistributedDataParallelCPU(module) Implements distributed data parallelism for CPU at the module level. This module supports the mpi and gloo backends. This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged. This module could be used in conjunction with the DistributedSampler, (see :class torch.utils.data.distributed.DistributedSampler) which will load a subset of the original datset for each node with the same batch size. So strong scaling should be configured like this: n = 1, batch size = 12 n = 2, batch size = 64 n = 4, batch size = 32 n = 8, batch size = 16 Creation of this class requires the distributed package to be already initialized in the process group mode (see torch.distributed.init_process_group()). Warning Constructor, forward method, and differentiation of the output (or a function of the output of this module) is a distributed synchronization point. Take that into account in case different node might be executing different code. Warning This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Warning This module assumes all gradients are dense. Warning This module doesn’t work with torch.autograd.grad() (i.e. it will only work if gradients are to be accumulated in .grad attributes of parameters). Warning Forward and backward hooks defined on module and its submodules won’t be invoked anymore, unless the hooks are initialized in the forward() method. Note Parameters are broadcast between nodes in the init() function. The module performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all nodes in the same way. Parameters: module – module to be parallelized Example: >>> torch.distributed.init_process_group(world_size=4, init_method='...') >>> net = torch.nn.DistributedDataParallelCPU(model) Utilities clipgrad_norm torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2) Clips gradient norm of an iterable of parameters. The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place. Parameters: parameters (Iterable__[Tensor] or Tensor) – an iterable of Tensors or a single Tensor that will have gradients normalized max_norm (float or int) – max norm of the gradients norm_type (float or int) – type of the used p-norm. Can be 'inf' for infinity norm. Returns: Total norm of the parameters (viewed as a single vector). clipgrad_value torch.nn.utils.clip_grad_value_(parameters, clip_value) Clips gradient of an iterable of parameters at specified value. Gradients are modified in-place. Parameters: parameters (Iterable__[Tensor] or Tensor) – an iterable of Tensors or a single Tensor that will have gradients normalized clip_value (float or int) – maximum allowed value of the gradients The gradients are clipped in the range [-clip_value, clip_value] parameters_to_vector torch.nn.utils.parameters_to_vector(parameters) Convert parameters to one vector Parameters: parameters (Iterable__[Tensor]) – an iterator of Tensors that are the parameters of a model. Returns: The parameters represented by a single vector --- --- vector_to_parameters torch.nn.utils.vector_to_parameters(vec, parameters) Convert one vector to the parameters Parameters: vec (Tensor) – a single vector represents the parameters of a model. parameters (Iterable__[Tensor]) – an iterator of Tensors that are the parameters of a model. weight_norm torch.nn.utils.weight_norm(module, name='weight', dim=0) Applies weight normalization to a parameter in the given module. Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This replaces the parameter specified by name (e.g. “weight”) with two parameters: one specifying the magnitude (e.g. “weight_g”) and one specifying the direction (e.g. “weight_v”). Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every forward() call. By default, with dim=0, the norm is computed independently per output channel/plane. To compute a norm over the entire weight tensor, use dim=None. See https://arxiv.org/abs/1602.07868 Parameters: module (nn.Module) – containing module name (str, optional) – name of weight parameter dim (int, optional) – dimension over which to compute the norm Returns: The original module with the weight norm hook Example: >>> m = weight_norm(nn.Linear(20, 40), name='weight') Linear (20 -> 40) >>> m.weight_g.size() torch.Size([40, 1]) >>> m.weight_v.size() torch.Size([40, 20]) remove_weight_norm torch.nn.utils.remove_weight_norm(module, name='weight') Removes the weight normalization reparameterization from a module. Parameters: module (nn.Module) – containing module name (str, optional) – name of weight parameter Example >>> m = weight_norm(nn.Linear(20, 40)) >>> remove_weight_norm(m) spectral_norm torch.nn.utils.spectral_norm(module, name='weight', n_power_iterations=1, eps=1e-12, dim=None) Applies spectral normalization to a parameter in the given module. Spectral normalization stabilizes the training of discriminators (critics) in Generaive Adversarial Networks (GANs) by rescaling the weight tensor with spectral norm of the weight matrix calculated using power iteration method. If the dimension of the weight tensor is greater than 2, it is reshaped to 2D in power iteration method to get spectral norm. This is implemented via a hook that calculates spectral norm and rescales weight before every forward() call. See Spectral Normalization for Generative Adversarial Networks . Parameters: module (nn.Module) – containing module name (str, optional) – name of weight parameter n_power_iterations (int, optional) – number of power iterations to calculate spectal norm eps (float, optional) – epsilon for numerical stability in calculating norms dim (int, optional) – dimension corresponding to number of outputs, the default is 0, except for modules that are instances of ConvTranspose1/2/3d, when it is 1 Returns: The original module with the spectal norm hook Example: >>> m = spectral_norm(nn.Linear(20, 40)) Linear (20 -> 40) >>> m.weight_u.size() torch.Size([20]) remove_spectral_norm torch.nn.utils.remove_spectral_norm(module, name='weight') Removes the spectral normalization reparameterization from a module. Parameters: module (nn.Module) – containing module name (str, optional) – name of weight parameter Example >>> m = spectral_norm(nn.Linear(40, 10)) >>> remove_spectral_norm(m) PackedSequence torch.nn.utils.rnn.PackedSequence(data, batch_sizes=None) Holds the data and list of batch_sizes of a packed sequence. All RNN modules accept packed sequences as inputs. Note Instances of this class should never be created manually. They are meant to be instantiated by functions like pack_padded_sequence(). Batch sizes represent the number elements at each sequence step in the batch, not the varying sequence lengths passed to pack_padded_sequence(). For instance, given data abc and x the PackedSequence would contain data axbc with batch_sizes=[2,1,1]. | Variables: | data (Tensor) – Tensor containing packed sequence batch_sizes (Tensor) – Tensor of integers holding information about the batch size at each sequence step pack_padded_sequence torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False) Packs a Tensor containing padded sequences of variable length. Input can be of size T x B x * where T is the length of the longest sequence (equal to lengths[0]), B is the batch size, and * is any number of dimensions (including 0). If batch_first is True B x T x * inputs are expected. The sequences should be sorted by length in a decreasing order, i.e. input[:,0] should be the longest sequence, and input[:,B-1] the shortest one. Note This function accepts any input that has at least two dimensions. You can apply it to pack the labels, and use the output of the RNN with them to compute the loss directly. A Tensor can be retrieved from a PackedSequence object by accessing its .data attribute. Parameters: input (Tensor) – padded batch of variable length sequences. lengths (Tensor) – list of sequences lengths of each batch element. batch_first (bool, optional) – if True, the input is expected in B x T x * format. Returns: a PackedSequence object pad_packed_sequence torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0, total_length=None) Pads a packed batch of variable length sequences. It is an inverse operation to pack_padded_sequence(). The returned Tensor’s data will be of size T x B x *, where T is the length of the longest sequence and B is the batch size. If batch_first is True, the data will be transposed into B x T x * format. Batch elements will be ordered decreasingly by their length. Note total_length is useful to implement the pack sequence -&gt; recurrent network -&gt; unpack sequence pattern in a Module wrapped in DataParallel. See this FAQ section for details. Parameters: sequence (PackedSequence) – batch to pad batch_first (bool, optional) – if True, the output will be in B x T x * format. padding_value (float, optional) – values for padded elements. total_length (int, optional) – if not None, the output will be padded to have length total_length. This method will throw ValueError if total_length is less than the max sequence length in sequence. Returns: Tuple of Tensor containing the padded sequence, and a Tensor containing the list of lengths of each sequence in the batch. pad_sequence torch.nn.utils.rnn.pad_sequence(sequences, batch_first=False, padding_value=0) Pad a list of variable length Tensors with zero pad_sequence stacks a list of Tensors along a new dimension, and pads them to equal length. For example, if the input is list of sequences with size L x * and if batch_first is False, and T x B x * otherwise. B is batch size. It is equal to the number of elements in sequences. T is length of the longest sequence. L is length of the sequence. * is any number of trailing dimensions, including none. Example >>> from torch.nn.utils.rnn import pad_sequence >>> a = torch.ones(25, 300) >>> b = torch.ones(22, 300) >>> c = torch.ones(15, 300) >>> pad_sequence([a, b, c]).size() torch.Size([25, 3, 300]) Note This function returns a Tensor of size T x B x * or B x T x * where T is the length of the longest sequence. This function assumes trailing dimensions and type of all the Tensors in sequences are same. Parameters: sequences (list[Tensor]) – list of variable length sequences. batch_first (bool, optional) – output will be in B x T x * if True, or in T x B x * otherwise padding_value (float, optional) – value for padded elements. Default: 0. Returns: Tensor of size T x B x * if batch_first is False. Tensor of size B x T x * otherwise pack_sequence torch.nn.utils.rnn.pack_sequence(sequences) Packs a list of variable length Tensors sequences should be a list of Tensors of size L x *, where L is the length of a sequence and * is any number of trailing dimensions, including zero. They should be sorted in the order of decreasing length. Example >>> from torch.nn.utils.rnn import pack_sequence >>> a = torch.tensor([1,2,3]) >>> b = torch.tensor([4,5]) >>> c = torch.tensor([6]) >>> pack_sequence([a, b, c]) PackedSequence(data=tensor([ 1, 4, 6, 2, 5, 3]), batch_sizes=tensor([ 3, 2, 1])) Parameters: sequences (list[Tensor]) – A list of sequences of decreasing length. Returns: a PackedSequence object --- --- 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"nn.functional.html":{"url":"nn.functional.html","title":"torch.nn.functional","keywords":"","body":"torch.nn.functional 译者：hijkzzz 卷积函数 conv1d torch.nn.functional.conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor 对由多个输入平面组成的输入信号进行一维卷积. 有关详细信息和输出形状, 请参见Conv1d. 注意 在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置torch.backends.cudn .deterministic = True来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 Reproducibility 了解背景. 参数: input – 输入张量, 形状为 ) weight – 卷积核, 形状为 ) bias – 可选的偏置, 形状为 ). 默认值: None stride – 卷积核的步幅, 可以是单个数字或一个元素元组(sW,). 默认值: 1 padding – 在输入的两边隐式加零. 可以是单个数字或一个元素元组(padW, ). 默认值: 0 dilation – 核元素之间的空洞. 可以是单个数字或单元素元组(dW,). 默认值: 1 groups – 将输入分组, 应该可以被组的数目整除. 默认值: 1 例子: >>> filters = torch.randn(33, 16, 3) >>> inputs = torch.randn(20, 16, 50) >>> F.conv1d(inputs, filters) conv2d torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor 对由多个输入平面组成的输入图像应用二维卷积. 有关详细信息和输出形状, 请参见Conv2d. 注意 在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置torch.backends.cudn .deterministic = True来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 Reproducibility 了解背景. 参数: input – 输入张量, 形状为 ) weight – 卷积核, 形状为 ) bias – 可选的偏置, 形状为 ). 默认值: None stride – 卷积核的步幅, 可以是单个数字或一个元素元组 (sH, sW). 默认值: 1 padding – 在输入的两边隐式加零. 可以是单个数字或一个元素元组 (padH, padW). 默认值: 0 dilation – 核元素之间的空洞. 可以是单个数字或单元素元组 (dH, dW). 默认值: 1 groups – 将输入分组, 应该可以被组的数目整除. 默认值: 1 例子: >>> # With square kernels and equal stride >>> filters = torch.randn(8,4,3,3) >>> inputs = torch.randn(1,4,5,5) >>> F.conv2d(inputs, filters, padding=1) conv3d torch.nn.functional.conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor 对由多个输入平面组成的输入图像应用三维卷积. 有关详细信息和输出形状, 请参见 Conv3d. 注意 在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置torch.backends.cudn .deterministic = True来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 Reproducibility 了解背景. 参数: input – 输入张量, 形状为 ) weight – 卷积核, 形状为 ) bias – 可选的偏置, 形状为 ). 默认值: None stride – 卷积核的步幅, 可以是单个数字或一个元素元组 (sT, sH, sW). 默认值: 1 padding – 在输入的两边隐式加零. 可以是单个数字或一个元素元组 (padT, padH, padW). 默认值: 0 dilation – 核元素之间的空洞. 可以是单个数字或单元素元组 (dT, dH, dW). 默认值: 1 groups – 将输入分组, 应该可以被组的数目整除. 默认值: 1 例子: >>> filters = torch.randn(33, 16, 3, 3, 3) >>> inputs = torch.randn(20, 16, 50, 10, 20) >>> F.conv3d(inputs, filters) conv_transpose1d torch.nn.functional.conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) → Tensor 对由多个输入平面组成的输入信号应用一维转置卷积操作, 有时也称为反卷积. 有关详细信息和输出形状, 请参见 ConvTranspose1d 注意 在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置torch.backends.cudn .deterministic = True来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 Reproducibility 了解背景. 参数: input – 输入张量, 形状为 ) weight – 卷积核, 形状为 ) bias – 可选的偏置, 形状为 ). 默认值: None stride – 卷积核的步幅, 可以是单个数字或一个元素元组 (sW,). 默认值: 1 padding – 输入中的每个维度的两边都将添加零填充kernel_size - 1 - padding. 可以是单个数字或元组 (padW,). 默认值: 0 output_padding – 添加到输出形状中每个维度的一侧的额外大小. 可以是单个数字或元组 (out_padW). 默认值: 0 groups – 将输入分组, 应该可以被组的数目整除. 默认值: 1 dilation – 核元素之间的空洞. 可以是单个数字或单元素元组 (dW,). 默认值: 1 例子: >>> inputs = torch.randn(20, 16, 50) >>> weights = torch.randn(16, 33, 5) >>> F.conv_transpose1d(inputs, weights) conv_transpose2d torch.nn.functional.conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) → Tensor 对由多个输入平面组成的输入图像应用二维转置卷积操作, 有时也称为反卷积. 有关详细信息和输出形状, 请参见 ConvTranspose2d. 注意 在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置torch.backends.cudn .deterministic = True来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 Reproducibility 了解背景. 参数: input – 输入张量, 形状为 ) weight – 卷积核, 形状为 ) bias –可选的偏置, 形状为 ). 默认值: None stride – 卷积核的步幅, 可以是单个数字或一个元素元组 (sH, sW). 默认值: 1 padding – 输入中的每个维度的两边都将添加零填充kernel_size - 1 - padding. 可以是单个数字或元组 (padH, padW). 默认值: 0 output_padding – 添加到输出形状中每个维度的一侧的额外大小. 可以是单个数字或元组 (out_padH, out_padW). 默认值: 0 groups – 将输入分组, 应该可以被组的数目整除. 默认值: 1 dilation – 核元素之间的空洞. 可以是单个数字或单元素元组 (dH, dW). 默认值: 1 例子: >>> # With square kernels and equal stride >>> inputs = torch.randn(1, 4, 5, 5) >>> weights = torch.randn(4, 8, 3, 3) >>> F.conv_transpose2d(inputs, weights, padding=1) conv_transpose3d torch.nn.functional.conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) → Tensor 对由多个输入平面组成的输入图像应用一个三维转置卷积操作, 有时也称为反卷积 有关详细信息和输出形状, 请参见 ConvTranspose3d. 注意 在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置torch.backends.cudn .deterministic = True来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 Reproducibility 了解背景. 参数: input – 输入张量, 形状为 ) weight – 卷积核, 形状为 ) bias –可选的偏置, 形状为 ). 默认值: None stride – 卷积核的步幅, 可以是单个数字或一个元素元组 (sT, sH, sW). 默认值: 1 padding – 输入中的每个维度的两边都将添加零填充kernel_size - 1 - padding. 可以是单个数字或元组 (padT, padH, padW). 默认值: 0 output_padding – 添加到输出形状中每个维度的一侧的额外大小. 可以是单个数字或元组 (out_padT, out_padH, out_padW). 默认值: 0 groups – 将输入分组, 应该可以被组的数目整除. 默认值: 1 dilation – 核元素之间的空洞. 可以是单个数字或单元素元组 (dT, dH, dW). 默认值: 1 例子: >>> inputs = torch.randn(20, 16, 50, 10, 20) >>> weights = torch.randn(16, 33, 3, 3, 3) >>> F.conv_transpose3d(inputs, weights) unfold torch.nn.functional.unfold(input, kernel_size, dilation=1, padding=0, stride=1) 从批量的输入张量中提取滑动局部块. 警告 目前, 仅支持四维（4D）的输入张量(批量的类似图像的张量). 细节请参阅 torch.nn.Unfold fold torch.nn.functional.fold(input, output_size, kernel_size, dilation=1, padding=0, stride=1) 将一组滑动局部块数组合成一个大的张量. 警告 目前, 仅支持四维（4D）的输入张量(批量的类似图像的张量). 细节请参阅 torch.nn.Fold 池化函数 avg_pool1d torch.nn.functional.avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) → Tensor 对由多个输入平面组成的输入信号应用一维平均池化. 有关详细信息和输出形状, 请参见 AvgPool1d. 参数: input – 输入张量, 形状为 ) kernel_size – 窗口的大小. 可以是单个数字或元组 ) stride – 窗户的步幅. 可以是单个数字或元组 (sW,). 默认值: kernel_size padding – 在输入的两边隐式加零. 可以是单个数字或一个元素元组 (padW,). 默认值: 0 ceil_mode – 如果 True, 将用 ceil 代替 floor计算输出形状. 默认值: False count_include_pad – 如果 True, 将在平均计算中包括零填充. 默认值: True 例子: >>> # pool of square window of size=3, stride=2 >>> input = torch.tensor([[[1,2,3,4,5,6,7]]]) >>> F.avg_pool1d(input, kernel_size=3, stride=2) tensor([[[ 2., 4., 6.]]]) avg_pool2d torch.nn.functional.avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) → Tensor 在 区域应用二维平均池化, 步幅为 . 输出特征的数量等于输入平面的数量. 有关详细信息和输出形状, 请参见 AvgPool2d. 参数: input – input tensor ) kernel_size – 池化区域的大小, 可以是一个数字或者元组 ) stride – 池化步幅, 可以是一个数字或者元组 (sH, sW). 默认值: kernel_size padding – 在输入的两边隐式加零. 可以是单个数字或一个元素元组 (padH, padW). 默认值: 0 ceil_mode – 如果 True, 将用 ceil 代替 floor计算输出形状. 默认值: False count_include_pad – 如果 True, 将在平均计算中包括零填充. 默认值: True avg_pool3d torch.nn.functional.avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) → Tensor 应 区域应用三维平均池化, 步幅为 . 输出特征的数量等于 . 有关详细信息和输出形状, 请参见 AvgPool3d. 参数: input – 输入张量 ) kernel_size – 池化区域的大小, 可以是一个数字或者元组 ) stride – 池化步幅, 可以是一个数字或者元组 (sT, sH, sW). 默认值: kernel_size padding – 在输入的两边隐式加零. 可以是单个数字或一个元素元组 (padT, padH, padW), 默认值: 0 ceil_mode – 如果 True, 将用 ceil 代替 floor计算输出形状. 默认值: False count_include_pad – 如果 True, 将在平均计算中包括零填充. 默认值: True max_pool1d torch.nn.functional.max_pool1d(*args, **kwargs) 对由多个输入平面组成的输入信号应用一维最大池化. 详情见 MaxPool1d. max_pool2d torch.nn.functional.max_pool2d(*args, **kwargs) 对由多个输入平面组成的输入信号应用二维最大池化. 详情见 MaxPool2d. max_pool3d torch.nn.functional.max_pool3d(*args, **kwargs) 对由多个输入平面组成的输入信号上应用三维最大池化. 详情见 MaxPool3d. max_unpool1d torch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, padding=0, output_size=None) 计算MaxPool1d的偏逆. 请参见 MaxUnpool1d. max_unpool2d torch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, padding=0, output_size=None) 计算MaxPool2d的偏逆. 详情见 MaxUnpool2d. max_unpool3d torch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, padding=0, output_size=None) 计算的MaxPool3d偏逆. 详情见 MaxUnpool3d. lp_pool1d torch.nn.functional.lp_pool1d(input, norm_type, kernel_size, stride=None, ceil_mode=False) 在由多个输入平面组成的输入信号上应用一维幂平均池化. 如果所有输入的p次方的和为零, 梯度也为零. 详情见 LPPool1d. lp_pool2d torch.nn.functional.lp_pool2d(input, norm_type, kernel_size, stride=None, ceil_mode=False) 在由多个输入平面组成的输入信号上应用二维幂平均池化. 如果所有输入的p次方的和为零, 梯度也为零. 详情见 LPPool2d. adaptive_max_pool1d torch.nn.functional.adaptive_max_pool1d(*args, **kwargs) 在由多个输入平面组成的输入信号上应用一维自适应最大池化. 请参见 AdaptiveMaxPool1d和输出形状. 参数: output_size – 目标输出的大小(单个整数) return_indices – 是否返回池化索引. 默认值: False adaptive_max_pool2d torch.nn.functional.adaptive_max_pool2d(*args, **kwargs) 在由多个输入平面组成的输入信号上应用二维自适应最大池. 请参见 AdaptiveMaxPool2d 和输出形状. 参数: output_size – 目标输出的大小(单个整数 或者 双整数元组) return_indices – 是否返回池化索引. 默认值: False adaptive_max_pool3d torch.nn.functional.adaptive_max_pool3d(*args, **kwargs) 在由多个输入平面组成的输入信号上应用三维自适应最大池. 请参见 AdaptiveMaxPool3d和输出形状. 参数: output_size – 目标输出的大小(单个整数 或者 三整数元组) return_indices – 是否返回池化索引. 默认值: False adaptive_avg_pool1d torch.nn.functional.adaptive_avg_pool1d(input, output_size) → Tensor 在由多个输入平面组成的输入信号上应用一维自适应平均池化. 请参见 AdaptiveAvgPool1d 了解详情和输出的形状. 参数: output_size – 输出目标大小(单个整数) adaptive_avg_pool2d torch.nn.functional.adaptive_avg_pool2d(input, output_size) 在由多个输入平面组成的输入信号上应用二维自适应平均池化. 请参见 AdaptiveAvgPool2d 了解详情和输出的形状. 参数: output_size – 输出目标大小(单个整数 或者 双整数元组) adaptive_avg_pool3d torch.nn.functional.adaptive_avg_pool3d(input, output_size) 在由多个输入平面组成的输入信号上应用三维自适应平均池化. 请参见 AdaptiveAvgPool3d 了解详情和输出的形状. 参数: output_size – 输出目标大小(单个整数 或者 三整数元组) 非线性激活函数 threshold torch.nn.functional.threshold(input, threshold, value, inplace=False) 为输入元素的每个元素设置阈值. 请参见 Threshold. torch.nn.functional.threshold_(input, threshold, value) → Tensor 就地版的 threshold(). relu torch.nn.functional.relu(input, inplace=False) → Tensor 逐元素应用整流线性单元函数. 请参见 ReLU. torch.nn.functional.relu_(input) → Tensor 就地版的 relu(). hardtanh torch.nn.functional.hardtanh(input, min_val=-1., max_val=1., inplace=False) → Tensor 逐元素应用hardtanh函数. 请参见 Hardtanh. torch.nn.functional.hardtanh_(input, min_val=-1., max_val=1.) → Tensor 原地版的 hardtanh(). relu6 torch.nn.functional.relu6(input, inplace=False) → Tensor 逐元素应用函数 %20%3D%20%5Cmin(%5Cmax(0%2Cx)%2C%206)). 请参见 ReLU6. elu torch.nn.functional.elu(input, alpha=1.0, inplace=False) 逐元素应用 %20%3D%20%5Cmax(0%2Cx)%20%2B%20%5Cmin(0%2C%20%5Calpha%20*%20(%5Cexp(x)%20-%201))). 请参见 ELU. torch.nn.functional.elu_(input, alpha=1.) → Tensor 就地版的 elu(). selu torch.nn.functional.selu(input, inplace=False) → Tensor 逐元素应用 %20%3D%20scale%20%20(%5Cmax(0%2Cx)%20%2B%20%5Cmin(0%2C%20%5Calpha%20%20(%5Cexp(x)%20-%201)))), 其中 并且 . 请参见 SELU. celu torch.nn.functional.celu(input, alpha=1., inplace=False) → Tensor 逐元素应用 %20%3D%20%5Cmax(0%2Cx)%20%2B%20%5Cmin(0%2C%20%5Calpha%20*%20(%5Cexp(x%2F%5Calpha)%20-%201))). 请参见 CELU. leaky_relu torch.nn.functional.leaky_relu(input, negative_slope=0.01, inplace=False) → Tensor 逐元素应用 %20%3D%20%5Cmax(0%2C%20x)%20%2B%20%5Ctext%7Bnegative%5C_slope%7D%20*%20%5Cmin(0%2C%20x)) 请参见 LeakyReLU. torch.nn.functional.leaky_relu_(input, negative_slope=0.01) → Tensor 就地版的 leaky_relu(). prelu torch.nn.functional.prelu(input, weight) → Tensor 逐元素应用函数 %20%3D%20%5Cmax(0%2Cx)%20%2B%20%5Ctext%7Bweight%7D%20*%20%5Cmin(0%2Cx)) 其中，权重是可学习的参数. 请参见 PReLU. rrelu torch.nn.functional.rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) → Tensor 随机的 leaky ReLU. 请参见 RReLU. torch.nn.functional.rrelu_(input, lower=1./8, upper=1./3, training=False) → Tensor 就地版的 rrelu(). glu torch.nn.functional.glu(input, dim=-1) → Tensor 门控线性单元. 计算: ) 其中inpuy沿dim分成两半, 形成A和B. 见 Language Modeling with Gated Convolutional Networks. 参数: input (Tensor) – 输入张量 dim (int) – 用于分割输入的维度 logsigmoid torch.nn.functional.logsigmoid(input) → Tensor 逐元素应用 %20%3D%20%5Clog%20%5Cleft(%5Cfrac%7B1%7D%7B1%20%2B%20%5Cexp(-x_i)%7D%5Cright)) 请参见 LogSigmoid. hardshrink torch.nn.functional.hardshrink(input, lambd=0.5) → Tensor 逐元素应用hardshrink函数 请参见 Hardshrink. tanhshrink torch.nn.functional.tanhshrink(input) → Tensor 逐元素应用, %20%3D%20x%20-%20%5Ctext%7BTanh%7D(x)) 请参见 Tanhshrink. softsign torch.nn.functional.softsign(input) → Tensor 逐元素应用, the function %20%3D%20%5Cfrac%7Bx%7D%7B1%20%2B%20%7Cx%7C%7D) 请参见 Softsign. softplus torch.nn.functional.softplus(input, beta=1, threshold=20) → Tensor softmin torch.nn.functional.softmin(input, dim=None, _stacklevel=3, dtype=None) 应用 softmin 函数. 注意 %20%3D%20%5Ctext%7BSoftmax%7D(-x)). 数学公式见softmax定义 请参见 Softmin. 参数: input (Tensor) – 输入 dim (int) – 计算softmin的维度(因此dim上每个切片的和为1). dtype (torch.dtype, 可选的) – 返回tenosr的期望数据类型. 如果指定了参数, 输入张量在执行::param操作之前被转换为dtype. 这对于防止数据类型溢出非常有用. 默认值: None. softmax torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None) 应用 softmax 函数. Softmax定义为: %20%3D%20%5Cfrac%7Bexp(x_i)%7D%7B%5Csum_j%20exp(x_j)%7D) 它应用于dim上的所有切片, 并将对它们进行重新缩放, 使元素位于(0,1)范围内, 和为1. 请参见 Softmax. 参数: input (Tensor) – 输入 dim (int) – 将计算softmax的维度. dtype (torch.dtype, 可选的) – 返回tenosr的期望数据类型. :如果指定了参数, 输入张量在执行::param操作之前被转换为dtype. 这对于防止数据类型溢出非常有用. 默认值: None. 注意 这个函数不能直接处理NLLLoss, NLLLoss要求日志在Softmax和它自己之间计算. 使用log_softmax来代替(它更快，并且具有更好的数值属性). softshrink torch.nn.functional.softshrink(input, lambd=0.5) → Tensor 逐元素应用 soft shrinkage 函数 请参见 Softshrink. gumbel_softmax torch.nn.functional.gumbel_softmax(logits, tau=1.0, hard=False, eps=1e-10) 采样自Gumbel-Softmax分布, 并可选择离散化. 参数: logits – [batch_size, num_features] 非规范化对数概率 tau – 非负的对抗强度 hard – 如果 True, 返回的样本将会离散为 one-hot 向量, 但将会是可微分的，就像是在自动求导的soft样本一样 返回值: 从 Gumbel-Softmax 分布采样的 tensor, 形状为 batch_size x num_features . 如果 hard=True, 返回值是 one-hot 编码, 否则, 它们就是特征和为1的概率分布 约束: 目前仅支持二维的 logits 输入张量, 形状为 batch_size x num_features 基于 https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb , (MIT license) log_softmax torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None) 应用 softmax 和对数运算. 虽然在数学上等价于log(softmax(x)), 但分开执行这两个操作比较慢, 而且在数值上不稳定. 这个函数使用另一种公式来正确计算输出和梯度. 请参见 LogSoftmax. 参数: input (Tensor) – 输入 dim (int) – 计算log_softmax的维度. dtype (torch.dtype, 可选的) – 返回张量的期望数据类型. :如果指定了参数, 输入张量在执行::param操作之前被转换为dtype. 这对于防止数据类型溢出非常有用. 默认值: None. tanh torch.nn.functional.tanh(input) → Tensor 逐元素应用 %20%3D%20%5Ctanh(x)%20%3D%20%5Cfrac%7B%5Cexp(x)%20-%20%5Cexp(-x)%7D%7B%5Cexp(x)%20%2B%20%5Cexp(-x)%7D) 请参见 Tanh. sigmoid torch.nn.functional.sigmoid(input) → Tensor 逐元素应用函数 %20%3D%20%5Cfrac%7B1%7D%7B1%20%2B%20%5Cexp(-x)%7D) 请参见 Sigmoid. 规范化函数 batch_norm torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, bias=None, training=False, momentum=0.1, eps=1e-05) 对一批数据中的每个通道应用批量标准化. 请参见 BatchNorm1d, BatchNorm2d, BatchNorm3d. instance_norm torch.nn.functional.instance_norm(input, running_mean=None, running_var=None, weight=None, bias=None, use_input_stats=True, momentum=0.1, eps=1e-05) 对批中每个数据样本中的每个通道应用实例规范化. 请参见 InstanceNorm1d, InstanceNorm2d, InstanceNorm3d. layer_norm torch.nn.functional.layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-05) 对最后特定数量的维度应用layer规范化. 请参见 LayerNorm. local_response_norm torch.nn.functional.local_response_norm(input, size, alpha=0.0001, beta=0.75, k=1.0) 对由多个输入平面组成的输入信号进行局部响应归一化, 其中通道占据第二维. 跨通道应用标准化. 请参见 LocalResponseNorm. normalize torch.nn.functional.normalize(input, p=2, dim=1, eps=1e-12, out=None) 对指定维度执行 规范化. 对于一个尺寸为 )的输入张量, 每一 -元素向量 沿着维度 dim 被转换为 %7D.%0D%0A%0D%0A) 对于默认参数, 它使用沿维度的欧几里得范数进行标准化. 参数: input – 任意形状的输入张量 p (float) – 范数公式中的指数值. 默认值: 2 dim (int) – 进行规约的维度. 默认值: 1 eps (float) – 避免除以零的小值. 默认值: 1e-12 out (Tensor, 可选的) – 输出张量. 如果 out 被设置, 此操作不可微分. 线性函数 linear torch.nn.functional.linear(input, weight, bias=None) 对传入数据应用线性转换: . 形状: Input: ) * 表示任意数量的附加维度 Weight: ) Bias: ) Output: ) bilinear torch.nn.functional.bilinear(input1, input2, weight, bias=None) Dropout 函数 dropout torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False) 在训练过程中, 使用伯努利分布的样本, 随机地用概率p将输入张量的一些元素归零. 请参见 Dropout. 参数: p – 清零概率. 默认值: 0.5 training – 如果 True 使用 dropout. 默认值: True inplace – 如果设置为 True, 将会原地操作. 默认值: False alpha_dropout torch.nn.functional.alpha_dropout(input, p=0.5, training=False, inplace=False) 对输入应用 alpha dropout. 请参见 AlphaDropout. dropout2d torch.nn.functional.dropout2d(input, p=0.5, training=True, inplace=False) 随机归零输入张量的整个通道 (一个通道是一个二维特征图, 例如, 在批量输入中第j个通道的第i个样本是一个二维张量的输入[i,j]). 每次前向传递时, 每个信道都将被独立清零. 用概率 p 从 Bernoulli 分布采样. 请参见 Dropout2d. 参数: p – 通道清零的概率. 默认值: 0.5 training – 使用 dropout 如果设为 True. 默认值: True inplace – 如果设置为 True, 将会做原地操作. 默认值: False dropout3d torch.nn.functional.dropout3d(input, p=0.5, training=True, inplace=False) 随机归零输入张量的整个通道 (一个通道是一个三维特征图, 例如, 在批量输入中第j个通道的第i个样本是一个三维张量的输入[i,j]). 每次前向传递时, 每个信道都将被独立清零. 用概率 p 从 Bernoulli 分布采样. 请参见 Dropout3d. 参数: p – 通道清零的概率. 默认值: 0.5 training – 使用 dropout 如果设为 True. 默认值: True inplace – 如果设置为 True, 将会做原地操作. 默认值: False 稀疏函数 embedding torch.nn.functional.embedding(input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False) 一个简单的查找表, 查找固定字典中的embedding(嵌入)内容和大小. 这个模块通常用于使用索引检索单词嵌入. 模块的输入是索引列表和嵌入矩阵, 输出是相应的单词嵌入. 请参见 torch.nn.Embedding. 参数: input (LongTensor) – 包含嵌入矩阵中的索引的张量 weight (Tensor) – 嵌入矩阵的行数等于可能的最大索引数+ 1, 列数等于嵌入大小 padding_idx (int, 可选的) – 如果给定, 每当遇到索引时, 在padding_idx (初始化为零)用嵌入向量填充输出. max_norm (float, 可选的) – 如果给定, 则将范数大于max_norm的每个嵌入向量重新规范化, 得到范数max_norm. 注意:这将修改适当的weight. norm_type (float, 可选的) – 用于计算max_norm选项的p范数的p. 默认 2. scale_grad_by_freq (boolean__, 可选的) – 如果给定, 这将通过小批处理中单词频率的倒数来缩放梯度. 默认 False. sparse (bool, 可选的) – 如果值为 True, 梯度 w.r.t. weight 将会是一个稀疏 tensor. 请看 torch.nn.Embedding有关稀疏梯度的更多详细信息. 形状: Input: 包含要提取的索引的任意形状的长张量 Weight: 浮点型嵌入矩阵, 形状为 (V, embedding_dim), V = maximum index + 1 并且 embedding_dim = the embedding size Output: (*, embedding_dim), * 是输入形状 例子: >>> # a batch of 2 samples of 4 indices each >>> input = torch.tensor([[1,2,4,5],[4,3,2,9]]) >>> # an embedding matrix containing 10 tensors of size 3 >>> embedding_matrix = torch.rand(10, 3) >>> F.embedding(input, embedding_matrix) tensor([[[ 0.8490, 0.9625, 0.6753], [ 0.9666, 0.7761, 0.6108], [ 0.6246, 0.9751, 0.3618], [ 0.4161, 0.2419, 0.7383]], [[ 0.6246, 0.9751, 0.3618], [ 0.0237, 0.7794, 0.0528], [ 0.9666, 0.7761, 0.6108], [ 0.3385, 0.8612, 0.1867]]]) >>> # example with padding_idx >>> weights = torch.rand(10, 3) >>> weights[0, :].zero_() >>> embedding_matrix = weights >>> input = torch.tensor([[0,2,0,5]]) >>> F.embedding(input, embedding_matrix, padding_idx=0) tensor([[[ 0.0000, 0.0000, 0.0000], [ 0.5609, 0.5384, 0.8720], [ 0.0000, 0.0000, 0.0000], [ 0.6262, 0.2438, 0.7471]]]) embedding_bag torch.nn.functional.embedding_bag(input, weight, offsets=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, mode='mean', sparse=False) 计算嵌入bags的和、平均值或最大值, 而不实例化中间嵌入. 请参见 torch.nn.EmbeddingBag 参数: input (LongTensor) – 包含嵌入矩阵的索引的bags张量 weight (Tensor) – 嵌入矩阵的行数等于可能的最大索引数+ 1, 列数等于嵌入大小 offsets (LongTensor__, 可选的) – 仅当input为一维时使用. offsets确定输入中每个bag(序列)的起始索引位置 max_norm (float, 可选的) – 如果给定此参数, 范数大于max_norm的每个嵌入向量将被重新规格化为范数max_norm. 注意:这将就地修改weight norm_type (float, 可选的) – The p in the p-norm to compute for the max_norm option. 默认 2. scale_grad_by_freq (boolean__, 可选的) – 如果给定此参数, 这将通过小批处理中单词频率的倒数来缩放梯度. 默认值 False. 注意:当mode=\"max\"时不支持此选项. mode (string__, 可选的) – \"sum\", \"mean\" or \"max\". 指定减少bag的方法. 默认值: \"mean\" sparse (bool, 可选的) – 如果True, 梯度w.r.t.权值就是一个稀疏张量.请参见 torch.nn.Embedding 关于稀疏梯度. 注意: 此选项不支持 mode=\"max\". 形状: input (LongTensor) 和 offsets (LongTensor, 可选的) 如果 input 是二维的, 形状为 B x N,它将被视为每个固定长度N的B个bag(序列), 这将根据模式以某种方式返回B个聚合值. 在本例中, offsets被忽略, 并且要求为None 如果 input 是一维的, 形状为 N 它将被视为多个bag(序列)的串联. offsets必须是一个一维tensor, 其中包含input中每个bag的起始索引位置. 因此, 对于形状B的偏移量, 输入将被视为有B个bag. 空bags( 即, 具有0长度)将返回由0填充的向量 weight (Tensor): 模块的可学习权重, 形状 (num_embeddings x embedding_dim) output: 聚合的嵌入值, 形状 B x embedding_dim 例子: >>> # an Embedding module containing 10 tensors of size 3 >>> embedding_matrix = torch.rand(10, 3) >>> # a batch of 2 samples of 4 indices each >>> input = torch.tensor([1,2,4,5,4,3,2,9]) >>> offsets = torch.tensor([0,4]) >>> F.embedding_bag(embedding_matrix, input, offsets) tensor([[ 0.3397, 0.3552, 0.5545], [ 0.5893, 0.4386, 0.5882]]) 距离函数 pairwise_distance torch.nn.functional.pairwise_distance(x1, x2, p=2.0, eps=1e-06, keepdim=False) 请参见 torch.nn.PairwiseDistance cosine_similarity torch.nn.functional.cosine_similarity(x1, x2, dim=1, eps=1e-8) → Tensor 返回x1和x2之间的余弦相似度, 沿dim计算 %7D%0D%0A%0D%0A) 参数: x1 (Tensor) – 第一个输入. x2 (Tensor) – 第二个输入(大小和 x1 匹配). dim (int, 可选的) – 维度. 默认值: 1 eps (float, 可选的) – 非常小的值避免除以0. 默认值: 1e-8 形状: Input: ) 其中D在dim位置. Output: ) 其中1在dim位置. 例子: >>> input1 = torch.randn(100, 128) >>> input2 = torch.randn(100, 128) >>> output = F.cosine_similarity(input1, input2) >>> print(output) pdist torch.nn.functional.pdist(input, p=2) → Tensor 计算输入中每对行向量之间的p范数距离. 这与torch.norm(input[:, None] - input, dim=2, p=p)的上三角形部分（不包括对角线）相同. 如果行是连续的, 则此函数将更快 如果输入具有形状 则输出将具有形状 ). 这个函数相当于 scipy.spatial.distance.pdist(input, ‘minkowski’, p=p) 如果 ). 当 它等价于 scipy.spatial.distance.pdist(input, ‘hamming’) * M. 当 , 最相近的scipy函数是 scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max()). 参数: input – 输入张量, 形状为 . p – 计算每个向量对之间的p范数距离的p值 . 损失函数 binary_cross_entropy torch.nn.functional.binary_cross_entropy(input, target, weight=None, size_average=None, reduce=None, reduction='mean') 计算目标和输出之间二进制交叉熵的函数. 请参见 BCELoss. 参数: input – 任意形状的张量 target – 与输入形状相同的张量 weight (Tensor, 可选的) – 手动重新调整权重, 如果提供, 它重复来匹配输入张量的形状 size_average (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 批处理中的每个损失元素的平均损失. 注意, 对于某些损失, 每个样本有多个元素. 如果size_average设置为False, 则对每个小批的损失进行汇总. reduce为False时忽略. 默认值: True reduce (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 根据size_average, 对每个小批量的观察结果的损失进行平均或求和. 当reduce为False时, 返回每批元素的损失并忽略size_average. 默认值: True reduction (string__, 可选的) – 指定要应用于输出的reduction：'none'| 'mean'| 'sum'. 'none'：没有reduction, 'mean'：输出的总和将除以输出中的元素数量 'sum'：输出将被求和. 注意：size_average和reduce正在被弃用, 同时, 指定这两个args中的任何一个都将覆盖reduce. 默认值：'mean', 默认值: ‘mean’ 例子: >>> input = torch.randn((3, 2), requires_grad=True) >>> target = torch.rand((3, 2), requires_grad=False) >>> loss = F.binary_cross_entropy(F.sigmoid(input), target) >>> loss.backward() binary_cross_entropy_with_logits torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None) 计算目标和输出logits之间的二进制交叉熵的函数. 请参见 BCEWithLogitsLoss. 参数: input – 任意形状的张量 target – 与输入形状相同的张量 weight (Tensor, 可选的) – 手动重新调整权重, 如果提供, 它重复来匹配输入张量的形状 size_average (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 批处理中的每个损失元素的平均损失. 注意, 对于某些损失, 每个样本有多个元素. 如果size_average设置为False, 则对每个小批的损失进行汇总. reduce为False时忽略. 默认值: True reduce (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 根据size_average, 对每个小批量的观察结果的损失进行平均或求和. 当reduce为False时, 返回每批元素的损失并忽略size_average. 默认值: True reduction (string__, 可选的) – 指定要应用于输出的reduction：'none'| 'mean'| 'sum'. 'none'：没有reduction, 'mean'：输出的总和将除以输出中的元素数量 'sum'：输出将被求和. 注意：size_average和reduce正在被弃用, 同时, 指定这两个args中的任何一个都将覆盖reduce. 默认值：'mean', 默认值: ‘mean’ pos_weight (Tensor, 可选的) – 正例样本的权重. 必须是长度等于类数的向量. 例子: >>> input = torch.randn(3, requires_grad=True) >>> target = torch.empty(3).random_(2) >>> loss = F.binary_cross_entropy_with_logits(input, target) >>> loss.backward() poisson_nll_loss torch.nn.functional.poisson_nll_loss(input, target, log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean') 泊松负对数似然损失. 请参见 PoissonNLLLoss. 参数: input – 潜在泊松分布的期望. target – 随机抽样 ). log_input – 如果为True, 则损失计算为 %20-%20%5Ctext%7Btarget%7D%20%20%5Ctext%7Binput%7D), 如果为False, 则损失计算为 ![](http://latex.codecogs.com/gif.latex?%5Ctext%7Binput%7D%20-%20%5Ctext%7Btarget%7D%20%20%5Clog(%5Ctext%7Binput%7D%2B%5Ctext%7Beps%7D)). 默认值: True full – 是否计算全部损失, 即. 加入Stirling近似项. 默认值: False %20-%20%5Ctext%7Btarget%7D%20%2B%200.5%20%20%5Clog(2%20%20%5Cpi%20*%20%5Ctext%7Btarget%7D)). size_average (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 批处理中的每个损失元素的平均损失. 注意, 对于某些损失, 每个样本有多个元素. 如果size_average设置为False, 则对每个小批的损失进行汇总. reduce为False时忽略. 默认值: True eps (float, 可选的) – 一个小值避免求值 ) 当 log_input=False. 默认值: 1e-8 reduce (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 根据size_average, 对每个小批量的观察结果的损失进行平均或求和. 当reduce为False时, 返回每批元素的损失并忽略size_average. 默认值: True reduction (string__, 可选的) – 指定要应用于输出的reduction：'none'| 'mean'| 'sum'. 'none'：没有reduction, 'mean'：输出的总和将除以输出中的元素数量 'sum'：输出将被求和. 注意：size_average和reduce正在被弃用, 同时, 指定这两个args中的任何一个都将覆盖reduce. 默认值：'mean', 默认值: ‘mean’ cosine_embedding_loss torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') → Tensor 请参见 CosineEmbeddingLoss. cross_entropy torch.nn.functional.cross_entropy(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') 此函数结合了 log_softmax 和 nll_loss. 请参见 CrossEntropyLoss. 参数: input (Tensor) – ) 其中 C = 类别数 或者在二维损失的情况下为 ), 或者 ) 当 在k维损失的情况下 target (Tensor) – ) 其中每个值都在 范围内, 或者 ) 其中 在k维损失情况下. weight (Tensor, 可选的) – 给每个类别的手动重定权重. 如果给定, 必须是大小为C的张量 size_average (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 批处理中的每个损失元素的平均损失. 注意, 对于某些损失, 每个样本有多个元素. 如果size_average设置为False, 则对每个小批的损失进行汇总. reduce为False时忽略. 默认值: True ignore_index (int, 可选的) – 指定一个被忽略的目标值，该目标值不影响输入梯度。当 size_average 取值为 True, 损失平均在不可忽略的目标上. 默认值: -100 reduce (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 根据size_average, 对每个小批量的观察结果的损失进行平均或求和. 当reduce为False时, 返回每批元素的损失并忽略size_average. 默认值: True reduction (string__, 可选的) – 指定要应用于输出的reduction：'none'| 'mean'| 'sum'. 'none'：没有reduction, 'mean'：输出的总和将除以输出中的元素数量 'sum'：输出将被求和. 注意：size_average和reduce正在被弃用, 同时, 指定这两个args中的任何一个都将覆盖reduce. 默认值：'mean', 默认值: ‘mean’ 例子: >>> input = torch.randn(3, 5, requires_grad=True) >>> target = torch.randint(5, (3,), dtype=torch.int64) >>> loss = F.cross_entropy(input, target) >>> loss.backward() ctc_loss torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean') 联结主义时间分类损失. 请参见 CTCLoss. 注意 在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置torch.backends.cudn .deterministic = True来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 Reproducibility 了解背景. 注意 当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于Reproducibility的注释. 参数: log_probs – ) 其中 C = 字母表中包括空格在内的字符数, T = 输入长度, and N = 批次数量. 输出的对数概率(e.g. 获得于torch.nn.functional.log_softmax()). targets – ) or (sum(target_lengths)). 目标（不能为空）. 在第二种形式中，假定目标是串联的。 input_lengths – ). 输入的长度 (必须 ) target_lengths – ). 目标的长度 blank (int, 可选的) – 空白的标签. 默认 . reduction (string__, 可选的) - 指定要应用于输出的reduction：'none'| 'mean'| 'sum'. 'none'：不会应用reduce, 'mean'：输出损失将除以目标长度, 然后得到批次的平均值. 默认值：'mean' 例子: >>> log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_() >>> targets = torch.randint(1, 20, (16, 30), dtype=torch.long) >>> input_lengths = torch.full((16,), 50, dtype=torch.long) >>> target_lengths = torch.randint(10,30,(16,), dtype=torch.long) >>> loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths) >>> loss.backward() hinge_embedding_loss torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean') → Tensor 请参见 HingeEmbeddingLoss. kl_div torch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction='mean') Kullback-Leibler divergence 损失. 请参见 KLDivLoss 参数: input – 任意形状的张量 target – 和输入形状相同的张量 size_average (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 批处理中的每个损失元素的平均损失. 注意, 对于某些损失, 每个样本有多个元素. 如果size_average设置为False, 则对每个小批的损失进行汇总. reduce为False时忽略. 默认值: True reduce (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 根据size_average, 对每个小批量的观察结果的损失进行平均或求和. 当reduce为False时, 返回每批元素的损失并忽略size_average. 默认值: True reduction (string__, 可选的) – 指定要应用于输出的缩减：'none'| 'batchmean'| 'sum'| 'mean'. 'none'：不会应用reduction 'batchmean'：输出的总和将除以batchsize 'sum'：输出将被加总 'mean'：输出将除以输出中的元素数 默认值：'mean' :param 注::size average和reduce正在被弃用, 同时, 指定这两个arg中的一个将覆盖reduce. :param 注::reduce = mean不返回真实的kl散度值, 请使用:reduce = batchmean, 它符合kl的数学定义. 在下一个主要版本中, “mean”将被修改为与“batchmean”相同. l1_loss torch.nn.functional.l1_loss(input, target, size_average=None, reduce=None, reduction='mean') → Tensor 该函数取元素的绝对值差的平均值。 请参见 L1Loss. mse_loss torch.nn.functional.mse_loss(input, target, size_average=None, reduce=None, reduction='mean') → Tensor 计算元素的均方误差. 请参见 MSELoss. margin_ranking_loss torch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') → Tensor 请参见 MarginRankingLoss. multilabel_margin_loss torch.nn.functional.multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') → Tensor 请参见 MultiLabelMarginLoss. multilabel_soft_margin_loss torch.nn.functional.multilabel_soft_margin_loss(input, target, weight=None, size_average=None) → Tensor 请参见 MultiLabelSoftMarginLoss. multi_margin_loss torch.nn.functional.multi_margin_loss(input, target, p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean') multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None, reduce=None, reduction=’mean’) -> Tensor 请参见 MultiMarginLoss. nll_loss torch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') 负的对数似然函数. 请参见 NLLLoss. 参数: input – ) C = 类别的数量 或者 ) 在二维损失的情况下, 或者 ) 在K维损失的情况下. target – ) 每个值是 , 或者 ) K维损失. weight (Tensor, 可选的) – 给每个类别的手动重定权重. 如果给定, 必须是大小为C的张量 size_average (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 批处理中的每个损失元素的平均损失. 注意, 对于某些损失, 每个样本有多个元素. 如果size_average设置为False, 则对每个小批的损失进行汇总. reduce为False时忽略. 默认值: True ignore_index (int, 可选的) – 指定一个被忽略的目标值, 该值不会影响输入梯度. 当size_average为True时, 损耗在未忽略的目标上平均. 默认值: -100 reduce (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 根据size_average, 对每个小批量的观察结果的损失进行平均或求和. 当reduce为False时, 返回每批元素的损失并忽略size_average. 默认值: True reduction (string__, 可选的) – 指定要应用于输出的reduction：'none'| 'mean'| 'sum'. 'none'：没有reduction, 'mean'：输出的总和将除以输出中的元素数量 'sum'：输出将被求和. 注意：size_average和reduce正在被弃用, 同时, 指定这两个args中的任何一个都将覆盖reduce. 默认值：'mean', 默认值: ‘mean’ 例子: >>> # input is of size N x C = 3 x 5 >>> input = torch.randn(3, 5, requires_grad=True) >>> # each element in target has to have 0 >> target = torch.tensor([1, 0, 4]) >>> output = F.nll_loss(F.log_softmax(input), target) >>> output.backward() smooth_l1_loss torch.nn.functional.smooth_l1_loss(input, target, size_average=None, reduce=None, reduction='mean') 如果绝对元素误差低于1, 则使用平方项, 否则使用L1项的函数. 请参见 SmoothL1Loss. soft_margin_loss torch.nn.functional.soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') → Tensor 请参见 SoftMarginLoss. triplet_margin_loss torch.nn.functional.triplet_margin_loss(anchor, positive, negative, margin=1.0, p=2, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean') 请参见 TripletMarginLoss 视觉函数 pixel_shuffle torch.nn.functional.pixel_shuffle() 重新排列张量中的元素, 从形状 ) 到 ). 请参见 PixelShuffle. 参数: input (Tensor) – 输入张量 upscale_factor (int) – 提高空间解析度的参数 例子: >>> input = torch.randn(1, 9, 4, 4) >>> output = torch.nn.functional.pixel_shuffle(input, 3) >>> print(output.size()) torch.Size([1, 1, 12, 12]) pad torch.nn.functional.pad(input, pad, mode='constant', value=0) 用于填充张量. Pading size: 要填充的维度数为 %7D%7D%7B2%7D%5Cright%5Crfloor)填充的维度从最后一个维度开始向前移动. 例如, 填充输入tensor的最后一个维度, 所以 pad 形如 (padLeft, padRight); 填充最后 2 个维度, 使用 (padLeft, padRight, padTop, padBottom); 填充最后 3 个维度, 使用 (padLeft, padRight, padTop, padBottom, padFront, padBack). Padding mode: 请参见 torch.nn.ConstantPad2d, torch.nn.ReflectionPad2d, and torch.nn.ReplicationPad2d 有关每个填充模式如何工作的具体示例. Constant padding 已经实现于任意维度. 复制填充用于填充5D输入张量的最后3个维度, 或4D输入张量的最后2个维度, 或3D输入张量的最后一个维度. 反射填充仅用于填充4D输入张量的最后两个维度, 或者3D输入张量的最后一个维度. 注意 当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于Reproducibility的注释. 参数: input (Tensor) – N维张量 pad (tuple) – m个元素的元组, 其中 输入维数，且m是偶数 mode – ‘constant’, ‘reflect’ or ‘replicate’. 默认值: ‘constant’ value – 用“常量”填充来填充值. 默认值: 0 例子: >>> t4d = torch.empty(3, 3, 4, 2) >>> p1d = (1, 1) # pad last dim by 1 on each side >>> out = F.pad(t4d, p1d, \"constant\", 0) # effectively zero padding >>> print(out.data.size()) torch.Size([3, 3, 4, 4]) >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2) >>> out = F.pad(t4d, p2d, \"constant\", 0) >>> print(out.data.size()) torch.Size([3, 3, 8, 4]) >>> t4d = torch.empty(3, 3, 4, 2) >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3) >>> out = F.pad(t4d, p3d, \"constant\", 0) >>> print(out.data.size()) torch.Size([3, 9, 7, 3]) interpolate torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None) 向下/向上采样输入到给定的size或给定的scale_factor 由 mode 指定插值的算法. 目前支持时间, 空间和体积上采样, 即预期输入为三维、四维或五维形状. 输入维度形式: mini-batch x channels x [可选的 depth] x [可选的 height] x width. 可用于上采样的模式是: nearest, linear (仅三维), bilinear (仅四维), trilinear (仅五维), area 参数: input (Tensor) – 输入张量 size (int or Tuple__[int] or Tuple__[int, int] or Tuple__[int, int, int]) – 输出尺寸. scale_factor (float or Tuple__[float]) – 空间大小的乘数. 如果是元组, 则必须匹配输入大小. mode (string) – 上采样算法: ‘nearest’ | ‘linear’ | ‘bilinear’ | ‘trilinear’ | ‘area’. 默认值: ‘nearest’ align_corners (bool, 可选的) – 如果为True, 则输入和输出张量的角像素对齐, 从而保留这些像素的值. 仅在 mode 是 linear, bilinear, 或者 trilinear 时生效. 默认值: False 警告 align_corners = True时, 线性插值模式(linear, bilinear, and trilinear)不会按比例对齐输出和输入像素, 因此输出值可能取决于输入大小. 这是0.3.1版之前这些模式的默认行为.此后, 默认行为为align_corners = False. 有关这如何影响输出的具体示例, 请参见上例. 注意 当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于Reproducibility的注释. upsample torch.nn.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None) 将输入采样到给定size或给定的scale_factor 警告 此函数已被弃用, 取而代之的是 torch.nn.functional.interpolate(). 等价于 nn.functional.interpolate(...). 注意 当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于Reproducibility的注释. 用于上采样的算法由 mode 确定. 目前支持时间, 空间和体积上采样, 即预期输入为三维、四维或五维形状. 输入维度形式: mini-batch x channels x [可选的 depth] x [可选的 height] x width. 可用于上采样的模式是: nearest, linear (仅三维), bilinear (仅四维), trilinear (仅五维), area 参数: input (Tensor) – 输入张量 size (int or Tuple__[int] or Tuple__[int, int] or Tuple__[int, int, int]) – 输出尺寸. scale_factor (int) – 空间大小的乘数. 必须是整数. mode (string) – 上采样算法: ‘nearest’ | ‘linear’| ‘bilinear’ | ‘trilinear’. 默认值: ‘nearest’ align_corners (bool, 可选的) – 如果为True, 则输入和输出张量的角像素对齐, 从而保留这些像素的值. 仅在 mode 是 linear, bilinear, 或者 trilinear 时生效. 默认值: False 警告 align_corners = True时, 线性插值模式(linear, bilinear, and trilinear)不会按比例对齐输出和输入像素, 因此输出值可能取决于输入大小. 这是0.3.1版之前这些模式的默认行为.此后, 默认行为为align_corners = False. 有关这如何影响输出的具体示例, 请参见 Upsample upsample_nearest torch.nn.functional.upsample_nearest(input, size=None, scale_factor=None) 使用最近邻的像素值对输入进行上采样. 警告 不推荐使用此函数, 而使用 torch.nn.functional.interpolate(). 等价于h nn.functional.interpolate(..., mode='nearest'). 目前支持空间和体积上采样 (即 inputs 是 4 或者 5 维的). 参数: input (Tensor) – 输入 size (int or Tuple__[int, int] or Tuple__[int, int, int]) – 输出空间大小. scale_factor (int) – 空间大小乘法器。必须是整数。 注意 当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于Reproducibility的注释. upsample_bilinear torch.nn.functional.upsample_bilinear(input, size=None, scale_factor=None) 使用双线性上采样对输入进行上采样. 警告 不推荐使用此函数, 而使用 torch.nn.functional.interpolate(). 等价于 nn.functional.interpolate(..., mode='bilinear', align_corners=True). 期望输入是空间的 (四维). 用 upsample_trilinear 对体积 (五维) 输入. 参数: input (Tensor) – 输入 size (int or Tuple__[int, int] or Tuple__[int, int, int]) – 输出空间大小. scale_factor (int) – 空间大小乘法器。 注意 当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于Reproducibility的注释. grid_sample torch.nn.functional.grid_sample(input, grid, mode='bilinear', padding_mode='zeros') 给定input 和流场 grid, 使用 input 和 grid 中的像素位置计算output. 目前, 仅支持 spatial (四维) 和 volumetric (五维) input. 在 spatial (4四维) 的情况下, 对于 input 形如 ) 和 grid 形如 ), 输出的形状为 ). 对于每个输出位置 output[n, :, h, w], 大小为2的向量 grid[n, h, w] 指定 input 的像素位置 x 和 y, 用于插值输出值 output[n, :, h, w]. 对于 5D 的 inputs, grid[n, d, h, w] 指定 x, y, z 像素位置用于插值 output[n, :, d, h, w]. mode 参数指定 nearest or bilinear 插值方法. grid 大多数值应该处于 [-1, 1]. 这是因为像素位置由input 空间维度标准化.例如, 值 x = -1, y = -1 是 input 的左上角, 值 x = 1, y = 1 是 input 的右下角. 如果 grid 有 [-1, 1] 之外的值, 那些坐标将由 padding_mode 定义. 选项如下 padding_mode=\"zeros\": 用 0 代替边界外的值, padding_mode=\"border\": 用 border 值代替, padding_mode=\"reflection\": 对于超出边界的值, 用反射的值. 对于距离边界较远的位置, 它会一直被反射, 直到到达边界, 例如(归一化)像素位置x = -3.5被-1反射, 变成x' = 2.5, 然后被边界1反射, 变成x'' = -0.5. 注意 该功能常用于空间变换网络的构建. 注意 当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于Reproducibility的注释. 参数: input (Tensor) – 形状为 )的输入 (四维情形) 或形状为) 的输入（五维情形） grid (Tensor) – 形状为) 的流场(四维情形) 或者 ) （五维情形） mode (str) – 插值模式计算输出值'双线性' | '最接近'. 默认值: ‘bilinear’ padding_mode (str) – 外部网格值' zeros ' | ' border ' | ' reflection '的填充模式. 默认值: ‘zeros’ 返回值: 输出张量 返回类型: 输出 (Tensor) affine_grid torch.nn.functional.affine_grid(theta, size) 在给定一批仿射矩阵theta的情况下生成二维流场. 通常与grid_sample()一起使用以实现空间变换器网络. 参数: theta (Tensor) – 输入的仿射矩阵 () size (torch.Size) – 目标图像输出的大小 () 例子: torch.Size((32, 3, 24, 24)) 返回值: 输出tensor, 形状为 () 返回类型: output (Tensor) 数据并行函数 (multi-GPU, distributed) data_parallel torch.nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None) 在设备id中给定的gpu上并行计算模块(输入). 这是DataParallel模块的函数版本. 参数: module (Module) – 要并行评估的模块 inputs (tensor) – 模块的输入 device_ids (list of python:int or torch.device) – 用于复制模块的GPU id output_device (list of python:int or torch.device) – 输出的GPU位置使用 -1表示CPU. (默认值: device_ids[0]) 返回值: 一个张量, 包含位于输出设备上的模块(输入)的结果 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"nn.init.html":{"url":"nn.init.html","title":"torch.nn.init","keywords":"","body":"torch.nn.init 译者：GeneZC torch.nn.init.calculate_gain(nonlinearity, param=None) 返回给定非线性函数的推荐的增益值。对应关系如下表： 非线性函数 增益 Linear / Identity Conv{1,2,3}D Sigmoid Tanh ReLU Leaky Relu 参数： nonlinearity – 非线性函数 (nn.functional 中的名字) param – 对应非线性函数的可选参数 例子 >>> gain = nn.init.calculate_gain('leaky_relu') torch.nn.init.uniform_(tensor, a=0, b=1) 用均匀分布 ) 初始化输入 Tensor。 参数： tensor – n 维 torch.Tensor a – 均匀分布的下界 b – 均匀分布的上界 例子 >>> w = torch.empty(3, 5) >>> nn.init.uniform_(w) torch.nn.init.normal_(tensor, mean=0, std=1) 用正态分布 ) 初始化输入 Tensor。 参数： tensor – n 维 torch.Tensor mean – 正态分布的均值 std – 正态分布的标准差 例子 >>> w = torch.empty(3, 5) >>> nn.init.normal_(w) torch.nn.init.constant_(tensor, val) 用常数 初始化输入 Tensor。 参数： tensor – n 维 torch.Tensor val – 用以填入张量的常数 例子 >>> w = torch.empty(3, 5) >>> nn.init.constant_(w, 0.3) torch.nn.init.eye_(tensor) 用单位矩阵初始化 2 维输入 Tensor。 保持输入张量输入 Linear 时的独一性，并且越多越好. 参数： tensor – 2 维 torch.Tensor 例子 >>> w = torch.empty(3, 5) >>> nn.init.eye_(w) torch.nn.init.dirac_(tensor) 用狄拉克δ函数初始化 {3, 4, 5} 维输入 Tensor。 保持输入张量输入 Convolutional 时的独一性，并且越多通道越好。 参数： tensor – {3, 4, 5} 维 torch.Tensor 例子 >>> w = torch.empty(3, 16, 5, 5) >>> nn.init.dirac_(w) torch.nn.init.xavier_uniform_(tensor, gain=1) 用论文 “Understanding the difficulty of training deep feedforward neural networks” - Glorot, X. & Bengio, Y. (2010) 中提及的均匀分布初始化输入 Tensor。初始化后的张量中的值采样自 ) 且 也被称作 Glorot 初始化。 参数： tensor – n 维 torch.Tensor gain – 可选缩放因子 例子 >>> w = torch.empty(3, 5) >>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu')) torch.nn.init.xavier_normal_(tensor, gain=1) 用论文 “Understanding the difficulty of training deep feedforward neural networks” - Glorot, X. & Bengio, Y. (2010) 中提及的正态分布初始化输入 Tensor。初始化后的张量中的值采样自 ) 且 也被称作 Glorot initialization。 参数： tensor – n 维 torch.Tensor gain – 可选缩放因子 例子 >>> w = torch.empty(3, 5) >>> nn.init.xavier_normal_(w) torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu') 用论文 “Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification” - He, K. et al. (2015) 中提及的均匀分布初始化输入 Tensor。初始化后的张量中的值采样自 ) 且 %20%5Ctimes%20%5Ctext%7Bfan%5C_in%7D%7D%7D%0D%0A%0D%0A) 也被称作 He initialization。 参数： tensor – n 维 torch.Tensor a – 该层后面一层的整流函数中负的斜率 (默认为 0，此时为 Relu) mode – ‘fan_in’ (default) 或者 ‘fan_out’。使用fan_in保持weights的方差在前向传播中不变；使用fan_out保持weights的方差在反向传播中不变。 nonlinearity – 非线性函数 (nn.functional 中的名字)，推荐只使用 ‘relu’ 或 ‘leaky_relu’ (default)。 例子 >>> w = torch.empty(3, 5) >>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu') torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu') 用论文 “Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification” - He, K. et al. (2015) 中提及的正态分布初始化输入 Tensor。初始化后的张量中的值采样 ) 且 %20%5Ctimes%20%5Ctext%7Bfan%5C_in%7D%7D%7D%0D%0A%0D%0A) 也被称作 He initialization。 参数： tensor – n 维 torch.Tensor a – 该层后面一层的整流函数中负的斜率 (默认为 0，此时为 Relu) mode – ‘fan_in’ (default) 或者 ‘fan_out’。使用fan_in保持weights的方差在前向传播中不变；使用fan_out保持weights的方差在反向传播中不变。 nonlinearity – 非线性函数 (nn.functional 中的名字)，推荐只使用 ‘relu’ 或 ‘leaky_relu’ (default)。 例子 >>> w = torch.empty(3, 5) >>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu') torch.nn.init.orthogonal_(tensor, gain=1) 用论文 “Exact solutions to the nonlinear dynamics of learning in deep linear neural networks” - Saxe, A. et al. (2013) 中描述的（半）正定矩阵初始化输入 Tensor。输入张量必须至少有 2 维，如果输入张量的维度大于 2， 则对后续维度进行放平操作。 参数： tensor – n 维 torch.Tensor，且 gain – 可选缩放因子 例子 >>> w = torch.empty(3, 5) >>> nn.init.orthogonal_(w) torch.nn.init.sparse_(tensor, sparsity, std=0.01) 用论文 “Deep learning via Hessian-free optimization” - Martens, J. (2010). 提及的稀疏矩阵初始化 2 维输入 Tensor，且使用正态分布 ) 初始化非零元素。 参数： tensor – n 维 torch.Tensor sparsity – 每一行置零元素的比例 std – 初始化非零元素时使用正态分布的标准差 例子 >>> w = torch.empty(3, 5) >>> nn.init.sparse_(w, sparsity=0.1) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"optim.html":{"url":"optim.html","title":"torch.optim","keywords":"","body":"torch.optim 译者：ApacheCN 是一个实现各种优化算法的包。已经支持最常用的方法，并且界面足够通用，因此将来可以轻松集成更复杂的方法。 如何使用优化器 要使用，您必须构造一个优化器对象，该对象将保持当前状态并将根据计算的渐变更新参数。 构建它 要构造一个你必须给它一个包含参数的迭代（所有应该是Variable s）来优化。然后，您可以指定特定于优化程序的选项，例如学习率，重量衰减等。 注意 如果您需要通过.cuda()将模型移动到GPU，请在为其构建优化器之前执行此操作。 .cuda()之后的模型参数与调用之前的参数不同。 通常，在构造和使用优化程序时，应确保优化参数位于一致的位置。 例： optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9) optimizer = optim.Adam([var1, var2], lr = 0.0001) 每个参数选项 s还支持指定每个参数选项。要做到这一点，不要传递一个可迭代的Variable，而是传递一个可迭代的s。它们中的每一个都将定义一个单独的参数组，并且应包含params键，其中包含属于它的参数列表。其他键应与优化程序接受的关键字参数匹配，并将用作此组的优化选项。 Note 您仍然可以将选项作为关键字参数传递。它们将在未覆盖它们的组中用作默认值。当您只想改变单个选项，同时保持参数组之间的所有其他选项保持一致时，这非常有用。 例如，当想要指定每层学习速率时，这非常有用： optim.SGD([ {'params': model.base.parameters()}, {'params': model.classifier.parameters(), 'lr': 1e-3} ], lr=1e-2, momentum=0.9) 这意味着model.base的参数将使用1e-2的默认学习速率，model.classifier的参数将使用1e-3的学习速率，0.9的动量将用于所有参数 采取优化步骤 所有优化器都实现了一个更新参数的方法。它可以以两种方式使用： optimizer.step() 这是大多数优化器支持的简化版本。一旦使用例如计算梯度，就可以调用该函数。 backward()。 Example: for input, target in dataset: optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() optimizer.step() optimizer.step(closure) 一些优化算法，例如Conjugate Gradient和LBFGS需要多次重新评估函数，因此您必须传入一个允许它们重新计算模型的闭包。闭合应清除梯度，计算损失并返回。 Example: for input, target in dataset: def closure(): optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() return loss optimizer.step(closure) 算法 class torch.optim.Optimizer(params, defaults) 所有优化器的基类。 警告 需要将参数指定为具有在运行之间一致的确定性排序的集合。不满足这些属性的对象的示例是字典值的集合和迭代器。 参数： params （ iterable ） - s或s的可迭代。指定应优化的张量。 默认值 - （dict）：包含优化选项默认值的dict（当参数组未指定它们时使用）。 add_param_group(param_group) 将参数组添加到s param_groups。 当微调预先训练的网络时，这可以是有用的，因为冻结层可以被训练并且被添加到训练进展中。 Parameters: param_group （） - 指定应该与组一起优化的张量 优化选项。 （特异性） - load_state_dict(state_dict) 加载优化器状态。 参数： state_dict （） - 优化器状态。应该是从调用返回的对象。 state_dict() 以...格式返回优化程序的状态。 它包含两个条目： state - a dict holding current optimization state. Its content 优化器类之间有所不同。 param_groups - 包含所有参数组的dict step(closure) 执行单个优化步骤（参数更新）。 Parameters: 闭包（可调用） - 一个重新评估模型并返回损失的闭包。大多数优化器都是可选的。 zero_grad() 清除所有优化s的渐变。 class torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0) 实现Adadelta算法。 已在 ADADELTA中提出：自适应学习速率方法。 Parameters: params （ iterable ） - 可迭代参数以优化或决定参数组 rho （， 可选） - 用于计算平方梯度运行平均值的系数（默认值：0.9） eps （， 可选） - 术语加入分母以提高数值稳定性（默认值：1e-6） lr （， 可选） - 在应用于参数之前缩放增量的系数（默认值：1.0） weight_decay （， 可选） - 体重衰减（L2惩罚）（默认值：0） step(closure=None) 执行单个优化步骤。 Parameters: 关闭（可调用 ， 可选） - 一个重新评估模型并返回损失的闭包。 class torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0) 实现Adagrad算法。 已经在自适应子梯度方法中提出了在线学习和随机优化。 Parameters: params （ iterable ） - 可迭代参数以优化或决定参数组 lr （， 可选） - 学习率（默认值：1e-2） lr_decay （， 可选） - 学习率衰减（默认值：0） weight_decay （， 可选） - 体重衰减（L2惩罚）（默认值：0） step(closure=None) Performs a single optimization step. Parameters: closure (callable__, optional) – A closure that reevaluates the model and returns the loss. class torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False) 实现Adam算法。 已在 Adam中提出：随机优化方法。 Parameters: params （ iterable ） - 可迭代参数以优化或决定参数组 lr （， 可选） - 学习率（默认值：1e-3） beta （元组 [， ] __， 任选） - 用于计算运行平均值的系数渐变及其方形（默认值：（0.9,0.999）） eps （， 可选） - 术语加入分母以提高数值稳定性（默认值：1e-8） weight_decay （， 可选） - 体重衰减（L2惩罚）（默认值：0） amsgrad （布尔 ， 可选） - 是否使用该算法的AMSGrad变体关于亚当及其后的收敛（默认值：False） step(closure=None) Performs a single optimization step. Parameters: closure (callable__, optional) – A closure that reevaluates the model and returns the loss. class torch.optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08) 实现适用于稀疏张量的懒惰版Adam算法。 在此变体中，只有渐变中显示的时刻才会更新，并且只有渐变的那些部分才会应用于参数。 Parameters: params （ iterable ） - 可迭代参数以优化或决定参数组 lr （， 可选） - 学习率（默认值：1e-3） beta （元组 [， ] __， 任选） - 用于计算运行平均值的系数渐变及其方形（默认值：（0.9,0.999）） eps （， 可选） - 术语加入分母以提高数值稳定性（默认值：1e-8） step(closure=None) Performs a single optimization step. Parameters: closure (callable__, optional) – A closure that reevaluates the model and returns the loss. class torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0) 实现Adamax算法（基于无穷大规范的Adam的变体）。 It has been proposed in Adam: A Method for Stochastic Optimization. Parameters: params （ iterable ） - 可迭代参数以优化或决定参数组 lr （， 可选） - 学习率（默认值：2e-3） beta （元组 [， ] __， 任选） - 用于计算运行平均值的系数渐变和它的正方形 eps （， 可选） - 术语加入分母以提高数值稳定性（默认值：1e-8） weight_decay （， 可选） - 体重衰减（L2惩罚）（默认值：0） step(closure=None) Performs a single optimization step. Parameters: closure (callable__, optional) – A closure that reevaluates the model and returns the loss. class torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0) 实现平均随机梯度下降。 已经在中通过平均来加速随机近似。 Parameters: params （ iterable ） - 可迭代参数以优化或决定参数组 lr （， 可选） - 学习率（默认值：1e-2） lambd （， 可选） - 衰变期限（默认值：1e-4） alpha （， 可选） - eta更新的权力（默认值：0.75） t0 （， 可选） - 开始平均的点（默认值：1e6） weight_decay （， 可选） - 体重衰减（L2惩罚）（默认值：0） step(closure=None) Performs a single optimization step. Parameters: closure (callable__, optional) – A closure that reevaluates the model and returns the loss. class torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-05, tolerance_change=1e-09, history_size=100, line_search_fn=None) 实现L-BFGS算法。 Warning 此优化器不支持每个参数选项和参数组（只能有一个）。 Warning 现在所有参数都必须在一台设备上。这将在未来得到改善。 Note 这是一个内存密集型优化器（它需要额外的param_bytes * (history_size + 1)字节）。如果它不适合内存尝试减少历史记录大小，或使用不同的算法。 Parameters: lr （） - 学习率（默认值：1） max_iter （） - 每个优化步骤的最大迭代次数（默认值：20） max_eval （） - 每个优化步骤的最大函数评估数（默认值：max_iter * 1.25）。 tolerance_grad （） - 一阶最优性的终止容差（默认值：1e-5）。 tolerance_change （） - 功能值/参数更改的终止容差（默认值：1e-9）。 history_size （） - 更新历史记录大小（默认值：100）。 step(closure) Performs a single optimization step. Parameters: 闭包（可调用） - 一个重新评估模型并返回损失的闭包。 class torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False) 实现RMSprop算法。 G. Hinton在他的课程中提出的建议。 中心版本首先出现在生成具有回归神经网络的序列中。 Parameters: params （ iterable ） - 可迭代参数以优化或决定参数组 lr （， 可选） - 学习率（默认值：1e-2） 动量（， 可选） - 动量因子（默认值：0） alpha （， 可选） - 平滑常数（默认值：0.99） eps （， 可选） - 术语加入分母以提高数值稳定性（默认值：1e-8） 居中（， 可选） - 如果True计算居中的RMSProp，则通过估计其方差对梯度进行归一化 weight_decay （， 可选） - 体重衰减（L2惩罚）（默认值：0） step(closure=None) Performs a single optimization step. Parameters: closure (callable__, optional) – A closure that reevaluates the model and returns the loss. class torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50)) 实现弹性反向传播算法。 Parameters: params （ iterable ） - 可迭代参数以优化或决定参数组 lr （， 可选） - 学习率（默认值：1e-2） etas （ Tuple [， ] __， 任选） - 对（etaminus，etaplis） ，这是乘法增加和减少因子（默认值：（0.5,1.2）） step_sizes （ Tuple [， ] __， 任选） - 一对最小和最大允许步长（默认值：（1e-6,50）） step(closure=None) Performs a single optimization step. Parameters: closure (callable__, optional) – A closure that reevaluates the model and returns the loss. class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False) 实现随机梯度下降（可选择带动量）。 Nesterov动量是基于关于初始化和动量在深度学习中的重要性的公式。 Parameters: params （ iterable ） - 可迭代参数以优化或决定参数组 lr （） - 学习率 动量（， 可选） - 动量因子（默认值：0） weight_decay （， 可选） - 体重衰减（L2惩罚）（默认值：0） 阻尼（， 可选） - 抑制动量（默认值：0） nesterov （， 可选） - 启用Nesterov动量（默认值：False） 例 >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9) >>> optimizer.zero_grad() >>> loss_fn(model(input), target).backward() >>> optimizer.step() Note 使用Momentum / Nesterov实施SGD与Sutskever等有所不同。人。和其他一些框架中的实现。 考虑到Momentum的具体情况，更新可以写成 其中p，g，v分别表示参数，梯度，速度和动量。 这与Sutskever等人形成鲜明对比。人。和其他采用表格更新的框架 Nesterov版本经过类似修改。 step(closure=None) Performs a single optimization step. Parameters: closure (callable__, optional) – A closure that reevaluates the model and returns the loss. 如何调整学习率 torch.optim.lr_scheduler提供了几种根据时期数调整学习率的方法。允许基于一些验证测量来降低动态学习速率。 class torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1) 将每个参数组的学习速率设置为给定函数的初始lr倍。当last_epoch = -1时，将初始lr设置为lr。 Parameters: 优化器（） - 包装优化器。 lr_lambda （函数 或） - 一个函数，它计算给定整数参数时期的乘法因子，或这些函数的列表，优化器中每个组一个.param_groups。 last_epoch （） - 最后一个纪元的索引。默认值：-1。 Example >>> # Assuming optimizer has two groups. >>> lambda1 = lambda epoch: epoch // 30 >>> lambda2 = lambda epoch: 0.95 ** epoch >>> scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2]) >>> for epoch in range(100): >>> scheduler.step() >>> train(...) >>> validate(...) load_state_dict(state_dict) 加载调度程序状态。 Parameters: state_dict （） - 调度程序状态。应该是从调用返回的对象。 state_dict() 将调度程序的状态作为a返回。 它包含自我中每个变量的条目。 dict 不是优化器。学习率lambda函数只有在它们是可调用对象时才会被保存，而不是它们是函数或lambdas。 class torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1) 将每个参数组的学习速率设置为每个step_size epochs由gamma衰减的初始lr。当last_epoch = -1时，将初始lr设置为lr。 Parameters: 优化器（） - 包装优化器。 step_size （） - 学习率衰减的时期。 gamma （） - 学习率衰减的乘法因子。默认值：0.1。 last_epoch （） - 最后一个纪元的索引。默认值：-1。 Example >>> # Assuming optimizer uses lr = 0.05 for all groups >>> # lr = 0.05 if epoch >> # lr = 0.005 if 30 >> # lr = 0.0005 if 60 >> # ... >>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1) >>> for epoch in range(100): >>> scheduler.step() >>> train(...) >>> validate(...) class torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1) 一旦纪元数达到其中一个里程碑，将每个参数组的学习速率设置为由伽玛衰减的初始lr。当last_epoch = -1时，将初始lr设置为lr。 Parameters: 优化器（） - 包装优化器。 里程碑（） - 时代指数列表。必须增加。 gamma （） - 学习率衰减的乘法因子。默认值：0.1。 last_epoch （） - 最后一个纪元的索引。默认值：-1。 Example >>> # Assuming optimizer uses lr = 0.05 for all groups >>> # lr = 0.05 if epoch >> # lr = 0.005 if 30 >> # lr = 0.0005 if epoch >= 80 >>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1) >>> for epoch in range(100): >>> scheduler.step() >>> train(...) >>> validate(...) class torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1) 将每个参数组的学习率设置为每个时期由伽玛衰减的初始lr。当last_epoch = -1时，将初始lr设置为lr。 Parameters: 优化器（） - 包装优化器。 gamma （） - 学习率衰减的乘法因子。 last_epoch （） - 最后一个纪元的索引。默认值：-1。 class torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1) 使用余弦退火计划设置每个参数组的学习速率，其中设置为初始lr，并且是自SGDR上次重启以来的纪元数： 当last_epoch = -1时，将初始lr设置为lr。 已在 SGDR中提出：具有暖启动的随机梯度下降。请注意，这仅实现SGDR的余弦退火部分，而不是重启。 Parameters: 优化器（） - 包装优化器。 T_max （） - 最大迭代次数。 eta_min （） - 最低学习率。默认值：0。 last_epoch （） - 最后一个纪元的索引。默认值：-1。 class torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08) 当指标停止改进时降低学习率。一旦学习停滞，模型通常会将学习率降低2-10倍。该调度程序读取度量数量，如果“耐心”数量的时期没有看到改善，则学习速率降低。 Parameters: 优化器（） - 包装优化器。 模式（） - min，max之一。在min模式下，当监控量停止下降时，lr将减少;在max模式下，当监控量停止增加时，它将减少。默认值：'min'。 factor （） - 学习率降低的因素。 new_lr = lr * factor。默认值：0.1。 耐心（） - 没有改善的时期数，之后学习率会降低。例如，如果patience = 2，那么我们将忽略没有改进的前2个时期，并且如果损失仍然没有改善那么将仅在第3个时期之后减少LR。默认值：10。 verbose （） - 如果True，每次更新都会向stdout输出一条消息。默认值：False。 阈值（） - 测量新最佳值的阈值，仅关注重大变化。默认值：1e-4。 threshold_mode （） - rel，abs之一。在rel模式下，dynamic_threshold ='max'模式下的最佳（1 +阈值）或min模式下的最佳（1 - 阈值）。在abs模式下，dynamic_threshold = max模式下的最佳+阈值或min模式下的最佳阈值。默认值：'rel'。 冷却时间（） - 在减少lr之后恢复正常操作之前要等待的时期数。默认值：0。 min_lr （或） - 标量或标量列表。所有参数组或每组的学习率的下限。默认值：0。 eps （） - 应用于lr的最小衰减。如果新旧lr之间的差异小于eps，则忽略更新。默认值：1e-8。 Example >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9) >>> scheduler = ReduceLROnPlateau(optimizer, 'min') >>> for epoch in range(10): >>> train(...) >>> val_loss = validate(...) >>> # Note that step should be called after validate() >>> scheduler.step(val_loss) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"autograd.html":{"url":"autograd.html","title":"torch.autograd","keywords":"","body":"Automatic differentiation package - torch.autograd 译者：gfjiangly torch.autograd 提供类和函数，实现任意标量值函数的自动微分。 它要求对已有代码的最小改变---你仅需要用requires_grad=True关键字为需要计算梯度的声明Tensor。 torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None) 计算被给张量关于图的叶节点的梯度和。 图使用链式法则微分。如何任何tensors是非标量（例如他们的数据不止一个元素）并且要求梯度，函数要额外指出grad_tensors。它应是一个匹配长度的序列，包含可微函数关于相应张量的梯度（None是一个对所有张量可接受的值，不需要梯度张量）。 此函数在叶节点累积梯度 - 你可能需要在调用前把它初始化为0. 参数： tensors (Tensor序列) – 计算导数的张量。 grad_tensors ([_Tensor](tensors.html#torch.Tensor \"torch.Tensor\") 或 None序列_) – 关于相应张量每个元素的梯度。标量张量或不需要梯度的可用None指定。如果None对所有grad_tensors可接受，则此参数可选。 retain_graph (bool, 可选) – 如果False，用于计算梯度的图将被释放。请注意，在几乎所有情况下，不需要将此选项设置为真，而且通常可以更有效地解决问题。默认为create_graph值。 create_graph (bool, 可选) – 如果True，则构造导数图，以便计算更高阶导数，默认False。 torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False) 计算和返回输出关于输入的梯度和。 grad_outputs 应是长度匹配输出的序列，包含关于输出每个元素的预计算梯度。如果一个输出不要求梯度，则梯度是None。 如果only_inputs是True，此函数将仅返回关于指定输入的梯度list。如果此参数是False，则关于其余全部叶子的梯度仍被计算，并且将累加到.grad属性中。 参数: outputs (Tensor序列) – 可微函数输出 inputs (Tensor序列) – 关于将返回梯度的输入(不累加到.grad)。 grad_outputs (Tensor序列) – 关于每个输入的梯度。标量张量或不需要梯度的可用None指定。如果None对所有grad_tensors可接受，则此参数可选。默认：None。 retain_graph (bool, 可选) – 如果False，用于计算梯度的图将被释放。请注意，在几乎所有情况下，不需要将此选项设置为真，而且通常可以更有效地解决问题。默认为create_graph值。 create_graph (bool, 可选) – 如果True，则构造导数图，以便计算更高阶导数，默认False。 allow_unused (bool, 可选) – 如果False, 当计算输出出错时指明不使用的输入 (因此它们的梯度一直是0)。 默认False。 局部禁用梯度计算 class torch.autograd.no_grad 禁用梯度计算的上下文管理器。 当你确认不会调用 Tensor.backward()，对于推断禁用梯度计算是有用的。它将减少计算的内存消耗，否则会有requires_grad=True。在这个模式中，每个计算结果将导致requires_grad=False, 即便输入有requires_grad=True。 函数还可作为装饰器。 示例： >>> x = torch.tensor([1], requires_grad=True) >>> with torch.no_grad(): ... y = x * 2 >>> y.requires_grad False >>> @torch.no_grad() ... def doubler(x): ... return x * 2 >>> z = doubler(x) >>> z.requires_grad False class torch.autograd.enable_grad 使能梯度计算的上下文管理器。 在一个no_grad上下文中使能梯度计算。在no_grad外部此上下文管理器无影响 函数还可作为装饰器。 示例： >>> x = torch.tensor([1], requires_grad=True) >>> with torch.no_grad(): ... with torch.enable_grad(): ... y = x * 2 >>> y.requires_grad True >>> y.backward() >>> x.grad >>> @torch.enable_grad() ... def doubler(x): ... return x * 2 >>> with torch.no_grad(): ... z = doubler(x) >>> z.requires_grad True class torch.autograd.set_grad_enabled(mode) 设置梯度计算打开或关闭的上下文管理器。 set_grad_enabled将基于它的参数mode使用或禁用梯度。它也能作为一个上下文管理器或函数使用。 参数: mode (bool) – 标记是否使能梯度（True），或使能（False）。这能被用在有条件的使能梯度。 示例： >>> x = torch.tensor([1], requires_grad=True) >>> is_train = False >>> with torch.set_grad_enabled(is_train): ... y = x * 2 >>> y.requires_grad False >>> torch.set_grad_enabled(True) >>> y = x * 2 >>> y.requires_grad True >>> torch.set_grad_enabled(False) >>> y = x * 2 >>> y.requires_grad False 关于Tensors的原位操作 在autograd中支持原位操作是一件很难的事，并且我们在大多数情况下不鼓励使用它们。Autograd积极的缓冲区释放和重用使其非常高效，实际上原位操作会大幅降低内存使用量的情况非常少。你可能永远不会使用它们，除非正在很大的内存压力下操作。 就地正确性检查 全部的Tensor保持追踪应用到它们身上的原位操作，并且如果实现检测到在任何一个函数中，一个tensor为反向传播保存，但是随后被原位修改，一旦反向传播开始将抛出一个错误。此设计确保如果你正在使用原位操作函数并且没有看到任何错误，你可以确保计算的梯度是正确的。 Variable (弃用) 警告 Variable API已经被弃用。对张量使用自动求导不再需要Variable。Autograd自动支持requires_grad参数设置成True的张量。以下是有关更改内容的快速指南： Variable(tensor) 和Variable(tensor, requires_grad)仍然和预期一样工作，但它们返回Tensors代替Variables。 var.data 和 tensor.data是一回事。 方法如var.backward(), var.detach(), var.register_hook()现在在tensors上使用相同的名字起作用。 此外，现在可以使用诸如torch.randn(), torch.zeros(), torch.ones()等工厂方法创建requires_grad=True的张量，如下所示： autograd_tensor = torch.randn((2, 3, 4), requires_grad=True) 张量自动求导函数 class torch.Tensor backward(gradient=None, retain_graph=None, create_graph=False) 计算当前张量关于图叶节点的梯度。 图使用链式反则微分。如果张量是非标量并且要求梯度，函数额外要求指梯度。它应是一个匹配类型和位置的张量，含有可微函数关于它本身的梯度。 此函数在叶节点累加梯度-你可能需要在调用前将它初始化为0。 参数： gradient (Tensor 或 None) – 关于张量的梯度。如果它是一个张量，它将被自动转换成不要求梯度的张量，除非create_graph是True。标量张量或不需要梯度的可用None指定。如果None对所有grad_tensors可接受，则此参数可选。 retain_graph (bool, 可选) – 如果False，用于计算梯度的图将被释放。请注意，在几乎所有情况下，不需要将此选项设置为真，而且通常可以更有效地解决问题。默认为create_graph值。 create_graph (bool, 可选) – 如果True，则构造导数图，以便计算更高阶导数，默认False。 detach() 返回一个新的Tensor，从当前图中分离出来。 结果不要求梯度。 注意 返回的张量与原始张量使用相同的数据。关于它们中任一个原位修改将被看见，并且可能在正确性检查中触发错误。 detach_() 从创建它的图中分离张量，使其成为叶。不能就地分离视图。 grad 此属性默认None，并且调用backward()计算自身梯度时第一时间成为一个Tensor。此属性将含计算的梯度，以后调用backward()将累加提到到自身。 is_leaf 按惯例，所有requires_grad=False的张量将是叶节点张量 如果张量是由用户创建，requires_grad的张量也是叶节点张量。这意味着它们不是一个操作的结果，并且grad_fn是None。 仅叶节点张量在调用backward()时填充它们的grad。为得到从非叶节点张量填充的梯度，你可以使用retain_grad(). 示例： >>> a = torch.rand(10, requires_grad=True) >>> a.is_leaf True >>> b = torch.rand(10, requires_grad=True).cuda() >>> b.is_leaf False # b 是由cpu Tensor投入cuda Tensor的操作创建的 >>> c = torch.rand(10, requires_grad=True) + 2 >>> c.is_leaf False # c 是由加操作创建的 >>> d = torch.rand(10).cuda() >>> d.is_leaf True # d 不要求梯度，所以没有创建它的操作 (被自动求导引擎追踪) >>> e = torch.rand(10).cuda().requires_grad_() >>> e.is_leaf True # e 要求梯度并且没有创建它的操作 >>> f = torch.rand(10, requires_grad=True, device=\"cuda\") >>> f.is_leaf True # f 要求梯度并且没有创建它的操作 register_hook(hook) 注册一个反向钩子 此钩子每次在对应张量梯度被计算时调用。此钩子应有下面鲜明特征： hook(grad) -> Tensor or None 此钩子不应该修改它的参数，但它能可选地返回一个新的用于替代 grad的梯度。 此函数返回一个句柄，其句柄方法为handle.remove()，用于从模块中删除钩子。 示例： >>> v = torch.tensor([0., 0., 0.], requires_grad=True) >>> h = v.register_hook(lambda grad: grad * 2) # double the gradient >>> v.backward(torch.tensor([1., 2., 3.])) >>> v.grad 2 4 6 [torch.FloatTensor of size (3,)] >>> h.remove() # removes the hook requires_grad 如果梯度需要为此张量计算则是True，否则为False 注意 事实是梯度需要为此张量计算不意味着grad属性将被填充，更多细节见is_leaf。 retain_grad() 为非叶节点张量使能.grad属性 Function class torch.autograd.Function 记录操作历史，定义可微操作公式。 在Tensor上执行的每个操作都会创建一个新的函数对象，执行计算，记录它发生的。历史记录以函数的DAG形式保留，DAG的边表示数据的依赖性（input &lt;- output）。然后，当backward被调用，图按拓扑顺序被处理，通过调用每个Function对象的backward()方法，并且传递梯度给下一个Function。 通常，用户与函数交互的唯一方式是通过创建子类和定义新操作。这是一种被推荐的扩展torch.autograd的方式。 每个函数对象只能使用一次（在正向传递中）。 示例： >>> class Exp(Function): >>> >>> @staticmethod >>> def forward(ctx, i): >>> result = i.exp() >>> ctx.save_for_backward(result) >>> return result >>> >>> @staticmethod >>> def backward(ctx, grad_output): >>> result, = ctx.saved_tensors >>> return grad_output * result static backward(ctx, *grad_outputs) 定义一个公式计算操作导数。 此函数被所有子类重载。 它必须接受一个上下文ctx作为第一个参数，随后是forward()返回的大量输出，并且它应返回尽可能多的张量，作为forward()函数输入。每个参数是关于被给输出的梯度，并且每个返回值是关于相应输入的梯度。 ctx上下文可用于恢复保存在前向传播过程的梯度。它有一个ctx.needs_input_grad属性，作为一个代表每个输入是否需要梯度的布尔元组。 static forward(ctx, *args, **kwargs) 执行操作。 此函数被所有子类重载。 它必须接受一个上下文ctx作为第一个参数，随后是任意数量的参数（tensor或其它类型）。 此上下文可被用来存储张量，随后可在反向传播过程取出。 数值梯度检查 torch.autograd.gradcheck(func, inputs, eps=1e-06, atol=1e-05, rtol=0.001, raise_exception=True) 通过小的有限差分与关于浮点类型且requires_grad=True的输入张量来检查计算的梯度。 在数组梯度和分析梯度之间检查使用allclose()。 注意 默认值为双精度输入设计。如果输入欠精度此检查有可能失败，例如，FloatTensor。 警告 如果在输入中任何被检查的张量有重叠的内存，换句话说，指向相同内存地址的不同切片（例如，从torch.expand()），此检查将有可能失败，因为在这个索引通过点扰动计算的数值梯度将改变在全部其它索引处共享内存地址的值。 参数 func (function) – 一个Python函数，输入是张量，返回一个张量或张量元组 inputs (张量元组 or Tensor) – func函数输入 eps (float, 可选) – 有限差分的扰动 atol (float, 可选) – 绝对容差 rtol (float, 可选) – 相对容差 raise_exception (bool, 可选) – 指示如果检查失败是否抛出一个异常。此异常给出关于失败的确切性质的更多信息。这在梯度检查调试时是有用的。 返回: 如果所有差都满足全部闭合条件，则为True torch.autograd.gradgradcheck(func, inputs, grad_outputs=None, eps=1e-06, atol=1e-05, rtol=0.001, gen_non_contig_grad_outputs=False, raise_exception=True) 通过小的有限差分与关于在输入中张量的分析梯度，检查已计算梯度的梯度，并且在requires_grad=True 情况下，grad_outputs是浮点类型。 此函数检查通过计算到给定grad_outputs的梯度的反向传播是否正确。 在数值梯度和分析梯度之间使用allclose()检查。 注意 默认值为双精度输入设计。如果输入欠精度此检查有可能失败，例如，FloatTensor。 警告 如果在输入中任何被检查的张量有重叠的内存，换句话说，指向相同内存地址的不同切片（例如，从torch.expand()），此检查将有可能失败，因为在这个索引通过点扰动计算的数值梯度将改变在全部其它索引处共享内存地址的值。 参数： func (function) – 一个Python函数，输入是张量，返回一个张量或张量元组 inputs (张量元组 or Tensor) – func函数输入 grad_outputs (tuple of Tensor or Tensor, 可选) – The gradients with respect to the function’s outputs. eps (float, 可选) – 有限差分的扰动 atol (float, 可选) – 绝对容差 rtol (float, 可选) – 相对容差 gen_non_contig_grad_outputs (bool, 可选) – 如果 grad_outputs 是 None 并且 gen_non_contig_grad_outputs 是 True，随机生成的梯度输出是不连续的 raise_exception (bool, 可选) – 指示如果检查失败是否抛出一个异常。此异常给出关于失败的确切性质的更多信息。这在梯度检查调试时是有用的。 返回: 如果所有差都满足全部闭合条件，则为True Profiler Autograd 包含一个事件探查器，让你洞察在你的模型中不同操作的代价-CPU和GPU中都有。现在有两种模式实现-CPU-仅使用profile，和使用emit_nvtx的nvprof（注册CPU和GPU活动） class torch.autograd.profiler.profile(enabled=True, use_cuda=False) 上下文管理器管理autograd事件探查器状态和保持一份汇总结果。 参数: enabled (bool, 可选) – 设置成False 让此上下文管理一个 no-op. 默认：True。 use_cuda (bool, 可选) – 使用cudaEvent API也可以启用CUDA事件的计时。 每个张量操作增加大约4us的开销。默认: False 示例： >>> x = torch.randn((1, 1), requires_grad=True) >>> with torch.autograd.profiler.profile() as prof: ... y = x ** 2 ... y.backward() >>> # 注意：为简洁起见，删除了一些列 ... print(prof) ------------------------------------- --------------- --------------- Name CPU time CUDA time ------------------------------------- --------------- --------------- PowConstant 142.036us 0.000us N5torch8autograd9GraphRootE 63.524us 0.000us PowConstantBackward 184.228us 0.000us MulConstant 50.288us 0.000us PowConstant 28.439us 0.000us Mul 20.154us 0.000us N5torch8autograd14AccumulateGradE 13.790us 0.000us N5torch8autograd5CloneE 4.088us 0.000us export_chrome_trace(path) 将EventList导出为Chrome跟踪工具文件。 检查点随后被加载和检查在chrome://tracing URL。 参数: path (str) – 将写入跟踪的路径。 key_averages() 平均键上的所有函数事件. 返回: 一个包含FunctionEventAvg对象的EventList。 table(sort_by=None) 将EventList打印为格式良好的表。 参数: sort_by (str, optional) – 用来排序事件的属性。默认以它们被注册时顺序打印。 合法的关键字包括：cpu_time, cuda_time, cpu_time_total, cuda_time_total, count。 返回: 一个包含表格的字符串。 --- --- total_average() 平均化全部事件。 返回: 一个FunctionEventAvg事件。 class torch.autograd.profiler.emit_nvtx(enabled=True) 让每个自动求导操作发出在一个NVTX范围内的上下文管理器。 当在nvprof下运行程序是有用的： nvprof --profile-from-start off -o trace_name.prof -- 不幸地，没有办法强制nvprof将它收集的数据输出到磁盘，所以对于CUDA分析，必须使用此上下文管理器来声明nvprof跟踪并等待进程在检查之前退出。然后，可使用NVIDIA可视化Profiler(nvvp)来显示时间线，或torch.autograd.profiler.load_nvprof()可加载结果以供检查，例如：在Python REPL中。 参数: enabled (bool, 可选) – 设置成False 让此上下文管理一个 no-op. 默认：True。 示例： >>> with torch.cuda.profiler.profile(): ... model(x) # Warmup CUDA memory allocator and profiler ... with torch.autograd.profiler.emit_nvtx(): ... model(x) Forward-backward correlation 在Nvidia Visual Profiler中查看使用emit_nvtx创建的配置文件时，将每个反向传递操作与相应的前向传递操作相关联可能很困难。 为了简化此任务，emit_nvtx将序列号信息附加到它生成的范围。 在前向传递过程，每个函数范围都用seq=&lt;N&gt;进行修饰。 seq是一个运行计数器，每次创建一个新的反向Function对象时会递增，并对前向不可见。 因此，与每个前向函数范围相关联的seq=&lt;N&gt;注释告诉你，如果此前向函数创建了反向的Function对象，则反向对象将收到序列号N。在反向传递过程，顶层范围包装每个C++反向函数的apply()调用都用不可见的stashed seq=&lt;M&gt;进行修饰。 M是创建反向对象的序列号。 通过比较在反向不可见的序列号和在前向的序列号，你可以跟踪哪个前向操作创建了每个反向函数。 在反向传递期间执行的任何函数也用seq=&lt;N&gt;进行修饰。 在默认反向（使用create_graph=False）时，此信息无关紧要，事实上，对于所有此类函数，N可能只是0。 作为将这些Function对象与早期的向前传递相关联的方法，只有与反向Function对象的apply()方法关联的顶级范围才有用。 Double-backward 另一方面，如果正在进行create_graph=True的反向传递（换句话说，如果你设置为double-backward），则在反向期间执行每个被给一个非零，有用的seq=&lt;N&gt;的函数。 这些函数本身可以稍后在double-backward期间创建Function对象来执行，就像在前向传递中原始函数所做的一样。 反向和double-backward的关系在概念上与前向和反向的关系相同：函数仍然发出当前序列号标记的范围，它们创建的Function对象仍然存储那些序列号，并且在最终的double-backward期间 向后，Function对象的apply()范围仍然用 stashed seq数字标记，可以与反向传递中的seq数字进行比较。 torch.autograd.profiler.load_nvprof(path) 打开一个nvprof跟踪文件并且解析autograd注释。 参数: path (str) – nvprof跟踪路径 异常检测 class torch.autograd.detect_anomaly 上下文管理器，为自动求导引擎使能异常检测。 这做了两件事：- 在启用检测的情况下运行前向传递将允许反向传递打印创建失败的反向函数的前向操作跟踪。 - 任何生成“nan”值的反向计算都会引发错误。 示例 >>> import torch >>> from torch import autograd >>> class MyFunc(autograd.Function): ... @staticmethod ... def forward(ctx, inp): ... return inp.clone() ... @staticmethod ... def backward(ctx, gO): ... # Error during the backward pass ... raise RuntimeError(\"Some error in backward\") ... return gO.clone() >>> def run_fn(a): ... out = MyFunc.apply(a) ... return out.sum() >>> inp = torch.rand(10, 10, requires_grad=True) >>> out = run_fn(inp) >>> out.backward() Traceback (most recent call last): File \"\", line 1, in File \"/your/pytorch/install/torch/tensor.py\", line 93, in backward torch.autograd.backward(self, gradient, retain_graph, create_graph) File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward allow_unreachable=True) # allow_unreachable flag File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply return self._forward_cls.backward(self, *args) File \"\", line 8, in backward RuntimeError: Some error in backward >>> with autograd.detect_anomaly(): ... inp = torch.rand(10, 10, requires_grad=True) ... out = run_fn(inp) ... out.backward() Traceback of forward call that caused the error: File \"tmp.py\", line 53, in out = run_fn(inp) File \"tmp.py\", line 44, in run_fn out = MyFunc.apply(a) Traceback (most recent call last): File \"\", line 4, in File \"/your/pytorch/install/torch/tensor.py\", line 93, in backward torch.autograd.backward(self, gradient, retain_graph, create_graph) File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward allow_unreachable=True) # allow_unreachable flag File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply return self._forward_cls.backward(self, *args) File \"\", line 8, in backward RuntimeError: Some error in backward class torch.autograd.set_detect_anomaly(mode) 上下文管理器，为自动求导引擎设置异常检测开或关。 set_detect_anomaly将基于它的参数mode使能或禁用自动求导异常检测。它也能作为一个上下文管理器或函数使用。 异常检测行为细节见上面detect_anomaly。 参数: mode (bool) – 标记是否使能异常检测（True），或禁用（False）。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed.html":{"url":"distributed.html","title":"torch.distributed","keywords":"","body":"分布式通信包 - torch.distributed 译者：univeryinli 后端 torch.distributed 支持三个后端，每个后端具有不同的功能。下表显示哪些功能可用于CPU/CUDA张量。仅当用于构建PyTorch的实现支持时，MPI才支持CUDA。 后端 gloo mpi nccl 设备 CPU GPU CPU GPU CPU GPU --- --- --- --- --- --- --- 发送 ✓ ✘ ✓ ? ✘ ✘ 接收 ✓ ✘ ✓ ? ✘ ✘ 广播 ✓ ✓ ✓ ? ✘ ✓ all_reduce ✓ ✓ ✓ ? ✘ ✓ reduce ✓ ✘ ✓ ? ✘ ✓ all_gather ✓ ✘ ✓ ? ✘ ✓ 收集 ✓ ✘ ✓ ? ✘ ✘ 分散 ✓ ✘ ✓ ? ✘ ✘ 屏障 ✓ ✘ ✓ ? ✘ ✓ PyTorch附带的后端 目前PyTorch分发版仅支持Linux。默认情况下，Gloo和NCCL后端构建并包含在PyTorch的分布之中（仅在使用CUDA构建时为NCCL）。MPI是一个可选的后端，只有从源代码构建PyTorch时才能包含它。（例如，在安装了MPI的主机上构建PyTorch） 哪个后端使用？ 在过去，我们经常被问到：“我应该使用哪个后端？”。 经验法则 使用NCCL后端进行分布式 GPU 训练。 使用Gloo后端进行分布式 CPU 训练。 具有InfiniBand互连的GPU主机 使用NCCL，因为它是目前唯一支持InfiniBand和GPUDirect的后端。 GPU主机与以太网互连 使用NCCL，因为它目前提供最佳的分布式GPU训练性能，特别是对于多进程单节点或多节点分布式训练。如果您遇到NCCL的任何问题，请使用Gloo作为后备选项。（请注意，Gloo目前运行速度比GPU的NCCL慢。） 具有InfiniBand互连的CPU主机 如果您的InfiniBand在IB上已启用IP，请使用Gloo，否则请使用MPI。我们计划在即将发布的版本中为Gloo添加InfiniBand支持。 具有以太网互连的CPU主机 除非您有特殊原因要使用MPI，否则请使用Gloo。 常见的环境变量 选择要使用的网络接口 默认情况下，NCCL和Gloo后端都会尝试查找用于通信的网络接口。但是，从我们的经验来看，并不总能保证这一点。因此，如果您在后端遇到任何问题而无法找到正确的网络接口。您可以尝试设置以下环境变量（每个变量适用于其各自的后端）： NCCL_SOCKET_IFNAME, 比如 export NCCL_SOCKET_IFNAME=eth0 GLOO_SOCKET_IFNAME, 比如 export GLOO_SOCKET_IFNAME=eth0 其他NCCL环境变量 NCCL还提供了许多用于微调目的的环境变量 常用的包括以下用于调试目的： export NCCL_DEBUG=INFO export NCCL_DEBUG_SUBSYS=ALL 有关NCCL环境变量的完整列表，请参阅NVIDIA NCCL的官方文档 基本 torch.distributed包为在一台或多台机器上运行的多个计算节点上的多进程并行性提供PyTorch支持和通信原语。类 torch.nn.parallel.DistributedDataParallel()基于此功能构建，以提供同步分布式训练作为包装器任何PyTorch模型。这与 Multiprocessing package - torch.multiprocessing 和 torch.nn.DataParallel() 因为它支持多个联网的机器，并且用户必须为每个进程显式启动主训练脚本的单独副本。 在单机同步的情况下，torch.distributed 或者 torch.nn.parallel.DistributedDataParallel() 与其他数据并行方法相比，包装器仍然具有优势，包含 torch.nn.DataParallel(): 每个进程都维护自己的优化器，并在每次迭代时执行完整的优化步骤。虽然这可能看起来是多余的，但由于梯度已经聚集在一起并且在整个过程中平均，因此对于每个过程都是相同的，这意味着不需要参数广播步骤，减少了在节点之间传输张量所花费的时间。 每个进程都包含一个独立的Python解释器，消除了额外的解释器开销和来自单个Python进程驱动多个执行线程，模型副本或GPU的“GIL-thrashing”。这对于大量使用Python运行时的模型尤其重要，包括具有循环层或许多小组件的模型。 初始化 这个包在调用其他的方法之前，需要使用 torch.distributed.init_process_group() 函数进行初始化。这将阻止所有进程加入。 torch.distributed.init_process_group(backend, init_method='env://', timeout=datetime.timedelta(seconds=1800), **kwargs) 初始化默认的分布式进程组，这也将初始化分布式程序包 参数: backend (str or Backend) – 后端使用。根据构建时配置，有效值包括 mpi，gloo和nccl。该字段应该以小写字符串形式给出(例如\"gloo\")，也可以通过Backend访问属性(例如Backend.GLOO)。 init_method (str, optional) – 指定如何初始化进程组的URL。 world_size (int, optional) – 参与作业的进程数。 rank (int, optional) – 当前流程的排名。 timeout (timedelta__, optional) – 针对进程组执行的操作超时，默认值等于30分钟，这仅适用于gloo后端。 group_name (str, optional__, deprecated) – 团队名字。 要启用backend == Backend.MPI，PyTorch需要在支持MPI的系统上从源构建，这同样适用于NCCL。 class torch.distributed.Backend 类似枚举的可用后端类：GLOO，NCCL和MPI。 这个类的值是小写字符串，例如“gloo”。它们可以作为属性访问，例如Backend.NCCL。 可以直接调用此类来解析字符串，例如，Backend（backend_str）将检查backend_str是否有效，如果是，则返回解析的小写字符串。它也接受大写字符串，例如`Backend（“GLOO”）return“gloo”。 注意 条目Backend.UNDEFINED存在但仅用作某些字段的初始值。用户既不应直接使用也不应假设存在。 torch.distributed.get_backend(group=) 返回给定进程组的后端 参数: group (ProcessGroup__, optional) – 要处理的进程组。默认值是常规主进程组。如果指定了另一个特定组，则调用进程必须是group的一部分。 返回: 给定进程组的后端作为小写字符串 --- --- torch.distributed.get_rank(group=) 返回当前进程组的排名 Rank是分配给分布式进程组中每个进程的唯一标识符。它们总是从0到world_size的连续整数。 参数: group (ProcessGroup__, optional) – 要处理的进程组 返回: 进程组-1的等级，如果不是该组的一部分 --- --- torch.distributed.get_world_size(group=) 返回当前进程组中的进程数 参数: group (ProcessGroup__, optional) – 要处理的进程组 返回: 进程组-1的世界大小，如果不是该组的一部分 --- --- torch.distributed.is_initialized() 检查是否已初始化默认进程组 torch.distributed.is_mpi_available() 检查MPI是否可用 torch.distributed.is_nccl_available() 检查NCCL是否可用 目前支持三种初始化方法： TCP初始化 有两种方法可以使用TCP进行初始化，这两种方法都需要从所有进程可以访问的网络地址和所需的world_size。第一种方法需要指定属于rank 0进程的地址。此初始化方法要求所有进程都具有手动指定的排名。 请注意，最新的分布式软件包中不再支持多播地址。group_name也被弃用了。 import torch.distributed as dist # 使用其中一台机器的地址 dist.init_process_group(backend, init_method='tcp://10.1.1.20:23456', rank=args.rank, world_size=4) 共享文件系统初始化 另一种初始化方法使用一个文件系统，该文件系统与组中的所有机器共享和可见，以及所需的world_size。URL应以file：//开头，并包含共享文件系统上不存在的文件（在现有目录中）的路径。如果文件不存在，文件系统初始化将自动创建该文件，但不会删除该文件。因此，下一步初始化 init_process_group() 在相同的文件路径发生之前您有责任确保清理文件。 请注意，在最新的分布式软件包中不再支持自动排名分配，并且也不推荐使用group_name。 警告 此方法假定文件系统支持使用fcntl进行锁定 - 大多数本地系统和NFS都支持它。 警告 此方法将始终创建该文件，并尽力在程序结束时清理并删除该文件。换句话说，每次进行初始化都需要创建一个全新的空文件，以便初始化成功。如果再次使用先前初始化使用的相同文件（不会被清除），则这是意外行为，并且经常会导致死锁和故障。因此，即使此方法将尽力清理文件，如果自动删除不成功，您有责任确保在训练结束时删除该文件以防止同一文件被删除 下次再次使用。如果你打算在相同的文件系统路径下多次调用 init_process_group() 的时候，就显得尤为重要了。换一种说法，如果那个文件没有被移除并且你再次调用 init_process_group()，那么失败是可想而知的。这里的经验法则是，每当调用init_process_group()的时候，确保文件不存在或为空。 import torch.distributed as dist # 应始终指定等级 dist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile', world_size=4, rank=args.rank) 环境变量初始化 此方法将从环境变量中读取配置，从而可以完全自定义信息的获取方式。要设置的变量是： MASTER_PORT - 需要; 必须是机器上的自由端口，等级为0。 MASTER_ADDR - 要求（0级除外）; 等级0节点的地址。 WORLD_SIZE - 需要; 可以在这里设置，也可以在调用init函数时设置。 RANK - 需要; 可以在这里设置，也可以在调用init函数时设置。 等级为0的机器将用于设置所有连接。 这是默认方法，意味着不必指定init_method（或者可以是env：//）。 组 默认情况下，集合体在默认组（也称为世界）上运行，并要求所有进程都进入分布式函数调用。但是，一些工作负载可以从更细粒度的通信中受益。这是分布式群体发挥作用的地方。new_group() 函数可用于创建新组，具有所有进程的任意子集。它返回一个不透明的组句柄，可以作为所有集合体的“group”参数给出（集合体是分布式函数，用于在某些众所周知的编程模式中交换信息）。 目前torch.distributed不支持创建具有不同后端的组。换一种说法，每一个正在被创建的组都会用相同的后端，只要你在 init_process_group() 里面声明清楚。 torch.distributed.new_group(ranks=None, timeout=datetime.timedelta(seconds=1800)) 创建一个新的分布式组 此功能要求主组中的所有进程（即属于分布式作业的所有进程）都进入此功能，即使它们不是该组的成员也是如此。此外，应在所有进程中以相同的顺序创建组。 参数: ranks (list[int]) – 小组成员的等级列表。 timeout (timedelta__, optional) – 针对进程组执行的操作超时，默认值等于30分钟，这仅适用于gloo后端。 返回: 分布式组的句柄，可以给予集体调用 点对点通信 torch.distributed.send(tensor, dst, group=, tag=0) 同步发送张量 参数: tensor (Tensor) – 准备发送的张量。 dst (int) – 目的地排名。 group (ProcessGroup__, optional) – 要处理的进程组。 tag (int, optional) – 标记以匹配发送与远程接收。 torch.distributed.recv(tensor, src=None, group=, tag=0) 同步接收张量 参数： tensor (Tensor) – 张量填充接收的数据。 src (int, optional) – 来源排名。如果未指定，将从任何流程收到。 group (ProcessGroup__, optional) – 要处理的进程组。 tag (int, optional) – 标记以匹配接收与远程发送。 返回: 发件人排名-1，如果不是该组的一部分 isend() 和 irecv() 使用时返回分布式请求对象。通常，此对象的类型未指定，因为它们永远不应手动创建，但它们保证支持两种方法： is_completed() - 如果操作已完成，则返回True。 wait() - 将阻止该过程，直到操作完成，is_completed（）保证一旦返回就返回True。 torch.distributed.isend(tensor, dst, group=, tag=0) 异步发送张量 参数: tensor (Tensor) – 准本发送的张量。 dst (int) – 目的地排名。 group (ProcessGroup__, optional) – 要处理的进程组。 tag (int, optional) – 标记以匹配发送与远程接收。 返回: 分布式请求对象。没有，如果不是该组的一部分 torch.distributed.irecv(tensor, src, group=, tag=0) 异步接收张量 参数: tensor (Tensor) – 张量填充接收的数据。 src (int) – 来源排名。 group (ProcessGroup__, optional) – 要处理的进程组。 tag (int, optional) – 标记以匹配接收与远程发送。 返回: 分布式请求对象。没有，如果不是该组的一部分 同步和异步集合操作 每个集合操作函数都支持以下两种操作： 同步操作 - 默认模式，当async_op设置为False时。当函数返回时，保证执行集合操作（如果它是CUDA操作，则不一定完成，因为所有CUDA操作都是异步的），并且可以调用任何进一步的函数调用，这取决于集合操作的数据。在同步模式下，集合函数不返回任何内容。 asynchronous operation - 当async_op设置为True时。集合操作函数返回分布式请求对象。通常，您不需要手动创建它，并且保证支持两种方法： is_completed() - 如果操作已完成，则返回True。 wait() - 将阻止该过程，直到操作完成。 集体职能 torch.distributed.broadcast(tensor, src, group=, async_op=False) 将张量广播到整个群体 tensor必须在参与集合体的所有进程中具有相同数量的元素。 参数: tensor (Tensor) – 如果src是当前进程的等级，则发送的数据，否则用于保存接收数据的张量。 src (int) – 来源排名。 group (ProcessGroup__, optional) – 要处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作。 返回: 异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分 torch.distributed.all_reduce(tensor, op=ReduceOp.SUM, group=, async_op=False) 减少所有机器上的张量数据，以便获得最终结果 调用tensor之后在所有进程中将按位相同。 参数: tensor (Tensor) – 集体的输入和输出。该功能就地运行。 op (optional) – 来自torch.distributed.ReduceOp枚举的值之一。指定用于逐元素减少的操作。 group (ProcessGroup__, optional) – 要处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作。 返回: 异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分 torch.distributed.reduce(tensor, dst, op=ReduceOp.SUM, group=, async_op=False) 减少所有机器的张量数据 只有排名为“dst”的进程才会收到最终结果。 参数: tensor (Tensor) – 集体的输入和输出。该功能就地运行。 dst (int) – 目的地排名。 op (optional) – 来自torch.distributed.ReduceOp枚举的值之一。指定用于逐元素减少的操作。 group (ProcessGroup__, optional) – 要处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作。 返回: 异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分 torch.distributed.all_gather(tensor_list, tensor, group=, async_op=False) 从列表中收集整个组的张量 参数： tensor_list (list[Tensor]) – 输出列表。它应包含正确大小的张量，用于集合的输出。 tensor (Tensor) – 从当前进程广播的张量。 group (ProcessGroup__, optional) – 要处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作。 返回: 异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分 torch.distributed.gather(tensor, gather_list, dst, group=, async_op=False) 在一个过程中收集张量列表 参数： tensor (Tensor) – 输入张量。 gather_list (list[Tensor]) – 用于接收数据的适当大小的张量列表。仅在接收过程中需要。 dst (int) – 目的地排名。除接收数据的进程外，在所有进程中都是必需的。 group (ProcessGroup__, optional) – 要处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作。 返回: 异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分 torch.distributed.scatter(tensor, scatter_list, src, group=, async_op=False) 将张量列表分散到组中的所有进程 每个进程只接收一个张量并将其数据存储在tensor参数中。 参数： tensor (Tensor) – 输出张量。 scatter_list (list[Tensor]) – 要分散的张量列表。仅在发送数据的过程中需要。 src (int) – 来源排名。除发送数据的进程外，在所有进程中都是必需的。 group (ProcessGroup__, optional) – 要处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作。 返回: 异步工作句柄，如果async_op设置为True。如果不是async_op或不是组的一部分，无 torch.distributed.barrier(group=, async_op=False) 同步所有进程 如果async_op为False，或者在wait（）上调用异步工作句柄，则此集合会阻止进程直到整个组进入此函数。 参数： group (ProcessGroup__, optional) – 要处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作。 返回: 异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分 class torch.distributed.ReduceOp 类似枚举的可用减少操作类：SUM，PRODUCT，MIN和MAX。 该类的值可以作为属性访问，例如，ReduceOp.SUM。它们用于指定减少集群的战略，例如 reduce(), all_reduce_multigpu()。 成员： SUM PRODUCT MIN MAX class torch.distributed.reduce_op 用于还原操作的不再使用的枚举类：SUM，PRODUCT，MIN和MAX。 建议使用ReduceOp 代替。 多GPU集群功能 如果每个节点上有多个GPU，则在使用NCCL和Gloo后端时，broadcast_multigpu() all_reduce_multigpu() reduce_multigpu() 和 all_gather_multigpu() 支持每个节点内多个GPU之间的分布式集合操作。这些功能可以潜在地提高整体分布式训练性能，并通过传递张量列表轻松使用。传递的张量列表中的每个张量需要位于调用该函数的主机的单独GPU设备上。请注意，张量列表的长度在所有分布式进程中需要相同。另请注意，目前只有NCCL后端支持多GPU集合功能。 例如，如果我们用于分布式训练的系统有2个节点，每个节点有8个GPU。在16个GPU中的每一个上，都有一个我们希望减少的张量，以下代码可以作为参考： 代码在节点0上运行 import torch import torch.distributed as dist dist.init_process_group(backend=\"nccl\", init_method=\"file:///distributed_test\", world_size=2, rank=0) tensor_list = [] for dev_idx in range(torch.cuda.device_count()): tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx)) dist.all_reduce_multigpu(tensor_list) 代码在节点1上运行 import torch import torch.distributed as dist dist.init_process_group(backend=\"nccl\", init_method=\"file:///distributed_test\", world_size=2, rank=1) tensor_list = [] for dev_idx in range(torch.cuda.device_count()): tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx)) dist.all_reduce_multigpu(tensor_list) 调用结束后，两个节点上的所有16个张量都将具有16的全减值。 torch.distributed.broadcast_multigpu(tensor_list, src, group=, async_op=False, src_tensor=0) 使用每个节点多个GPU张量将张量广播到整个组 tensor必须在参与集合体的所有进程的所有GPU中具有相同数量的元素。列表中的每个张量必须位于不同的GPU上。 目前仅支持nccl和gloo后端张量应该只是GPU张量 参数： tensor_list (List__[Tensor]) – 参与集群操作行动的张量。如果src是排名，那么tensor_list`（`tensor_list [src_tensor]`）的`src_tensor元素将被广播到src进程中的所有其他张量（在不同的GPU上）以及tensor_list中的所有张量其他非src进程。您还需要确保调用此函数的所有分布式进程的len（tensor_list）是相同的。 src (int) – 源排行。 group (ProcessGroup__, optional) – 要被处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作。 src_tensor (int, optional) – 源张量等级在tensor_list内。 返回: 异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分 torch.distributed.all_reduce_multigpu(tensor_list, op=ReduceOp.SUM, group=, async_op=False) 减少所有机器上的张量数据，以便获得最终结果。此功能可减少每个节点上的多个张量，而每个张量位于不同的GPU上。因此，张量列表中的输入张量需要是GPU张量。此外，张量列表中的每个张量都需要驻留在不同的GPU上。 在调用之后，tensor_list中的所有tensor在所有进程中都是按位相同的。 目前仅支持nccl和gloo后端，张量应仅为GPU张量。 参数： list (tensor) – 集体的输入和输出张量列表。该功能就地运行，并要求每个张量在不同的GPU上为GPU张量。您还需要确保调用此函数的所有分布式进程的len（tensor_list）是相同的。 op (optional) – 来自torch.distributed.ReduceOp枚举的值之一，并且指定一个逐元素减少的操作。 group (ProcessGroup__, optional) – 要处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作。 返回: 异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分 torch.distributed.reduce_multigpu(tensor_list, dst, op=ReduceOp.SUM, group=, async_op=False, dst_tensor=0) 减少所有计算机上多个GPU的张量数据。tensor_list中的每个张量应位于单独的GPU上。 只有级别为'dst的进程中的'tensor_list [dst_tensor]的GPU才会收到最终结果。 目前仅支持nccl后端张量应该只是GPU张量。 参数： tensor_list (List__[Tensor]) – 输入和输出集体的GPU张量。该功能就地运行，您还需要确保调用此函数的所有分布式进程的len（tensor_list）是相同的。 dst (int) – 目的地排名。 op (optional) – 来自torch.distributed.ReduceOp枚举的值之一。指定一个逐元素减少的操作。 group (ProcessGroup__, optional) – The process group to work on async_op (bool, optional) – 这个操作是否应该是异步操作。 dst_tensor (int, optional) – 目标张量在tensor_list中排名。 返回: 异步工作句柄，如果async_op设置为True。没有，否则 torch.distributed.all_gather_multigpu(output_tensor_lists, input_tensor_list, group=, async_op=False) 从列表中收集整个组的张量。tensor_list中的每个张量应位于单独的GPU上。 目前仅支持nccl后端张量应该只是GPU张量。 参数： output_tensor_lists (List[List__[Tensor]__]) – 输出列表。它应该在每个GPU上包含正确大小的张量，以用于集合的输出。例如 output_tensor_lists [i]包含驻留在input_tensor_list [i]的GPU上的all_gather结果。请注意，output_tensor_lists [i]的每个元素都具有world_size * len（input_tensor_list）的大小，因为该函数全部收集组中每个GPU的结果。要解释output_tensor_list [i]的每个元素，请注意等级k的input_tensor_list [j]将出现在output_tensor_list [i] [rank * world_size + j]中。还要注意len（output_tensor_lists），并且output_tensor_lists中的每个元素的大小（每个元素都是一个列表，因此len（output_tensor_lists [i]）`）对于调用此函数的所有分布式进程都需要相同。 input_tensor_list (List__[Tensor]) – 从当前进程广播的张量（在不同的GPU上）的列表。请注意，调用此函数的所有分布式进程的len（input_tensor_list）必须相同。 group (ProcessGroup__, optional) – 要处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作 返回: 异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分 启动实用程序 torch.distributed包还在torch.distributed.launch中提供了一个启动实用程序。此帮助实用程序可用于为每个节点启动多个进程以进行分布式训练。该实用程序还支持python2和python3。 torch.distributed.launch是一个模块，它在每个训练节点上产生多个分布式训练过程。 该实用程序可用于单节点分布式训练，其中将生成每个节点的一个或多个进程。该实用程序可用于CPU训练或GPU训练。如果该实用程序用于GPU训练，则每个分布式进程将在单个GPU上运行。这可以实现良好改进的单节点训练性能。它还可以用于多节点分布式训练，通过在每个节点上产生多个进程来获得良好改进的多节点分布式训练性能。这对于具有多个具有直接GPU支持的Infiniband接口的系统尤其有利，因为所有这些接口都可用于聚合通信带宽。 在单节点分布式训练或多节点分布式训练的两种情况下，该实用程序将为每个节点启动给定数量的进程（--nproc_per_node）。如果用于GPU训练，此数字需要小于或等于当前系统上的GPU数量（'nprocper_node`），并且每个进程将在单个GPU上运行，从_GPU 0到GPU（nproc_per_node - 1）。 如何使用这个模块： 单节点多进程分布式训练 >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of your training script) 多节点多进程分布式训练:(例如两个节点） 节点1：（IP：192.168.1.1，并且有一个空闲端口：1234） >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\" --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of your training script) 节点2： >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\" --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of your training script) 1.查找此模块提供的可选参数： >>> python -m torch.distributed.launch --help 重要告示： 1. 这种实用和多进程分布式（单节点或多节点）GPU训练目前仅使用NCCL分布式后端实现最佳性能。因此，NCCL后端是用于GPU训练的推荐后端。 2. 在您的训练程序中，您必须解析命令行参数：--local_rank = LOCAL_PROCESS_RANK，这将由此模块提供。如果您的训练计划使用GPU，则应确保您的代码仅在LOCAL_PROCESS_RANK的GPU设备上运行。这可以通过以下方式完成： 解析local_rank参数 >>> import argparse >>> parser = argparse.ArgumentParser() >>> parser.add_argument(\"--local_rank\", type=int) >>> args = parser.parse_args() 使用其中一个将您的设备设置为本地排名 >>> torch.cuda.set_device(arg.local_rank) # before your code runs 或者 >>> with torch.cuda.device(arg.local_rank): >>> # your code to run 3. 在您的训练计划中，您应该在开始时调用以下函数来启动分布式后端。您需要确保init_method使用env：//，这是该模块唯一支持的init_method。 torch.distributed.init_process_group(backend='YOUR BACKEND', init_method='env://') 4. 在您的训练计划中，您可以使用常规分布式功能或使用 torch.nn.parallel.DistributedDataParallel() 模块。如果您的训练计划使用GPU进行训练，并且您希望使用 torch.nn.parallel.DistributedDataParallel() 模块。这里是如何配置它。 model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[arg.local_rank], output_device=arg.local_rank) 请确保将device_ids参数设置为您的代码将在其上运行的唯一GPU设备ID。这通常是流程的本地排名。换句话说，device_ids需要是[args.local_rank]，output_device需要是'args.local_rank`才能使用这个实用程序。 警告 local_rank不是全局唯一的：它只对机器上的每个进程唯一。因此，不要使用它来决定是否应该，例如，写入网络文件系统，参考 https://github.com/pytorch/pytorch/issues/12042 例如，如果您没有正确执行此操作，事情可能会出错。 Spawn实用程序 在 torch.multiprocessing.spawn() 里面，torch.multiprocessing包还提供了一个spawn函数. 此辅助函数可用于生成多个进程。它通过传递您要运行的函数并生成N个进程来运行它。这也可以用于多进程分布式训练。 有关如何使用它的参考，请参阅 PyToch example - ImageNet implementation 请注意，此函数需要Python 3.4或更高版本。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributions.html":{"url":"distributions.html","title":"torch.distributions","keywords":"","body":"概率分布 - torch.distributions 译者：hijkzzz distributions 包含可参数化的概率分布和采样函数. 这允许构造用于优化的随机计算图和随机梯度估计器. 这个包一般遵循 TensorFlow Distributions 包的设计. 通常, 不可能直接通过随机样本反向传播. 但是, 有两种主要方法可创建可以反向传播的代理函数. 即得分函数估计器/似然比估计器/REINFORCE和pathwise derivative估计器. REINFORCE通常被视为强化学习中策略梯度方法的基础, 并且pathwise derivative估计器常见于变分自动编码器中的重新参数化技巧. 得分函数仅需要样本的值 , pathwise derivative 需要导数 . 接下来的部分将在一个强化学习示例中讨论这两个问题. 有关详细信息, 请参阅 Gradient Estimation Using Stochastic Computation Graphs . 得分函数 当概率密度函数相对于其参数可微分时, 我们只需要sample()和log_prob()来实现REINFORCE: 是参数, 是学习速率, 是奖励 并且 是在状态 以及给定策略 执行动作 的概率. 在实践中, 我们将从网络输出中采样一个动作, 将这个动作应用于一个环境中, 然后使用log_prob构造一个等效的损失函数. 请注意, 我们使用负数是因为优化器使用梯度下降, 而上面的规则假设梯度上升. 有了确定的策略, REINFORCE的实现代码如下: probs = policy_network(state) # Note that this is equivalent to what used to be called multinomial m = Categorical(probs) action = m.sample() next_state, reward = env.step(action) loss = -m.log_prob(action) * reward loss.backward() Pathwise derivative 实现这些随机/策略梯度的另一种方法是使用来自rsample()方法的重新参数化技巧, 其中参数化随机变量可以通过无参数随机变量的参数确定性函数构造. 因此, 重新参数化的样本变得可微分. 实现Pathwise derivative的代码如下: params = policy_network(state) m = Normal(*params) # Any distribution with .has_rsample == True could work based on the application action = m.rsample() next_state, reward = env.step(action) # Assuming that reward is differentiable loss = -reward loss.backward() 分布 class torch.distributions.distribution.Distribution(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None) 基类: object Distribution是概率分布的抽象基类. arg_constraints 从参数名称返回字典到 Constraint 对象（应该满足这个分布的每个参数）.不是张量的arg不需要出现在这个字典中. batch_shape 返回批量参数的形状. cdf(value) 返回value处的累积密度/质量函数估计. | 参数: | value (Tensor) – | entropy() 返回分布的熵, 批量的形状为 batch_shape. | 返回值: | Tensor 形状为 batch_shape. | enumerate_support(expand=True) 返回包含离散分布支持的所有值的张量. 结果将在维度0上枚举, 所以结果的形状将是 (cardinality,) + batch_shape + event_shape (对于单变量分布 event_shape = ()). 注意, 这在lock-step中枚举了所有批处理张量[[0, 0], [1, 1], …]. 当 expand=False, 枚举沿着维度 0进行, 但是剩下的批处理维度是单维度, [[0], [1], ... 遍历整个笛卡尔积的使用 itertools.product(m.enumerate_support()). | 参数: | expand (bool) – 是否扩展对批处理dim的支持以匹配分布的 batch_shape. | | 返回值: | 张量在维上0迭代. | event_shape 返回单个样本的形状 (非批量). expand(batch_shape, _instance=None) 返回一个新的分布实例(或填充派生类提供的现有实例), 其批处理维度扩展为 batch_shape. 这个方法调用 expand 在分布的参数上. 因此, 这不会为扩展的分布实例分配新的内存. 此外, 第一次创建实例时, 这不会在中重复任何参数检查或参数广播在 __init__.py. 参数: batch_shape (torch.Size) – 所需的扩展尺寸. _instance – 由需要重写.expand的子类提供的新实例. | 返回值: | 批处理维度扩展为batch_size的新分布实例. | icdf(value) 返回按value计算的反向累积密度/质量函数. | 参数: | value (Tensor) – | log_prob(value) 返回按value计算的概率密度/质量函数的对数. | 参数: | value (Tensor) – | mean 返回分布的平均值. perplexity() 返回分布的困惑度, 批量的关于 batch_shape. | 返回值: | 形状为 batch_shape 的张量. | rsample(sample_shape=torch.Size([])) 如果分布的参数是批量的, 则生成sample_shape形状的重新参数化样本或sample_shape形状的批量重新参数化样本. sample(sample_shape=torch.Size([])) 如果分布的参数是批量的, 则生成sample_shape形状的样本或sample_shape形状的批量样本. sample_n(n) 如果分布参数是分批的, 则生成n个样本或n批样本. stddev 返回分布的标准差. support 返回Constraint 对象表示该分布的支持. variance 返回分布的方差. ExponentialFamily class torch.distributions.exp_family.ExponentialFamily(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None) 基类: torch.distributions.distribution.Distribution 指数族是指数族概率分布的抽象基类, 其概率质量/密度函数的形式定义如下 表示自然参数, 表示充分统计量, 是给定族的对数归一化函数 是carrier measure. 注意 该类是Distribution类与指数族分布之间的中介, 主要用于检验.entropy()和解析KL散度方法的正确性. 我们使用这个类来计算熵和KL散度使用AD框架和Bregman散度 (出自: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families). entropy() 利用对数归一化器的Bregman散度计算熵的方法. Bernoulli class torch.distributions.bernoulli.Bernoulli(probs=None, logits=None, validate_args=None) 基类: torch.distributions.exp_family.ExponentialFamily 创建参数化的伯努利分布, 根据 probs 或者 logits (但不是同时都有). 样本是二值的 (0 或者 1). 取值 1 伴随概率 p , 或者 0 伴随概率 1 - p. 例子: >>> m = Bernoulli(torch.tensor([0.3])) >>> m.sample() # 30% chance 1; 70% chance 0 tensor([ 0.]) 参数: probs (Number__, Tensor) – the probabilty of sampling 1 logits (Number__, Tensor) – the log-odds of sampling 1 arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} entropy() enumerate_support(expand=True) expand(batch_shape, _instance=None) has_enumerate_support = True log_prob(value) logits mean param_shape probs sample(sample_shape=torch.Size([])) support = Boolean() variance Beta class torch.distributions.beta.Beta(concentration1, concentration0, validate_args=None) 基类: torch.distributions.exp_family.ExponentialFamily Beta 分布, 参数为 concentration1 和 concentration0. 例子: >>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5])) >>> m.sample() # Beta distributed with concentration concentration1 and concentration0 tensor([ 0.1046]) 参数: concentration1 (float or Tensor) – 分布的第一个浓度参数（通常称为alpha） concentration0 (float or Tensor) – 分布的第二个浓度参数(通常称为beta) arg_constraints = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)} concentration0 concentration1 entropy() expand(batch_shape, _instance=None) has_rsample = True log_prob(value) mean rsample(sample_shape=()) support = Interval(lower_bound=0.0, upper_bound=1.0) variance Binomial class torch.distributions.binomial.Binomial(total_count=1, probs=None, logits=None, validate_args=None) 基类: torch.distributions.distribution.Distribution 创建一个Binomial 分布, 参数为 total_count 和 probs 或者 logits (但不是同时都有使用). total_count 必须和 [probs] 之间可广播(#torch.distributions.binomial.Binomial.probs \"torch.distributions.binomial.Binomial.probs\")/logits. 例子: >>> m = Binomial(100, torch.tensor([0 , .2, .8, 1])) >>> x = m.sample() tensor([ 0., 22., 71., 100.]) >>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8])) >>> x = m.sample() tensor([[ 4., 5.], [ 7., 6.]]) 参数: total_count (int or Tensor) – 伯努利试验次数 probs (Tensor) – 事件概率 logits (Tensor) – 事件 log-odds arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0), 'total_count': IntegerGreaterThan(lower_bound=0)} enumerate_support(expand=True) expand(batch_shape, _instance=None) has_enumerate_support = True log_prob(value) logits mean param_shape probs sample(sample_shape=torch.Size([])) support variance Categorical class torch.distributions.categorical.Categorical(probs=None, logits=None, validate_args=None) 基类: torch.distributions.distribution.Distribution 创建一个 categorical 分布, 参数为 probs 或者 logits (但不是同时都有). 注意 它等价于从 torch.multinomial() 的采样. 样本是整数来自 K 是 probs.size(-1). 如果 probs 是 1D 的, 长度为K, 每个元素是在该索引处对类进行抽样的相对概率. 如果 probs 是 2D 的, 它被视为一组相对概率向量. 注意 probs 必须是非负的、有限的并且具有非零和, 并且它将被归一化为和为1. 请参阅: torch.multinomial() 例子: >>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ])) >>> m.sample() # equal probability of 0, 1, 2, 3 tensor(3) 参数: probs (Tensor) – event probabilities logits (Tensor) – event log probabilities arg_constraints = {'logits': Real(), 'probs': Simplex()} entropy() enumerate_support(expand=True) expand(batch_shape, _instance=None) has_enumerate_support = True log_prob(value) logits mean param_shape probs sample(sample_shape=torch.Size([])) support variance Cauchy class torch.distributions.cauchy.Cauchy(loc, scale, validate_args=None) 基类: torch.distributions.distribution.Distribution 样本来自柯西(洛伦兹)分布. 均值为0的独立正态分布随机变量之比服从柯西分布. 例子: >>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0])) >>> m.sample() # sample from a Cauchy distribution with loc=0 and scale=1 tensor([ 2.3214]) 参数: loc (float or Tensor) – 分布的模态或中值. scale (float or Tensor) – half width at half maximum. arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} cdf(value) entropy() expand(batch_shape, _instance=None) has_rsample = True icdf(value) log_prob(value) mean rsample(sample_shape=torch.Size([])) support = Real() variance Chi2 class torch.distributions.chi2.Chi2(df, validate_args=None) 基类: torch.distributions.gamma.Gamma 创建由形状参数df参数化的Chi2分布. 这完全等同于 Gamma(alpha=0.5*df, beta=0.5) 例子: >>> m = Chi2(torch.tensor([1.0])) >>> m.sample() # Chi2 distributed with shape df=1 tensor([ 0.1046]) | 参数: | df (float or Tensor) – 分布的形状参数 | arg_constraints = {'df': GreaterThan(lower_bound=0.0)} df expand(batch_shape, _instance=None) Dirichlet class torch.distributions.dirichlet.Dirichlet(concentration, validate_args=None) 基类: torch.distributions.exp_family.ExponentialFamily 创建一个 Dirichlet 分布, 参数为concentration. 例子: >>> m = Dirichlet(torch.tensor([0.5, 0.5])) >>> m.sample() # Dirichlet distributed with concentrarion concentration tensor([ 0.1046, 0.8954]) | 参数: | concentration (Tensor) – 分布的浓度参数（通常称为alpha） | arg_constraints = {'concentration': GreaterThan(lower_bound=0.0)} entropy() expand(batch_shape, _instance=None) has_rsample = True log_prob(value) mean rsample(sample_shape=()) support = Simplex() variance Exponential class torch.distributions.exponential.Exponential(rate, validate_args=None) 基类: torch.distributions.exp_family.ExponentialFamily 创建由rate参数化的指数分布. 例子: >>> m = Exponential(torch.tensor([1.0])) >>> m.sample() # Exponential distributed with rate=1 tensor([ 0.1046]) | 参数: | rate (float or Tensor) – rate = 1 / 分布的scale | arg_constraints = {'rate': GreaterThan(lower_bound=0.0)} cdf(value) entropy() expand(batch_shape, _instance=None) has_rsample = True icdf(value) log_prob(value) mean rsample(sample_shape=torch.Size([])) stddev support = GreaterThan(lower_bound=0.0) variance FisherSnedecor class torch.distributions.fishersnedecor.FisherSnedecor(df1, df2, validate_args=None) 基类: torch.distributions.distribution.Distribution 创建由df1和df2参数化的Fisher-Snedecor分布 例子: >>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0])) >>> m.sample() # Fisher-Snedecor-distributed with df1=1 and df2=2 tensor([ 0.2453]) 参数: df1 (float or Tensor) – 自由度参数1 df2 (float or Tensor) – 自由度参数2 arg_constraints = {'df1': GreaterThan(lower_bound=0.0), 'df2': GreaterThan(lower_bound=0.0)} expand(batch_shape, _instance=None) has_rsample = True log_prob(value) mean rsample(sample_shape=torch.Size([])) support = GreaterThan(lower_bound=0.0) variance Gamma class torch.distributions.gamma.Gamma(concentration, rate, validate_args=None) 基类: torch.distributions.exp_family.ExponentialFamily 创建由concentration和rate参数化的伽马分布. . 例子: >>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0])) >>> m.sample() # Gamma distributed with concentration=1 and rate=1 tensor([ 0.1046]) 参数: concentration (float or Tensor) – 分布的形状参数（通常称为alpha） rate (float or Tensor) – rate = 1 / 分布scale (通常称为beta ) arg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'rate': GreaterThan(lower_bound=0.0)} entropy() expand(batch_shape, _instance=None) has_rsample = True log_prob(value) mean rsample(sample_shape=torch.Size([])) support = GreaterThan(lower_bound=0.0) variance Geometric class torch.distributions.geometric.Geometric(probs=None, logits=None, validate_args=None) 基类: torch.distributions.distribution.Distribution 创建由probs参数化的几何分布, 其中probs是伯努利试验成功的概率. 它表示概率在 次伯努利试验中, 前 试验失败, 然后成功. 样本是非负整数 [0, ). 例子: >>> m = Geometric(torch.tensor([0.3])) >>> m.sample() # underlying Bernoulli has 30% chance 1; 70% chance 0 tensor([ 2.]) 参数: probs (Number__, Tensor) – 抽样1的概率 . 必须是在范围 (0, 1] logits (Number__, Tensor) – 抽样 1的log-odds. arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} entropy() expand(batch_shape, _instance=None) log_prob(value) logits mean probs sample(sample_shape=torch.Size([])) support = IntegerGreaterThan(lower_bound=0) variance Gumbel class torch.distributions.gumbel.Gumbel(loc, scale, validate_args=None) 基类: torch.distributions.transformed_distribution.TransformedDistribution 来自Gumbel分布的样本. Examples: >>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0])) >>> m.sample() # sample from Gumbel distribution with loc=1, scale=2 tensor([ 1.0124]) 参数: loc (float or Tensor) – 分布的位置参数 scale (float or Tensor) – 分布的scale 参数 arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} entropy() expand(batch_shape, _instance=None) mean stddev support = Real() variance HalfCauchy class torch.distributions.half_cauchy.HalfCauchy(scale, validate_args=None) 基类: torch.distributions.transformed_distribution.TransformedDistribution 创建scale参数化的半正态分布: X ~ Cauchy(0, scale) Y = |X| ~ HalfCauchy(scale) 例子: >>> m = HalfCauchy(torch.tensor([1.0])) >>> m.sample() # half-cauchy distributed with scale=1 tensor([ 2.3214]) | 参数: | scale (float or Tensor) – 完全柯西分布的scale | arg_constraints = {'scale': GreaterThan(lower_bound=0.0)} cdf(value) entropy() expand(batch_shape, _instance=None) has_rsample = True icdf(prob) log_prob(value) mean scale support = GreaterThan(lower_bound=0.0) variance HalfNormal class torch.distributions.half_normal.HalfNormal(scale, validate_args=None) 基类: torch.distributions.transformed_distribution.TransformedDistribution 创建按scale参数化的半正态分布: X ~ Normal(0, scale) Y = |X| ~ HalfNormal(scale) 例子: >>> m = HalfNormal(torch.tensor([1.0])) >>> m.sample() # half-normal distributed with scale=1 tensor([ 0.1046]) | 参数: | scale (float or Tensor) – 完全正态分布的scale | arg_constraints = {'scale': GreaterThan(lower_bound=0.0)} cdf(value) entropy() expand(batch_shape, _instance=None) has_rsample = True icdf(prob) log_prob(value) mean scale support = GreaterThan(lower_bound=0.0) variance Independent class torch.distributions.independent.Independent(base_distribution, reinterpreted_batch_ndims, validate_args=None) 基类: torch.distributions.distribution.Distribution 重新解释一些分布的批量 dims 作为 event dims. 这主要用于改变log_prob()结果的形状.例如, 要创建与多元正态分布形状相同的对角正态分布(因此它们是可互换的), 您可以这样做: >>> loc = torch.zeros(3) >>> scale = torch.ones(3) >>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale)) >>> [mvn.batch_shape, mvn.event_shape] [torch.Size(()), torch.Size((3,))] >>> normal = Normal(loc, scale) >>> [normal.batch_shape, normal.event_shape] [torch.Size((3,)), torch.Size(())] >>> diagn = Independent(normal, 1) >>> [diagn.batch_shape, diagn.event_shape] [torch.Size(()), torch.Size((3,))] 参数: base_distribution (torch.distributions.distribution.Distribution) – 基础分布 reinterpreted_batch_ndims (int) –要重解释的批量dims的数量 arg_constraints = {} entropy() enumerate_support(expand=True) expand(batch_shape, _instance=None) has_enumerate_support has_rsample log_prob(value) mean rsample(sample_shape=torch.Size([])) sample(sample_shape=torch.Size([])) support variance Laplace class torch.distributions.laplace.Laplace(loc, scale, validate_args=None) 基类: torch.distributions.distribution.Distribution 创建参数化的拉普拉斯分布, 参数是 loc 和 :attr:’scale’. 例子: >>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0])) >>> m.sample() # Laplace distributed with loc=0, scale=1 tensor([ 0.1046]) 参数: loc (float or Tensor) – 分布均值 scale (float or Tensor) – 分布scale arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} cdf(value) entropy() expand(batch_shape, _instance=None) has_rsample = True icdf(value) log_prob(value) mean rsample(sample_shape=torch.Size([])) stddev support = Real() variance LogNormal class torch.distributions.log_normal.LogNormal(loc, scale, validate_args=None) 基类: torch.distributions.transformed_distribution.TransformedDistribution 创建参数化的对数正态分布, 参数为 loc 和 scale: X ~ Normal(loc, scale) Y = exp(X) ~ LogNormal(loc, scale) 例子: >>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0])) >>> m.sample() # log-normal distributed with mean=0 and stddev=1 tensor([ 0.1046]) 参数: loc (float or Tensor) – 分布对数平均值 scale (float or Tensor) – 分布对数的标准差 arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} entropy() expand(batch_shape, _instance=None) has_rsample = True loc mean scale support = GreaterThan(lower_bound=0.0) variance LowRankMultivariateNormal class torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(loc, cov_factor, cov_diag, validate_args=None) 基类: torch.distributions.distribution.Distribution 使用由cov_factor和cov_diag参数化的低秩形式的协方差矩阵创建多元正态分布: covariance_matrix = cov_factor @ cov_factor.T + cov_diag Example >>> m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([1, 0]), torch.tensor([1, 1])) >>> m.sample() # normally distributed with mean=`[0,0]`, cov_factor=`[1,0]`, cov_diag=`[1,1]` tensor([-0.2102, -0.5429]) 参数: loc (Tensor) – 分布的均值, 形状为 batch_shape + event_shape cov_factor (Tensor) – 协方差矩阵低秩形式的因子部分, 形状为 batch_shape + event_shape + (rank,) cov_diag (Tensor) – 协方差矩阵的低秩形式的对角部分, 形状为 batch_shape + event_shape 注意 避免了协方差矩阵的行列式和逆的计算, 当 cov_factor.shape[1] 由于 Woodbury matrix identity 和 matrix determinant lemma. 由于这些公式, 我们只需要计算小尺寸“capacitance”矩阵的行列式和逆: capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor arg_constraints = {'cov_diag': GreaterThan(lower_bound=0.0), 'cov_factor': Real(), 'loc': Real()} covariance_matrix entropy() expand(batch_shape, _instance=None) has_rsample = True log_prob(value) mean precision_matrix rsample(sample_shape=torch.Size([])) scale_tril support = Real() variance Multinomial class torch.distributions.multinomial.Multinomial(total_count=1, probs=None, logits=None, validate_args=None) 基类: torch.distributions.distribution.Distribution 创建由total_count和probs或logits（但不是两者）参数化的多项式分布. probs的最内层维度是对类别的索引. 所有其他维度索引批次. 注意 total_count 不需要指定, 当只有 log_prob() 被调用 注意 probs 必须是非负的、有限的并且具有非零和, 并且它将被归一化为和为1. sample() 所有参数和样本都需要一个共享的total_count. log_prob() 允许每个参数和样本使用不同的total_count. 例子: >>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.])) >>> x = m.sample() # equal probability of 0, 1, 2, 3 tensor([ 21., 24., 30., 25.]) >>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x) tensor([-4.1338]) 参数: total_count (int) – 试验次数 probs (Tensor) – 事件概率 logits (Tensor) – 事件对数概率 arg_constraints = {'logits': Real(), 'probs': Simplex()} expand(batch_shape, _instance=None) log_prob(value) logits mean param_shape probs sample(sample_shape=torch.Size([])) support variance MultivariateNormal class torch.distributions.multivariate_normal.MultivariateNormal(loc, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=None) 基类: torch.distributions.distribution.Distribution 创建由均值向量和协方差矩阵参数化的多元正态(也称为高斯)分布. 多元正态分布可以用正定协方差矩阵来参数化或者一个正定的精度矩阵 或者是一个正对角项的下三角矩阵 , 例如 . 这个三角矩阵可以通过协方差的Cholesky分解得到. 例子 >>> m = MultivariateNormal(torch.zeros(2), torch.eye(2)) >>> m.sample() # normally distributed with mean=`[0,0]` and covariance_matrix=`I` tensor([-0.2102, -0.5429]) 参数: loc (Tensor) – 分布的均值 covariance_matrix (Tensor) – 正定协方差矩阵 precision_matrix (Tensor) – 正定精度矩阵 scale_tril (Tensor) – 具有正值对角线的下三角协方差因子 注意 仅仅一个 covariance_matrix 或者 precision_matrix 或者 scale_tril 可被指定. 使用 scale_tril 会更有效率: 内部的所有计算都基于 scale_tril. 如果 covariance_matrix 或者 precision_matrix 已经被传入, 它仅用于使用Cholesky分解计算相应的下三角矩阵. arg_constraints = {'covariance_matrix': PositiveDefinite(), 'loc': RealVector(), 'precision_matrix': PositiveDefinite(), 'scale_tril': LowerCholesky()} covariance_matrix entropy() expand(batch_shape, _instance=None) has_rsample = True log_prob(value) mean precision_matrix rsample(sample_shape=torch.Size([])) scale_tril support = Real() variance NegativeBinomial class torch.distributions.negative_binomial.NegativeBinomial(total_count, probs=None, logits=None, validate_args=None) 基类: torch.distributions.distribution.Distribution 创建一个负二项分布, 即在达到total_count失败之前所需的独立相同伯努利试验的数量的分布. 每次伯努利试验成功的概率都是probs. 参数: total_count (float or Tensor) – 非负数伯努利试验停止的次数, 虽然分布仍然对实数有效 probs (Tensor) – 事件概率, 区间为 [0, 1) logits (Tensor) – 事件对数几率 - 成功概率的几率 arg_constraints = {'logits': Real(), 'probs': HalfOpenInterval(lower_bound=0.0, upper_bound=1.0), 'total_count': GreaterThanEq(lower_bound=0)} expand(batch_shape, _instance=None) log_prob(value) logits mean param_shape probs sample(sample_shape=torch.Size([])) support = IntegerGreaterThan(lower_bound=0) variance Normal class torch.distributions.normal.Normal(loc, scale, validate_args=None) 基类: torch.distributions.exp_family.ExponentialFamily 创建由loc和scale参数化的正态（也称为高斯）分布 例子: >>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0])) >>> m.sample() # normally distributed with loc=0 and scale=1 tensor([ 0.1046]) 参数: loc (float or Tensor) – 均值 (也被称为 mu) scale (float or Tensor) – 标准差(也被称为) sigma) arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} cdf(value) entropy() expand(batch_shape, _instance=None) has_rsample = True icdf(value) log_prob(value) mean rsample(sample_shape=torch.Size([])) sample(sample_shape=torch.Size([])) stddev support = Real() variance OneHotCategorical class torch.distributions.one_hot_categorical.OneHotCategorical(probs=None, logits=None, validate_args=None) 基类: torch.distributions.distribution.Distribution 创建一个由probs或logits参数化的One Hot Categorical 分布 样本是大小为 probs.size(-1)热编码向量. 注意 probs必须是非负的, 有限的并且具有非零和, 并且它将被归一化为总和为1. 请参见: torch.distributions.Categorical() 对于指定 probs 和 logits. 例子: >>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ])) >>> m.sample() # equal probability of 0, 1, 2, 3 tensor([ 0., 0., 0., 1.]) 参数: probs (Tensor) – event probabilities logits (Tensor) – event log probabilities arg_constraints = {'logits': Real(), 'probs': Simplex()} entropy() enumerate_support(expand=True) expand(batch_shape, _instance=None) has_enumerate_support = True log_prob(value) logits mean param_shape probs sample(sample_shape=torch.Size([])) support = Simplex() variance Pareto class torch.distributions.pareto.Pareto(scale, alpha, validate_args=None) 基类: torch.distributions.transformed_distribution.TransformedDistribution 来自Pareto Type 1分布的样本. 例子: >>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0])) >>> m.sample() # sample from a Pareto distribution with scale=1 and alpha=1 tensor([ 1.5623]) 参数: scale (float or Tensor) – 分布的Scale alpha (float or Tensor) – 分布的Shape arg_constraints = {'alpha': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)} entropy() expand(batch_shape, _instance=None) mean support variance Poisson class torch.distributions.poisson.Poisson(rate, validate_args=None) 基类: torch.distributions.exp_family.ExponentialFamily 创建按rate参数化的泊松分布 样本是非负整数, pmf是 例子: >>> m = Poisson(torch.tensor([4])) >>> m.sample() tensor([ 3.]) | 参数: | rate (Number__, Tensor) – rate 参数 | arg_constraints = {'rate': GreaterThan(lower_bound=0.0)} expand(batch_shape, _instance=None) log_prob(value) mean sample(sample_shape=torch.Size([])) support = IntegerGreaterThan(lower_bound=0) variance RelaxedBernoulli class torch.distributions.relaxed_bernoulli.RelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None) 基类: torch.distributions.transformed_distribution.TransformedDistribution 创建一个RelaxedBernoulli分布, 通过temperature参数化, 以及probs或logits（但不是两者）. 这是伯努利分布的松弛版本, 因此值在（0,1）中, 并且具有可重参数化的样本. 例子: >>> m = RelaxedBernoulli(torch.tensor([2.2]), torch.tensor([0.1, 0.2, 0.3, 0.99])) >>> m.sample() tensor([ 0.2951, 0.3442, 0.8918, 0.9021]) 参数: temperature (Tensor) – 松弛 temperature probs (Number__, Tensor) –采样 1 的概率 logits (Number__, Tensor) – 采样 1 的对数概率 arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} expand(batch_shape, _instance=None) has_rsample = True logits probs support = Interval(lower_bound=0.0, upper_bound=1.0) temperature RelaxedOneHotCategorical class torch.distributions.relaxed_categorical.RelaxedOneHotCategorical(temperature, probs=None, logits=None, validate_args=None) 基类: torch.distributions.transformed_distribution.TransformedDistribution 创建一个由温度参数化的RelaxedOneHotCategorical分布, 以及probs或logits. 这是OneHotCategorical分布的松弛版本, 因此它的样本是单一的, 并且可以重参数化. 例子: >>> m = RelaxedOneHotCategorical(torch.tensor([2.2]), torch.tensor([0.1, 0.2, 0.3, 0.4])) >>> m.sample() tensor([ 0.1294, 0.2324, 0.3859, 0.2523]) 参数: temperature (Tensor) – 松弛 temperature probs (Tensor) – 事件概率 logits (Tensor) –对数事件概率. arg_constraints = {'logits': Real(), 'probs': Simplex()} expand(batch_shape, _instance=None) has_rsample = True logits probs support = Simplex() temperature StudentT class torch.distributions.studentT.StudentT(df, loc=0.0, scale=1.0, validate_args=None) 基类: torch.distributions.distribution.Distribution 根据自由度df, 平均loc和scale创建学生t分布. 例子: >>> m = StudentT(torch.tensor([2.0])) >>> m.sample() # Student's t-distributed with degrees of freedom=2 tensor([ 0.1046]) 参数: df (float or Tensor) – 自由度 loc (float or Tensor) – 均值 scale (float or Tensor) – 分布的scale arg_constraints = {'df': GreaterThan(lower_bound=0.0), 'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} entropy() expand(batch_shape, _instance=None) has_rsample = True log_prob(value) mean rsample(sample_shape=torch.Size([])) support = Real() variance TransformedDistribution class torch.distributions.transformed_distribution.TransformedDistribution(base_distribution, transforms, validate_args=None) 基类: torch.distributions.distribution.Distribution Distribution类的扩展, 它将一系列变换应用于基本分布. 假设f是所应用变换的组成: X ~ BaseDistribution Y = f(X) ~ TransformedDistribution(BaseDistribution, f) log p(Y) = log p(X) + log |det (dX/dY)| 注意 .event_shape of a TransformedDistribution 是其基本分布及其变换的最大形状, 因为变换可以引入事件之间的相关性. 一个使用例子 TransformedDistribution: # Building a Logistic Distribution # X ~ Uniform(0, 1) # f = a + b * logit(X) # Y ~ f(X) ~ Logistic(a, b) base_distribution = Uniform(0, 1) transforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)] logistic = TransformedDistribution(base_distribution, transforms) 有关更多示例, 请查看有关实现 Gumbel, HalfCauchy, HalfNormal, LogNormal, Pareto, Weibull, RelaxedBernoulli 和 RelaxedOneHotCategorical arg_constraints = {} cdf(value) 通过逆变换和计算基分布的分数来计算累积分布函数. expand(batch_shape, _instance=None) has_rsample icdf(value) 使用transform(s)计算逆累积分布函数, 并计算基分布的分数. log_prob(value) 通过反转变换并使用基本分布的分数和日志abs det jacobian计算分数来对样本进行评分 rsample(sample_shape=torch.Size([])) 如果分布参数是批处理的, 则生成sample_shape形状的重新参数化样本或sample_shape形状的重新参数化样本批次. 首先从基本分布中采样, 并对列表中的每个变换应用transform() sample(sample_shape=torch.Size([])) 如果分布参数是批处理的, 则生成sample_shape形样本或sample_shape形样本批处理. 首先从基本分布中采样, 并对列表中的每个变换应用transform(). support Uniform class torch.distributions.uniform.Uniform(low, high, validate_args=None) 基类: torch.distributions.distribution.Distribution 从半开区间[low, high)生成均匀分布的随机样本 例子: >>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0])) >>> m.sample() # uniformly distributed in the range [0.0, 5.0) tensor([ 2.3418]) 参数: low (float or Tensor) – 下限（含）. high (float or Tensor) – 上限(排除). arg_constraints = {'high': Dependent(), 'low': Dependent()} cdf(value) entropy() expand(batch_shape, _instance=None) has_rsample = True icdf(value) log_prob(value) mean rsample(sample_shape=torch.Size([])) stddev support variance Weibull class torch.distributions.weibull.Weibull(scale, concentration, validate_args=None) 基类: torch.distributions.transformed_distribution.TransformedDistribution 来自双参数Weibull分布的样本. Example >>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0])) >>> m.sample() # sample from a Weibull distribution with scale=1, concentration=1 tensor([ 0.4784]) 参数: scale (float or Tensor) – Scale (lambda). concentration (float or Tensor) – Concentration (k/shape). arg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)} entropy() expand(batch_shape, _instance=None) mean support = GreaterThan(lower_bound=0.0) variance KL Divergence torch.distributions.kl.kl_divergence(p, q) 计算Kullback-Leibler散度 对于两个分布. 参数: p (Distribution) – Distribution 对象. q (Distribution) – Distribution 对象. | 返回值: | 批量的 KL 散度, 形状为 batch_shape. | | 返回类型： | Tensor | | 异常: | NotImplementedError – 如果分布类型尚未通过注册 register_kl(). | torch.distributions.kl.register_kl(type_p, type_q) 装饰器注册kl_divergence()的成对函数 @register_kl(Normal, Normal) def kl_normal_normal(p, q): # insert implementation here Lookup返回由子类排序的最具体(type,type)匹配. 如果匹配不明确, 则会引发RuntimeWarning. 例如, 解决模棱两可的情况 @register_kl(BaseP, DerivedQ) def kl_version1(p, q): ... @register_kl(DerivedP, BaseQ) def kl_version2(p, q): ... 你应该注册第三个最具体的实现, 例如: register_kl(DerivedP, DerivedQ)(kl_version1) # Break the tie. 参数: type_p (type) – 子类 Distribution. type_q (type) – 子类 Distribution. Transforms class torch.distributions.transforms.Transform(cache_size=0) 有可计算的log det jacobians进行可逆变换的抽象类. 它们主要用于 torch.distributions.TransformedDistribution. 缓存对于其反转昂贵或数值不稳定的变换很有用. 请注意, 必须注意记忆值, 因为可以颠倒自动记录图. 例如, 以下操作有或没有缓存: y = t(x) t.log_abs_det_jacobian(x, y).backward() # x will receive gradients. 但是, 由于依赖性反转, 缓存时会出现以下错误: y = t(x) z = t.inv(y) grad(z.sum(), [y]) # error because z is x 派生类应该实现_call()或_inverse()中的一个或两个. 设置bijective=True的派生类也应该实现log_abs_det_jacobian() | 参数: | cache_size (int) – 缓存大小. 如果为零, 则不进行缓存. 如果是, 则缓存最新的单个值. 仅支持0和1 | | Variables: | domain (Constraint) – 表示该变换有效输入的约束. codomain (Constraint) – 表示此转换的有效输出的约束, 这些输出是逆变换的输入. bijective (bool) – 这个变换是否是双射的. 变换 t 是双射的 如果 t.inv(t(x)) == x 并且 t(t.inv(y)) == y 对于每一个 x 和 y. 不是双射的变形应该至少保持较弱的伪逆属性 t(t.inv(t(x)) == t(x) and t.inv(t(t.inv(y))) == t.inv(y). sign (int or Tensor) – 对于双射单变量变换, 它应该是+1或-1, 这取决于变换是单调递增还是递减. event_dim (int) – 变换event_shape中相关的维数. 这对于逐点变换应该是0, 对于在矢量上共同作用的变换是1, 对于在矩阵上共同作用的变换是2, 等等. inv 返回逆Transform. 满足 t.inv.inv is t. sign 如果适用, 返回雅可比行列式的符号. 一般来说, 这只适用于双射变换. log_abs_det_jacobian(x, y) 计算 log det jacobian log |dy/dx| 给定输入和输出. class torch.distributions.transforms.ComposeTransform(parts) 在一个链中组合多个转换. 正在组合的转换负责缓存. | 参数: | parts (list of Transform) – 列表 transforms. | class torch.distributions.transforms.ExpTransform(cache_size=0) 转换通过映射 . class torch.distributions.transforms.PowerTransform(exponent, cache_size=0) 转换通过映射 . class torch.distributions.transforms.SigmoidTransform(cache_size=0) 转换通过映射 and . class torch.distributions.transforms.AbsTransform(cache_size=0) 转换通过映射 . class torch.distributions.transforms.AffineTransform(loc, scale, event_dim=0, cache_size=0) 通过逐点仿射映射进行转换 . 参数: loc (Tensor or float) – Location. scale (Tensor or float) – Scale. event_dim (int) – 可选的 event_shape 大小. T对于单变量随机变量, 该值应为零, 对于矢量分布, 1应为零, 对于矩阵的分布, 应为2. class torch.distributions.transforms.SoftmaxTransform(cache_size=0) 从无约束空间到单纯形的转换, 通过 然后归一化. 这不是双射的, 不能用于HMC. 然而, 这主要是协调的（除了最终的归一化）, 因此适合于坐标方式的优化算法. class torch.distributions.transforms.StickBreakingTransform(cache_size=0) 将无约束空间通过 stick-breaking 过程转化为一个额外维度的单纯形. 这种变换是Dirichlet分布的破棒构造中的迭代sigmoid变换:第一个逻辑通过sigmoid变换成第一个概率和所有其他概率, 然后这个过程重复出现. 这是双射的, 适合在HMC中使用; 然而, 它将坐标混合在一起, 不太适合优化. class torch.distributions.transforms.LowerCholeskyTransform(cache_size=0) 将无约束矩阵转换为具有非负对角项的下三角矩阵. 这对于根据Cholesky分解来参数化正定矩阵是有用的. Constraints The following constraints are implemented: constraints.boolean constraints.dependent constraints.greater_than(lower_bound) constraints.integer_interval(lower_bound, upper_bound) constraints.interval(lower_bound, upper_bound) constraints.lower_cholesky constraints.lower_triangular constraints.nonnegative_integer constraints.positive constraints.positive_definite constraints.positive_integer constraints.real constraints.real_vector constraints.simplex constraints.unit_interval class torch.distributions.constraints.Constraint constraints 的抽象基类. constraint对象表示变量有效的区域, 例如, 其中可以优化变量 check(value) 返回一个字节张量 sample_shape + batch_shape 指示值中的每个事件是否满足此约束. torch.distributions.constraints.dependent_property alias of torch.distributions.constraints._DependentProperty torch.distributions.constraints.integer_interval alias of torch.distributions.constraints._IntegerInterval torch.distributions.constraints.greater_than alias of torch.distributions.constraints._GreaterThan torch.distributions.constraints.greater_than_eq alias of torch.distributions.constraints._GreaterThanEq torch.distributions.constraints.less_than alias of torch.distributions.constraints._LessThan torch.distributions.constraints.interval alias of torch.distributions.constraints._Interval torch.distributions.constraints.half_open_interval alias of torch.distributions.constraints._HalfOpenInterval Constraint Registry PyTorch 提供两个全局 ConstraintRegistry 对象 , 链接 Constraint 对象到 Transform 对象. 这些对象既有输入约束, 也有返回变换, 但是它们对双射性有不同的保证. biject_to(constraint) 查找一个双射的 Transform 从 constraints.real 到给定的 constraint. 返回的转换保证具有 .bijective = True 并且应该实现了 .log_abs_det_jacobian(). transform_to(constraint) 查找一个不一定是双射的 Transform 从 constraints.real 到给定的 constraint. 返回的转换不保证实现 .log_abs_det_jacobian(). transform_to()注册表对于对概率分布的约束参数执行无约束优化非常有用, 这些参数由每个分布的.arg_constraints指示. 这些变换通常会过度参数化空间以避免旋转; 因此, 它们更适合像Adam那样的坐标优化算法 loc = torch.zeros(100, requires_grad=True) unconstrained = torch.zeros(100, requires_grad=True) scale = transform_to(Normal.arg_constraints['scale'])(unconstrained) loss = -Normal(loc, scale).log_prob(data).sum() biject_to() 注册表对于Hamiltonian Monte Carlo非常有用, 其中来自具有约束. .support的概率分布的样本在无约束空间中传播, 并且算法通常是旋转不变的 dist = Exponential(rate) unconstrained = torch.zeros(100, requires_grad=True) sample = biject_to(dist.support)(unconstrained) potential_energy = -dist.log_prob(sample).sum() 注意 一个 transform_to 和 biject_to 不同的例子是 constraints.simplex: transform_to(constraints.simplex) 返回一个 SoftmaxTransform 简单地对其输入进行指数化和归一化; 这是一种廉价且主要是坐标的操作, 适用于像SVI这样的算法. 相反, biject_to(constraints.simplex) 返回一个 StickBreakingTransform 将其输入生成一个较小维度的空间; 这是一种更昂贵的数值更少的数值稳定的变换, 但对于像HM​​C这样的算法是必需的. biject_to 和 transform_to 对象可以通过用户定义的约束进行扩展, 并使用.register()方法进行转换, 作为单例约束的函数 transform_to.register(my_constraint, my_transform) 或作为参数化约束的装饰器: @transform_to.register(MyConstraintClass) def my_factory(constraint): assert isinstance(constraint, MyConstraintClass) return MyTransform(constraint.param1, constraint.param2) 您可以通过创建新的ConstraintRegistry创建自己的注册表. class torch.distributions.constraint_registry.ConstraintRegistry 注册表, 将约束链接到转换. register(constraint, factory=None) 在此注册表注册一个 Constraint 子类. 用法: @my_registry.register(MyConstraintClass) def construct_transform(constraint): assert isinstance(constraint, MyConstraint) return MyTransform(constraint.arg_constraints) 参数: constraint (subclass of Constraint) – [Constraint]的子类(#torch.distributions.constraints.Constraint \"torch.distributions.constraints.Constraint\"), 或者派生类的对象. factory (callable) – 可调用对象, 输入 constraint 对象返回 Transform 对象. 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"hub.html":{"url":"hub.html","title":"torch.hub","keywords":"","body":"torch.hub 译者：kunwuz torch.hub.load(github, model, force_reload=False, *args, **kwargs) 从github上加载一个带有预训练权重的模型。 参数: github – 必需，一个字符串对象，格式为“repo_owner/repo_name[:tag_name]”，可选 tag/branch。如果未做指定，默认的 branch 是 master 。比方说: ‘pytorch/vision[:hub]’ model – 必须，一个字符串对象，名字在hubconf.py中定义。 force_reload – 可选， 是否丢弃现有缓存并强制重新下载。默认是：False。 *args – 可选， 可调用的model的相关args参数。 **kwargs – 可选， 可调用的model的相关kwargs参数。 返回: 一个有相关预训练权重的单一模型。 torch.hub.set_dir(d) 也可以将hub_dir设置为本地目录来保存中间模型和检查点文件。 如果未设置此参数,环境变量TORCH_HUB_DIR 会被首先搜寻，~/.torch/hub 将被创建并用作后备。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"jit.html":{"url":"jit.html","title":"torch.jit","keywords":"","body":"TorchScript 译者：keyianpai 创建 Torch 脚本代码 将追踪和脚本化结合起来 Torch 脚本语言参考 类型 表达式 语句 变量解析 python值的使用 调试 内置函数 Torch脚本是一种从PyTorch代码创建可序列化和可优化模型的方法。用Torch脚本编写的代码可以从Python进程中保存，并在没有Python依赖的进程中加载。 我们提供了一些工具帮助我们将模型从纯Python程序逐步转换为可以独立于Python运行的Torch脚本程序。Torch脚本程序可以在其他语言的程序中运行（例如，在独立的C ++程序中）。这使得我们可以使用熟悉的工具在PyTorch中训练模型，而将模型导出到出于性能和多线程原因不能将模型作为Python程序运行的生产环境中去。 class torch.jit.ScriptModule(optimize=True) ScriptModule与其内部的Torch脚本函数可以通过两种方式创建： 追踪： 使用torch.jit.trace。torch.jit.trace以现有模块或python函数和样例输入作为参数，它会运行该python函数，记录函数在所有张量上执行的操作，并将记录转换为Torch脚本方法以作为ScriptModule的forward方法。创建的模块包含原始模块的所有参数。 例： import torch def foo(x, y): return 2*x + y traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3))) 注意 追踪一个 函数 将生成一个ScriptModule，该ScriptModule中包含一个实现被追踪函数的forward方法，但不包含任何参数。 例： import torch import torchvision traced_net = torch.jit.trace(torchvision.models.resnet18(), torch.rand(1, 3, 224, 224)) 注意 追踪仅记录在给定张量上运行给定函数时执行的操作。因此，返回的ScriptModule在任何输入上将运行相同的追踪图。当你的模块需要根据输入和/或模块状态运行不同的操作集时，这会产生一些重要的影响。例如， >* 追踪不会记录if语句或循环之类的控制流。当这个控制流在你的模块中保持不变时，这很好，它通常只是内联配置决策。但有时控制流实际上是模型本身的一部分。例如，序列到序列转换中的beam搜索是对（可变）输入序列长度的循环。 >*在返回的ScriptModule中，在training和eval模式中具有不同行为的操作将始终表现为处于追踪期间的模式。 在上述情况下，脚本化是一个比追踪更好的选择。 脚本化 你可以使用Python语法直接编写Torch脚本代码。你可以使用torch.jit.script注释（对于函数）或torch.jit.script_method注释（对于ScriptModule子类的方法）来编写Torch脚本代码。通过注释，被注释函数的主体将直接转换为Torch脚本。 Torch脚本本身只是Python语言的一个子集，因此不是python中的所有特性都可以使用，但我们提供了足够的功能来计算张量并执行与控制相关的操作。 例: import torch @torch.jit.script def foo(x, y): if x.max() &gt; y.max(): r = x else: r = y return r 注意 脚本 函数 注释将构造带有一个forward方法的ScriptModule，该forward方法实现被注释函数，并且不包含任何参数。 例： import torch class MyModule(torch.jit.ScriptModule): def __init__(self, N, M): super(MyModule, self).__init__() self.weight = torch.nn.Parameter(torch.rand(N, M)) @torch.jit.script_method def forward(self, input): return self.weight.mv(input) 例： import torch import torch.nn as nn import torch.nn.functional as F from torch.jit import ScriptModule, script_method, trace class MyScriptModule(ScriptModule): def __init__(self): super(MyScriptModule, self).__init__() # 通过追踪产生ScriptModule的 conv1和conv2 self.conv1 = trace(nn.Conv2d(1, 20, 5), torch.rand(1, 1, 16, 16)) self.conv2 = trace(nn.Conv2d(20, 20, 5), torch.rand(1, 20, 16, 16)) @script_method def forward(self, input): input = F.relu(self.conv1(input)) input = F.relu(self.conv2(input)) return input save(filename) 保存离线版本的模块，以便将来在其他的进程中使用。保存的模块会序列化当前模块的所有方法和参数。保存的模块可以使用torch :: jit :: load（filename）加载到C ++ API中，也可以使用torch.jit.load（filename）加载到Python API中。 为了能够保存模块，当前模块不能调用原生python函数。也就是说要保存模块的所有子模块也必须是ScriptModules的子类。 危险 所有模块，不论其设备，在加载过程中始终都会加载到CPU中。这与torch.load()的语义不同，将来可能会发生变化。 torch.jit.load(f, map_location=None) 使用load加载之前用save保存的ScriptModule。 所有先前保存的模块，不论其设备，首先加载到CPU上，然后移动到之前保存它们的设备上。如果此操作失败（例如，运行时系统没有某些设备），则会引发异常。此时可以使用map_location参数将存储重新映射到另一组设备。与torch.load()相比，此函数中的map_location被简化为只接受字符串（例如'cpu'，'cuda：0'）或torch.device（例如，torch.device（'cpu'）） 参数： f – 文件类对象（必须实现read，readline，tell和seek），或为文件名的字符串 map_location – 可以是一个字符串（例如，'cpu'，'cuda：0'），一个设备（例如，torch.device（'cpu'）） 返回值: ScriptModule 对象. 例： >>> torch.jit.load('scriptmodule.pt') # 从io.BytesIO对象加载ScriptModule >>> with open('scriptmodule.pt', 'rb') as f: buffer = io.BytesIO(f.read()) # 将所有张量加载到原来的设备上 >>> torch.jit.load(buffer) # 用设备将所有张量加载到CPU上 >>> torch.jit.load(buffer, map_location=torch.device('cpu')) # 用字符串将所有张量加载到CPU上 >>> torch.jit.load(buffer, map_location='cpu') torch.jit.trace(func, example_inputs, optimize=True, check_trace=True, check_inputs=None, check_tolerance=1e-05, _force_outplace=False) 追踪一个函数并返回一个使用即时编译优化过的可执行追踪。 警告 追踪仅正确记录不依赖于数据的函数和模块（例如，对张量中的数据进行条件判断），并且没有任何未追踪的外部依赖性（例如，执行输入/输出或访问全局变量）。如果你追踪此类模型，则可能会在随后的模型调用中静默获取不正确的结果。当执行可能生成错误追踪的内容时，追踪器将尝试发出警告。 参数： func (callable or torch.nn.Module) – 将使用example_inputs作为输入运行的python函数或torch.nn.Module。参数和返回值必须是Tensor或（嵌套的）包含张量的元组。 example_inputs (tuple) – 在追踪时将传递给函数的示例输入元组。假设被追踪操作支持这些类型和形状的情况下，生成的追踪可以在不同类型和形状的输入下运行。 example_inputs也可以是单个Tensor，这种情况下，它会自动包装到元组中。 关键字参数： optimize (bool, optional) – 是否应用优化。默认值：True。 check_trace (bool, optional) – 检查被追踪代码在相同输入下输出是否相同。默认值：True。你可以在某些情况下禁用此功能。例如，你的网络包含非确定性操作，或者你确定网络正确。 check_inputs (list of tuples__, optional) – 应该用于根据预期检查追踪的输入参数元组列表。每个元组相当于一个将在args中指定的输入参数集合。为获得最佳结果，请传递一组检查输入表示你期望网络接受的形状和输入类型范围。如果未指定，则用原来的args检查。 check_tolerance (float, optional) – 在检查过程中使用的浮点比较容差。用于放松检查严格性。 返回值： 含有forward()方法的ScriptModule对象，该方法包含被追踪代码。当func是torch.nn.Module时，返回的ScriptModule具有与原始模块相同的子模块和参数集。 例： >>> def f(x): ... return x * 2 >>> traced_f = torch.jit.trace(f, torch.rand(1)) 在许多情况下，追踪或脚本是转换模型的更简单方法。我们允许你将追踪和脚本组合使用以满足模型特定部分的特定要求。 脚本函数可以调用被追踪函数。当你需要使用控制流控制简单的前馈模型时，这尤其有用。例如，序列到序列模型的beam搜索通常将以脚本编写，但可以调用使用追踪生成的编码器模块。 例： import torch def foo(x, y): return 2 * x + y traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3))) @torch.jit.script def bar(x): return traced_foo(x, x) 被追踪函数也可以调用脚本函数。当模型大体是一个前馈网络，只有模型的一小部分需要一些控制流时，这也很有用。由追踪函数调用的脚本函数内部的控制流会被正确地保留。 例： import torch @torch.jit.script def foo(x, y): if x.max() > y.max(): r = x else: r = y return r def bar(x, y, z): return foo(x, y) + z traced_bar = torch.jit.trace(bar, (torch.rand(3), torch.rand(3), torch.rand(3)) 组合也适用于模块，例如可以从脚本模块的方法调用追踪来生成子模块： 例： import torch import torchvision class MyScriptModule(torch.jit.ScriptModule): def __init__(self): super(MyScriptModule, self).__init__() self.means = torch.nn.Parameter(torch.tensor([103.939, 116.779, 123.68]) .resize_(1, 3, 1, 1)) self.resnet = torch.jit.trace(torchvision.models.resnet18(), torch.rand(1, 3, 224, 224)) @torch.jit.script_method def forward(self, input): return self.resnet(input - self.means) Torch脚本是Python的一个子集，可以直接编写（使用@script注释），也可以通过追踪从Python代码自动生成。使用追踪时，代码会自动转换为Python的这个子集，方法是仅记录和执行张量上的实际运算符，并丢弃其他Python代码。 使用@script注释直接编写Torch脚本时，程序员必须只使用Torch脚本支持的Python子集。本节以语言参考的形式介绍Torch脚本支持的功能。本参考中未提及的Python的其他功能都不是Torch脚本的一部分。 作为Python的一个子集，任何有效的Torch脚本函数也是一个有效的Python函数。因此你可以删除@script注释后使用标准Python工具（如pdb）调试函数。反之则不然：有许多有效的python程序不是有效的Torch脚本程序。Torch脚本专注于在Torch中表示神经网络模型所需的Python特性。 PYTORCH_JIT=1 设置环境变量PYTORCH_JIT = 0将禁用所有脚本和追踪注释。如果在ScriptModule中遇到难以调试的错误，则可以使用此标志强制使用原生Python运行所有内容。此时可使用pdb之类的工具调试代码。 Torch脚本与完整Python语言之间的最大区别在于Torch脚本仅支持表达神经网络模型所需的一些类型。特别地，Torch脚本支持： Tensor 具有任何dtype，维度或backend的PyTorch张量。 Tuple[T0, T1, ...] 包含子类型T0，T1等的元组（例如Tuple [Tensor，Tensor]）。 int 标量整数 float 标量浮点数 List[T] 所有成员都是T类型的列表T 与Python不同，Torch脚本函数中的每个变量都必须具有一个静态类型。这样以便于优化Torch脚本功能。 例： @torch.jit.script def an_error(x): if x: r = torch.rand(1) else: r = 4 return r # 类型不匹配：在条件为真时r为Tensor类型 # 而为假时却是int类型 默认情况下，Torch脚本函数的所有参数都为Tensor类型，因为这是模块中最常用的类型。要将Torch脚本函数的参数指定为另一种类型，可以通过MyPy风格的注释使用上面列出的类型： 例： @torch.jit.script def foo(x, tup): # type: (int, Tuple[Tensor, Tensor]) -> Tensor t0, t1 = tup return t0 + t1 + x print(foo(3, (torch.rand(3), torch.rand(3)))) 注意 也可以使用Python 3类型注释来注释类型。在示例中，我们使用基于注释的注释来确保对Python 2的兼容性。 Torch脚本支持以下Python表达式 字面常量 True, False, None, 'string literals', \"string literals\", 字面值3（解释为int）3.4（解释为float） 变量 a 注意 请参阅变量解析，了解变量的解析方式。 元组构造 (3, 4), (3,) 列表构造 [3, 4], [], [torch.rand(3), torch.rand(4)] 注意 空列表具有类型List[Tensor] 。其他列表字面常量的类型由成员的类型推出。 算术运算符 a + b a - b a * b a / b a ^ b a @ b 比较运算符 a == b a != b a a > b a a >= b 逻辑运算符 a and b a or b not b 索引 t[0] t[-1] t[0:2] t[1:] t[:1] t[:] t[0, 1] t[0, 1:2] t[0, :1] t[-1, 1:, 0] t[1:, -1, 0] t[i:j, i] 注意 Torch脚本目前不支持原地修改张量，因此对张量索引只能出现在表达式的右侧。 函数调用 调用内置函数：torch.rand(3, dtype=torch.int) 调用其他脚本函数： import torch @torch.jit.script def foo(x): return x + 1 @torch.jit.script def bar(x): return foo(x) 方法调用 调用内置类型的方法，如tensor: x.mm(y) 在ScriptModule中定义Script方法时，使用@script_method批注。Script方法可以调用模块内其他方法或子模块的方法。 直接调用子模块（例如self.resnet（input））等同于调用其forward方法（例如self.resnet.forward（input）） import torch class MyScriptModule(torch.jit.ScriptModule): def __init__(self): super(MyScriptModule, self).__init__() self.means = torch.nn.Parameter(torch.tensor([103.939, 116.779, 123.68]) .resize_(1, 3, 1, 1)) self.resnet = torch.jit.trace(torchvision.models.resnet18(), torch.rand(1, 3, 224, 224)) @torch.jit.script_method def helper(self, input): return self.resnet(input - self.means) @torch.jit.script_method def forward(self, input): return self.helper(input) If 表达式 x if x > y else y 类型转换 float(ten), int(3.5), bool(ten) 访问模块参数 self.my_parameter self.my_submodule.my_parameter Torch脚本支持以下类型的语句： 简单赋值 a = b a += b # short-hand for a = a + b, does not operate in-place on a a -= b 模式匹配赋值 a, b = tuple_or_list a, b, *c = a_tuple Print 语句 print(\"the result of an add:\", a + b) If 语句 if a &lt; 4: r = -a elif a &lt; 3: r = a + a else: r = 3 * a While 循环 a = 0 while a &lt; 4: print(a) a += 1 带 range 的for循环 x = 0 for i in range(10): x *= i 注意 脚本目前不支持对一般可迭代对象（如列表或张量）进行迭代，也不支持range起始与增量参数，这些将在未来版本中添加。 对元组的for循环： tup = (3, torch.rand(4)) for x in tup: print(x) 注意 对于元组循环将展开循环，为元组的每个成员生成一个循环体。循环体内必须确保每个成员类型正确。 对常量 torch.nn.ModuleList 的for循环 class SubModule(torch.jit.ScriptModule): def __init__(self): super(Sub, self).__init__() self.weight = nn.Parameter(torch.randn(2)) @torch.jit.script_method def forward(self, input): return self.weight + input class MyModule(torch.jit.ScriptModule): __constants__ = ['mods'] def __init__(self): super(MyModule, self).__init__() self.mods = torch.nn.ModuleList([SubModule() for i in range(10)]) @torch.jit.script_method def forward(self, v): for module in self.mods: v = m(v) return v 注意 要在@script_method中使用模块列表，必须通过将属性的名称添加到类型的__constants__列表来将其标记为常量。ModuleList上的for循环在编译时使用常量模块列表的每个成员展开循环体。 Return 语句 return a, b 注意 return语句必须作为函数的最后一个成员，而不能出现在函数的其他位置。此限制将在以后删除。 Torch脚本支持Python变量解析（即作用域）规则的子集。局部变量的行为与Python中的相同，但变量必须在函数的所有路径中具有相同类型。如果变量在if语句的不同侧具有不同的类型，则在if语句结束后使用它会抱错。 类似地，如果仅在函数的某些执行路径上定义变量也会出错。 例： @torch.jit.script def foo(x): if x 定义函数的非局部变量在编译时解析为Python值。然后，用Python值的使用中的规则将这些值转换为Torch脚本值。 为了使编写Torch脚本更方便，我们允许脚本代码引用周围的Python值。例如，当我们引用torch时，Torch脚本编译器实际上在声明函数时将其解析为Python的torch模块。这些Python值不是Torch脚本的一部分，它们在编译时被转换成Torch脚本支持的原始类型。本节介绍在Torch脚本中访问Python值时使用的规则。它们依赖于引用的python值的动态类型。 函数 Torch脚本可以调用python函数。此功能在将模型逐步转换为脚本时非常有用。可以将模型中的函数逐个转成脚本，保留对其余Python函数的调用。这样，在逐步转换的过程中你可以随时检查模型的正确性。 例： def foo(x): print(\"I am called with {}\".format(x)) import pdb; pdb.set_trace() return x @torch.jit.script def bar(x) return foo(x + 1) 注意 不能在包含Python函数调用的ScriptModule上调用save。该功能仅用于调试，应在保存之前删除调用或将其转换为脚本函数。 Python模块的属性查找 Torch脚本可以在模块上查找属性。像torch.add这样的内置函数就以这种方式访问。这允许Torch脚本调用其他模块中定义的函数。 Python 中定义的常量 Torch脚本还提供了使用Python常量的方法。这可用于将超参数硬编码到函数中，或用于定义通用常量。有两种方法可以指定Python值为常量。 查找的值为模块的属性,例如：math.pi 可以将ScriptModule的属性标记为常量，方法是将其列为类的__constants__属性成员： 例： class Foo(torch.jit.ScriptModule): __constants__ = ['a'] def __init__(self): super(Foo, self).__init__(False) self.a = 1 + 4 @torch.jit.ScriptModule def forward(self, input): return self.a + input 支持的Python常量值有 int bool torch.device torch.layout torch.dtype 包含支持类型的元组 torch.nn.ModuleList ，可以将其用在Torch 脚本for循环中 禁用JIT以方便调试 可以通过将PYTORCH_JIT环境变量值设置为0禁用所有JIT模式（追踪和脚本化）以便在原始Python中调试程序。下面是一个示例脚本： @torch.jit.script def scripted_fn(x : torch.Tensor): for i in range(12): x = x + x return x def fn(x): x = torch.neg(x) import pdb; pdb.set_trace() return scripted_fn(x) traced_fn = torch.jit.trace(fn, (torch.rand(4, 5),)) traced_fn(torch.rand(3, 4)) 为了使用PDB调试此脚本。我们可以全局禁用JIT，这样我们就可以将@script函数作为普通的python函数调用而不会编译它。如果上面的脚本名为disable_jit_example.py，我们这样调用它： $ PYTORCH_JIT=0 python disable_jit_example.py 这样,我们就能够作为普通的Python函数步入@script函数。 解释图 TorchScript使用静态单一指派（SSA）中间表示（IR）来表示计算。这种格式的指令包括ATen（PyTorch的C ++后端）运算符和其他原始运算符，包括循环和条件的控制流运算符。举个例子： @torch.jit.script def foo(len): # type: (int) -> torch.Tensor rv = torch.zeros(3, 4) for i in range(len): if i 具有单个forward方法的ScriptModule具有graph属性，你可以使用该图来检查表示计算的IR。如果ScriptModule有多个方法，则需要访问方法本身的.graph属性。例如我们可以通过访问.bar.graph来检查ScriptModule上名为bar的方法的图。 上面的示例脚本生成图形： graph(%len : int) { %13 : float = prim::Constant[value=1]() %10 : int = prim::Constant[value=10]() %2 : int = prim::Constant[value=4]() %1 : int = prim::Constant[value=3]() %3 : int[] = prim::ListConstruct(%1, %2) %4 : int = prim::Constant[value=6]() %5 : int = prim::Constant[value=0]() %6 : int[] = prim::Constant[value=[0, -1]]() %rv.1 : Dynamic = aten::zeros(%3, %4, %5, %6) %8 : int = prim::Constant[value=1]() %rv : Dynamic = prim::Loop(%len, %8, %rv.1) block0(%i : int, %12 : Dynamic) { %11 : int = aten::lt(%i, %10) %rv.4 : Dynamic = prim::If(%11) block0() { %14 : int = prim::Constant[value=1]() %rv.2 : Dynamic = aten::sub(%12, %13, %14) -> (%rv.2) } block1() { %16 : int = prim::Constant[value=1]() %rv.3 : Dynamic = aten::add(%12, %13, %16) -> (%rv.3) } %19 : int = prim::Constant[value=1]() -> (%19, %rv.4) } return (%rv); } 以指令％rv.1：Dynamic = aten :: zeros（％3，％4，％5，％6）为例。％rv.1：Dynamic将输出分配给名为rv.1的（唯一）值，该值是动态类型，即我们不知道它的具体形状。aten :: zeros是运算符（相当于torch.zeros），它的输入列表（％3，％4，％5，％6）指定范围中的哪些值应作为输入传递。内置函数（如aten :: zeros）的模式可以在内置函数中找到。 注意，运算符也可以有关联的block，如prim :: Loop和prim :: If运算符。在图形打印输出中，这些运算符被格式化以反映与其等价的源代码形式，以便于调试。 可以检查图以确认ScriptModule描述的计算是正确的，方法如下所述。 追踪的边缘情况 在一些边缘情况下一些Python函数/模块的追踪不能代表底层代码。这些情况可以包括： 追踪依赖于输入的控制流（例如张量形状） 追踪张量视图的就地操作（例如，在分配的左侧进行索引） 请注意，这些情况在将来版本中可能是可追踪的。 自动追踪检查 通过在torch.jit.trace()API上使用check_inputs，是自动捕获追踪中错误的一种方法。 check_inputs是用于重新追踪计算并验证结果的输入元组列表。例如： def loop_in_traced_fn(x): result = x[0] for i in range(x.size(0)): result = result * x[i] return result inputs = (torch.rand(3, 4, 5),) check_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)] traced = torch.jit.trace(loop_in_traced_fn, inputs, check_inputs=check_inputs) 上面代码会为我们提供以下诊断信息： ERROR: Graphs differed across invocations! Graph diff: graph(%0 : Dynamic) { %1 : int = prim::Constant[value=0]() %2 : int = prim::Constant[value=0]() %3 : Dynamic = aten::select(%0, %1, %2) %4 : int = prim::Constant[value=0]() %5 : int = prim::Constant[value=0]() %6 : Dynamic = aten::select(%0, %4, %5) %7 : Dynamic = aten::mul(%3, %6) %8 : int = prim::Constant[value=0]() %9 : int = prim::Constant[value=1]() %10 : Dynamic = aten::select(%0, %8, %9) %11 : Dynamic = aten::mul(%7, %10) %12 : int = prim::Constant[value=0]() %13 : int = prim::Constant[value=2]() %14 : Dynamic = aten::select(%0, %12, %13) %15 : Dynamic = aten::mul(%11, %14) + %16 : int = prim::Constant[value=0]() + %17 : int = prim::Constant[value=3]() + %18 : Dynamic = aten::select(%0, %16, %17) + %19 : Dynamic = aten::mul(%15, %18) - return (%15); ? ^ + return (%19); ? ^ } 此消息表明，我们第一次追踪函数和使用check_inputs追踪函数时的计算存在差异。事实上，loop_in_traced_fn体内的循环取决于输入x的形状，因此当我们输入不同形状的x时，轨迹会有所不同。 在这种情况下，可以使用脚本捕获此类数据相关控制流： def fn(x): result = x[0] for i in range(x.size(0)): result = result * x[i] return result inputs = (torch.rand(3, 4, 5),) check_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)] scripted_fn = torch.jit.script(fn) print(scripted_fn.graph) for input_tuple in [inputs] + check_inputs: torch.testing.assert_allclose(fn(*input_tuple), scripted_fn(*input_tuple)) 上面代码会为我们提供以下信息： graph(%x : Dynamic) { %1 : int = prim::Constant[value=0]() %2 : int = prim::Constant[value=0]() %result.1 : Dynamic = aten::select(%x, %2, %1) %4 : int = aten::size(%x, %1) %5 : int = prim::Constant[value=1]() %result : Dynamic = prim::Loop(%4, %5, %result.1) block0(%i : int, %7 : Dynamic) { %9 : int = prim::Constant[value=0]() %10 : Dynamic = aten::select(%x, %9, %i) %result.2 : Dynamic = aten::mul(%7, %10) %12 : int = prim::Constant[value=1]() -> (%12, %result.2) } return (%result); } 追踪器警告 追踪器会在追踪计算中对有问题的模式生成警告。例如，追踪包含在Tensor的切片（视图）上就地赋值操作的函数： def fill_row_zero(x): x[0] = torch.rand(*x.shape[1:2]) return x traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),)) print(traced.graph) 这会出现如下警告和一个简单返回输入的图： fill_row_zero.py:4: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe. x[0] = torch.rand(*x.shape[1:2]) fill_row_zero.py:6: TracerWarning: Output nr 1\\. of the traced function does not match the corresponding output of the Python function. Detailed error: Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 1] (0.09115803241729736 vs. 0.6782537698745728) and 3 other locations (33.00%) traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),)) graph(%0 : Float(3, 4)) { return (%0); } 我们可以通过使用torch.cat返回结果张量避免就地更新问题： def fill_row_zero(x): x = torch.cat((torch.rand(1, *x.shape[1:2]), x[1:2]), dim=0) return x traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),)) print(traced.graph) Torch脚本支持部分PyTorch内置张量和神经网络函数。 Tensor上的大多数方法以及torch命名空间中的函数都可用。 torch.nn.functional中的许多函数也可用。 我们目前不提供像 Linear 或 Conv 模块之类内置ScriptModule,此功能将在未来开发。目前我们建议使用torch.jit.trace将标准的torch.nn模块转换为ScriptModule。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"multiprocessing.html":{"url":"multiprocessing.html","title":"torch.multiprocessing","keywords":"","body":"多进程包 - torch.multiprocessing 译者：hijkzzz torch.multiprocessing 是一个本地 multiprocessing 模块的包装. 它注册了自定义的reducers, 并使用共享内存为不同的进程在同一份数据上提供共享的视图. 一旦 tensor/storage 被移动到共享内存 (见 share_memory_()), 将其发送到任何进程不会造成拷贝开销. 此 API 100% 兼容原生模块 - 所以足以将 import multiprocessing 改成 import torch.multiprocessing 使得所有的 tensors 通过队列发送或者使用其它共享机制, 移动到共享内存. 因为 APIs 的相似性, 我们没有为此包提供足够的文档, 所以推荐参考非常优秀的原生进程模块文档. 警告 如果主进程意外退出 (比如 因为一个信号的到来), Python’s multiprocessing 有时候会无法请理它的子进程. 这是一个众所周知的警告, 因此，如果你在中断解释器后发现任何资源泄漏，这可能意味着你刚刚发生了这种情况. 策略管理 torch.multiprocessing.get_all_sharing_strategies() 返回当前系统支持的共享策略的集合. torch.multiprocessing.get_sharing_strategy() 返回当前的 CPU tensors 共享策略. torch.multiprocessing.set_sharing_strategy(new_strategy) 设置一个新的 CPU tensors 共享策略. 参数: new_strategy (str) – 选定策略的名字. 必须是 get_all_sharing_strategies() 的返回值中的一个. 共享 CUDA tensors 在进程间共享 CUDA tensors 仅仅在 Python 3 中被支持, 使用 spawn 或者 forkserver 启动方法. multiprocessing 在 Python 2 中只能使用 fork 创建新进程, 然而 CUDA 运行时不支持它. 警告 CUDA API要求导出到其他进程的分配只要被其他进程使用就保持有效. 您应该小心，并确保共享的CUDA tensor在必要时不会超出范围. 共享模型参数不应该是一个问题，但是传递其他类型的数据应该小心。注意，此限制不适用于共享CPU内存. 共享策略 本节简要概述不同的共享策略是如何工作的。注意，它只适用于CPU tensor——CUDA tensor总是使用CUDA API，因为这是它们可以共享的唯一方式。 文件描述符 - file_descriptor 注意 这是默认策略(macOS和OS X因为不支持除外) 该策略将使用文件描述符作为共享内存句柄。每当一个存储被移动到共享内存时，从shm open获得的文件描述符就会被对象缓存，当它被发送到其他进程时，文件描述符就会被传输(例如通过UNIX套接字)到它。接收者还将缓存文件描述符并mmap它，以获得存储数据上的共享视图。 请注意，如果共享了很多tensor，那么这种策略将在大多数情况下打开大量的文件描述符。如果您的系统对打开的文件描述符的数量限制很低，并且您不能提高它们的数量，那么您应该使用file_system策略。 文件系统 - file_system 该策略将使用指定给shm open的文件名来标识共享内存区域。这样做的好处是不需要实现缓存从中获得的文件描述符，但同时容易导致共享内存泄漏。文件不能在创建之后立即删除，因为其他进程需要访问它来打开它们的视图。如果进程致命地崩溃或被杀死，并且不调用存储析构函数，那么文件将保留在系统中。这是非常严重的，因为它们会一直使用内存，直到系统重新启动，或者重新手动释放。 为了解决共享内存文件泄漏的问题，torch.multiprocessing将生成一个名为torch_shm_manager的守护进程，它将自己与当前进程组隔离，并跟踪所有共享内存分配。连接到它的所有进程退出后，它将等待一段时间以确保没有新的连接，并将遍历组分配的所有共享内存文件。如果它发现其中任何一个仍然存在，就会解除它们的分配。我们对这种方法进行了测试，证明它对各种故障都具有鲁棒性。 不过，如果您的系统有足够高的限制，并且file_descriptor是受支持的策略，我们不建议切换到这个策略。 Spawning 子线程 注意 仅支持 Python >= 3.4. 依赖于 spawn 启动方法(在 Python 的 multiprocessing 包中)。 通过创建进程实例并调用join来等待它们完成，可以生成大量子进程来执行某些功能。这种方法在处理单个子进程时工作得很好，但在处理多个进程时可能会出现问题。 也就是说，顺序连接进程意味着它们将顺序终止。如果没有，并且第一个进程没有终止，那么进程终止将不被注意。 此外，没有用于错误传播的本地工具. 下面的spawn函数解决了这些问题，并负责错误传播、无序终止，并在检测到其中一个错误时主动终止进程. torch.multiprocessing.spawn(fn, args=(), nprocs=1, join=True, daemon=False) Spawns nprocs 进程运行 fn 使用参数 args. 如果其中一个进程以非零退出状态退出，则会杀死其余进程，并引发异常，导致终止。在子进程中捕获异常的情况下，将转发该异常，并将其跟踪包含在父进程中引发的异常中。 参数: fn (function) – 函数被称为派生进程的入口点。必须在模块的顶层定义此函数，以便对其进行pickle和派生。这是多进程强加的要求。 该函数称为fn(i， *args)，其中i是进程索引，args是传递的参数元组。 args (tuple) – 传递给 fn 的参数. nprocs (int) – 派生的进程数. join (bool) – 执行一个阻塞的join对于所有进程. daemon (bool) – 派生进程守护进程标志。如果设置为True，将创建守护进程. 返回值: None 如果 join 是 True, SpawnContext 如果 join 是 False class torch.multiprocessing.SpawnContext 由 spawn() 返回, 当 join=False. join(timeout=None) 尝试连接此派生上下文中的一个或多个进程。如果其中一个进程以非零退出状态退出，则此函数将杀死其余进程，并引发异常，导致第一个进程退出。 返回 True如果所有进程正常退出, False 如果有更多的进程需要 join. Parameters: timeout (float) – 放弃等待的最长时间. 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"random.html":{"url":"random.html","title":"torch.random","keywords":"","body":"torch.random torch.random.fork_rng(devices=None, enabled=True, _caller='fork_rng', _devices_kw='devices')[source] 福克斯的RNG，所以，当你返回时，RNG复位的状态，这是以前英寸 参数 * **devices** （可迭代CUDA编号） - CUDA设备针对其叉的RNG。 CPU RNG状态始终分叉。默认情况下， `fork_rng（） `运行在所有设备上，但会发出警告，如果你的机器有很多的设备，因为该功能将运行非常缓慢在这种情况下。如果您明确指定的设备，这个警告将被抑制 * **enabled** （[bool](https://docs.python.org/3/library/functions.html#bool \"\\(in Python v3.7\\)\")） - 如果`假 `时，RNG没有分叉。这是很容易禁用上下文管理，而不必删除它，并在它之下取消缩进Python代码便利的说法。 torch.random.get_rng_state()[source] 返回随机数发生器状态作为 torch.ByteTensor 。 torch.random.initial_seed()[source] 返回初始种子用于产生随机数作为一个Python 长。 torch.random.manual_seed(seed)[source] 设置生成随机数种子。返回 torch.Generator 对象。 参数 **seed** （[int](https://docs.python.org/3/library/functions.html#int） - 所需的种子。 torch.random.seed()[source] 设置用于产生随机数，以非确定性的随机数种子。返回用于播种RNG一个64位的数。 torch.random.set_rng_state(new_state)[source] 设置随机数生成器的状态。 参数 **new_state**(torch.ByteTensor) - 期望状态 随机数发生器 torch.random.get_rng_state()[source] Returns the random number generator state as a torch.ByteTensor. torch.random.set_rng_state(new_state)[source] Sets the random number generator state. Parameters **new_state** ( _torch.ByteTensor_ ) – The desired state torch.random.manual_seed(seed)[source] Sets the seed for generating random numbers. Returns a torch.Generator object. Parameters seed ( int) – The desired seed. torch.random.seed()[source] Sets the seed for generating random numbers to a non-deterministic random number. Returns a 64 bit number used to seed the RNG. torch.random.initial_seed()[source] Returns the initial seed for generating random numbers as a Python long. torch.random.fork_rng(devices=None, enabled=True, _caller='fork_rng', _devices_kw='devices')[source] Forks the RNG, so that when you return, the RNG is reset to the state that it was previously in. Parameters * **devices** ( _iterable of CUDA IDs_ ) – CUDA devices for which to fork the RNG. CPU RNG state is always forked. By default, `fork_rng()`operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed * **enabled** ([ _bool_](https://docs.python.org/3/library/functions.html#bool \"\\(in Python v3.7\\)\")) – if `False`, the RNG is not forked. This is a convenience argument for easily disabling the context manager without having to delete it and unindent your Python code under it. 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"bottleneck.html":{"url":"bottleneck.html","title":"torch.utils.bottleneck","keywords":"","body":"torch.utils.bottleneck 译者: belonHan torch.utils.bottleneck是 调试瓶颈bottleneck时首先用到的工具.它总结了python分析工具与PyTorch自动梯度分析工具在脚本运行中情况. 在命令行运行如下命令 python -m torch.utils.bottleneck /path/to/source/script.py [args] 其中 [args] 是script.py脚本的参数(任意个数).运行python -m torch.utils.bottleneck -h命令获取更多帮助说明. 警告 请确保脚本在分析时能够在有限时间内退出. 警告 当运行CUDA代码时，由于CUDA内核的异步特性, cProfile的输出 和cpu模式的autograd分析工具可能无法显示正确的计时: 报告的CPU时间 是用于启动内核的时间,不包括在GPU上执行的时间。 在常规cpu模式分析器下，同步操作是非常昂贵的。在这种无法准确计时的情况下，可以使用cuda模式的autograd分析工具。 注意 选择查看哪个分析工具的输出结果(CPU模式还是CUDA模式) ,首先应确定脚本是不是CPU密集型CPU-bound(“CPU总时间远大于CUDA总时间”)。如果是cpu密集型，选择查看cpu模式的结果。相反，如果大部分时间都运行在GPU上，再查看CUDA分析结果中相应的CUDA操作。 当然，实际情况取决于您的模型，可能会更复杂，不属于上面两种极端情况。除了分析结果之外,可以尝试使用nvprof命令查看torch.autograd.profiler.emit_nvtx()的结果.然而需要注意NVTX的开销是非常高的,时间线经常会有严重的偏差。 警告 如果您在分析CUDA代码, bottleneck运行的第一个分析工具 (cProfile),它的时间中会包含CUDA的启动(CUDA缓存分配)时间。当然，如果CUDA启动时间远小于代码的中瓶颈,这就被可以忽略。 更多更复杂关于分析工具的使用方法(比如多GPU),请点击https://docs.python.org/3/library/profile.html 或者 torch.autograd.profiler.profile(). 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"checkpoint.html":{"url":"checkpoint.html","title":"torch.utils.checkpoint","keywords":"","body":"torch.utils.checkpoint 译者: belonHan 注意 checkpointing的实现方法是在向后传播期间重新运行已被checkpint的前向传播段。 所以会导致像RNG这类(模型)的持久化的状态比实际更超前。默认情况下，checkpoint包含了使用RNG状态的逻辑(例如通过dropout)，与non-checkpointed传递相比,checkpointed具有更确定的输出。RNG状态的存储逻辑可能会导致一定的性能损失。如果不需要确定的输出，设置全局标志(global flag) torch.utils.checkpoint.preserve_rng_state=False 忽略RNG状态在checkpoint时的存取。 torch.utils.checkpoint.checkpoint(function, *args) checkpoint模型或模型的一部分 checkpoint通过计算换内存空间来工作。与向后传播中存储整个计算图的所有中间激活不同的是，checkpoint不会保存中间激活部分，而是在反向传递中重新计算它们。它被应用于模型的任何部分。 具体来说，在正向传播中，function将以torch.no_grad()方式运行 ，即不存储中间激活,但保存输入元组和 function的参数。在向后传播中，保存的输入变量以及 function会被取回，并且function在正向传播中被重新计算.现在跟踪中间激活，然后使用这些激活值来计算梯度。 Warning 警告 Checkpointing 在 torch.autograd.grad()中不起作用, 仅作用于 torch.autograd.backward(). 警告 如果function在向后执行和前向执行不同，例如,由于某个全局变量，checkpoint版本将会不同，并且无法被检测到。 参数: function - 描述在模型的正向传递或模型的一部分中运行的内容。它也应该知道如何处理作为元组传递的输入。例如，在LSTM中，如果用户通过 ，应正确使用第一个输入作为第二个输入(activation, hidden)functionactivationhidden args – 包含输入的元组function Returns: 输出 torch.utils.checkpoint.checkpoint_sequential(functions, segments, *inputs) 用于checkpoint sequential模型的辅助函数 Sequential模型按顺序执行模块/函数。因此，我们可以将这样的模型划分为不同的段(segment)，并对每个段进行checkpoint。除最后一段外的所有段都将以torch.no_grad()方式运行，即，不存储中间活动。将保存每个checkpoint段的输入，以便在向后传递中重新运行该段。 checkpointing工作方式: checkpoint(). 警告 Checkpointing无法作用于torch.autograd.grad(), 只作用于torch.autograd.backward(). 参数: functions – 按顺序执行的模型， 一个 torch.nn.Sequential对象,或者一个由modules或functions组成的list。 segments – 段的数量 inputs – 输入,Tensor组成的元组 Returns: 按顺序返回每个*inputs的结果 例子 >>> model = nn.Sequential(...) >>> input_var = checkpoint_sequential(model, chunks, input_var) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"cpp_extension.html":{"url":"cpp_extension.html","title":"torch.utils.cpp_extension","keywords":"","body":"torch.utils.cpp_extension torch.utils.cpp_extension.``CppExtension( name , sources , *args , **kwargs )[source] 创建setuptools.Extension为C ++。 一个创建setuptools.Extension与最低限度（但通常​​足以）参数建立一个C ++扩展的便捷方法。 所有参数被转发到setuptools.Extension构造。 例 >>> from setuptools import setup >>> from torch.utils.cpp_extension import BuildExtension, CppExtension >>> setup( name='extension', ext_modules=[ CppExtension( name='extension', sources=['extension.cpp'], extra_compile_args=['-g']), ], cmdclass={ 'build_ext': BuildExtension }) torch.utils.cpp_extension.``CUDAExtension( name , sources , *args , **kwargs )[source] 创建setuptools.Extension [HTG3用于CUDA / C ++。 一个创建setuptools.Extension与最低限度（但通常​​足以）参数建立一个CUDA / C ++扩展的便捷方法。这包括CUDA包括路径，库路径和运行时库。 All arguments are forwarded to the setuptools.Extensionconstructor. Example >>> from setuptools import setup >>> from torch.utils.cpp_extension import BuildExtension, CUDAExtension >>> setup( name='cuda_extension', ext_modules=[ CUDAExtension( name='cuda_extension', sources=['extension.cpp', 'extension_kernel.cu'], extra_compile_args={'cxx': ['-g'], 'nvcc': ['-O2']}) ], cmdclass={ 'build_ext': BuildExtension }) torch.utils.cpp_extension.``BuildExtension( *args , **kwargs )[source] 自定义setuptools的构建扩展。 此setuptools.build_ext亚类需要经过所需要的最小编译器标志的护理（例如，-std = C ++ 11）以及作为混合的C ++ / CUDA汇编（和一般为CUDA文件的支持）。 当使用 BuildExtension，它被允许提供一个字典extra_compile_args（而不是通常的列表）从语言映射（CXX或CUDA）来的额外的编译标志来提供给编译器的列表。这使得可以为混合编译期间提供不同的标记到C ++和CUDA编译器。 torch.utils.cpp_extension.``load( name , sources , extra_cflags=None , extra_cuda_cflags=None , extra_ldflags=None , extra_include_paths=None , build_directory=None , verbose=False , with_cuda=None , is_python_module=True )[source] 加载PyTorch C ++扩展刚刚在时间（JIT）。 要加载的扩展，一个忍者构建文件被发射时，其被用来编译该给定源集成到一个动态库。该库随后被加载到当前的Python程序作为一个模块，并从该函数返回，以备使用。 默认情况下，目录到构建文件发出并编译得到的库来为& LT ; TMP & GT ; / torch_extensions / [ - - ] LT ;名称& GT ;，其中& LT ; TMP & GT ;为当前平台上的临时文件夹并& LT ;名称& GT ;扩展的名称。这个位置可以通过两种方式来覆盖。首先，如果TORCH_EXTENSIONS_DIR环境变量被设置，它取代& LT ; TMP & GT ; / torch_extensions和所有的扩展会被编译成这个目录的子文件夹。其次，如果被提供的BUILD_DIRECTORY参数给此函数，它会覆盖整个路径，即，库将被直接编译到该文件夹​​。 编译源代码，默认的系统编译器（C ++）被使用，其可以通过设置CXX环境变量被重写。传递额外的参数来编译过程，EXTRA_CFLAGS或`可以提供EXTRA_LDFLAGS。例如，为了与编译优化您的扩展，通过EXTRA_CFLAGS = [ ' - O3']。您也可以使用EXTRA_CFLAGS`进一步通过包括目录。 提供CUDA支持混合编译。简单地传递CUDA源文件（.CU或.cuh）与其他来源的沿。这些文件将被检测并与NVCC，而不是C ++编译器编译。这包括使CUDA lib64目录作为一个库的目录，和链接cudart。可以传递附加标志通过至NVCC extra_cuda_cflags，就像EXTRA_CFLAGS为C ++。为寻找CUDA安装目录各种试探被使用，通常做工精细。如果不是，设置CUDA_HOME环境变量是最安全的选择。 Parameters 名 - 扩展的名称来构建。这必须是相同pybind11模块的名称！ 来源 - 相对或绝对路径到C ++源文件的列表。 EXTRA_CFLAGS - 编译器标志的可选列表转发到构建。 extra_cuda_cflags - 编译器标志的可选列表了建设CUDA源时，NVCC。 EXTRA_LDFLAGS - 连接标志的可选列表转发到构建。 extra_include_paths - 包括目录的可选列表转发到构建。 BUILD_DIRECTORY - 可选路径为构建工作空间使用。 冗长 - 若真，接通的负载的步骤详细日志记录。 with_cuda - 确定CUDA头和库是否被添加到该生成。如果设置为无（默认），该值被自动确定基于的.CU或[HTG11存在] .cuh在来源。其设置为 TRUE`给力CUDA头文件和库包括在内。 is_python_module - 若真（默认），出口所产生的共享库的Python模块。如果假，将其加载到处理作为一个纯动态库。 Returns 如果is_python_module是真，返回加载PyTorch扩展作为一个Python模块。如果is_python_module是假返回任何（共享库加载到过程作为副作用）。 Example >>> from torch.utils.cpp_extension import load >>> module = load( name='extension', sources=['extension.cpp', 'extension_kernel.cu'], extra_cflags=['-O2'], verbose=True) torch.utils.cpp_extension.``load_inline( name , cpp_sources , cuda_sources=None , functions=None , extra_cflags=None , extra_cuda_cflags=None , extra_ldflags=None , extra_include_paths=None , build_directory=None , verbose=False , with_cuda=None , is_python_module=True )[source] 装载来自串源的PyTorch C ++扩展刚刚在时间（JIT）。 此函数的行为完全一样 负载（），但需要它的来源字符串而不是文件名。这些字符串存储到文件中生成目录，之后， load_inline的行为（） 是相同的 负载（）。 参见测试使用此功能的好例子。 源可省略典型的非直列C ++扩展的两个必需的部分：必要的头包括，以及所述（pybind11）绑定代码。更精确地，字符串传递给cpp_sources首先连接成单个的.cpp文件。该文件然后用前缀的#include & LT ;torch/ extension.h & GT ;。 此外，如果功能提供参数，绑定将被自动指定为每个功能产生。 功能可以是函数名的列表，或者从功能名称来文档字符串的字典映射。如果给出一个列表，每个函数的名称作为它的文档字符串。 cuda_sources 被连接到一个单独的.CU文件，并通过torch/ types.h中预先考虑在来源 ，cuda.h和 cuda_runtime.h包括。的的.cpp和.CU的文件被单独编译，但最终连接到单个库。注意，没有绑定在 cuda_sources本身为函数生成的。绑定到一个CUDA内核，你必须创建一个C ++函数调用它，无论是申报或cpp_sources 的一个定义这个C ++函数（且在HTG36包括它的名字] 功能）。 参见 负载（）为以下省略的参数的描述。 Parameters cpp_sources - 一个字符串或字符串的列表中，含有C ++源代码。 cuda_sources - 一个字符串或字符串的列表，包含CUDA源代码。 功能 - 要为其生成功能绑定函数名称的列表。如果字典中给出，它应该映射函数名的文档字符串（否则只是函数名）。 with_cuda - 确定CUDA头和库是否被添加到该生成。如果设置为无（默认），该值被自动确定基于是否cuda_sources提供。其设置为 TRUE`给力CUDA头文件和库包括在内。 Example >>> from torch.utils.cpp_extension import load_inline >>> source = ''' at::Tensor sin_add(at::Tensor x, at::Tensor y) { return x.sin() + y.sin(); } ''' >>> module = load_inline(name='inline_extension', cpp_sources=[source], functions=['sin_add']) torch.utils.cpp_extension.``include_paths( cuda=False )[source] 获取包括建立一个C ++或CUDA扩展所需的路径。 Parameters CUDA - CUDA专用如果真，包括包括路径。 Returns 名单包括路径字符串。 torch.utils.cpp_extension.``check_compiler_abi_compatibility( compiler )[source] 验证给定的编译器与PyTorch ABI兼容。 Parameters 编译 （ STR ） - 编译器可执行文件的名称来检查（例如，克++）。必须在shell进程可执行文件。 Returns FALSE如果编译器（可能）ABI-不符合PyTorch，否则真。 torch.utils.cpp_extension.``verify_ninja_availability()[source] 返回真如果忍者打造系统可在系统上。 Next Previous ©版权所有2019年，Torch 贡献者。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"data.html":{"url":"data.html","title":"torch.utils.data","keywords":"","body":"torch.utils.data PyTorch数据加载程序的核心是 torch.utils.data.DataLoader 类。它表示在数据集上可迭代的Python，并支持 映射样式和迭代样式的数据集（map-style and iterable-style datasets） 自定义数据加载顺序（customizing data loading order） 自动批次（automatic batching） 单进程和多进程数据加载（single- and multi-process data loading） 自动内存锁（automatic memory pinning） 这些选项是由DataLoader的构造函数参数配置的，具有签名: ​​ DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, ​ batch_sampler=None, num_workers=0, collate_fn=None, ​ pin_memory=False, drop_last=False, timeout=0, ​ worker_init_fn=None) 下面几节将详细描述这些选项的功能和用法。 数据集类型 DataLoader构造函数最重要的参数是dataset，它表示要从中加载数据的dataset对象。PyTorch支持两种不同类型的数据集: 版图式数据集, 迭代式的数据集. 版图式数据集 版图式数据集实现了 __getitem__() 和 __len__() 协议，并表示从(可能不是完整的)索引/键到数据样本的映射。 例如，当使用 dataset[idx]访问这样的数据集时，可以从磁盘上的文件夹中读取 idx-th i图像及其对应的标签。 参见Dataset了解更多细节。 可迭代式的数据集 可迭代式数据集的 一个子类的实例IterableDataset实现了 __iter__() 协议和代表了数据样本可迭代。这种类型的数据集特别适合这样的情况：随机读取非常高代价，甚至是不可能的，并且批大小取决于获取的数据。 例如，这样的数据集在被访问 iter(dataset)，可以返回从数据库、远程服务器甚至实时生成的日志读取的数据流。 参见 IterableDataset了解更多详情。 注意 当使用 multi-process data loading. 的IterableDataset 时。在每个工作进程上复制相同的数据集对象，因此必须对副本进行不同的配置，以避免重复数据。有关如何实现此目的，请参见IterableDataset 文档。 数据加载顺序和采样器 对于iterable风格的数据集，数据加载顺序完全由用户定义的iterable控制。这允许更容易地实现块读取和动态批处理大小(例如，每次生成一个批处理样例)。 本节的其余部分涉及map-style datasets。torch.utils.data.Sampler 类用于指定数据加载中使用的索引/键的顺序。它们表示数据集索引上的可迭代对象。例如，在随机梯度像样(SGD)的常见情况下，一个 Sampler 可以随机排列一个索引列表，并一次产生一个，或产生一小部分用于小型批量SGD的索引。 顺序采样器或打乱采样器将根据 DataLoader 的' shuffle '参数自动构建。或者，用户可以使用‘sampler’参数来指定一个自定义的Sampler 对象，该对象每次都会生成下一个要获取的索引/键。 一个自定义的 Sampler ，一次生成一批索引的列表，可以作为' batch_sampler '参数传递。自动批处理也可以通过“batch_size”和“drop_last”参数启用。参见下一节 获得更多的细节。 请注意 “sampler”和“batch_sampler”都与迭代式数据集不兼容，因为这样的数据集没有键或索引的概念。 加载批处理和非批处理数据 DataLoader支持通过参数batch_size、drop_last和batch_sampler将单个获取的数据样本自动整理成批。 自动批处理(默认) 这是最常见的情况，它对应于获取少量数据并将其整理成成批的样本，即，包含一个维度为批处理维度(通常是第一个维度)的张量。 当“batch_size”(默认为“1”)不是“None”时，数据加载器将生成成批的样本，而不是单个样本。“batch_size”和“drop_last”参数用于指定数据加载器如何获取批量数据集键。对于地图样式的数据集，用户也可以指定“batch_sampler”，它一次生成一个键列表。 请注意 “batch_size”和“drop_last”参数主要用于从“sampler”构造“batch_sampler”。对于地图样式的数据集，“采样器”要么由用户提供，要么基于“shuffle”参数构造。对于迭代式数据集，“采样器”是一个虚拟的无限数据集。有关采样器的更多信息，请参见本节 请注意 当从具有多个处理的迭代式数据集中获取数据时，drop_last参数将删除每个工作区的数据集副本的最后一批未完成的数据。 使用来自sampler的索引获取样本列表之后，作为collate_fn参数传递的函数被用来将样本列表整理成批量。 在这种情况下，从一个地图样式的数据集加载大致相当于: for indices in batch_sampler: yield collate_fn([dataset[i] for i in indices]) 和从一个迭代式数据集加载大致相当于: dataset_iter = iter(dataset) for indices in batch_sampler: yield collate_fn([next(dataset_iter) for _ in indices]) 自定义 collate_fn可用于自定义排序规则，例如，将顺序数据填充到批处理的最大长度。参见本节 了解更多关于 collate_fn.的信息。 禁用自动批处理 在某些情况下，用户可能希望在数据集代码中手动处理批处理，或者只加载单个示例。例如，直接加载成批数据(例如，从数据库中批量读取数据或读取连续的内存块)，或者批量大小依赖于数据，或者程序设计用于处理单个样本，这样做的成本更低。在这些场景下，最好不要使用自动批处理(其中使用' collate_fn '对样本进行排序)，而是让数据加载器直接返回' dataset '对象的每个成员。 当“batch_size”和“batch_sampler”都是“None”(batch_sampler的默认值已经是“None”)时，自动批处理将被禁用。从' dataset '获得的每个样例都使用作为' collate_fn '参数传递的函数进行处理。 当自动批处理被禁用时，默认的' collate_fn '只是将NumPy数组转换为PyTorch张量，而不改变其他内容。 In this case, loading from a map-style dataset is roughly equivalent with: 在这种情况下，从一个map-style dataset加载大致相当于: for index in sampler: yield collate_fn(dataset[index]) 从一个iterable-style dataset集加载大致相当于: for data in iter(dataset): yield collate_fn(data) 见这一节更多关于collate_fn。 Working with collate_fn 启用或禁用自动批处理时，' collate_fn '的使用略有不同。 当自动批处理被禁用，' collate_fn '与每个单独的数据样本一起被调用，输出由数据加载器迭代器产生。在本例中，默认' collate_fn '只是转换PyTorch张量中的NumPy数组。 启用自动批处理时，每次使用数据样本列表调用' collate_fn '。预期它会将输入样例整理成一个批，以便从数据加载器迭代器生成。本节的其余部分将在本例中描述默认' collate_fn '的行为。 例如，如果每个数据样本包含一个3通道图像和一个完整的类标签，即，数据集的每个元素都返回一个元组' (image, class_index) '，默认的' collate_fn '将这样的元组列表整理成成批处理的图像张量和成批处理的类标签张量的一个元组。特别是，默认的“collate_fn”具有以下属性: 它总是预先添加一个新的维度作为批处理维度。 它自动将NumPy数组和Python数值转换为PyTorch张量。 它保留了数据结构，例如，如果每个样本是一个字典，它将输出一个字典，该字典具有相同的一组键，但将批量张量作为值(如果不能将值转换为张量，则输出列表)。列表s、元组s、名称元组s也是如此。 用户可以使用自定义的“collate_fn”来实现自定义的批处理，例如，根据第一个维度以外的维度进行排序，填充不同长度的序列，或者添加对自定义数据类型的支持。 Single- and Multi-process Data Loading 一个DataLoader 默认使用单进程数据加载。 在Python进程中，全局解释器锁(GIL)会阻止真正的跨线程完全并行化Python代码。为了避免使用数据加载阻塞计算代码，PyTorch提供了一个简单的开关来执行多进程数据加载，只需将参数' num_workers '设置为正整数。 单进程数据加载(默认) 在这种模式下，在初始化' DataLoader '的过程中完成数据获取。因此，数据加载可能会阻塞计算。但是，当用于在进程之间共享数据的资源(例如，共享内存、文件描述符)有限时，或者当整个数据集很小并且可以完全加载到内存中时，这种模式可能是首选的。此外，单进程加载通常显示更多可读的错误跟踪，因此对于调试非常有用。 Multi-process data loading多进程数据加载 将参数' num_workers '设置为正整数将打开多进程数据加载，并使用指定的加载工作进程数量。 在这种模式下，每次创建' DataLoader '的迭代器(例如，当您调用' enumerate(DataLoader) ')时，就会创建' num_workers '工作者进程。此时，' dataset '、' collate_fn '和' worker_init_fn '被传递给每个worker，它们用于初始化和获取数据。这意味着数据集访问及其内部IO、转换(包括' collate_fn ')在工作进程中运行。 torch.utils.data.get_worker_info()返回工作进程中的各种有用信息(包括工作进程id、数据集副本、初始种子等)，并在主进程中返回' None '。用户可以在数据集代码和/或‘worker_init_fn’中使用这个函数来单独配置每个数据集副本，并确定代码是否在工作进程中运行。例如，这对于数据集分片特别有帮助。 对于 map-style 数据集，主进程使用 sampler 生成索引并将它们发送给工作者。因此，任何随机洗牌都是在主进程中完成的，它通过为load分配索引来引导装载。 For iterable-style datasets, since each worker process gets a replica of the dataset object, naive multi-process loading will often result in duplicated data. Using torch.utils.data.get_worker_info() and/or worker_init_fn, users may configure each replica independently. (See IterableDataset documentations for how to achieve this. ) For similar reasons, in multi-process loading, the drop_last argument drops the last non-full batch of each worker’s iterable-style dataset replica. 对于迭代风格的数据集，由于每个工作进程都获得一个“dataset”对象的副本，所以简单的多进程加载通常会导致重复的数据。使用torch.utils.data.get_worker_info()](https://pytorch.org/docs/stable/data.html#torch.utils.data.get_worker_info)'或 worker_init_fn,，用户可以独立配置每个副本。(参见 IterableDataset 出于类似的原因，在多进程加载过程中，' drop_last '参数会删除每个worker的迭代式数据集副本的最后一批非完整数据。 一旦到达迭代的末尾，或者当迭代器变成垃圾收集时，Workers就会被关闭。 警告 它一般不建议恢复在多进程加载CUDA张量，因为许多微妙之处使用CUDA和多分享CUDA张量（见多处理 CUDA）。相反，我们建议使用自动存储器钉扎（即，设置pin_memory =真），这使得能够快速数据传输到支持CUDA的GPU。 特定于平台的行为 由于工人依靠Python的多重处理 ，工人启动在Windows上U不同于nix。 在Unix上，fork() 是默认的multiprocessing 启动方法。使用“fork()”，儿童工作者通常可以通过克隆的地址空间直接访问 dataset 和Python参数函数。 在Windows中，产卵（）为默认 多处理启动方法。使用重生（），另一种解释是推出是运行在主脚本，然后由接收数据集内部职工功能， collat​​e_fn和通过 泡菜序列的其它参数。 在Windows上，spawn()是默认的多处理启动方法(multiprocessing )。使用spawn() ，启动另一个解释器，它运行主脚本，然后启动内部的worker函数，它通过 pickle 序列化接收数据集、collate_fn和其他参数。 这种独立的序列化意味着，你应该采取两个步骤，以确保与Windows兼容，同时使用多进程数据加载: 将主脚本的大部分代码封装在 if __name__ == '__main__': block, 中，以确保在启动每个工作进程时不会再次运行(很可能会产生错误)。您可以将数据集和DataLoader 实例创建逻辑放在这里，因为它不需要在workers中重新执行。 确保任何自定义的collate_fn, worker_init_fn 或数据集代码都被声明为顶层定义，并在 __main__ 检查之外。这确保它们在工作进程中可用。(这是必需的，因为函数仅作为引用进行pickle，而不是作为字节码。) 多进程数据加载的随机性 默认情况下，每个worker将其PyTorch种子设置为base_seed + worker_id，其中base_seed是由使用其RNG的主进程生成的长种子(因此，强制使用RNG状态)。但是，其他库的种子可能在初始化worker (w.g.)时被复制。，导致每个worker返回相同的随机数。(参见FAQ中的这个 部分 )。 In worker_init_fn, you may access the PyTorch seed set for each worker with either torch.utils.data.get_worker_info().seed or torch.initial_seed(), and use it to seed other libraries before data loading. 在worker_init_fn,你可以访问PyTorch种子为每个工具人与 torch.utils.data.get_worker_info().seed 或 torch.initial_seed(),并使用它的种子数据加载之前其他库。 Memory Pinning 当来自固定(页面锁定)内存时，GPU副本的主机速度要快得多。参见使用固定内存缓冲区了解更多关于何时以及如何使用固定内存的细节。 对于数据加载，将' pin_memory=True '传递给 DataLoader 将自动将获取的数据张量放入固定内存中，从而能够更快地将数据传输到支持cuda的gpu。 默认的内存固定逻辑只识别张量、映射和包含张量的迭代器。默认情况下,如果把逻辑看到一批自定义类型(这将发生如果你有一批“collate_fn”,返回一个自定义类型),或者如果你批的每个元素是一个自定义类型,将逻辑不会认出他们,它会返回这批没有固定的内存(或这些元素)。要为自定义批处理或数据类型启用内存固定，请在自定义类型上定义' pin_memory() '方法。 See the example below. 请参见下面的例子。 例： class SimpleCustomBatch: def __init__(self, data): transposed_data = list(zip(*data)) self.inp = torch.stack(transposed_data[0], 0) self.tgt = torch.stack(transposed_data[1], 0) # custom memory pinning method on custom type def pin_memory(self): self.inp = self.inp.pin_memory() self.tgt = self.tgt.pin_memory() return self def collate_wrapper(batch): return SimpleCustomBatch(batch) inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5) tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5) dataset = TensorDataset(inps, tgts) loader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper, pin_memory=True) for batch_ndx, sample in enumerate(loader): print(sample.inp.is_pinned()) print(sample.tgt.is_pinned()) CLASStorch.utils.data.``DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None) ​ 数据加载程序。组合一个数据集和一个采样器，并在给定的数据集上提供一个可迭代的。 DataLoader支持地图样式和迭代样式的数据集，支持单进程或多进程加载、自定义加载顺序以及可选的自动批处理(排序)和内存固定。 看torch.utils.data 。有关更多详细信息，请参阅数据文档页。 Parameters ​ dataset (Dataset) - 从该数据集到加载数据。 batch_size (int, optional) ） - 如何每批许多样品加载（默认值：1）。 shuffle (bool, optional) ） - 设置为真为具有在每个历元改组的数据（默认值：假）。 sampler (Sampler, optional) ） - 定义从数据集中得出样品的策略。如果指定，洗牌必须假 [HTG17。 batch_sampler （ 取样 ， 可选 ） - 象取样，但在同一时间返回一批指标。互斥与的batch_size，洗牌，取样和drop_last。 num_workers （ INT ， 可选 ） - 多少子过程用于数据加载。 0意味着数据将在主处理加载。 （默认值：0） collat​​e_fn （ 可调用 ， 可选 ） - 合并的样本的列表，以形成小批量张量（S）的。使用从图式集装批处理时使用。 pin_memory （ 布尔 ， 可选 ） - 如果真，数据装载将在返回之前复制到张量CUDA固定内存。如果数据元素是一个自定义类型，或你的collat​​e_fn返回一批即自定义类型，见下面的例子。 drop_last （ 布尔 ， 可选 ） - 设置为真放弃最后一批不全，如果数据集大小不是由批量大小整除。如果假和数据集的大小是不是批量大小整除，则最后一批将较小。 （默认值：假） timeout（ 数字 ， 可选 ） - 如果为正，则为从工作者收集批的超时值。应该是非负的。(默认值:0) worker_init_fn （ 可调用 ， 可选 ） - 如果不是' None '，则在播种之后和数据加载之前，以工作者id (' [0, num_workers - 1] '中的int)作为输入，在每个工作者子进程上调用它。(默认:“没有一个”) Warning 如果使用 spawn 启动方法，则worker_init_fn 不能是一个不可修改的对象，例如lambda函数。有关PyTorch中多处理的更多细节，请参见Multiprocessing best practices 。 Note len(dataloader) 启发式是基于所用采样器的长度。当“dataset”是一个IterableDataset时，将使用一个无限采样器，它的 __len__() 没有实现，因为实际长度取决于可迭代和多进程加载配置。因此，除非使用地图样式的数据集，否则不应该查询此方法。有关这两种数据集的详细信息，请参见 Dataset Types CLASStorch.utils.data.``Dataset 表示数据集的抽象类。 所有表示从键到数据样本的映射的数据集都应该继承它。所有的子类都应该覆盖__getitem__()，支持为给定的键获取数据样本。子类也可以选择性地覆盖 __len__()预计返回数据集的大小由许多Sampler 实现和默认选项DataLoader. Note 的DataLoader缺省构建一个索引采样能产生整数指数。为了使它与地图式的数据集与非整指数/键的作用，必须提供自定义采样。 DataLoader 默认情况下构造一个索引采样器，生成完整的索引。要使它与具有非完整索引/键的地图样式数据集一起工作，必须提供自定义采样器。 classtorch.utils.data.``IterableDataset[source] ​ 可迭代的数据集。 代表数据样本的迭代所有数据集应该继承它。当数据来自一个数据集流的这种形式是特别有用的。 所有子类应该overrite __iter __（），这将返回样本的迭代在该数据集。 当一个子类使用具有 的DataLoader，在数据集中的每个项目将被从得到的 的DataLoader迭代器。当num_workers & GT ; 0，每个工作进程将具有数据集对象的不同拷贝，因此通常希望独立地配置每个拷贝，以避免从工人返回重复数据。get_worker_info（），在一个工作进程调用时，返回关于工人的信息。它可以在任一使用的数据集的__iter __（）方法或 的DataLoader的worker_init_fn选项来修改每个副本的行为。 实施例1：在所有工人分裂工作量__iter __（）： ​ >>> class MyIterableDataset(torch.utils.data.IterableDataset): ... def __init__(self, start, end): ... super(MyIterableDataset).__init__() ... assert end > start, \"this example code only works with end >= start\" ... self.start = start ... self.end = end ... ... def __iter__(self): ... worker_info = torch.utils.data.get_worker_info() ... if worker_info is None: # single-process data loading, return the full iterator ... iter_start = self.start ... iter_end = self.end ... else: # in a worker process ... # split workload ... per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers))) ... worker_id = worker_info.id ... iter_start = self.start + worker_id * per_worker ... iter_end = min(iter_start + per_worker, self.end) ... return iter(range(iter_start, iter_end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7) >>> # Single-process loading >>> print(list(torch.utils.data.DataLoader(ds, num_workers=0))) [3, 4, 5, 6] >>> # Mult-process loading with two worker processes >>> # Worker 0 fetched [3, 4]. Worker 1 fetched [5, 6]. >>> print(list(torch.utils.data.DataLoader(ds, num_workers=2))) [3, 5, 4, 6] >>> # With even more workers >>> print(list(torch.utils.data.DataLoader(ds, num_workers=20))) [3, 4, 5, 6] 实施例2：使用worker_init_fn在所有工人之间分配工作负载: >>> class MyIterableDataset(torch.utils.data.IterableDataset): ... def __init__(self, start, end): ... super(MyIterableDataset).__init__() ... assert end > start, \"this example code only works with end >= start\" ... self.start = start ... self.end = end ... ... def __iter__(self): ... return iter(range(self.start, self.end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7) >>> # Single-process loading >>> print(list(torch.utils.data.DataLoader(ds, num_workers=0))) [3, 4, 5, 6] >>> >>> # Directly doing multi-process loading yields duplicate data >>> print(list(torch.utils.data.DataLoader(ds, num_workers=2))) [3, 3, 4, 4, 5, 5, 6, 6] >>> # Define a `worker_init_fn` that configures each dataset copy differently >>> def worker_init_fn(worker_id): ... worker_info = torch.utils.data.get_worker_info() ... dataset = worker_info.dataset # the dataset copy in this worker process ... overall_start = dataset.start ... overall_end = dataset.end ... # configure the dataset to only process the split workload ... per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers))) ... worker_id = worker_info.id ... dataset.start = overall_start + worker_id * per_worker ... dataset.end = min(dataset.start + per_worker, overall_end) ... >>> # Mult-process loading with the custom `worker_init_fn` >>> # Worker 0 fetched [3, 4]. Worker 1 fetched [5, 6]. >>> print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn))) [3, 5, 4, 6] >>> # With even more workers >>> print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn))) [3, 4, 5, 6] classtorch.utils.data.``TensorDataset( *tensors )[source] ​ 数据集包装张量。 每个样品将沿所述第一维度的索引张量进行检索。 Parameters ​ *tensors (Tensor) - 具有所述第一尺寸的大小相同张量。 classtorch.utils.data.``ConcatDataset( datasets )[source] ​ 数据集作为多个数据集的串联。 这个类是组装不同的现有数据集是有用的。 Parameters ​ datasets (sequence) 数据集 （ 序列 ） - 数据集的列表要连接 classtorch.utils.data.``ChainDataset( datasets )[source] ​ 数据集chainning多个 IterableDataset秒。 这个类是组装不同的现有数据集流是有用的。该chainning操作上即时完成的，因此串联与此类大型数据集将是有效的。 Parameters ​ 数据集 （ IterableDataset 的迭代） - 数据集链接在一起 classtorch.utils.data.``Subset( dataset , indices )[source] ​ 在指定的索引数据集的子集。 Parameters ​ 数据集 （ 数据集 ） - 整个数据集 指数 （ 序列 ） - 在整个组索引选择的子集 torch.utils.data.``get_worker_info()[source] ​ 返回当前 的DataLoader迭代工作进程的信息。 当一个工人叫，这将返回保证具有以下属性的对象： ID：当前作业人员ID。 num_workers：工人的总数。 种子：当前工人随机种子集。此值由主进程RNG和工人的ID来确定。参见 的DataLoader的更多细节的文档。 数据集：数据集对象在 这里 过程的副本。请注意，这将是在不同的进程比一个主处理不同的对象。 当主过程调用，这将返回无。 Note 用于worker_init_fn经过DataLoader时,这种方法可能是有用的设置每个工作进程不同,例如,使用worker_id配置数据集对象只读取一个特定部分的分片数据集,或其他使用种子种子库中使用数据集的代码(例如,NumPy)。 torch.utils.data.``random_split( dataset , lengths )[source] ​ 随机分割数据集到给定长度的非重叠的新的数据集。 Parameters ​ dataset （ 数据集 ） - 数据集要被分割 lengths （ 序列 ） - 要产生裂缝的长度 classtorch.utils.data.``Sampler( data_source )[source] ​ 基类的所有取样。 每采样的子类必须提供一个 __iter__() 的方法，提供一种方式来迭代数据集的元素的索引，和 __len__() 方法，它返回所返回的迭代器的长度。 Note 的__len __（）方法并不严格 的DataLoader必需的，但在涉及任何计算预期的 的DataLoader的长度。 classtorch.utils.data.``SequentialSampler( data_source )[source] ​ 顺序地将样品的元素，总是以相同的顺序。 Parameters ​ DATA_SOURCE （ 数据集 ） - 数据集以从采样 classtorch.utils.data.``RandomSampler( data_source , replacement=False , num_samples=None )[source] ​ 样品元件中随机。如果不更换，然后从一个洗牌的数据集进行采样。如果具有置换，然后用户可指定num_samples绘制。 Parameters ​ data_source ( Dataset) – dataset to sample from replacement（ 布尔 ） - 样品绘制替换如果真，默认=False num_samples （ INT ） - 样本的数目来绘制，默认=LEN（数据集）。该参数应该当替换是真仅被指定。 classtorch.utils.data.``SubsetRandomSampler( indices )[source] ​ 随机样本元素从指数的定列表，无需更换。 Parameters ​ indices (sequence) - 索引的序列 classtorch.utils.data.``WeightedRandomSampler( weights , num_samples , replacement=True )[source] ​ 从样品元素[0,..,len(weights)-1]` 与给定的概率（权重）。 Parameters ​ weights（ 序列 ） - 权重的顺序，没有必要总结到一个 num_samples （ INT ） - 样本的数目来绘制 replacement （ 布尔 ） - 如果真，样品绘制更换。如果不是，他们绘制无需更换，这意味着当指数样本绘制为行，不能再为该行画出。 例 >>> list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True)) [0, 0, 0, 1, 0] >>> list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False)) [0, 1, 4, 3, 2] 包装另一个采样，以产生小批量指数。 Parameters ​ sampler（ 取样 ） - 基采样器。 batch_size (int) - 小批量的大小。 drop_last (bool) - 如果真，采样器将下降的最后一批，如果它的规模将是小于的batch_size Example >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False)) [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True)) [[0, 1, 2], [3, 4, 5], [6, 7, 8]] 限制数据加载到数据集子集的采样器。 它与torch.nn.parallel.DistributedDataParallel.特别有用。在这种情况下，每个进程可以将DistributedSampler实例作为DataLoader采样器传递，并加载原始数据集的一个子集，该子集是它独有的。 Note 数据集被认为是恒定的大小。 Parameters ​ dataset - 数据集用于采样。 num_replicas （ 可选 ） - 的参与分布式训练的进程数。 rank（ 可选 ） - num_replicas内的当前过程的秩。 shuffle（ 可选 ） - 如果为true（默认值），采样器将会洗牌指数 Next Previous ©版权所有2019年，Torch 贡献者。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"dlpack.html":{"url":"dlpack.html","title":"torch.utils.dlpack","keywords":"","body":"torch.utils.dlpack torch.utils.dlpack.from_dlpack( dlpack ) → Tensor 解码DLPack到张量。 Parameters **dlpack** - 与dltensor一个PyCapsule对象 张量将与dlpack表示的对象共享存储器。请注意，每个dlpack只能使用一次消耗。 torch.utils.dlpack.to_dlpack( tensor ) → PyCapsule 返回表示张量DLPack。 Parameters **tensor** - 要导出的张量 该dlpack共享内存的张量。请注意，每个dlpack只能使用一次消耗。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"model_zoo.html":{"url":"model_zoo.html","title":"torch.utils.model_zoo","keywords":"","body":"torch.utils.model_zoo 译者：BXuan694 torch.utils.model_zoo.load_url(url, model_dir=None, map_location=None, progress=True) 由给定URL加载Torch序列化对象。 如果该对象已经存在于model_dir中，将被反序列化并返回。URL的文件名部分应该遵循约定filename-.ext，其中是文件内容的SHA256哈希的前八位或更多位数。（哈希用于确保唯一的名称并验证文件的内容） model_dir默认为$TORCH_HOME/models，其中$TORCH_HOME默认是~/.torch。如果不需要默认目录，可以通过环境变量$TORCH_MODEL_ZOO指定其它的目录。 参数： url（string）– 要下载的对象的URL链接 model_dir（string , 可选）– 保存下载对象的目录 map_location（可选）– 函数或字典，指定如何重新映射存储位置（见torch.load） progress（bool, 可选）– 是否向标准输出展示进度条 示例 >>> state_dict = torch.utils.model_zoo.load_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth') 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"tensorboard.html":{"url":"tensorboard.html","title":"torch.utils.tensorboard","keywords":"","body":"torch.utils.tensorboard 在进一步讨论之前，可以在https://www.tensorflow.org/tensorboard/上找到有关TensorBoard的更多详细信息。 一旦你安装TensorBoard，这些工具让您登录PyTorch模型和指标纳入了TensorBoard UI中的可视化的目录。标量，图像，柱状图，曲线图，和嵌入可视化都支持PyTorch模型和张量以及Caffe2网和斑点。 SummaryWriter类是记录TensorBoard使用和可视化数据的主入口。例如：​ import torch import torchvision from torch.utils.tensorboard import SummaryWriter from torchvision import datasets, transforms # Writer will output to ./runs/ directory by default writer = SummaryWriter() transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) trainset = datasets.MNIST('mnist_train', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) model = torchvision.models.resnet50(False) # Have ResNet model take in grayscale rather than RGB model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) images, labels = next(iter(trainloader)) grid = torchvision.utils.make_grid(images) writer.add_image('images', grid, 0) writer.add_graph(model, images) writer.close() 然后可以用TensorBoard可视化，这应该是安装和运行的有： pip install tensorboard tensorboard --logdir=runs 一次实验可以记录很多信息。为了避免混乱的UI，并有更好的聚类的结果，我们可以通过分层命名来对图进行分组。例如，“Loss/train”和“Loss/test”将被分组在一起，而“Accuracy/train”和“Accuracy/test”将分别在TensorBoard接口分组。 from torch.utils.tensorboard import SummaryWriter import numpy as np writer = SummaryWriter() for n_iter in range(100): writer.add_scalar('Loss/train', np.random.random(), n_iter) writer.add_scalar('Loss/test', np.random.random(), n_iter) writer.add_scalar('Accuracy/train', np.random.random(), n_iter) writer.add_scalar('Accuracy/test', np.random.random(), n_iter) 预期结果： classtorch.utils.tensorboard.writer.``SummaryWriter( log_dir=None , comment='' , purge_step=None , max_queue=10 , flush_secs=120 , filename_suffix='' )[source] 将条目直接写入log_dir中的事件文件中，供TensorBoard使用。 在 SummaryWriter SummaryWriter类提供了一个高级API，可以在给定的目录中创建事件文件，并向其中添加摘要和事件。该类异步更新文件内容。训练程序调用方法直接从训练循环中向文件添加数据，而不会减慢训练速度。 __init__( log_dir=None , comment='' , purge_step=None , max_queue=10 , flush_secs=120 , filename_suffix='' )[source] 创建 SummaryWriter 将写出事件和摘要的事件文件。 Parameters ​ log_dir (string) - 保存目录位置。缺省值是运行/ CURRENT_DATETIME_HOSTNAME ，在每次运行后，其改变。采用分层文件夹结构容易运行之间的比较。例如通过在 ‘runs/exp1’, ‘runs/exp2’等，对每一个新实验进行比较。 comment (string) - 添加到默认log_dir后缀的注释log_dir。如果分配了log_dir，则此参数无效 purge_step (int) -当日志记录在步骤T+XT+X崩溃并在步骤TT重新启动时，global_step大于或等于TT的任何事件将被清除并从TensorBoard中隐藏。注意，崩溃和恢复的实验应该具有相同的log_dir。 max_queue (int) - 在其中一个“add”调用强制刷新到磁盘之前，挂起事件和摘要的队列大小。默认为10项。 flush_secs (int) - 将事件挂起和将摘要刷新到磁盘的频率（秒）。默认值是每两分钟一次。 filename_suffix (string) - 添加到日志目录中所有事件文件名的后缀。在tensorboard.summary.writer.event_file_writer.EventFileWriter中有更多细节。 例子： from torch.utils.tensorboard import SummaryWriter # create a summary writer with automatically generated folder name. writer = SummaryWriter() # folder location: runs/May04_22-14-54_s-MacBook-Pro.local/ # create a summary writer using the specified folder name. writer = SummaryWriter(\"my_experiment\") # folder location: my_experiment # create a summary writer with comment appended. writer = SummaryWriter(comment=\"LR_0.1_BATCH_16\") # folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/ add_scalar( tag , scalar_value , global_step=None , walltime=None )[source] ​ 标量数据添加到汇总。 Parameters ​ tag (string) - 数据标识符 scalar_value (float or string/blobname) - 值保存 global_step (int) –- 记录的全局步长值 walltime (float) – 可选覆盖默认的walltime (time.time())，在事件一轮后的几秒内覆盖 Examples: ​​ from torch.utils.tensorboard import SummaryWriter ​ writer = SummaryWriter() ​ x = range(100) ​ for i in x: ​ writer.add_scalar('y=2x', i * 2, i) ​ writer.close() Expected result: add_scalars( main_tag , tag_scalar_dict , global_step=None , walltime=None )[source] ​ 向摘要添加许多标量数据。 注意，此函数还将记录标量保存在内存中。在极端情况下，它会让你的内存爆满。 Parameters ​ main_tag (string) – 标记的父名称 tag_scalar_dict (dict) – 存储标记和相应值的键值对 global_step (int) – 要记录的全局步骤值 walltime (float) – 可选的覆盖默认的walltime (time.time())事件历元后 Examples: from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter() r = 5 for i in range(100): writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r), 'xcosx':i*np.cos(i/r), 'tanx': np.tan(i/r)}, i) writer.close() # This call adds three values to the same scalar plot with the tag # 'run_14h' in TensorBoard's scalar section. Expected result: add_histogram( tag , values , global_step=None , bins='tensorflow' , walltime=None , max_bins=None )[source] ​ 添加柱状图总结。 Parameters ​ tag (string) – 数据标识符 values (torch.Tensor, numpy.array**, or string/blobname) – 值构建直方图 global_step (int) – 要记录的全局步长值 bins (string) – One of {‘tensorflow’,’auto’, ‘fd’, …}. 这决定了bins的制作方式。您可以在以下地址找到其他选项: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) – 可选覆盖默认的walltime (time.time())事件历元后的 Examples: ​ from torch.utils.tensorboard import SummaryWriter import numpy as np writer = SummaryWriter() for i in range(10): x = np.random.random(1000) writer.add_histogram('distribution centers', x + i, i) writer.close() Expected result: add_image( tag , img_tensor , global_step=None , walltime=None , dataformats='CHW' )[source] ​ 将图像数据添加到摘要中。 请注意，这需要 pillow 包装。 Parameters ​ tag ( string ) – Data identifier img_tensor （ torch.Tensor ， numpy.array 或 串/ blobname ） - 图像数据 global_step ( int) – Global step value to record walltime ( float) – Optional override default walltime (time.time()) seconds after epoch of event Shape: ​ img_tensor:默认值为(3,H,W) (3,H,W)。您可以使用 torchvision.utils.make_grid() 将一批张量转换成3xHxW格式，或者调用 add_images ，让我们来完成这项工作。(1,H,W) (1,H,W) (H,W) (H,W) (H,W) (H,W) (H,W,3) (H,W,3)张量也是可以的，只要传递了相应的 dataformats 参数。例如:CHW, HWC, HW。 Examples: from torch.utils.tensorboard import SummaryWriter import numpy as np img = np.zeros((3, 100, 100)) img[0] = np.arange(0, 10000).reshape(100, 100) / 10000 img[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000 img_HWC = np.zeros((100, 100, 3)) img_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 img_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000 writer = SummaryWriter() writer.add_image('my_image', img, 0) # If you have non-default dimension setting, set the dataformats argument. writer.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC') writer.close() Expected result: add_images( tag , img_tensor , global_step=None , walltime=None , dataformats='NCHW' )[source] ​ 成批的图像数据添加到汇总。 Note that this requires the pillowpackage. 请注意，这需要pillowpackage。 Parameters ​ tag ( string ) – Data identifier img_tensor ( torch.Tensor , numpy.array , or string/blobname ) – Image data global_step ( int) – Global step value to record walltime ( float) – Optional override default walltime (time.time()) seconds after epoch of event dataformats （ 串 ） - 形式的NCHW，NHWC，CHW，HWC，HW，WH等的图像数据格式规范 Shape: ​ img_tensor：默认为 （ N ， 3 ， H ， W ） （N，3，H，W） （ N ， 3 ， H ， W ） 。如果dataformats被指定，其他形状将被接受。例如NCHW或NHWC。 Examples: ​ from torch.utils.tensorboard import SummaryWriter import numpy as np img_batch = np.zeros((16, 3, 100, 100)) for i in range(16): img_batch[i, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 / 16 * i img_batch[i, 1] = (1 - np.arange(0, 10000).reshape(100, 100) / 10000) / 16 * i writer = SummaryWriter() writer.add_images('my_image_batch', img_batch, 0) writer.close() Expected result: add_figure( tag , figure , global_step=None , close=True , walltime=None )[source] ​ 渲染matplotlib图成图像并将其添加到汇总。 注意，这需要的matplotlib包。 Parameters ​ tag ( string ) – Data identifier [HTG0图（ matplotlib.pyplot.figure ） - 图或数字的列表 global_step ( int) – Global step value to record 关闭 （ 布尔 ） - 标志自动关闭该图 walltime ( float) – Optional override default walltime (time.time()) seconds after epoch of event add_video( tag , vid_tensor , global_step=None , fps=4 , walltime=None )[source] ​ 视频数据添加到汇总。 注意，这需要的moviepy包。 Parameters ​ tag (string) – Data identifier vid_tensor (torch.Tensor) – Video data global_step (int) – Global step value to record fps (float or int) – Frames per second walltime (float) – Optional override default walltime (time.time()) seconds after epoch of event Shape: ​ vid_tensor： （ N ， T ， C ， H ， W ） （N，T，C，H，W） （ N ， T ， C ， H ， W ） 。的值应该位于[0,255]为式 UINT8 或[0,1]类型浮动。 add_audio( tag , snd_tensor , global_step=None , sample_rate=44100 , walltime=None )[source] ​ 音频数据添加到汇总。 Parameters ​ tag ( string ) – Data identifier snd_tensor （ torch.Tensor ） - 声音数据 global_step ( int) – Global step value to record SAMPLE_RATE （ INT ） - 以Hz采样率 walltime ( float) – Optional override default walltime (time.time()) seconds after epoch of event Shape: ​ snd_tensor： （ 1 ， L ） （1，L） （ 1 ， L ） 。值应该[-1,1]之间。 add_text( tag , text_string , global_step=None , walltime=None )[source] ​ 文本数据添加到汇总。 Parameters ​ tag ( string ) – Data identifier text_string的 （ 串 ） - 字符串，以节省 global_step ( int) – Global step value to record walltime ( float) – Optional override default walltime (time.time()) seconds after epoch of event Examples: ​​ writer.add_text('lstm', 'This is an lstm', 0) ​ writer.add_text('rnn', 'This is an rnn', 10) add_graph( model , input_to_model=None , verbose=False )[source] ​ 图数据添加到汇总。 Parameters ​ 模型 （ torch.nn.Module ） - 模型绘制。 input_to_model （ torch.Tensor 或 torch.Tensor 的列表中） - 的变量或变量的元组被输送。 冗长 （ 布尔 ） - 是否打印图形结构在控制台。 add_embedding( mat , metadata=None , label_img=None , global_step=None , tag='default' , metadata_header=None )[source] ​ 添加投影数据嵌入到总结。 Parameters ​ 垫 （ torch.Tensor 或 numpy.array ） - 甲矩阵，每一行都是特征向量数据点 元数据 （ 列表 ） - 标签的列表，每个元件将转换为串 label_img （ torch.Tensor ） - 图像对应于每个数据点 global_step ( int) – Global step value to record 标记 （ 串 ） - 名称为嵌入 Shape: ​ 垫： （ N ， d ） （N，d） （ N ， d ） ，其中N是数据的数和d是特征尺寸 label_img： （ N ， C ， H ， W ） （N，C，H，W） （ N ， C ， H ， W ） Examples: import keyword import torch meta = [] while len(meta)add_pr_curve( tag , labels , predictions , global_step=None , num_thresholds=127 , weights=None , walltime=None )[source] 添加精确召回曲线。绘制精确召回曲线可以了解模型在不同阈值设置下的性能。使用此函数，可以为每个目标提供基本真实值标记 (T/F) 和预测置信度（通常是模型的输出）TensorBoard UI将允许您交互地选择阈值。 Parameters ​ tag (string) –– Data identifier labels (torch.Tensor, numpy.array**, or string/blobname) – 地面实测数据。每个元素的二进制标签。 predictions (torch.Tensor, numpy.array**, or string/blobname) – 该元素被分类为真概率。值应在[0，1] global_step ( int) – Global step value to record num_thresholds (int) – 用于绘制曲线的阈值的数量。 walltime ( float) – Optional override default walltime (time.time()) seconds after epoch of event Examples: ​ from torch.utils.tensorboard import SummaryWriter import numpy as np labels = np.random.randint(2, size=100) # binary label predictions = np.random.rand(100) writer = SummaryWriter() writer.add_pr_curve('pr_curve', labels, predictions, 0) writer.close() add_custom_scalars( layout)[source] 通过收集“scalars”中的图表标记创建特殊图表。请注意，对于每个summarywriter（）对象，此函数只能调用一次。因为它只向tensorboard提供元数据，所以可以在训练循环之前或之后调用该函数。 Parameters ​ layout (dict) - {categoryName: charts}，其中charts也是一个字典{chartName: ListOfProperties}。ListOfProperties中的第一个元素是图表的类型(多行或空白中的一个)，第二个元素应该是包含add_scalar函数中使用的标记的列表，这些标记将被收集到新图表中。 Examples: ​​ layout = {'Taiwan':{'twse':['Multiline',['twse/0050', 'twse/2330']]}, ​ 'USA':{ 'dow':['Margin', ['dow/aaa', 'dow/bbb', 'dow/ccc']], ​ 'nasdaq':['Margin', ['nasdaq/aaa', 'nasdaq/bbb', 'nasdaq/ccc']]}} ​​ writer.add_custom_scalars(layout) add_mesh( tag , vertices , colors=None , faces=None , config_dict=None , global_step=None , walltime=None )[source] ​ 将网格或三维点云添加到TensorBoard。可视化基于three.js，因此它允许用户与呈现的对象交互。除了顶点、面等基本定义外，用户还可以进一步提供相机参数、照明条件等，高级使用请查看https://threejs.org/docs/index.html manual/en/introduction/creating-a-scene。 Parameters ​ tag ( string ) – Data identifier 顶点 （ torch.Tensor ） - 三维坐标列表的顶点。 颜色 （ torch.Tensor ） - 为每个顶点的色彩 面 （ torch.Tensor ） - 每个三角形内的顶点指数。 （可选的） config_dict - 字典与ThreeJS类的名称和结构。 global_step ( int) – Global step value to record walltime ( float) – Optional override default walltime (time.time()) seconds after epoch of event Shape: ​ vertices: (B, N, 3) （ B ， N ， 3 ） （B，N，3） （ B ， N ， 3 ） 。 （分批，number_of_vertices，通道） colors: (B, N, 3) （ B ， N ， 3 ） （B，N，3） （ B ， N ， 3 ） 。的值应该位于[0,255]为式 UINT8 或[0,1]类型浮动。 faces: (B, N, 3)（ B ， N ， 3 ） （B，N，3） （ B ， N ， 3 ） 。的值应该位于[0，number_of_vertices]为式 UINT8 。 Examples: ​ from torch.utils.tensorboard import SummaryWriter vertices_tensor = torch.as_tensor([ [1, 1, 1], [-1, -1, 1], [1, -1, -1], [-1, 1, -1], ], dtype=torch.float).unsqueeze(0) colors_tensor = torch.as_tensor([ [255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 0, 255], ], dtype=torch.int).unsqueeze(0) faces_tensor = torch.as_tensor([ [0, 2, 3], [0, 3, 1], [0, 1, 2], [1, 3, 2], ], dtype=torch.int).unsqueeze(0) writer = SummaryWriter() writer.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor) writer.close() flush()[source] ​ 刷新事件文件到磁盘。调用此方法，以确保所有未决事件已被写入磁盘。 close()[source] 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"onnx.html":{"url":"onnx.html","title":"torch.onnx","keywords":"","body":"torch.onnx 译者：guobaoyo 示例:从Pytorch到Caffe2的端对端AlexNet模型 这里是一个简单的脚本程序,它将一个在 torchvision 中已经定义的预训练 AlexNet 模型导出到 ONNX 格式. 它会运行一次,然后把模型保存至 alexnet.onnx: import torch import torchvision dummy_input = torch.randn(10, 3, 224, 224, device='cuda') model = torchvision.models.alexnet(pretrained=True).cuda() # 可以根据模块图形的数值设置输入输出的显示名称。这些设置不会改变此图形的语义。只是会变得更加可读了。 #该网络的输入包含了输入的扁平表(flat list)。也就是说传入forward()里面的值，其后是扁平表的参数。你可以指定一部分名字，例如指定一个比该模块输入数量更少的表，随后我们会从一开始就设定名字。 input_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ] output_names = [ \"output1\" ] torch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, input_names=input_names, output_names=output_names) 得到的 alexnet.onnx 是一个 protobuf 二值文件, 它包含所导出模型 ( 这里是 AlexNet )中网络架构和网络参数. 关键参数 verbose=True 会使导出过程中打印出的网络更可读: #这些是网络的输入和参数，包含了我们之前设定的名称。 graph(%actual_input_1 : Float(10, 3, 224, 224) %learned_0 : Float(64, 3, 11, 11) %learned_1 : Float(64) %learned_2 : Float(192, 64, 5, 5) %learned_3 : Float(192) # ---- 为了简介可以省略 ---- %learned_14 : Float(1000, 4096) %learned_15 : Float(1000)) { # 每个声明都包含了一些输出张量以及他们的类型，以及即将运行的操作符（并且包含它的属性，例如核部分，步长等等）它的输入张量（%actual_input_1, %learned_0, %learned_1） %17 : Float(10, 64, 55, 55) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%actual_input_1, %learned_0, %learned_1), scope: AlexNet/Sequential[features]/Conv2d[0] %18 : Float(10, 64, 55, 55) = onnx::Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1] %19 : Float(10, 64, 27, 27) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2] # ---- 为了简洁可以省略 ---- %29 : Float(10, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12] #动态意味着它的形状是未知的。这可能是因为我们的执行操作或者其形状大小是否确实为动态的而受到了限制。（这一点我们想在将来的版本中修复） %30 : Dynamic = onnx::Shape(%29), scope: AlexNet %31 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%30), scope: AlexNet %32 : Long() = onnx::Squeeze[axes=[0]](%31), scope: AlexNet %33 : Long() = onnx::Constant[value={9216}](), scope: AlexNet # ---- 为了简洁可以省略 ---- %output1 : Float(10, 1000) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%45, %learned_14, %learned_15), scope: AlexNet/Sequential[classifier]/Linear[6] return (%output1); } 你可以使用 onnx 库验证 protobuf, 并且用 conda 安装 onnx conda install -c conda-forge onnx 然后运行: import onnx # 载入onnx模块 model = onnx.load(\"alexnet.onnx\") #检查IR是否良好 onnx.checker.check_model(model) #输出一个图形的可读表示方式 onnx.helper.printable_graph(model.graph) 为了能够使用 caffe2 运行脚本，你需要安装 Caffe2. 如果你之前没有安装,请参照 安装指南。 一旦这些安装完成,你就可以在后台使用 Caffe2 : # ...接着上面的继续 import onnx_caffe2.backend as backend import numpy as np rep = backend.prepare(model, device=\"CUDA:0\") #或者 \"CPU\" #后台运行Caffe2： #rep.predict_net是该网络的Caffe2 protobuf #rep.workspace是该网络的Caffe2 workspace #（详见类“onnx_caffe2.backend.Workspace”） outputs = rep.run(np.random.randn(10, 3, 224, 224).astype(np.float32)) #为了多输入地运行该网络，应该传递元组而不是一个单元格。 print(outputs[0]) 之后,我们还会提供其它框架的后端支持. 局限 ONNX 导出器是一个基于轨迹的导出器，这意味着它执行时需要运行一次模型，然后导出实际参与运算的运算符。这也意味着，如果你的模型是动态的，例如，改变一些依赖于输入数据的操作，这时的导出结果是不准确的。同样，一个轨迹可能只对一个具体的输入尺寸有效 (这就是我们在轨迹中需要有明确的输入的原因之一。) 我们建议检查模型的轨迹，确保被追踪的运算符是合理的。 Pytorch和Caffe2中的一些运算符经常有着数值上的差异.根据模型的结构,这些差异可能是微小的,但它们会在表现上产生很大的差别 (尤其是对于未训练的模型。)之后，为了帮助你在准确度要求很高的情况中，能够轻松地避免这些差异带来的影响，我们计划让Caffe2能够直接调用Torch的运算符. 支持的运算符 以下是已经被支持的运算符: add (不支持非零α) sub (不支持非零α) mul div cat mm addmm neg sqrt tanh sigmoid mean sum prod t expand (只有在扩展onnx操作符之前可以使用，例如add) transpose view split squeeze prelu (不支持输入通道之间的单重共享) threshold (不支持非零值阈值/非零值) leaky_relu glu softmax (只支持dim=-1) avg_pool2d (不支持ceil_mode) log_softmax unfold (为ATen-Caffe2集成作实验支撑) elu concat abs index_select pow clamp max min eq gt lt ge le exp sin cos tan asin acos atan permute Conv BatchNorm MaxPool1d (不支持ceil_mode) MaxPool2d (不支持ceil_mode) MaxPool3d (不支持ceil_mode) Embedding (不支持可选参数) RNN ConstantPadNd Dropout FeatureDropout (不支持训练模式) Index (支持常量整数和元组索引) 上面的运算符足够导出下面的模型: AlexNet DCGAN DenseNet Inception (注意:该模型对操作符十分敏感) ResNet SuperResolution VGG word_language_model 为操作符增加导出支持是一种 提前的用法。为了实现这一点，开发者需要掌握PyTorch的源代码。请按照这个网址链接 去下载PyTorch。如果您想要的运算符已经在ONNX标准化了，那么支持对导出此类运算符的操作（为运算符添加符号函数）就很容易了。为了确认运算符是否已经被标准化，请检查ONNX 操作符列表.如果这个操作符是ATen操作符，这就意味着你可以在 torch/csrc/autograd/generated/VariableType.h找到它的定义。(在PyTorch安装文件列表的合成码中可见)，你应该在 torch/onnx/symbolic.py里面加上符号并且遵循下面的指令： 在 torch/onnx/symbolic.py里面定义符号。确保该功能与在ATen操作符在VariableType.h的功能相同。 第一个参数总是ONNX图形参数，参数的名字必须与 VariableType.h里的匹配，因为调度是依赖于关键字参数完成的。 参数排序不需要严格与VariableType.h匹配，首先的张量一定是输入的张量，然后是非张量参数。 在符号功能里，如果操作符已经在ONNX标准化了，我们只需要创建一个代码去表示在图形里面的ONNX操作符。 如果输入参数是一个张量，但是ONNX需要的是一个标量形式的输入，我们需要做个转化。_scalar可以帮助我们将一个张量转化为一个python标量，并且_if_scalar_type_as函数可以将python标量转化为PyTorch张量。 如果操作符是一个非ATen操作符，那么符号功能需要加在相应的PyTorch函数类中。请阅读下面的指示： 在相应的函数类中创建一个符号函数命名为symbolic。 第一个参数总是导出ONNX图形参数。 参数的名字除了第一个必须与前面的形式严格匹配。 输出元组大小必须与前面的形式严格匹配。 在符号功能中，如果操作符已经在ONNX标准化了，我们只需要创建一个代码去表示在图形里面的ONNX操作符。 符号功能应该在Python里面配置好。所有的这些与Python方法相关的功能都通过C++-Python绑定配置好，且上者提供的界面直观地显示如下： def operator/symbolic(g, *inputs): \"\"\" 修改图像(例如使用 \"op\")，加上代表这个PyTorch功能的ONNX操作符，并且返回一个指定的ONNX输出值或者元组值，这些值与最开始PyTorch返回的自动求导功能相关(或者如果ONNX不支持输出，则返回none。 ). 参数： g (图形)：写入图形的ONNX表示方法。 inputs (值...)：该值的列表表示包含这个功能的输入的可变因素。 \"\"\" class Value(object): \"\"\"代表一个在ONNX里计算的中间张量。\"\"\" def type(self): \"\"\"返回值的类型\"\"\" class Type(object): def sizes(self): \"\"\"返回代表这个张量大小形状的整数元组\"\"\" class Graph(object): def op(self, opname, *inputs, **attrs): \"\"\" 闯将一个ONNX操作符'opname'，将'args'作为输入和属性'kwargs'并且将它作为当前图形的节点，返回代表这个操作符的单一输出值(详见`outputs`多关键参数返回节点)。 操作符的设置和他们输入属性详情请见 https://github.com/onnx/onnx/blob/master/docs/Operators.md 参数： opname (字符串)：ONNX操作符的名字，例如`Abs`或者`Add`。 args (值...)：该操作符的输入经常被作为`symbolic`定义参数输入。 kwargs：该ONNX操作符的属性键名根据以下约定：`alpha_f` 代表着`alpha`具有`f`的属性。有效的类型说明符是 `f`（float），`i`（int），`s`（string）或`t`（Tensor）。使用float类型指定的属性接受单个float或float列表（例如，对于带有整数列表的`dims`属性，你可以称其为'dims_i`）。 outputs (证书，可选)：这个运算符返回的输出参数的数量，默认情况下，假定运算符返回单个输出。 如果`输出`不止一个，这个功能将会返回一个输出值的元组，代表着每个ONNX操作符的输出的位置。 \"\"\" ONNX的图形C++定义详情请见torch/csrc/jit/ir.h。 这是一个处理elu操作符缺少符号函数的例子。我们尝试导出模型并查看错误消息，如下所示： UserWarning: ONNX export failed on elu because torch.onnx.symbolic.elu does not exist RuntimeError: ONNX export failed: Couldn't export operator elu 导出失败，因为PyTorch不支持导出elu操作符。 我们发现virtual Tensor elu（const Tensor＆input，Scalar alpha，bool inplace）const override;```VariableType.h。 这意味着elu是一个ATen操作符。 我们可以参考ONNX操作运算符列表，并且确认 Elu 在ONNX中已经被标准化。我们将以下行添加到symbolic.py： def elu(g, input, alpha, inplace=False): return g.op(\"Elu\", input, alpha_f=_scalar(alpha)) 现在PyTorch能够导出elu操作符： 在下面的链接中有更多的例子： symbolic.py, tensor.py, padding.py. 用于指定运算符定义的接口是实验性的; 喜欢尝试的用户应该注意，API可能会在未来的界面中发生变化。 功能函数 torch.onnx.export(*args, **kwargs) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"torchvision/":{"url":"torchvision/","title":"torchvision","keywords":"","body":"TORCHVISION 该torchvision软件包包括流行的数据集，模型体系结构和用于计算机视觉的常见图像转换。 Package Reference torchvision.datasets MNIST Fashion-MNIST KMNIST EMNIST QMNIST FakeData COCO LSUN ImageFolder DatasetFolder ImageNet CIFAR STL10 SVHN PhotoTour SBU Flickr VOC Cityscapes SBD USPS Kinetics-400 HMDB51 UCF101 torchvision.io Video torchvision.models Classification Semantic Segmentation Object Detection, Instance Segmentation and Person Keypoint Detection Video classification torchvision.ops torchvision.transforms Transforms on PIL Image Transforms on torch.*Tensor Conversion Transforms Generic Transforms Functional Transforms torchvision.utils torchvision.``get_image_backend()[SOURCE] Gets the name of the package used to load images torchvision.``set_image_backend(backend)[SOURCE] Specifies the package used to load images. Parameters backend (string) – Name of the image backend. one of {‘PIL’, ‘accimage’}. The accimage package uses the Intel IPP library. It is generally faster than PIL, but does not support as many operations. 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"}}