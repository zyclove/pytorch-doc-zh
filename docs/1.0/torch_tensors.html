
<!DOCTYPE HTML>
<html lang="zh-hans" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Tensors · Pytorch 中文文档</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        <meta name="author" content="ApacheCN">
        
        
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-insert-logo/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-code/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-pageview-count/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-emphasize/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-alerts/style.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-auto-scroll-table/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-page-toc-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-tbfed-pagefooter/footer.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-advanced-emoji/emoji-website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search-pro/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="torch_random_sampling.html" />
    
    
    <link rel="prev" href="torch.html" />
    

    <style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
    <script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"}]};
    </script>

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="输入并搜索" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" >
            
                <span>
            
                    
                    中文教程
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="tut_getting_started.html">
            
                <a href="tut_getting_started.html">
            
                    
                    起步
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1.1" data-path="deep_learning_60min_blitz.html">
            
                <a href="deep_learning_60min_blitz.html">
            
                    
                    PyTorch 深度学习: 60 分钟极速入门
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1.1.1" data-path="blitz_tensor_tutorial.html">
            
                <a href="blitz_tensor_tutorial.html">
            
                    
                    什么是 PyTorch？
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.1.2" data-path="blitz_autograd_tutorial.html">
            
                <a href="blitz_autograd_tutorial.html">
            
                    
                    Autograd：自动求导
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.1.3" data-path="blitz_neural_networks_tutorial.html">
            
                <a href="blitz_neural_networks_tutorial.html">
            
                    
                    神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.1.4" data-path="blitz_cifar10_tutorial.html">
            
                <a href="blitz_cifar10_tutorial.html">
            
                    
                    训练分类器
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.1.5" data-path="blitz_data_parallel_tutorial.html">
            
                <a href="blitz_data_parallel_tutorial.html">
            
                    
                    可选：数据并行处理
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.1.2" data-path="data_loading_tutorial.html">
            
                <a href="data_loading_tutorial.html">
            
                    
                    数据加载和处理教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.3" data-path="pytorch_with_examples.html">
            
                <a href="pytorch_with_examples.html">
            
                    
                    用例子学习 PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.4" data-path="transfer_learning_tutorial.html">
            
                <a href="transfer_learning_tutorial.html">
            
                    
                    迁移学习教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.5" data-path="deploy_seq2seq_hybrid_frontend_tutorial.html">
            
                <a href="deploy_seq2seq_hybrid_frontend_tutorial.html">
            
                    
                    混合前端的 seq2seq 模型部署
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.6" data-path="saving_loading_models.html">
            
                <a href="saving_loading_models.html">
            
                    
                    Saving and Loading Models
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.7" data-path="nn_tutorial.html">
            
                <a href="nn_tutorial.html">
            
                    
                    What is torch.nn really?
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="tut_image.html">
            
                <a href="tut_image.html">
            
                    
                    图像
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.2.1" data-path="finetuning_torchvision_models_tutorial.html">
            
                <a href="finetuning_torchvision_models_tutorial.html">
            
                    
                    Torchvision 模型微调
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.2" data-path="spatial_transformer_tutorial.html">
            
                <a href="spatial_transformer_tutorial.html">
            
                    
                    空间变换器网络教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.3" data-path="neural_style_tutorial.html">
            
                <a href="neural_style_tutorial.html">
            
                    
                    使用 PyTorch 进行图像风格转换
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.4" data-path="fgsm_tutorial.html">
            
                <a href="fgsm_tutorial.html">
            
                    
                    对抗性示例生成
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.5" data-path="super_resolution_with_caffe2.html">
            
                <a href="super_resolution_with_caffe2.html">
            
                    
                    使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="tut_text.html">
            
                <a href="tut_text.html">
            
                    
                    文本
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.3.1" data-path="chatbot_tutorial.html">
            
                <a href="chatbot_tutorial.html">
            
                    
                    聊天机器人教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.2" data-path="char_rnn_generation_tutorial.html">
            
                <a href="char_rnn_generation_tutorial.html">
            
                    
                    使用字符级别特征的 RNN 网络生成姓氏
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.3" data-path="char_rnn_classification_tutorial.html">
            
                <a href="char_rnn_classification_tutorial.html">
            
                    
                    使用字符级别特征的 RNN 网络进行姓氏分类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4" data-path="deep_learning_nlp_tutorial.html">
            
                <a href="deep_learning_nlp_tutorial.html">
            
                    
                    Deep Learning for NLP with Pytorch
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.3.4.1" data-path="nlp_pytorch_tutorial.html">
            
                <a href="nlp_pytorch_tutorial.html">
            
                    
                    PyTorch 介绍
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4.2" data-path="nlp_deep_learning_tutorial.html">
            
                <a href="nlp_deep_learning_tutorial.html">
            
                    
                    使用 PyTorch 进行深度学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4.3" data-path="nlp_word_embeddings_tutorial.html">
            
                <a href="nlp_word_embeddings_tutorial.html">
            
                    
                    Word Embeddings: Encoding Lexical Semantics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4.4" data-path="nlp_sequence_models_tutorial.html">
            
                <a href="nlp_sequence_models_tutorial.html">
            
                    
                    序列模型和 LSTM 网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4.5" data-path="nlp_advanced_tutorial.html">
            
                <a href="nlp_advanced_tutorial.html">
            
                    
                    Advanced: Making Dynamic Decisions and the Bi-LSTM CRF
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.3.5" data-path="seq2seq_translation_tutorial.html">
            
                <a href="seq2seq_translation_tutorial.html">
            
                    
                    基于注意力机制的 seq2seq 神经网络翻译
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="tut_generative.html">
            
                <a href="tut_generative.html">
            
                    
                    生成
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.4.1" data-path="dcgan_faces_tutorial.html">
            
                <a href="dcgan_faces_tutorial.html">
            
                    
                    DCGAN Tutorial
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="tut_reinforcement_learning.html">
            
                <a href="tut_reinforcement_learning.html">
            
                    
                    强化学习
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.5.1" data-path="reinforcement_q_learning.html">
            
                <a href="reinforcement_q_learning.html">
            
                    
                    Reinforcement Learning (DQN) Tutorial
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.6" data-path="tut_extending_pytorch.html">
            
                <a href="tut_extending_pytorch.html">
            
                    
                    扩展 PyTorch
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.6.1" data-path="numpy_extensions_tutorial.html">
            
                <a href="numpy_extensions_tutorial.html">
            
                    
                    用 numpy 和 scipy 创建扩展
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.2" data-path="cpp_extension.html">
            
                <a href="cpp_extension.html">
            
                    
                    Custom C++   and CUDA Extensions
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.3" data-path="torch_script_custom_ops.html">
            
                <a href="torch_script_custom_ops.html">
            
                    
                    Extending TorchScript with Custom C++   Operators
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.7" data-path="tut_production_usage.html">
            
                <a href="tut_production_usage.html">
            
                    
                    生产性使用
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.7.1" data-path="dist_tuto.html">
            
                <a href="dist_tuto.html">
            
                    
                    Writing Distributed Applications with PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.2" data-path="aws_distributed_training_tutorial.html">
            
                <a href="aws_distributed_training_tutorial.html">
            
                    
                    使用 Amazon AWS 进行分布式训练
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.3" data-path="ONNXLive.html">
            
                <a href="ONNXLive.html">
            
                    
                    ONNX 现场演示教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.4" data-path="cpp_export.html">
            
                <a href="cpp_export.html">
            
                    
                    在 C++ 中加载 PYTORCH 模型
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.8" data-path="tut_other_language.html">
            
                <a href="tut_other_language.html">
            
                    
                    其它语言中的 PyTorch
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.8.1" data-path="cpp_frontend.html">
            
                <a href="cpp_frontend.html">
            
                    
                    使用 PyTorch C++ 前端
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" >
            
                <span>
            
                    
                    中文文档
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="docs_notes.html">
            
                <a href="docs_notes.html">
            
                    
                    注解
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1.1" data-path="notes_autograd.html">
            
                <a href="notes_autograd.html">
            
                    
                    自动求导机制
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.2" data-path="notes_broadcasting.html">
            
                <a href="notes_broadcasting.html">
            
                    
                    广播语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.3" data-path="notes_cuda.html">
            
                <a href="notes_cuda.html">
            
                    
                    CUDA 语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.4" data-path="notes_extending.html">
            
                <a href="notes_extending.html">
            
                    
                    Extending PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.5" data-path="notes_faq.html">
            
                <a href="notes_faq.html">
            
                    
                    Frequently Asked Questions
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.6" data-path="notes_multiprocessing.html">
            
                <a href="notes_multiprocessing.html">
            
                    
                    Multiprocessing best practices
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.7" data-path="notes_randomness.html">
            
                <a href="notes_randomness.html">
            
                    
                    Reproducibility
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.8" data-path="notes_serialization.html">
            
                <a href="notes_serialization.html">
            
                    
                    Serialization semantics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.9" data-path="notes_windows.html">
            
                <a href="notes_windows.html">
            
                    
                    Windows FAQ
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="docs_package_ref.html">
            
                <a href="docs_package_ref.html">
            
                    
                    包参考
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.2.1" data-path="torch.html">
            
                <a href="torch.html">
            
                    
                    torch
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter active" data-level="1.3.2.1.1" data-path="torch_tensors.html">
            
                <a href="torch_tensors.html">
            
                    
                    Tensors
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.1.2" data-path="torch_random_sampling.html">
            
                <a href="torch_random_sampling.html">
            
                    
                    Random sampling
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.1.3" data-path="torch_serialization_parallelism_utilities.html">
            
                <a href="torch_serialization_parallelism_utilities.html">
            
                    
                    Serialization, Parallelism, Utilities
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.1.4" data-path="torch_math_operations.html">
            
                <a href="torch_math_operations.html">
            
                    
                    Math operations
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.2.1.4.1" data-path="torch_math_operations_pointwise_ops.html">
            
                <a href="torch_math_operations_pointwise_ops.html">
            
                    
                    Pointwise Ops
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.1.4.2" data-path="torch_math_operations_reduction_ops.html">
            
                <a href="torch_math_operations_reduction_ops.html">
            
                    
                    Reduction Ops
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.1.4.3" data-path="torch_math_operations_comparison_ops.html">
            
                <a href="torch_math_operations_comparison_ops.html">
            
                    
                    Comparison Ops
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.1.4.4" data-path="torch_math_operations_spectral_ops.html">
            
                <a href="torch_math_operations_spectral_ops.html">
            
                    
                    Spectral Ops
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.1.4.5" data-path="torch_math_operations_other_ops.html">
            
                <a href="torch_math_operations_other_ops.html">
            
                    
                    Other Operations
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.1.4.6" data-path="torch_math_operations_blas_lapack_ops.html">
            
                <a href="torch_math_operations_blas_lapack_ops.html">
            
                    
                    BLAS and LAPACK Operations
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.2.2" data-path="tensors.html">
            
                <a href="tensors.html">
            
                    
                    torch.Tensor
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.3" data-path="tensor_attributes.html">
            
                <a href="tensor_attributes.html">
            
                    
                    Tensor Attributes
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.4" data-path="type_info.html">
            
                <a href="type_info.html">
            
                    
                    数据类型信息
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.5" data-path="sparse.html">
            
                <a href="sparse.html">
            
                    
                    torch.sparse
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.6" data-path="cuda.html">
            
                <a href="cuda.html">
            
                    
                    torch.cuda
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.7" data-path="storage.html">
            
                <a href="storage.html">
            
                    
                    torch.Storage
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.8" data-path="nn.html">
            
                <a href="nn.html">
            
                    
                    torch.nn
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.9" data-path="nn_functional.html">
            
                <a href="nn_functional.html">
            
                    
                    torch.nn.functional
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.10" data-path="nn_init.html">
            
                <a href="nn_init.html">
            
                    
                    torch.nn.init
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.11" data-path="optim.html">
            
                <a href="optim.html">
            
                    
                    torch.optim
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.12" data-path="autograd.html">
            
                <a href="autograd.html">
            
                    
                    Automatic differentiation package - torch.autograd
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.13" data-path="distributed.html">
            
                <a href="distributed.html">
            
                    
                    Distributed communication package - torch.distributed
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.14" data-path="distributions.html">
            
                <a href="distributions.html">
            
                    
                    Probability distributions - torch.distributions
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.15" data-path="jit.html">
            
                <a href="jit.html">
            
                    
                    Torch Script
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.16" data-path="multiprocessing.html">
            
                <a href="multiprocessing.html">
            
                    
                    多进程包 - torch.multiprocessing
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.17" data-path="bottleneck.html">
            
                <a href="bottleneck.html">
            
                    
                    torch.utils.bottleneck
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.18" data-path="checkpoint.html">
            
                <a href="checkpoint.html">
            
                    
                    torch.utils.checkpoint
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.19" data-path="docs_cpp_extension.html">
            
                <a href="docs_cpp_extension.html">
            
                    
                    torch.utils.cpp_extension
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.20" data-path="data.html">
            
                <a href="data.html">
            
                    
                    torch.utils.data
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.21" data-path="dlpack.html">
            
                <a href="dlpack.html">
            
                    
                    torch.utils.dlpack
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.22" data-path="hub.html">
            
                <a href="hub.html">
            
                    
                    torch.hub
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.23" data-path="model_zoo.html">
            
                <a href="model_zoo.html">
            
                    
                    torch.utils.model_zoo
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.24" data-path="onnx.html">
            
                <a href="onnx.html">
            
                    
                    torch.onnx
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.25" data-path="distributed_deprecated.html">
            
                <a href="distributed_deprecated.html">
            
                    
                    Distributed communication package (deprecated) - torch.distributed.deprecated
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="docs_torchvision_ref.html">
            
                <a href="docs_torchvision_ref.html">
            
                    
                    torchvision 参考
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.3.1" data-path="torchvision_datasets.html">
            
                <a href="torchvision_datasets.html">
            
                    
                    torchvision.datasets
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.2" data-path="torchvision_models.html">
            
                <a href="torchvision_models.html">
            
                    
                    torchvision.models
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.3" data-path="torchvision_transforms.html">
            
                <a href="torchvision_transforms.html">
            
                    
                    torchvision.transforms
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.4" data-path="torchvision_utils.html">
            
                <a href="torchvision_utils.html">
            
                    
                    torchvision.utils
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本书使用 GitBook 发布
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >Tensors</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h2 id="tensors">Tensors</h2>
<blockquote>
<p>&#x8BD1;&#x8005;&#xFF1A;<a href="https://github.com/dyywinner" target="_blank">dyywinner</a>
       <a href="https://github.com/infdahai" target="_blank">cluster</a></p>
</blockquote>
<pre><code class="lang-py">torch.is_tensor(obj)
</code></pre>
<p>&#x5982;&#x679C;<code>obj</code>&#x662F;&#x4E00;&#x4E2A;PyTorch&#x5F20;&#x91CF;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;True.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>obj</strong> (<em>Object</em>) &#x2013; Object to test</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">torch.is_storage(obj)
</code></pre>
<p>&#x5982;&#x679C;<code>obj</code>&#x662F;&#x4E00;&#x4E2A;PyTorch &#x5B58;&#x50A8;&#x5BF9;&#x8C61;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;True.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>obj</strong> (<em>Object</em>) &#x2013; Object to test</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">torch.set_default_dtype(d)
</code></pre>
<p>&#x5C06;<code>d</code>&#x8BBE;&#x7F6E;&#x4E3A;&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;&#x7C7B;&#x578B;(dtype). &#x8BE5;&#x7C7B;&#x578B;&#x5C06;&#x5728; <a href="#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> &#x4E2D;&#x4F5C;&#x4E3A;&#x7C7B;&#x578B;&#x63A8;&#x65AD;&#x7684;&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;&#x7C7B;&#x578B;&#x3002;&#x521D;&#x59CB;&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;&#x7C7B;&#x578B;&#x4E3A;<code>torch.float32</code>&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>d</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>) &#x2013; &#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;&#x7C7B;&#x578B;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.tensor([<span class="hljs-number">1.2</span>, <span class="hljs-number">3</span>]).dtype           <span class="hljs-comment"># &#x521D;&#x59CB;&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;&#x7C7B;&#x578B;&#x4E3A; torch.float32</span>
floating point <span class="hljs-keyword">is</span> torch.float32
torch.float32
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.set_default_dtype(torch.float64)
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.tensor([<span class="hljs-number">1.2</span>, <span class="hljs-number">3</span>]).dtype           <span class="hljs-comment"># &#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x6D6E;&#x70B9;&#x7C7B;&#x578B;&#x7684;&#x5F20;&#x91CF;</span>
torch.float64
</code></pre>
<pre><code class="lang-py">torch.get_default_dtype() &#x2192; torch.dtype
</code></pre>
<p>&#x83B7;&#x53D6;&#x5F53;&#x524D;&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;&#x7C7B;&#x578B; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>.</p>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.get_default_dtype()  <span class="hljs-comment"># &#x521D;&#x59CB;&#x7684;&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;&#x7C7B;&#x578B;&#x662F; torch.float32</span>
torch.float32
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.set_default_dtype(torch.float64)
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.get_default_dtype()  <span class="hljs-comment"># &#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;&#x7C7B;&#x578B;&#x4E3A; torch.float64</span>
torch.float64
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.set_default_tensor_type(torch.FloatTensor)  <span class="hljs-comment"># &#x8BBE;&#x7F6E;&#x9ED8;&#x8BA4;&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x4E5F;&#x4F1A;&#x5F71;&#x54CD;&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;&#x7C7B;&#x578B;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.get_default_dtype()  <span class="hljs-comment"># &#x53D8;&#x5316;&#x5230; torch.float32( &#x6B64;&#x7C7B;&#x578B;(dtype)&#x6765;&#x81EA;&#x4E8E;torch.FloatTensor )</span>
torch.float32
</code></pre>
<pre><code class="lang-py">torch.set_default_tensor_type(t)
</code></pre>
<p>&#x8BBE;&#x7F6E;&#x9ED8;&#x8BA4;&#x7684; <code>torch.Tensor</code> &#x7C7B;&#x578B;&#x5230;&#x6D6E;&#x70B9;&#x5F20;&#x91CF;&#x7C7B;&#x578B; <a href="#torch.t" title="torch.t"><code>t</code></a>. &#x8BE5;&#x7C7B;&#x578B;&#x5C06;&#x5728; <a href="#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> &#x4E2D;&#x4F5C;&#x4E3A;&#x7C7B;&#x578B;&#x63A8;&#x65AD;&#x7684;&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;&#x7C7B;&#x578B;&#x3002;
&#x521D;&#x59CB;&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x4E3A; <code>torch.FloatTensor</code>.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>t</strong> (<a href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.7)" target="_blank"><em>type</em></a> <em>or</em> <em>string</em>) &#x2013; &#x6D6E;&#x70B9;&#x5F20;&#x91CF;&#x7684;&#x7C7B;&#x578B;&#x6216;&#x8005;&#x5B83;&#x7684;&#x540D;&#x79F0;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.tensor([<span class="hljs-number">1.2</span>, <span class="hljs-number">3</span>]).dtype    <span class="hljs-comment"># &#x521D;&#x59CB;&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;&#x7C7B;&#x578B;&#x4E3A; torch.float32</span>
torch.float32
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.set_default_tensor_type(torch.DoubleTensor)
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.tensor([<span class="hljs-number">1.2</span>, <span class="hljs-number">3</span>]).dtype    <span class="hljs-comment"># &#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x6D6E;&#x70B9;&#x5F20;&#x91CF;</span>
torch.float64
</code></pre>
<pre><code class="lang-py">torch.numel(input) &#x2192; int
</code></pre>
<p>&#x8FD4;&#x56DE; <code>input</code> &#x5F20;&#x91CF;&#x4E2D;&#x5143;&#x7D20;&#x603B;&#x6570;.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.numel(a)
<span class="hljs-number">120</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.zeros(<span class="hljs-number">4</span>,<span class="hljs-number">4</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.numel(a)
<span class="hljs-number">16</span>
</code></pre>
<pre><code class="lang-py">torch.set_printoptions(precision=<span class="hljs-keyword">None</span>, threshold=<span class="hljs-keyword">None</span>, edgeitems=<span class="hljs-keyword">None</span>, linewidth=<span class="hljs-keyword">None</span>, profile=<span class="hljs-keyword">None</span>)
</code></pre>
<p>&#x8BBE;&#x7F6E;&#x6253;&#x5370;&#x9009;&#x9879;. &#x4ECE; NumPy &#x527D;&#x7A83;&#x8FC7;&#x6765;&#x7684;&#xFF08;&#x6ED1;&#x7A3D;&#xFF09;</p>
<p>Parameters: </p>
<ul>
<li><strong>precision</strong> &#x2013; &#x6D6E;&#x70B9;&#x8F93;&#x51FA;&#x7684;&#x6709;&#x6548;&#x4F4D;&#x6570; (&#x9ED8;&#x8BA4;&#x4E3A; 4).</li>
<li><strong>threshold</strong> &#x2013; &#x8F93;&#x51FA;&#x65F6;&#x7684;&#x9608;&#x503C;&#xFF0C;&#x5F53;&#x6570;&#x7EC4;&#x5143;&#x7D20;&#x603B;&#x548C;&#x8D85;&#x8FC7;&#x9608;&#x503C;&#xFF0C;&#x4F1A;&#x88AB;&#x622A;&#x65AD;&#x8F93;&#x51FA; (&#x9ED8;&#x8BA4;&#x4E3A; 1000).</li>
<li><strong>edgeitems</strong> &#x2013; &#x6BCF;&#x4E2A;&#x7EF4;&#x5EA6;&#x6240;&#x7EDF;&#x8BA1;&#x7684;&#x6570;&#x7EC4;&#x6761;&#x76EE;&#x6570;&#x91CF;(&#x9ED8;&#x8BA4;&#xFF1A;3).</li>
<li><strong>linewidth</strong> &#x2013; &#x6BCF;&#x4E00;&#x884C;&#x8F93;&#x51FA;&#x7684;&#x5B57;&#x7B26;&#x957F;&#x5EA6; (&#x9ED8;&#x8BA4;&#x4E3A;80). &#x9608;&#x503C;&#x77E9;&#x9635;&#x5C06;&#x5FFD;&#x7565;&#x8BE5;&#x53C2;&#x6570;.</li>
<li><strong>profile</strong> &#x2013; &#x6253;&#x5370;&#x8F93;&#x51FA;&#x7684;&#x7F8E;&#x89C2;&#x7A0B;&#x5EA6; &#x9ED8;&#x8BA4;&#x503C;&#x4E3A;Sane. &#x53EF;&#x4EE5;&#x7528;&#x540E;&#x9762;&#x62EC;&#x53F7;&#x4E2D;&#x7684;&#x9009;&#x9879;&#x8986;&#x76D6;( <code>default</code>, <code>short</code>, <code>full</code>).</li>
</ul>
<pre><code class="lang-py">torch.set_flush_denormal(mode) &#x2192; bool
</code></pre>
<p>CPU&#x4E0D;&#x652F;&#x6301;&#x975E;&#x89C4;&#x683C;&#x5316;&#x6D6E;&#x70B9;&#x6570; .</p>
<p>&#x5982;&#x679C;&#x4F60;&#x7684;&#x7CFB;&#x7EDF;&#x652F;&#x6301;&#x975E;&#x89C4;&#x683C;&#x5316;&#x6570;&#x5B57;&#x6A21;&#x5F0F;(flush denormal mode)&#x5E76;&#x4E14;&#x53EF;&#x6210;&#x529F;&#x914D;&#x7F6E;&#x8BE5;&#x6A21;&#x5F0F;&#x5219;&#x8FD4;&#x56DE; <code>True</code> . <a href="#torch.set_flush_denormal" title="torch.set_flush_denormal"><code>set_flush_denormal()</code></a> &#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#x5728;&#x652F;&#x6301;SSE3&#x7684;x86&#x67B6;&#x6784;.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>mode</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a>) &#x2013; &#x662F;&#x5426;&#x5F00;&#x542F;  flush denormal mode</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.set_flush_denormal(<span class="hljs-keyword">True</span>)
<span class="hljs-keyword">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.tensor([<span class="hljs-number">1e-323</span>], dtype=torch.float64)
tensor([ <span class="hljs-number">0.</span>], dtype=torch.float64)
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.set_flush_denormal(<span class="hljs-keyword">False</span>)
<span class="hljs-keyword">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.tensor([<span class="hljs-number">1e-323</span>], dtype=torch.float64)
tensor(<span class="hljs-number">9.88131e-324</span> *
 [ <span class="hljs-number">1.0000</span>], dtype=torch.float64)
</code></pre>
<h3 id="creation-ops">Creation Ops</h3>
<p>Note</p>
<p>&#x968F;&#x673A;&#x91C7;&#x6837;&#x521B;&#x9020;&#x968F;&#x673A;&#x6570;&#x7684;&#x65B9;&#x5F0F;&#x5728; <a href="#random-sampling">Random sampling</a> &#x5217;&#x4E3E;&#x3002;&#x5176;&#x4E2D;&#x5305;&#x62EC; <a href="#torch.rand" title="torch.rand"><code>torch.rand()</code></a> <a href="#torch.rand_like" title="torch.rand_like"><code>torch.rand_like()</code></a> <a href="#torch.randn" title="torch.randn"><code>torch.randn()</code></a> <a href="#torch.randn_like" title="torch.randn_like"><code>torch.randn_like()</code></a> <a href="#torch.randint" title="torch.randint"><code>torch.randint()</code></a> <a href="#torch.randint_like" title="torch.randint_like"><code>torch.randint_like()</code></a> <a href="#torch.randperm" title="torch.randperm"><code>torch.randperm()</code></a> . &#x4F60;&#x53EF;&#x4EE5;&#x4F7F;&#x7528; <a href="#torch.empty" title="torch.empty"><code>torch.empty()</code></a> &#xFF0C;&#x5E76;&#x4F7F;&#x7528; <a href="#inplace-random-sampling">In-place random sampling</a> &#x65B9;&#x6CD5;&#x53BB;&#x4ECE;&#x66F4;&#x5BBD;&#x6CDB;&#x7684;&#x8303;&#x56F4;&#x91C7;&#x6837;,&#x751F;&#x6210; <a href="tensors.html#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> .</p>
<pre><code class="lang-py">torch.tensor(data, dtype=<span class="hljs-keyword">None</span>, device=<span class="hljs-keyword">None</span>, requires_grad=<span class="hljs-keyword">False</span>) &#x2192; Tensor
</code></pre>
<p>&#x7528; <code>data</code> &#x6784;&#x5EFA;&#x5F20;&#x91CF;.</p>
<p>Warning</p>
<p><a href="#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> &#x4F1A;&#x62F7;&#x8D1D; <code>data</code>. &#x5982;&#x679C;&#x4F60;&#x6709;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;( <code>data</code> )&#xFF0C;&#x5E76;&#x4E14;&#x60F3;&#x8981;&#x907F;&#x514D;&#x62F7;&#x8D1D;, &#x8BF7;&#x4F7F;&#x7528; <a href="tensors.html#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code>torch.Tensor.requires_grad_()</code></a> &#x6216;&#x8005; <a href="autograd.html#torch.Tensor.detach" title="torch.Tensor.detach"><code>torch.Tensor.detach()</code></a>. &#x5982;&#x679C;&#x4F60;&#x6709;&#x4E00;&#x4E2A;NumPy&#x6570;&#x7EC4;(<code>ndarray</code>) &#x5E76;&#x4E14;&#x60F3;&#x8981;&#x907F;&#x514D;&#x62F7;&#x8D1D;, &#x8BF7;&#x4F7F;&#x7528; <a href="#torch.from_numpy" title="torch.from_numpy"><code>torch.from_numpy()</code></a>.</p>
<p>Warning</p>
<p>&#x5F53; data &#x4E3A;&#x4E00;&#x4E2A;&#x540D;&#x4E3A; <code>x</code> &#x7684;&#x5F20;&#x91CF;&#xFF0C; <a href="#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> &#x8BFB;&#x53D6; &#x2018;the data&#x2019; (&#x65E0;&#x8BBA;&#x4F20;&#x8F93;&#x4E86;&#x4EC0;&#x4E48;), &#x90FD;&#x4F1A;&#x6784;&#x5EFA;&#x4E00;&#x4E2A; leaf variable(&#x8BA1;&#x7B97;&#x56FE;&#x6A21;&#x578B;&#x4E2D;&#x4E8B;&#x5148;&#x521B;&#x5EFA;&#x7684;&#x3001;&#x800C;&#x975E;&#x8FD0;&#x7B97;&#x5F97;&#x5230;&#x7684;&#x53D8;&#x91CF;). &#x56E0;&#x6B64; <code>torch.tensor(x)</code> &#x7B49;&#x4EF7;&#x4E8E; <code>x.clone().detach()</code> &#xFF0C;&#x540C;&#x65F6; <code>torch.tensor(x, requires_grad=True)</code> &#x7B49;&#x4EF7;&#x4E8E; <code>x.clone().detach().requires_grad_(True)</code>. &#x6211;&#x4EEC;&#x63A8;&#x8350;&#x8FD9;&#x79CD;&#x4F7F;&#x7528; <code>clone()</code> and <code>detach()</code> &#x7684;&#x5199;&#x6CD5;.</p>
<p>Parameters: </p>
<ul>
<li><strong>data</strong> (<em>array_like</em>) &#x2013; &#x521D;&#x59CB;&#x5316;&#x5F20;&#x91CF;&#x7684;&#x6570;&#x636E;. &#x5141;&#x8BB8;&#x7684;&#x7C7B;&#x578B;&#x6709; list, tuple, NumPy <code>ndarray</code>, scalar(&#x6807;&#x91CF;), &#x4EE5;&#x53CA;&#x5176;&#x4ED6;&#x7C7B;&#x578B;.</li>
<li><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x6240;&#x8981;&#x6C42;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;. &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x6B64;&#x53C2;&#x6570;&#x4E3A; <code>None</code>,&#x4ECE; <code>data</code>&#x4E2D;&#x63A8;&#x65AD;&#x6570;&#x636E;&#x7C7B;&#x578B;.</li>
<li><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x6240;&#x8981;&#x6C42;&#x7684;&#x786C;&#x4EF6;. &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x6B64;&#x53C2;&#x6570;&#x4E3A; <code>None</code>,&#x5BF9;&#x5F53;&#x524D;&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x4F7F;&#x7528;&#x5F53;&#x524D;&#x786C;&#x4EF6;(&#x53C2;&#x8003; <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> &#x53EF;&#x4EE5;&#x4E3A; &#x63D0;&#x4F9B;CPU&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x7684;CPU&#x548C; &#x652F;&#x6301;CUDA&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x7684;CUDA&#x8BBE;&#x5907;.</li>
<li><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5BF9;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x81EA;&#x52A8;&#x6C42;&#x5BFC;&#x65F6;&#x662F;&#x5426;&#x9700;&#x8981;&#x8BB0;&#x5F55;&#x64CD;&#x4F5C;. &#x9ED8;&#x8BA4;: <code>False</code>.</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.tensor([[<span class="hljs-number">0.1</span>, <span class="hljs-number">1.2</span>], [<span class="hljs-number">2.2</span>, <span class="hljs-number">3.1</span>], [<span class="hljs-number">4.9</span>, <span class="hljs-number">5.2</span>]])
tensor([[ <span class="hljs-number">0.1000</span>,  <span class="hljs-number">1.2000</span>],
       [ <span class="hljs-number">2.2000</span>,  <span class="hljs-number">3.1000</span>],
       [ <span class="hljs-number">4.9000</span>,  <span class="hljs-number">5.2000</span>]])

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.tensor([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])  <span class="hljs-comment"># &#x8F93;&#x5165;&#x6570;&#x636E;&#x63A8;&#x65AD;</span>
tensor([ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.tensor([[<span class="hljs-number">0.11111</span>, <span class="hljs-number">0.222222</span>, <span class="hljs-number">0.3333333</span>]],
                 dtype=torch.float64,
                 device=torch.device(<span class="hljs-string">&apos;cuda:0&apos;</span>))  <span class="hljs-comment"># &#x521B;&#x5EFA;&#x4E00;&#x4E2A; torch.cuda.DoubleTensor</span>
tensor([[ <span class="hljs-number">0.1111</span>,  <span class="hljs-number">0.2222</span>,  <span class="hljs-number">0.3333</span>]], dtype=torch.float64, 
device=<span class="hljs-string">&apos;cuda:0&apos;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.tensor(<span class="hljs-number">3.14159</span>)  <span class="hljs-comment"># &#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x6807;&#x91CF; (&#x96F6;&#x7EF4;&#x5F20;&#x91CF;)</span>
tensor(<span class="hljs-number">3.1416</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.tensor([])  <span class="hljs-comment"># &#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x7A7A;&#x5F20;&#x91CF; (&#x5F62;&#x72B6;&#x662F; (0,))</span>
tensor([])
</code></pre>
<pre><code class="lang-py">torch.sparse_coo_tensor(indices, values, size=<span class="hljs-keyword">None</span>, dtype=<span class="hljs-keyword">None</span>, device=<span class="hljs-keyword">None</span>, requires_grad=<span class="hljs-keyword">False</span>) &#x2192; Tensor
</code></pre>
<p>&#x7528;&#x975E;0&#x5143;&#x7D20;&#x503C;<code>values</code>&#x548C;&#x4E0B;&#x6807;<code>indices</code>&#x5728;COO(&#x987A;&#x5E8F;&#x6807;&#x6CE8;)&#x6784;&#x5EFA;&#x4E00;&#x4E2A;&#x7A00;&#x758F;&#x77E9;&#x9635;&#x3002;&#x4E00;&#x4E2A;&#x7A00;&#x758F;&#x5411;&#x91CF;&#x53EF;&#x4EE5;&#x662F;<code>&#x672A;&#x5408;&#x5E76;&#x7684;</code>(<code>uncoalesced</code>), &#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;,&#x5728;&#x7D22;&#x5F15;&#x4E2D;&#x4F1A;&#x5B58;&#x5728;&#x6709;&#x91CD;&#x590D;&#x5750;&#x6807; ,&#x8FD9;&#x4E2A;&#x7D22;&#x5F15;&#x7684;&#x503C;&#x662F;&#x6240;&#x6709;&#x91CD;&#x590D;&#x503C;&#x6570;&#x91CF;&#x7684;&#x548C;: <a href="https://pytorch.org/docs/stable/sparse.html" target="_blank">torch.sparse</a>.</p>
<p>Parameters: </p>
<ul>
<li><strong>indices</strong> (<em>array_like</em>) &#x2013; &#x7ED9;&#x5F20;&#x91CF;&#x521D;&#x59CB;&#x5316;&#x6570;&#x636E;&#x3002;&#x53EF;&#x4EE5;&#x662F;&#x5217;&#x8868;,&#x5143;&#x7EC4;,Numpy&#x77E9;&#x9635;(<code>ndarry</code>&#x7C7B;&#x578B;),&#x6807;&#x91CF;&#x548C;&#x5176;&#x4ED6;&#x7C7B;&#x578B;&#x3002;&#x4E4B;&#x540E;&#x5C06;&#x5728;&#x5185;&#x90E8;&#x88AB;&#x6620;&#x5C04;&#x6210;<code>torch.LongTensor</code>&#x3002; &#x56E0;&#x6B64;&#x77E9;&#x9635;&#x4E2D;&#x7684;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x4E0B;&#x6807;&#x7684;&#x5750;&#x6807;,&#x9700;&#x8981;&#x662F;&#x4E8C;&#x7EF4;&#x7684;&#xFF0C;&#x5E76;&#x4E14;&#x7B2C;&#x4E00;&#x7EF4;&#x662F;&#x5F20;&#x91CF;&#x7684;&#x7EF4;&#x5EA6;&#xFF0C;&#x7B2C;&#x4E8C;&#x7EF4;&#x662F;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x6570;&#x91CF;&#x3002;</li>
<li><strong>values</strong> (<em>array_like</em>) &#x2013; &#x521D;&#x59CB;&#x5316;&#x5F20;&#x91CF;&#x7684;&#x503C;&#x3002;&#x53EF;&#x4EE5;&#x662F;&#x5217;&#x8868;,&#x5143;&#x7EC4;,Numpy&#x77E9;&#x9635;(<code>ndarry</code>&#x7C7B;&#x578B;),&#x6807;&#x91CF;&#x548C;&#x5176;&#x4ED6;&#x7C7B;&#x578B;&#x3002;</li>
<li><strong>size</strong> (list, tuple, or <code>torch.Size</code>, optional) &#x2013; &#x7A00;&#x758F;&#x77E9;&#x9635;&#x7684;&#x5F62;&#x72B6;&#x3002;&#x5982;&#x679C;&#x4E0D;&#x63D0;&#x4F9B;size&#x5F62;&#x72B6;&#xFF0C;&#x5C06;&#x4F1A;&#x88AB;&#x81EA;&#x52A8;&#x4F18;&#x5316;&#x4E3A;&#x53EF;&#x4EE5;&#x88C5;&#x4E0B;&#x6240;&#x6709;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x7684;&#x6700;&#x5C0F;&#x5927;&#x5C0F;&#x3002;</li>
<li><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; &#x5F20;&#x91CF;&#x8FD4;&#x56DE;&#x503C;&#x7684;&#x671F;&#x671B;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;&#x5982;&#x679C;&#x6CA1;&#x6709;&#xFF0C;&#x5219;&#x9ED8;&#x8BA4;&#x4E3A;<code>values</code>&#x3002;</li>
<li><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x6240;&#x8981;&#x6C42;&#x7684;&#x786C;&#x4EF6;. &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x6B64;&#x53C2;&#x6570;&#x4E3A; <code>None</code>,&#x5BF9;&#x5F53;&#x524D;&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x4F7F;&#x7528;&#x5F53;&#x524D;&#x786C;&#x4EF6;(&#x53C2;&#x8003; <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> &#x53EF;&#x4EE5;&#x4E3A; &#x63D0;&#x4F9B;CPU&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x7684;CPU&#x548C; &#x652F;&#x6301;CUDA&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x7684;CUDA&#x8BBE;&#x5907;.torch.set_default_tensor_type &quot;torch.set_default_tensor_type&quot;)). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</li>
<li><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5BF9;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x81EA;&#x52A8;&#x6C42;&#x5BFC;&#x65F6;&#x662F;&#x5426;&#x9700;&#x8981;&#x8BB0;&#x5F55;&#x64CD;&#x4F5C;. &#x9ED8;&#x8BA4;: <code>False</code>.</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>i = torch.tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                      [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>v = torch.tensor([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>], dtype=torch.float32)
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.sparse_coo_tensor(i, v, [<span class="hljs-number">2</span>, <span class="hljs-number">4</span>])
tensor(indices=tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                       [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>]]),
       values=tensor([<span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>]),
       size=(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>), nnz=<span class="hljs-number">3</span>, layout=torch.sparse_coo)

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.sparse_coo_tensor(i, v)  <span class="hljs-comment"># Shape inference</span>
tensor(indices=tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                       [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>]]),
       values=tensor([<span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>]),
       size=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), nnz=<span class="hljs-number">3</span>, layout=torch.sparse_coo)

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.sparse_coo_tensor(i, v, [<span class="hljs-number">2</span>, <span class="hljs-number">4</span>],
                            dtype=torch.float64,
                            device=torch.device(<span class="hljs-string">&apos;cuda:0&apos;</span>))
tensor(indices=tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                       [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>]]),
       values=tensor([<span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>]),
       device=<span class="hljs-string">&apos;cuda:0&apos;</span>, size=(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>), nnz=<span class="hljs-number">3</span>, dtype=torch.float64,
       layout=torch.sparse_coo)

<span class="hljs-comment"># &#x4F7F;&#x7528;&#x4E0B;&#x5217;&#x5E38;&#x91CF;&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x7A7A;&#x7A00;&#x758F;&#x5F20;&#x91CF;:</span>
<span class="hljs-comment">#   1\. sparse_dim + dense_dim = len(SparseTensor.shape)</span>
<span class="hljs-comment">#   2\. SparseTensor._indices().shape = (sparse_dim, nnz)</span>
<span class="hljs-comment">#   3\. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># &#x6BD4;&#x5982;&#xFF0C;&#x4F7F;&#x7528;nnz = 0, dense_dim = 0&#x548C;</span>
<span class="hljs-comment"># sparse_dim = 1 (&#x8FD9;&#x91CC;&#x7684;&#x4E0B;&#x6807;&#x662F;&#x4E00;&#x4E2A;&#x4E8C;&#x7EF4;&#x5F20;&#x91CF;&#x5F62;&#x72B6;shape = (1, 0)) &#x6765;&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x7A7A;&#x7A00;&#x758F;&#x77E9;&#x9635;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>S = torch.sparse_coo_tensor(torch.empty([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]), [], [<span class="hljs-number">1</span>])
tensor(indices=tensor([], size=(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>)),
       values=tensor([], size=(<span class="hljs-number">0</span>,)),
       size=(<span class="hljs-number">1</span>,), nnz=<span class="hljs-number">0</span>, layout=torch.sparse_coo)

<span class="hljs-comment"># &#x7136;&#x540E;&#x4F7F;&#x7528;nnz = 0, dense_dim = 1 &#x548C; sparse_dim = 1</span>
<span class="hljs-comment"># &#x6765;&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x7A7A;&#x7A00;&#x758F;&#x77E9;&#x9635;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>S = torch.sparse_coo_tensor(torch.empty([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]), torch.empty([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>]), [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])
tensor(indices=tensor([], size=(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>)),
       values=tensor([], size=(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>)),
       size=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), nnz=<span class="hljs-number">0</span>, layout=torch.sparse_coo)
</code></pre>
<pre><code class="lang-py">torch.as_tensor(data, dtype=<span class="hljs-keyword">None</span>, device=<span class="hljs-keyword">None</span>) &#x2192; Tensor
</code></pre>
<p>&#x8F6C;&#x6362; <code>data</code> &#x5230; <code>torch.Tensor</code> &#x7C7B;&#x578B;. &#x5982;&#x679C; <code>data</code> &#x5DF2;&#x7ECF;&#x662F;&#x76F8;&#x540C; <code>dtype</code> &#x548C; <code>device</code> &#x7684;&#x5F20;&#x91CF;&#xFF0C; &#x90A3;&#x5C31;&#x4E0D;&#x4F1A;&#x6267;&#x884C;&#x62F7;&#x8D1D;&#xFF0C;&#x5426;&#x5219;&#x5C06;&#x4F1A;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;(&#x5982;&#x679C; data <code>Tensor</code> &#x4E2D; <code>requires_grad=True</code>,&#x5219;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x8BA1;&#x7B97;&#x56FE;&#x4F1A;&#x88AB;&#x4FDD;&#x7559;)&#x76F8;&#x4F3C;&#x5730;&#xFF0C;&#x5982;&#x679C; data <code>dtype</code> &#x4E3A; <code>ndarry</code> &#x5E76;&#x4E14; <code>device</code> &#x4E3A;CPU&#xFF0C;&#x5219;&#x62F7;&#x8D1D;&#x4E0D;&#x4F1A;&#x53D1;&#x751F;&#x3002;</p>
<p>Parameters:</p>
<ul>
<li><strong>data</strong> (<em>array_like</em>) &#x2013; &#x63D0;&#x4F9B;&#x5F20;&#x91CF;&#x521D;&#x59CB;&#x5316;&#x7684;&#x6570;&#x636E;&#x7ED3;&#x6784;&#x3002;&#x53EF;&#x80FD;&#x662F;ist, tuple, NumPy <code>ndarray</code>, scalar &#x6216;&#x5176;&#x4ED6;&#x7C7B;&#x578B;&#x3002;</li>
<li><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; &#x63D0;&#x4F9B;&#x5F20;&#x91CF;&#x521D;&#x59CB;&#x5316;&#x7684;&#x503C;&#x3002;&#x53EF;&#x80FD;&#x662F;ist, tuple, NumPy <code>ndarray</code>, scalar &#x6216;&#x5176;&#x4ED6;&#x7C7B;&#x578B;&#x3002;</li>
<li><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x6240;&#x9700;&#x8981;&#x7684;&#x8BBE;&#x5907;&#x3002;&#x9ED8;&#x8BA4;&#xFF1A;&#x82E5;&#x4E3A;&#x7A7A;&#xFF0C;&#x5219;&#x5F53;&#x524D;&#x7684;&#x8BBE;&#x5907;&#x63D0;&#x4F9B;&#x7ED9;&#x9ED8;&#x8BA4;&#x5F20;&#x91CF;&#x7C7B;&#x578B;(see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> &#x5C06;&#x4E3A;&#x652F;&#x6301;CPU&#x5F20;&#x91CF;&#x7684;CPU&#x548C;&#x652F;&#x6301;CUDA&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x7684;CUDA&#x8BBE;&#x5907;&#x3002;</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>a = numpy.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>t = torch.as_tensor(a)
<span class="hljs-meta">&gt;&gt;&gt; </span>t
tensor([ <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>t[<span class="hljs-number">0</span>] = <span class="hljs-number">-1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>a
array([<span class="hljs-number">-1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>a = numpy.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>t = torch.as_tensor(a, device=torch.device(<span class="hljs-string">&apos;cuda&apos;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>t
tensor([ <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>t[<span class="hljs-number">0</span>] = <span class="hljs-number">-1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>a
array([<span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>])
</code></pre>
<pre><code class="lang-py">torch.from_numpy(ndarray) &#x2192; Tensor
</code></pre>
<p> &#x4ECE;&#x4E00;&#x4E2A; <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.15)" target="_blank"><code>numpy.ndarray</code></a> &#x521B;&#x5EFA;&#x4E00;&#x4E2A; <a href="tensors.html#torch.Tensor" title="torch.Tensor"><code>Tensor</code></a>.</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x548C; <code>ndarry</code> &#x5171;&#x4EAB;&#x76F8;&#x540C;&#x7684;&#x5185;&#x5B58;&#x3002; &#x5BF9;&#x5F20;&#x91CF;&#x7684;&#x4FEE;&#x9970;&#x5C06;&#x4F1A;&#x53CD;&#x6620;&#x5728; <code>ndarry</code>&#xFF0C;&#x53CD;&#x4E4B;&#x4EA6;&#x7136;&#x3002;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF; &#x5927;&#x5C0F;&#x4E0D;&#x53EF;&#x53D8;&#x3002;</p>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>a = numpy.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>t = torch.from_numpy(a)
<span class="hljs-meta">&gt;&gt;&gt; </span>t
tensor([ <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>t[<span class="hljs-number">0</span>] = <span class="hljs-number">-1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>a
array([<span class="hljs-number">-1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>])
</code></pre>
<pre><code class="lang-py">torch.zeros(*sizes, out=<span class="hljs-keyword">None</span>, dtype=<span class="hljs-keyword">None</span>, layout=torch.strided, device=<span class="hljs-keyword">None</span>, requires_grad=<span class="hljs-keyword">False</span>) &#x2192; Tensor
</code></pre>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x7528;&#x6807;&#x91CF; <code>0</code> &#x586B;&#x5145;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#x53EF;&#x53D8;&#x957F;&#x53C2;&#x6570; <code>sizes</code> &#x5B9A;&#x4E49;&#x4E86;&#x8BE5;&#x5F20;&#x91CF;&#x5F62;&#x72B6;(shape).</p>
<p>Parameters:</p>
<ul>
<li><strong>sizes</strong> (<em>int...</em>) &#x2013; &#x5B9A;&#x4E49;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x5F62;&#x72B6;&#x7684;&#x6574;&#x6570;&#x5E8F;&#x5217;. &#x53EF;&#x4EE5;&#x662F;&#x53EF;&#x53D8;&#x957F;&#x7684;&#x53C2;&#x6570; &#x6216;&#x8005;&#x662F;&#x50CF; &#x5217;&#x8868;&#x5143;&#x7EC4;&#x8FD9;&#x6837;&#x7684;&#x96C6;&#x5408;&#x3002;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; &#x8F93;&#x51FA;&#x5F20;&#x91CF;</li>
<li><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;. &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x4E3A; <code>None</code>, &#x4F7F;&#x7528;&#x5168;&#x5C40;&#x9ED8;&#x8BA4;&#x503C; (&#x53C2;&#x8003; <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</li>
<li><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x5C42;&#x6570;. Default: <code>torch.strided</code>.</li>
<li><h1 id="todo">TODO</h1>
</li>
<li><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x6240;&#x9700;&#x7684;&#x8BBE;&#x5907;. &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x4E3A; <code>None</code>, &#x5219;&#x5F53;&#x524D;&#x7684;&#x8BBE;&#x5907;&#x63D0;&#x4F9B;&#x7ED9;&#x9ED8;&#x8BA4;&#x5F20;&#x91CF;&#x7C7B;&#x578B;(see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> &#x5C06;&#x4E3A;&#x652F;&#x6301;CPU&#x5F20;&#x91CF;&#x7684;CPU&#x548C;&#x652F;&#x6301;CUDA&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x7684;CUDA&#x8BBE;&#x5907;&#x3002;</li>
<li><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; &#x81EA;&#x52A8;&#x68AF;&#x5EA6;&#x8BA1;&#x7B97;&#x662F;&#x5426;&#x9700;&#x8981;&#x8BB0;&#x5F55;&#x5728;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x4E0A;&#x7684;&#x64CD;&#x4F5C;&#x3002;&#x9ED8;&#x8BA4;: <code>False</code>.</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.zeros(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)
tensor([[ <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>],
 [ <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>]])

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.zeros(<span class="hljs-number">5</span>)
tensor([ <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>])
</code></pre>
<pre><code class="lang-py">torch.zeros_like(input, dtype=<span class="hljs-keyword">None</span>, layout=<span class="hljs-keyword">None</span>, device=<span class="hljs-keyword">None</span>, requires_grad=<span class="hljs-keyword">False</span>) &#x2192; Tensor
</code></pre>
<p>&#x8FD4;&#x56DE;&#x7528;&#x6807;&#x91CF;<code>0</code>&#x586B;&#x5145;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5927;&#x5C0F;&#x548C;<code>input</code>&#x7684;<code>size</code>&#x4E00;&#x6837;. <code>torch.zeros_like(input)</code> &#x7B49;&#x4EF7;&#x4E8E; <code>torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>.</p>
<p>Warning</p>
<p>&#x622A;&#x6B62;&#x5230; 0.4, &#x8BE5;&#x51FD;&#x6570;&#x4E0D;&#x518D;&#x652F;&#x6301;<code>out</code>&#x5173;&#x952E;&#x5B57;. &#x540C;&#x65F6;&#xFF0C;&#x8001;&#x7248;&#x7684; <code>torch.zeros_like(input, out=output)</code> &#x7B49;&#x4EF7;&#x4E8E; <code>torch.zeros(input.size(), out=output)</code>.</p>
<p>Parameters: </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; <code>input</code>&#x7684;<code>size</code>&#x5C5E;&#x6027;&#x51B3;&#x5B9A;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x5927;&#x5C0F;</li>
<li><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;. &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x4E3A; <code>None</code>, &#x4F7F;&#x7528;<code>input</code>&#x7684;<code>dtype</code>&#x5C5E;&#x6027; .</li>
<li><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x5C42;&#x6570;. Default: &#x9ED8;&#x8BA4;&#x4E3A;<code>input</code>&#x7684;<code>layout</code>&#x5C5E;&#x6027;.</li>
<li><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x6240;&#x9700;&#x7684;&#x8BBE;&#x5907;. &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x4E3A; <code>None</code>, &#x5219;&#x4E3A;<code>input</code>&#x7684;<code>device</code>&#x5C5E;&#x6027;.</li>
<li><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; &#x81EA;&#x52A8;&#x68AF;&#x5EA6;&#x8BA1;&#x7B97;&#x662F;&#x5426;&#x9700;&#x8981;&#x8BB0;&#x5F55;&#x5728;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x4E0A;&#x7684;&#x64CD;&#x4F5C;&#x3002;&#x9ED8;&#x8BA4;: <code>False</code>.</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.empty(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.zeros_like(input)
tensor([[ <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>],
 [ <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>]])
</code></pre>
<pre><code class="lang-py">torch.ones(*sizes, out=<span class="hljs-keyword">None</span>, dtype=<span class="hljs-keyword">None</span>, layout=torch.strided, device=<span class="hljs-keyword">None</span>, requires_grad=<span class="hljs-keyword">False</span>) &#x2192; Tensor
</code></pre>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x7528;&#x6807;&#x91CF; <code>1</code> &#x586B;&#x5145;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#x53EF;&#x53D8;&#x957F;&#x53C2;&#x6570; <code>sizes</code> &#x5B9A;&#x4E49;&#x4E86;&#x8BE5;&#x5F20;&#x91CF;&#x5F62;&#x72B6;(shape).</p>
<p>Parameters:</p>
<ul>
<li><strong>sizes</strong> (<em>int...</em>) &#x2013; &#x5B9A;&#x4E49;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x5F62;&#x72B6;&#x7684;&#x6574;&#x6570;&#x5E8F;&#x5217;. &#x53EF;&#x4EE5;&#x662F;&#x53EF;&#x53D8;&#x957F;&#x7684;&#x53C2;&#x6570; &#x6216;&#x8005;&#x662F;&#x50CF; &#x5217;&#x8868;&#x5143;&#x7EC4;&#x8FD9;&#x6837;&#x7684;&#x96C6;&#x5408;&#x3002;</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; &#x8F93;&#x51FA;&#x5F20;&#x91CF;</li>
<li><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;. &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x4E3A; <code>None</code>, &#x4F7F;&#x7528;&#x5168;&#x5C40;&#x9ED8;&#x8BA4;&#x503C; (&#x53C2;&#x8003; <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</li>
<li><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x5C42;&#x6570;. Default: <code>torch.strided</code>.</li>
<li><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x6240;&#x9700;&#x7684;&#x8BBE;&#x5907;. &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x4E3A; <code>None</code>, &#x5219;&#x5F53;&#x524D;&#x7684;&#x8BBE;&#x5907;&#x63D0;&#x4F9B;&#x7ED9;&#x9ED8;&#x8BA4;&#x5F20;&#x91CF;&#x7C7B;&#x578B;(see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> &#x5C06;&#x4E3A;&#x652F;&#x6301;CPU&#x5F20;&#x91CF;&#x7684;CPU&#x548C;&#x652F;&#x6301;CUDA&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x7684;CUDA&#x8BBE;&#x5907;&#x3002;</li>
<li><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; &#x81EA;&#x52A8;&#x68AF;&#x5EA6;&#x8BA1;&#x7B97;&#x662F;&#x5426;&#x9700;&#x8981;&#x8BB0;&#x5F55;&#x5728;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x4E0A;&#x7684;&#x64CD;&#x4F5C;&#x3002;&#x9ED8;&#x8BA4;: <code>False</code>.</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.ones(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)
tensor([[ <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>],
 [ <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>]])

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.ones(<span class="hljs-number">5</span>)
tensor([ <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>])
</code></pre>
<pre><code class="lang-py">torch.ones_like(input, dtype=<span class="hljs-keyword">None</span>, layout=<span class="hljs-keyword">None</span>, device=<span class="hljs-keyword">None</span>, requires_grad=<span class="hljs-keyword">False</span>) &#x2192; Tensor
</code></pre>
<p>&#x8FD4;&#x56DE;&#x7528;&#x6807;&#x91CF;<code>1</code>&#x586B;&#x5145;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5927;&#x5C0F;&#x548C;<code>input</code>&#x7684;<code>size</code>&#x4E00;&#x6837;. <code>torch.ones_like(input)</code> &#x7B49;&#x4EF7;&#x4E8E; <code>torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code></p>
<p>Warning</p>
<p>&#x622A;&#x6B62;&#x5230; 0.4, &#x8BE5;&#x51FD;&#x6570;&#x4E0D;&#x518D;&#x652F;&#x6301;<code>out</code>&#x5173;&#x952E;&#x5B57;. &#x540C;&#x65F6;&#xFF0C;&#x8001;&#x7248;&#x7684; <code>torch.ones_like(input, out=output)</code> &#x7B49;&#x4EF7;&#x4E8E; <code>torch.ones(input.size(), out=output)</code>.</p>
<p>Parameters: </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; <code>input</code>&#x7684;<code>size</code>&#x5C5E;&#x6027;&#x51B3;&#x5B9A;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x5927;&#x5C0F;</li>
<li><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;. &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x4E3A; <code>None</code>, &#x4F7F;&#x7528;<code>input</code>&#x7684;<code>dtype</code>&#x5C5E;&#x6027; .</li>
<li><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x5C42;&#x6570;. Default: &#x9ED8;&#x8BA4;&#x4E3A;<code>input</code>&#x7684;<code>layout</code>&#x5C5E;&#x6027;.</li>
<li><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x6240;&#x9700;&#x7684;&#x8BBE;&#x5907;. &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x4E3A; <code>None</code>, &#x5219;&#x4E3A;<code>input</code>&#x7684;<code>device</code>&#x5C5E;&#x6027;.</li>
<li><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; &#x81EA;&#x52A8;&#x68AF;&#x5EA6;&#x8BA1;&#x7B97;&#x662F;&#x5426;&#x9700;&#x8981;&#x8BB0;&#x5F55;&#x5728;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x4E0A;&#x7684;&#x64CD;&#x4F5C;&#x3002;&#x9ED8;&#x8BA4;: <code>False</code>.</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.empty(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.ones_like(input)
tensor([[ <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>],
 [ <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>]])
</code></pre>
<pre><code class="lang-py">torch.arange(start=<span class="hljs-number">0</span>, end, step=<span class="hljs-number">1</span>, out=<span class="hljs-keyword">None</span>, dtype=<span class="hljs-keyword">None</span>, layout=torch.strided, device=<span class="hljs-keyword">None</span>, requires_grad=<span class="hljs-keyword">False</span>) &#x2192; Tensor
</code></pre>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#xFF0C;&#x5927;&#x5C0F;&#x4E3A;<img src="img/93f0e014493a12cb3334b1c1f00517f4.jpg" alt="">&#xFF0C;&#x503C;&#x4E3A;&#x533A;&#x95F4; <code>[start,end)</code>&#x5185;&#xFF0C;&#x4EE5;<code>step</code>&#x4E3A;&#x6B65;&#x8DDD;,&#x4ECE;<code>start</code>&#x5F00;&#x59CB;&#x7684;&#x6570;&#x5217;.</p>
<p>&#x6CE8;&#x610F;: &#x975E;&#x6574;&#x578B;&#x6570; <code>step</code> &#x548C; <code>end</code> &#x6BD4;&#x8F83;&#x65F6;&#x5B58;&#x5728;&#x6D6E;&#x70B9;&#x56DB;&#x820D;&#x4E94;&#x5165;&#x8BEF;&#x5DEE;;&#x4E3A;&#x907F;&#x514D;&#x4E0D;&#x4E00;&#x81F4;&#xFF0C;&#x5EFA;&#x8BAE;&#x5728;<code>end</code>&#x540E;&#x9762;&#x52A0;&#x4E0A;&#x4E00;&#x4E2A;&#x5C0F;&#x7684;epsilon.</p>
<p><img src="img/9e6fd079c910af8a8a6486e341b02282.jpg" alt=""></p>
<p>Parameters: </p>
<ul>
<li><strong>start</strong> (<em>Number</em>) &#x2013; &#x70B9;&#x96C6;&#x7684;&#x8D77;&#x59CB;&#x503C;. &#x9ED8;&#x8BA4;&#x4E3A;<code>0</code>.</li>
<li><strong>end</strong> (<em>Number</em>) &#x2013; &#x70B9;&#x96C6;&#x7684;&#x7EC8;&#x503C;.</li>
<li><strong>step</strong> (<em>Number</em>) &#x2013; &#x6BCF;&#x5BF9;&#x76F8;&#x90BB;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB; . &#x9ED8;&#x8BA4;&#x4E3A; <code>1</code>.</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; &#x8F93;&#x51FA;&#x7684;&#x5F20;&#x91CF;</li>
<li><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;. &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x4E3A; <code>None</code>, &#x4F7F;&#x7528;&#x5168;&#x5C40;&#x9ED8;&#x8BA4;&#x503C;. (&#x53C2;&#x8003; <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). &#x82E5; <code>dtype</code> &#x672A;&#x63D0;&#x4F9B;, &#x5219;&#x4ECE;&#x5176;&#x4ED6;&#x8F93;&#x5165;&#x53C2;&#x6570;&#x63A8;&#x65AD;&#x6570;&#x636E;&#x7C7B;&#x578B;. &#x5982;&#x679C; <code>start</code>, <code>end</code>, <code>stop</code> &#x4E2D;&#x5B58;&#x5728;&#x6D6E;&#x70B9;&#x6570;, &#x5219; <code>dtype</code> &#x4F1A;&#x4F7F;&#x7528;&#x9ED8;&#x8BA4;&#x6570;&#x636E;&#x7C7B;&#x578B;, &#x8BF7;&#x67E5;&#x770B; <a href="#torch.get_default_dtype" title="torch.get_default_dtype"><code>get_default_dtype()</code></a>. &#x5426;&#x5219;,  <code>dtype</code> &#x4F1A;&#x4F7F;&#x7528; <code>torch.int64</code>.</li>
<li><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x5C42;&#x6570;. Default: <code>torch.strided</code>.</li>
<li><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x6240;&#x9700;&#x7684;&#x8BBE;&#x5907;. &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x4E3A; <code>None</code>, &#x5219;&#x5F53;&#x524D;&#x7684;&#x8BBE;&#x5907;&#x63D0;&#x4F9B;&#x7ED9;&#x9ED8;&#x8BA4;&#x5F20;&#x91CF;&#x7C7B;&#x578B;(see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> &#x5C06;&#x4E3A;&#x652F;&#x6301;CPU&#x5F20;&#x91CF;&#x7684;CPU&#x548C;&#x652F;&#x6301;CUDA&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x7684;CUDA&#x8BBE;&#x5907;&#x3002;</li>
<li><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; &#x81EA;&#x52A8;&#x68AF;&#x5EA6;&#x8BA1;&#x7B97;&#x662F;&#x5426;&#x9700;&#x8981;&#x8BB0;&#x5F55;&#x5728;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x4E0A;&#x7684;&#x64CD;&#x4F5C;&#x3002;&#x9ED8;&#x8BA4;: <code>False</code>.</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.arange(<span class="hljs-number">5</span>)
tensor([ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>)
tensor([ <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">2.5</span>, <span class="hljs-number">0.5</span>)
tensor([ <span class="hljs-number">1.0000</span>,  <span class="hljs-number">1.5000</span>,  <span class="hljs-number">2.0000</span>])
</code></pre>
<pre><code class="lang-py">torch.range(start=<span class="hljs-number">0</span>, end, step=<span class="hljs-number">1</span>, out=<span class="hljs-keyword">None</span>, dtype=<span class="hljs-keyword">None</span>, layout=torch.strided, device=<span class="hljs-keyword">None</span>, requires_grad=<span class="hljs-keyword">False</span>) &#x2192; Tensor
</code></pre>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#xFF0C;&#x5927;&#x5C0F;&#x4E3A;<img src="img/8df7b0b7ce71be5149c4526856d43f13.jpg" alt="">&#xFF0C;&#x503C;&#x4ECE;<code>start</code>&#x5230;<code>end</code>&#xFF0C;&#x4EE5;<code>step</code>&#x4E3A;&#x6B65;&#x8DDD;&#x7684;&#x6570;&#x5217;.</p>
<p><img src="img/df6d39cd8a83332eb174ec540be74326.jpg" alt=""></p>
<p>Warning</p>
<p>&#x8FD9;&#x4E2A;&#x51FD;&#x6570;&#x88AB;&#x5F03;&#x7528;&#xFF0C;&#x6539;&#x4E3A; <a href="#torch.arange" title="torch.arange"><code>torch.arange()</code></a>.</p>
<p>Parameters: </p>
<ul>
<li><strong>start</strong> (<em>Number</em>) &#x2013; &#x70B9;&#x96C6;&#x7684;&#x8D77;&#x59CB;&#x503C;. &#x9ED8;&#x8BA4;&#x4E3A;<code>0</code>.</li>
<li><strong>end</strong> (<em>Number</em>) &#x2013; &#x70B9;&#x96C6;&#x7684;&#x7EC8;&#x503C;.</li>
<li><strong>step</strong> (<em>Number</em>) &#x2013; &#x6BCF;&#x5BF9;&#x76F8;&#x90BB;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB; . &#x9ED8;&#x8BA4;&#x4E3A; <code>1</code>.</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; &#x8F93;&#x51FA;&#x7684;&#x5F20;&#x91CF;</li>
<li><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;. &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x4E3A; <code>None</code>, &#x4F7F;&#x7528;&#x5168;&#x5C40;&#x9ED8;&#x8BA4;&#x503C;. (&#x53C2;&#x8003; <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). &#x82E5; <code>dtype</code> &#x672A;&#x63D0;&#x4F9B;, &#x5219;&#x4ECE;&#x5176;&#x4ED6;&#x8F93;&#x5165;&#x53C2;&#x6570;&#x63A8;&#x65AD;&#x6570;&#x636E;&#x7C7B;&#x578B;. &#x5982;&#x679C; <code>start</code>, <code>end</code>, <code>stop</code> &#x4E2D;&#x5B58;&#x5728;&#x6D6E;&#x70B9;&#x6570;, &#x5219; <code>dtype</code> &#x4F1A;&#x4F7F;&#x7528;&#x9ED8;&#x8BA4;&#x6570;&#x636E;&#x7C7B;&#x578B;, &#x8BF7;&#x67E5;&#x770B; <a href="#torch.get_default_dtype" title="torch.get_default_dtype"><code>get_default_dtype()</code></a>. &#x5426;&#x5219;,  <code>dtype</code> &#x4F1A;&#x4F7F;&#x7528; <code>torch.int64</code>.</li>
<li><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x5C42;&#x6570;. Default: <code>torch.strided</code>.</li>
<li><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x6240;&#x9700;&#x7684;&#x8BBE;&#x5907;. &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x4E3A; <code>None</code>, &#x5219;&#x5F53;&#x524D;&#x7684;&#x8BBE;&#x5907;&#x63D0;&#x4F9B;&#x7ED9;&#x9ED8;&#x8BA4;&#x5F20;&#x91CF;&#x7C7B;&#x578B;(see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> &#x5C06;&#x4E3A;&#x652F;&#x6301;CPU&#x5F20;&#x91CF;&#x7684;CPU&#x548C;&#x652F;&#x6301;CUDA&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x7684;CUDA&#x8BBE;&#x5907;&#x3002;</li>
<li><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; &#x81EA;&#x52A8;&#x68AF;&#x5EA6;&#x8BA1;&#x7B97;&#x662F;&#x5426;&#x9700;&#x8981;&#x8BB0;&#x5F55;&#x5728;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x4E0A;&#x7684;&#x64CD;&#x4F5C;&#x3002;&#x9ED8;&#x8BA4;: <code>False</code>.</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.range(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>)
tensor([ <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.range(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0.5</span>)
tensor([ <span class="hljs-number">1.0000</span>,  <span class="hljs-number">1.5000</span>,  <span class="hljs-number">2.0000</span>,  <span class="hljs-number">2.5000</span>,  <span class="hljs-number">3.0000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">4.0000</span>])
</code></pre>
<pre><code class="lang-py">torch.linspace(start, end, steps=<span class="hljs-number">100</span>, out=<span class="hljs-keyword">None</span>, dtype=<span class="hljs-keyword">None</span>, layout=torch.strided, device=<span class="hljs-keyword">None</span>, requires_grad=<span class="hljs-keyword">False</span>) &#x2192; Tensor
</code></pre>
<p>&#x8FD4;&#x56DE;&#x5171;<code>steps</code>&#x6570;&#x91CF;&#x5728;<code>start</code> &#x548C; <code>end</code>&#x4E4B;&#x95F4;&#x7684;&#x7B49;&#x8DDD;&#x70B9;&#xFF0C;&#x4ECE;&#x800C;&#x7EC4;&#x6210;&#x7684;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;.</p>
<p>&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x5927;&#x5C0F;&#x4E3A;<code>steps</code>&#xFF0C;&#x7EF4;&#x5EA6;&#x4E3A;&#x4E00;&#x7EF4;.</p>
<p>Parameters: </p>
<ul>
<li><strong>start</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a>) &#x2013; &#x70B9;&#x96C6;&#x7684;&#x8D77;&#x59CB;&#x503C;. </li>
<li><strong>end</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a>) &#x2013;&#x70B9;&#x96C6;&#x7684;&#x7EC8;&#x503C;.</li>
<li><strong>steps</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013;  <code>start</code> &#x548C; <code>end</code>&#x4E4B;&#x95F4;&#x7684;&#x6837;&#x672C;&#x70B9;&#x6570;&#x76EE;. &#x9ED8;&#x8BA4;: <code>100</code>.</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; &#x8F93;&#x51FA;&#x5F20;&#x91CF;</li>
<li><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;. &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x4E3A; <code>None</code>, &#x4F7F;&#x7528;&#x5168;&#x5C40;&#x9ED8;&#x8BA4;&#x503C;. (&#x53C2;&#x8003; <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</li>
<li><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x5C42;&#x6570;. Default: <code>torch.strided</code>.</li>
<li><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x6240;&#x9700;&#x7684;&#x8BBE;&#x5907;. &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x4E3A; <code>None</code>, &#x5219;&#x5F53;&#x524D;&#x7684;&#x8BBE;&#x5907;&#x63D0;&#x4F9B;&#x7ED9;&#x9ED8;&#x8BA4;&#x5F20;&#x91CF;&#x7C7B;&#x578B;(see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> &#x5C06;&#x4E3A;&#x652F;&#x6301;CPU&#x5F20;&#x91CF;&#x7684;CPU&#x548C;&#x652F;&#x6301;CUDA&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x7684;CUDA&#x8BBE;&#x5907;&#x3002;</li>
<li><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; &#x81EA;&#x52A8;&#x68AF;&#x5EA6;&#x8BA1;&#x7B97;&#x662F;&#x5426;&#x9700;&#x8981;&#x8BB0;&#x5F55;&#x5728;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x4E0A;&#x7684;&#x64CD;&#x4F5C;&#x3002;&#x9ED8;&#x8BA4;: <code>False</code>.</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.linspace(<span class="hljs-number">3</span>, <span class="hljs-number">10</span>, steps=<span class="hljs-number">5</span>)
tensor([  <span class="hljs-number">3.0000</span>,   <span class="hljs-number">4.7500</span>,   <span class="hljs-number">6.5000</span>,   <span class="hljs-number">8.2500</span>,  <span class="hljs-number">10.0000</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.linspace(<span class="hljs-number">-10</span>, <span class="hljs-number">10</span>, steps=<span class="hljs-number">5</span>)
tensor([<span class="hljs-number">-10.</span>,  <span class="hljs-number">-5.</span>,   <span class="hljs-number">0.</span>,   <span class="hljs-number">5.</span>,  <span class="hljs-number">10.</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.linspace(start=<span class="hljs-number">-10</span>, end=<span class="hljs-number">10</span>, steps=<span class="hljs-number">5</span>)
tensor([<span class="hljs-number">-10.</span>,  <span class="hljs-number">-5.</span>,   <span class="hljs-number">0.</span>,   <span class="hljs-number">5.</span>,  <span class="hljs-number">10.</span>])
</code></pre>
<pre><code class="lang-py">torch.logspace(start, end, steps=<span class="hljs-number">100</span>, out=<span class="hljs-keyword">None</span>, dtype=<span class="hljs-keyword">None</span>, layout=torch.strided, device=<span class="hljs-keyword">None</span>, requires_grad=<span class="hljs-keyword">False</span>) &#x2192; Tensor
</code></pre>
<p>&#x8FD4;&#x56DE;&#x5171;&#x6709;<code>steps</code>&#x6570;&#x91CF;&#x7684;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#xFF0C;&#x70B9;&#x96C6;&#x7531;<img src="img/f8472cb905d226233b6c5b6ca382cf74.jpg" alt=""> &#x548C; <img src="img/5385eb329ab67a640482b03b99be6155.jpg" alt="">&#x4E4B;&#x95F4;&#x5BF9;&#x6570;&#x5206;&#x5E03;&#x7684;&#x70B9;&#x7EC4;&#x6210;.</p>
<p>&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x5927;&#x5C0F;&#x4E3A;<code>steps</code>&#xFF0C;&#x7EF4;&#x5EA6;&#x4E3A;&#x4E00;&#x7EF4;.</p>
<p>Parameters: </p>
<ul>
<li><strong>start</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a>) &#x2013; &#x70B9;&#x96C6;&#x7684;&#x8D77;&#x59CB;&#x503C;. </li>
<li><strong>end</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a>) &#x2013;&#x70B9;&#x96C6;&#x7684;&#x7EC8;&#x503C;.</li>
<li><strong>steps</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013;  <code>start</code> &#x548C; <code>end</code>&#x4E4B;&#x95F4;&#x7684;&#x6837;&#x672C;&#x70B9;&#x6570;&#x76EE;. &#x9ED8;&#x8BA4;: <code>100</code>.</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; &#x8F93;&#x51FA;&#x5F20;&#x91CF;</li>
<li><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;. &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x4E3A; <code>None</code>, &#x4F7F;&#x7528;&#x5168;&#x5C40;&#x9ED8;&#x8BA4;&#x503C;. (&#x53C2;&#x8003; <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</li>
<li><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x5C42;&#x6570;. Default: <code>torch.strided</code>.</li>
<li><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x6240;&#x9700;&#x7684;&#x8BBE;&#x5907;. &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x4E3A; <code>None</code>, &#x5219;&#x5F53;&#x524D;&#x7684;&#x8BBE;&#x5907;&#x63D0;&#x4F9B;&#x7ED9;&#x9ED8;&#x8BA4;&#x5F20;&#x91CF;&#x7C7B;&#x578B;(see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> &#x5C06;&#x4E3A;&#x652F;&#x6301;CPU&#x5F20;&#x91CF;&#x7684;CPU&#x548C;&#x652F;&#x6301;CUDA&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x7684;CUDA&#x8BBE;&#x5907;&#x3002;</li>
<li><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; &#x81EA;&#x52A8;&#x68AF;&#x5EA6;&#x8BA1;&#x7B97;&#x662F;&#x5426;&#x9700;&#x8981;&#x8BB0;&#x5F55;&#x5728;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x4E0A;&#x7684;&#x64CD;&#x4F5C;&#x3002;&#x9ED8;&#x8BA4;: <code>False</code>.</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.logspace(start=<span class="hljs-number">-10</span>, end=<span class="hljs-number">10</span>, steps=<span class="hljs-number">5</span>)
tensor([ <span class="hljs-number">1.0000e-10</span>,  <span class="hljs-number">1.0000e-05</span>,  <span class="hljs-number">1.0000e+00</span>,  <span class="hljs-number">1.0000e+05</span>,  <span class="hljs-number">1.0000e+10</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.logspace(start=<span class="hljs-number">0.1</span>, end=<span class="hljs-number">1.0</span>, steps=<span class="hljs-number">5</span>)
tensor([  <span class="hljs-number">1.2589</span>,   <span class="hljs-number">2.1135</span>,   <span class="hljs-number">3.5481</span>,   <span class="hljs-number">5.9566</span>,  <span class="hljs-number">10.0000</span>])
</code></pre>
<pre><code class="lang-py">torch.eye(n, m=<span class="hljs-keyword">None</span>, out=<span class="hljs-keyword">None</span>, dtype=<span class="hljs-keyword">None</span>, layout=torch.strided, device=<span class="hljs-keyword">None</span>, requires_grad=<span class="hljs-keyword">False</span>) &#x2192; Tensor
</code></pre>
<p>&#x8FD4;&#x56DE;&#x4E8C;&#x7EF4;&#x5F20;&#x91CF;&#xFF0C;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0A;&#x662F;1&#xFF0C;&#x5176;&#x5B83;&#x5730;&#x65B9;&#x662F;0.</p>
<p>Parameters: </p>
<ul>
<li><strong>n</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; the number of rows</li>
<li><strong>m</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; the number of columns with default being <code>n</code></li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor</li>
<li><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</li>
<li><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</li>
<li><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</li>
<li><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</li>
</ul>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>A 2-D tensor with ones on the diagonal and zeros elsewhere</th>
</tr>
</thead>
<tbody>
<tr>
<td>Return type:</td>
<td><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.eye(<span class="hljs-number">3</span>)
tensor([[ <span class="hljs-number">1.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>],
 [ <span class="hljs-number">0.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">0.</span>],
 [ <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">1.</span>]])
</code></pre>
<pre><code class="lang-py">torch.empty(*sizes, out=<span class="hljs-keyword">None</span>, dtype=<span class="hljs-keyword">None</span>, layout=torch.strided, device=<span class="hljs-keyword">None</span>, requires_grad=<span class="hljs-keyword">False</span>) &#x2192; Tensor
</code></pre>
<p>Returns a tensor filled with uninitialized data. The shape of the tensor is defined by the variable argument <code>sizes</code>.</p>
<p>Parameters: </p>
<ul>
<li><strong>sizes</strong> (<em>int...</em>) &#x2013; a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor</li>
<li><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</li>
<li><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</li>
<li><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</li>
<li><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.empty(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)
tensor(<span class="hljs-number">1.00000e-08</span> *
 [[ <span class="hljs-number">6.3984</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>]])
</code></pre>
<pre><code class="lang-py">torch.empty_like(input, dtype=<span class="hljs-keyword">None</span>, layout=<span class="hljs-keyword">None</span>, device=<span class="hljs-keyword">None</span>, requires_grad=<span class="hljs-keyword">False</span>) &#x2192; Tensor
</code></pre>
<p>Returns an uninitialized tensor with the same size as <code>input</code>. <code>torch.empty_like(input)</code> is equivalent to <code>torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>.</p>
<p>Parameters: </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the size of <code>input</code> will determine size of the output tensor</li>
<li><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</li>
<li><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</li>
<li><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</li>
<li><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.empty((<span class="hljs-number">2</span>,<span class="hljs-number">3</span>), dtype=torch.int64)
tensor([[ <span class="hljs-number">9.4064e+13</span>,  <span class="hljs-number">2.8000e+01</span>,  <span class="hljs-number">9.3493e+13</span>],
 [ <span class="hljs-number">7.5751e+18</span>,  <span class="hljs-number">7.1428e+18</span>,  <span class="hljs-number">7.5955e+18</span>]])
</code></pre>
<pre><code class="lang-py">torch.full(size, fill_value, out=<span class="hljs-keyword">None</span>, dtype=<span class="hljs-keyword">None</span>, layout=torch.strided, device=<span class="hljs-keyword">None</span>, requires_grad=<span class="hljs-keyword">False</span>) &#x2192; Tensor
</code></pre>
<p>Returns a tensor of size <code>size</code> filled with <code>fill_value</code>.</p>
<p>Parameters: </p>
<ul>
<li><strong>size</strong> (<em>int...</em>) &#x2013; a list, tuple, or <code>torch.Size</code> of integers defining the shape of the output tensor.</li>
<li><strong>fill_value</strong> &#x2013; the number to fill the output tensor with.</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor</li>
<li><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</li>
<li><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</li>
<li><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</li>
<li><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.full((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), <span class="hljs-number">3.141592</span>)
tensor([[ <span class="hljs-number">3.1416</span>,  <span class="hljs-number">3.1416</span>,  <span class="hljs-number">3.1416</span>],
 [ <span class="hljs-number">3.1416</span>,  <span class="hljs-number">3.1416</span>,  <span class="hljs-number">3.1416</span>]])
</code></pre>
<pre><code class="lang-py">torch.full_like(input, fill_value, out=<span class="hljs-keyword">None</span>, dtype=<span class="hljs-keyword">None</span>, layout=torch.strided, device=<span class="hljs-keyword">None</span>, requires_grad=<span class="hljs-keyword">False</span>) &#x2192; Tensor
</code></pre>
<p>Returns a tensor with the same size as <code>input</code> filled with <code>fill_value</code>. <code>torch.full_like(input, fill_value)</code> is equivalent to <code>torch.full_like(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device)</code>.</p>
<p>Parameters: </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the size of <code>input</code> will determine size of the output tensor</li>
<li><strong>fill_value</strong> &#x2013; the number to fill the output tensor with.</li>
<li><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</li>
<li><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</li>
<li><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</li>
<li><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</li>
</ul>
<h3 id="indexing-slicing-joining-mutating-ops">Indexing, Slicing, Joining, Mutating Ops</h3>
<pre><code class="lang-py">torch.cat(tensors, dim=<span class="hljs-number">0</span>, out=<span class="hljs-keyword">None</span>) &#x2192; Tensor
</code></pre>
<p>Concatenates the given sequence of <code>seq</code> tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.</p>
<p><a href="#torch.cat" title="torch.cat"><code>torch.cat()</code></a> can be seen as an inverse operation for <a href="#torch.split" title="torch.split"><code>torch.split()</code></a> and <a href="#torch.chunk" title="torch.chunk"><code>torch.chunk()</code></a>.</p>
<p><a href="#torch.cat" title="torch.cat"><code>torch.cat()</code></a> can be best understood via examples.</p>
<p>Parameters: </p>
<ul>
<li><strong>tensors</strong> (<em>sequence of Tensors</em>) &#x2013; any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension.</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; the dimension over which the tensors are concatenated</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>x
tensor([[ <span class="hljs-number">0.6580</span>, <span class="hljs-number">-1.0969</span>, <span class="hljs-number">-0.4614</span>],
 [<span class="hljs-number">-0.1034</span>, <span class="hljs-number">-0.5790</span>,  <span class="hljs-number">0.1497</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.cat((x, x, x), <span class="hljs-number">0</span>)
tensor([[ <span class="hljs-number">0.6580</span>, <span class="hljs-number">-1.0969</span>, <span class="hljs-number">-0.4614</span>],
 [<span class="hljs-number">-0.1034</span>, <span class="hljs-number">-0.5790</span>,  <span class="hljs-number">0.1497</span>],
 [ <span class="hljs-number">0.6580</span>, <span class="hljs-number">-1.0969</span>, <span class="hljs-number">-0.4614</span>],
 [<span class="hljs-number">-0.1034</span>, <span class="hljs-number">-0.5790</span>,  <span class="hljs-number">0.1497</span>],
 [ <span class="hljs-number">0.6580</span>, <span class="hljs-number">-1.0969</span>, <span class="hljs-number">-0.4614</span>],
 [<span class="hljs-number">-0.1034</span>, <span class="hljs-number">-0.5790</span>,  <span class="hljs-number">0.1497</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.cat((x, x, x), <span class="hljs-number">1</span>)
tensor([[ <span class="hljs-number">0.6580</span>, <span class="hljs-number">-1.0969</span>, <span class="hljs-number">-0.4614</span>,  <span class="hljs-number">0.6580</span>, <span class="hljs-number">-1.0969</span>, <span class="hljs-number">-0.4614</span>,  <span class="hljs-number">0.6580</span>,
 <span class="hljs-number">-1.0969</span>, <span class="hljs-number">-0.4614</span>],
 [<span class="hljs-number">-0.1034</span>, <span class="hljs-number">-0.5790</span>,  <span class="hljs-number">0.1497</span>, <span class="hljs-number">-0.1034</span>, <span class="hljs-number">-0.5790</span>,  <span class="hljs-number">0.1497</span>, <span class="hljs-number">-0.1034</span>,
 <span class="hljs-number">-0.5790</span>,  <span class="hljs-number">0.1497</span>]])
</code></pre>
<pre><code class="lang-py">torch.chunk(tensor, chunks, dim=<span class="hljs-number">0</span>) &#x2192; List of Tensors
</code></pre>
<p>Splits a tensor into a specific number of chunks.</p>
<p>Last chunk will be smaller if the tensor size along the given dimension <code>dim</code> is not divisible by <code>chunks</code>.</p>
<p>Parameters: </p>
<ul>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the tensor to split</li>
<li><strong>chunks</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; number of chunks to return</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; dimension along which to split the tensor</li>
</ul>
<pre><code class="lang-py">torch.gather(input, dim, index, out=<span class="hljs-keyword">None</span>) &#x2192; Tensor
</code></pre>
<p>Gathers values along an axis specified by <code>dim</code>.</p>
<p>For a 3-D tensor the output is specified by:</p>
<pre><code class="lang-py">out[i][j][k] = input[index[i][j][k]][j][k]  <span class="hljs-comment"># if dim == 0</span>
out[i][j][k] = input[i][index[i][j][k]][k]  <span class="hljs-comment"># if dim == 1</span>
out[i][j][k] = input[i][j][index[i][j][k]]  <span class="hljs-comment"># if dim == 2</span>
</code></pre>
<p>If <code>input</code> is an n-dimensional tensor with size <img src="img/cc51a9d31af24562d403cf82e9e26cb9.jpg" alt=""> and <code>dim = i</code>, then <code>index</code> must be an <img src="img/493731e423d5db62086d0b8705dda0c8.jpg" alt="">-dimensional tensor with size <img src="img/28ad1437dec91c9b5a9e20d1e044527a.jpg" alt=""> where <img src="img/e03000f13271a1cd4b262e91f1e8f846.jpg" alt=""> and <code>out</code> will have the same size as <code>index</code>.</p>
<p>Parameters: </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the source tensor</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; the axis along which to index</li>
<li><strong>index</strong> (<em>LongTensor</em>) &#x2013; the indices of elements to gather</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the destination tensor</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>t = torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.gather(t, <span class="hljs-number">1</span>, torch.tensor([[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]]))
tensor([[ <span class="hljs-number">1</span>,  <span class="hljs-number">1</span>],
 [ <span class="hljs-number">4</span>,  <span class="hljs-number">3</span>]])
</code></pre>
<pre><code class="lang-py">torch.index_select(input, dim, index, out=<span class="hljs-keyword">None</span>) &#x2192; Tensor
</code></pre>
<p>Returns a new tensor which indexes the <code>input</code> tensor along dimension <code>dim</code> using the entries in <code>index</code> which is a <code>LongTensor</code>.</p>
<p>The returned tensor has the same number of dimensions as the original tensor (<code>input</code>). The <code>dim</code>th dimension has the same size as the length of <code>index</code>; other dimensions have the same size as in the original tensor.</p>
<p>Note</p>
<p>The returned tensor does <strong>not</strong> use the same storage as the original tensor. If <code>out</code> has a different shape than expected, we silently change it to the correct shape, reallocating the underlying storage if necessary.</p>
<p>Parameters: </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; the dimension in which we index</li>
<li><strong>index</strong> (<em>LongTensor</em>) &#x2013; the 1-D tensor containing the indices to index</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>x
tensor([[ <span class="hljs-number">0.1427</span>,  <span class="hljs-number">0.0231</span>, <span class="hljs-number">-0.5414</span>, <span class="hljs-number">-1.0009</span>],
 [<span class="hljs-number">-0.4664</span>,  <span class="hljs-number">0.2647</span>, <span class="hljs-number">-0.1228</span>, <span class="hljs-number">-1.1068</span>],
 [<span class="hljs-number">-1.1734</span>, <span class="hljs-number">-0.6571</span>,  <span class="hljs-number">0.7230</span>, <span class="hljs-number">-0.6004</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>indices = torch.tensor([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.index_select(x, <span class="hljs-number">0</span>, indices)
tensor([[ <span class="hljs-number">0.1427</span>,  <span class="hljs-number">0.0231</span>, <span class="hljs-number">-0.5414</span>, <span class="hljs-number">-1.0009</span>],
 [<span class="hljs-number">-1.1734</span>, <span class="hljs-number">-0.6571</span>,  <span class="hljs-number">0.7230</span>, <span class="hljs-number">-0.6004</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.index_select(x, <span class="hljs-number">1</span>, indices)
tensor([[ <span class="hljs-number">0.1427</span>, <span class="hljs-number">-0.5414</span>],
 [<span class="hljs-number">-0.4664</span>, <span class="hljs-number">-0.1228</span>],
 [<span class="hljs-number">-1.1734</span>,  <span class="hljs-number">0.7230</span>]])
</code></pre>
<pre><code class="lang-py">torch.masked_select(input, mask, out=<span class="hljs-keyword">None</span>) &#x2192; Tensor
</code></pre>
<p>Returns a new 1-D tensor which indexes the <code>input</code> tensor according to the binary mask <code>mask</code> which is a <code>ByteTensor</code>.</p>
<p>The shapes of the <code>mask</code> tensor and the <code>input</code> tensor don&#x2019;t need to match, but they must be <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>Note</p>
<p>The returned tensor does <strong>not</strong> use the same storage as the original tensor</p>
<p>Parameters: </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input data</li>
<li><strong>mask</strong> (<a href="tensors.html#torch.ByteTensor" title="torch.ByteTensor"><em>ByteTensor</em></a>) &#x2013; the tensor containing the binary mask to index with</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>x
tensor([[ <span class="hljs-number">0.3552</span>, <span class="hljs-number">-2.3825</span>, <span class="hljs-number">-0.8297</span>,  <span class="hljs-number">0.3477</span>],
 [<span class="hljs-number">-1.2035</span>,  <span class="hljs-number">1.2252</span>,  <span class="hljs-number">0.5002</span>,  <span class="hljs-number">0.6248</span>],
 [ <span class="hljs-number">0.1307</span>, <span class="hljs-number">-2.0608</span>,  <span class="hljs-number">0.1244</span>,  <span class="hljs-number">2.0139</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>mask = x.ge(<span class="hljs-number">0.5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>mask
tensor([[ <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>],
 [ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">1</span>],
 [ <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>]], dtype=torch.uint8)
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.masked_select(x, mask)
tensor([ <span class="hljs-number">1.2252</span>,  <span class="hljs-number">0.5002</span>,  <span class="hljs-number">0.6248</span>,  <span class="hljs-number">2.0139</span>])
</code></pre>
<pre><code class="lang-py">torch.narrow(input, dimension, start, length) &#x2192; Tensor
</code></pre>
<p>Returns a new tensor that is a narrowed version of <code>input</code> tensor. The dimension <code>dim</code> is input from <code>start</code> to <code>start + length</code>. The returned tensor and <code>input</code> tensor share the same underlying storage.</p>
<p>Parameters: </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the tensor to narrow</li>
<li><strong>dimension</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; the dimension along which to narrow</li>
<li><strong>start</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; the starting dimension</li>
<li><strong>length</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; the distance to the ending dimension</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>x = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>], [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.narrow(x, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)
tensor([[ <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>],
 [ <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.narrow(x, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)
tensor([[ <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>],
 [ <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>],
 [ <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>]])
</code></pre>
<pre><code class="lang-py">torch.nonzero(input, out=<span class="hljs-keyword">None</span>) &#x2192; LongTensor
</code></pre>
<p>Returns a tensor containing the indices of all non-zero elements of <code>input</code>. Each row in the result contains the indices of a non-zero element in <code>input</code>.</p>
<p>If <code>input</code> has <code>n</code> dimensions, then the resulting indices tensor <code>out</code> is of size <img src="img/a6282f8c351e427a676fb9abf237e01f.jpg" alt="">, where <img src="img/d132b400654f0a1c0bf2cf921b391c8a.jpg" alt=""> is the total number of non-zero elements in the <code>input</code> tensor.</p>
<p>Parameters: </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor</li>
<li><strong>out</strong> (<em>LongTensor__,</em> <em>optional</em>) &#x2013; the output tensor containing indices</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.nonzero(torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]))
tensor([[ <span class="hljs-number">0</span>],
 [ <span class="hljs-number">1</span>],
 [ <span class="hljs-number">2</span>],
 [ <span class="hljs-number">4</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.nonzero(torch.tensor([[<span class="hljs-number">0.6</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],
 [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],
 [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.2</span>, <span class="hljs-number">0.0</span>],
 [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>,<span class="hljs-number">-0.4</span>]]))
tensor([[ <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>],
 [ <span class="hljs-number">1</span>,  <span class="hljs-number">1</span>],
 [ <span class="hljs-number">2</span>,  <span class="hljs-number">2</span>],
 [ <span class="hljs-number">3</span>,  <span class="hljs-number">3</span>]])
</code></pre>
<pre><code class="lang-py">torch.reshape(input, shape) &#x2192; Tensor
</code></pre>
<p>Returns a tensor with the same data and number of elements as <code>input</code>, but with the specified shape. When possible, the returned tensor will be a view of <code>input</code>. Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior.</p>
<p>See <a href="tensors.html#torch.Tensor.view" title="torch.Tensor.view"><code>torch.Tensor.view()</code></a> on when it is possible to return a view.</p>
<p>A single dimension may be -1, in which case it&#x2019;s inferred from the remaining dimensions and the number of elements in <code>input</code>.</p>
<p>Parameters: </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the tensor to be reshaped</li>
<li><strong>shape</strong> (<em>tuple of python:ints</em>) &#x2013; the new shape</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.arange(<span class="hljs-number">4.</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.reshape(a, (<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))
tensor([[ <span class="hljs-number">0.</span>,  <span class="hljs-number">1.</span>],
 [ <span class="hljs-number">2.</span>,  <span class="hljs-number">3.</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>b = torch.tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.reshape(b, (<span class="hljs-number">-1</span>,))
tensor([ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>])
</code></pre>
<pre><code class="lang-py">torch.split(tensor, split_size_or_sections, dim=<span class="hljs-number">0</span>)
</code></pre>
<p>Splits the tensor into chunks.</p>
<p>If <code>split_size_or_sections</code> is an integer type, then <a href="#torch.tensor" title="torch.tensor"><code>tensor</code></a> will be split into equally sized chunks (if possible). Last chunk will be smaller if the tensor size along the given dimension <code>dim</code> is not divisible by <code>split_size</code>.</p>
<p>If <code>split_size_or_sections</code> is a list, then <a href="#torch.tensor" title="torch.tensor"><code>tensor</code></a> will be split into <code>len(split_size_or_sections)</code> chunks with sizes in <code>dim</code> according to <code>split_size_or_sections</code>.</p>
<p>Parameters: </p>
<ul>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; tensor to split.</li>
<li><strong>split_size_or_sections</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>) or</em> <em>(</em><a href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)" target="_blank"><em>list</em></a><em>(</em><a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>)</em>) &#x2013; size of a single chunk or list of sizes for each chunk</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; dimension along which to split the tensor.</li>
</ul>
<pre><code class="lang-py">torch.squeeze(input, dim=<span class="hljs-keyword">None</span>, out=<span class="hljs-keyword">None</span>) &#x2192; Tensor
</code></pre>
<p>Returns a tensor with all the dimensions of <code>input</code> of size <code>1</code> removed.</p>
<p>For example, if <code>input</code> is of shape: <img src="img/e798b9a45934bd0742372a7daf0d72a4.jpg" alt=""> then the <code>out</code> tensor will be of shape: <img src="img/8ebcff51adfcadbe79dc480e6924d59b.jpg" alt="">.</p>
<p>When <code>dim</code> is given, a squeeze operation is done only in the given dimension. If <code>input</code> is of shape: <img src="img/86382e0be02bdfa9046461356e835f6a.jpg" alt="">, <code>squeeze(input, 0)</code> leaves the tensor unchanged, but <code>squeeze(input, 1)</code> will squeeze the tensor to the shape <img src="img/8f06e0620721d3fc0d796a659d8ecf03.jpg" alt="">.</p>
<p>Note</p>
<p>The returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other.</p>
<p>Parameters: </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; if given, the input will be squeezed only in this dimension</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>x = torch.zeros(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>x.size()
torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>y = torch.squeeze(x)
<span class="hljs-meta">&gt;&gt;&gt; </span>y.size()
torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>y = torch.squeeze(x, <span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>y.size()
torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>y = torch.squeeze(x, <span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>y.size()
torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>])
</code></pre>
<pre><code class="lang-py">torch.stack(seq, dim=<span class="hljs-number">0</span>, out=<span class="hljs-keyword">None</span>) &#x2192; Tensor
</code></pre>
<p>Concatenates sequence of tensors along a new dimension.</p>
<p>All tensors need to be of the same size.</p>
<p>Parameters: </p>
<ul>
<li><strong>seq</strong> (<em>sequence of Tensors</em>) &#x2013; sequence of tensors to concatenate</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; dimension to insert. Has to be between 0 and the number of dimensions of concatenated tensors (inclusive)</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor</li>
</ul>
<pre><code class="lang-py">torch.t(input) &#x2192; Tensor
</code></pre>
<p>Expects <code>input</code> to be a matrix (2-D tensor) and transposes dimensions 0 and 1.</p>
<p>Can be seen as a short-hand function for <code>transpose(input, 0, 1)</code>.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>x
tensor([[ <span class="hljs-number">0.4875</span>,  <span class="hljs-number">0.9158</span>, <span class="hljs-number">-0.5872</span>],
 [ <span class="hljs-number">0.3938</span>, <span class="hljs-number">-0.6929</span>,  <span class="hljs-number">0.6932</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.t(x)
tensor([[ <span class="hljs-number">0.4875</span>,  <span class="hljs-number">0.3938</span>],
 [ <span class="hljs-number">0.9158</span>, <span class="hljs-number">-0.6929</span>],
 [<span class="hljs-number">-0.5872</span>,  <span class="hljs-number">0.6932</span>]])
</code></pre>
<pre><code class="lang-py">torch.take(input, indices) &#x2192; Tensor
</code></pre>
<p>Returns a new tensor with the elements of <code>input</code> at the given indices. The input tensor is treated as if it were viewed as a 1-D tensor. The result takes the same shape as the indices.</p>
<p>Parameters: </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor</li>
<li><strong>indices</strong> (<em>LongTensor</em>) &#x2013; the indices into tensor</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>src = torch.tensor([[<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>],
 [<span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.take(src, torch.tensor([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>]))
tensor([ <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">8</span>])
</code></pre>
<pre><code class="lang-py">torch.transpose(input, dim0, dim1) &#x2192; Tensor
</code></pre>
<p>Returns a tensor that is a transposed version of <code>input</code>. The given dimensions <code>dim0</code> and <code>dim1</code> are swapped.</p>
<p>The resulting <code>out</code> tensor shares it&#x2019;s underlying storage with the <code>input</code> tensor, so changing the content of one would change the content of the other.</p>
<p>Parameters: </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor</li>
<li><strong>dim0</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; the first dimension to be transposed</li>
<li><strong>dim1</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; the second dimension to be transposed</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>x
tensor([[ <span class="hljs-number">1.0028</span>, <span class="hljs-number">-0.9893</span>,  <span class="hljs-number">0.5809</span>],
 [<span class="hljs-number">-0.1669</span>,  <span class="hljs-number">0.7299</span>,  <span class="hljs-number">0.4942</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.transpose(x, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)
tensor([[ <span class="hljs-number">1.0028</span>, <span class="hljs-number">-0.1669</span>],
 [<span class="hljs-number">-0.9893</span>,  <span class="hljs-number">0.7299</span>],
 [ <span class="hljs-number">0.5809</span>,  <span class="hljs-number">0.4942</span>]])
</code></pre>
<pre><code class="lang-py">torch.unbind(tensor, dim=<span class="hljs-number">0</span>) &#x2192; seq
</code></pre>
<p>Removes a tensor dimension.</p>
<p>Returns a tuple of all slices along a given dimension, already without it.</p>
<p>Parameters: </p>
<ul>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the tensor to unbind</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; dimension to remove</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.unbind(torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
<span class="hljs-meta">&gt;&gt;&gt; </span>                           [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>],
<span class="hljs-meta">&gt;&gt;&gt; </span>                           [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]]))
(tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]), tensor([<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]), tensor([<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]))
</code></pre>
<pre><code class="lang-py">torch.unsqueeze(input, dim, out=<span class="hljs-keyword">None</span>) &#x2192; Tensor
</code></pre>
<p>Returns a new tensor with a dimension of size one inserted at the specified position.</p>
<p>The returned tensor shares the same underlying data with this tensor.</p>
<p>A <code>dim</code> value within the range <code>[-input.dim() - 1, input.dim() + 1)</code> can be used. Negative <code>dim</code> will correspond to <a href="#torch.unsqueeze" title="torch.unsqueeze"><code>unsqueeze()</code></a> applied at <code>dim</code> = <code>dim + input.dim() + 1</code>.</p>
<p>Parameters: </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; the index at which to insert the singleton dimension</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor</li>
</ul>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.unsqueeze(x, <span class="hljs-number">0</span>)
tensor([[ <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.unsqueeze(x, <span class="hljs-number">1</span>)
tensor([[ <span class="hljs-number">1</span>],
 [ <span class="hljs-number">2</span>],
 [ <span class="hljs-number">3</span>],
 [ <span class="hljs-number">4</span>]])
</code></pre>
<pre><code class="lang-py">torch.where(condition, x, y) &#x2192; Tensor
</code></pre>
<p>Return a tensor of elements selected from either <code>x</code> or <code>y</code>, depending on <code>condition</code>.</p>
<p>The operation is defined as:</p>
<p><img src="img/731cf2092e0addecd6cde7e99f46763f.jpg" alt=""></p>
<p>Note</p>
<p>The tensors <code>condition</code>, <code>x</code>, <code>y</code> must be <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>Parameters: </p>
<ul>
<li><strong>condition</strong> (<a href="tensors.html#torch.ByteTensor" title="torch.ByteTensor"><em>ByteTensor</em></a>) &#x2013; When True (nonzero), yield x, otherwise yield y</li>
<li><strong>x</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; values selected at indices where <code>condition</code> is <code>True</code></li>
<li><strong>y</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; values selected at indices where <code>condition</code> is <code>False</code></li>
</ul>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>A tensor of shape equal to the broadcasted shape of <code>condition</code>, <code>x</code>, <code>y</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>Return type:</td>
<td><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>y = torch.ones(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>x
tensor([[<span class="hljs-number">-0.4620</span>,  <span class="hljs-number">0.3139</span>],
 [ <span class="hljs-number">0.3898</span>, <span class="hljs-number">-0.7197</span>],
 [ <span class="hljs-number">0.0478</span>, <span class="hljs-number">-0.1657</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.where(x &gt; <span class="hljs-number">0</span>, x, y)
tensor([[ <span class="hljs-number">1.0000</span>,  <span class="hljs-number">0.3139</span>],
 [ <span class="hljs-number">0.3898</span>,  <span class="hljs-number">1.0000</span>],
 [ <span class="hljs-number">0.0478</span>,  <span class="hljs-number">1.0000</span>]])
</code></pre>
<p><hr></p>
<div align="center">
    <p><a href="http://www.apachecn.org" target="_blank"><font face="KaiTi" size="6" color="red">&#x6211;&#x4EEC;&#x4E00;&#x76F4;&#x5728;&#x52AA;&#x529B;</font></a></p>
    <p><a href="https://github.com/apachecn/pytorch-doc-zh/" target="_blank">apachecn/pytorch-doc-zh</a></p>
    <p><iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=watch&amp;count=true&amp;v=2" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=star&amp;count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=fork&amp;count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <a target="_blank" href="shang.qq.com/wpa/qunwpa"><img border="0" src="http://data.apachecn.org/img/logo/ApacheCN-group.png" alt="ML | ApacheCN" title="ML | ApacheCN"></a></p>
</div>
 <div style="text-align:center;margin:0 0 10.5px;">
     <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
     <ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-3565452474788507" data-ad-slot="2543897000">
     </ins>
     <script>(adsbygoogle = window.adsbygoogle || []).push({});</script>

    <script>
      var _hmt = _hmt || [];
      (function() {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91";
        var s = document.getElementsByTagName("script")[0]; 
        s.parentNode.insertBefore(hm, s);
      })();
    </script>

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-102475051-10"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-102475051-10');
    </script>
</div>

<p><meta name="google-site-verification" content="pyo9N70ZWyh8JB43bIu633mhxesJ1IcwWCZlM3jUfFo"></p>
<p><iframe src="https://www.bilibili.com/read/cv2710377" style="display:none"></iframe>
<img src="http://t.cn/AiCoDHwb" hidden="hidden"></p>
<div>
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
    <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script>
    <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
    <div id="gitalk-container"></div>
    <script type="text/javascript">
        const gitalk = new Gitalk({
        clientID: '2e62dee5b9896e2eede6',
        clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53',
        repo: 'pytorch-doc-zh',
        owner: 'apachecn',
        admin: ['jiangzhonglian', 'wizardforcel'],
        id: md5(location.pathname),
        distractionFreeMode: false
        })
        gitalk.render('gitalk-container')
    </script>
</div>

<footer class="page-footer"><span class="copyright">Copyright &#xA9; ibooker.org.cn 2019 all right reserved&#xFF0C;&#x7531; ApacheCN &#x56E2;&#x961F;&#x63D0;&#x4F9B;&#x652F;&#x6301;</span><span class="footer-modification">&#x8BE5;&#x6587;&#x4EF6;&#x4FEE;&#x8BA2;&#x65F6;&#x95F4;&#xFF1A; 
2019-12-27 08:05:23
</span></footer>
<script>console.log("plugin-popup....");document.onclick = function(e){ e.target.tagName === "IMG" && window.open(e.target.src,e.target.src)}</script><style>img{cursor:pointer}</style>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="torch.html" class="navigation navigation-prev " aria-label="Previous page: torch">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="torch_random_sampling.html" class="navigation navigation-next " aria-label="Next page: Random sampling">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Tensors","level":"1.3.2.1.1","depth":4,"next":{"title":"Random sampling","level":"1.3.2.1.2","depth":4,"path":"torch_random_sampling.md","ref":"torch_random_sampling.md","articles":[]},"previous":{"title":"torch","level":"1.3.2.1","depth":3,"path":"torch.md","ref":"torch.md","articles":[{"title":"Tensors","level":"1.3.2.1.1","depth":4,"path":"torch_tensors.md","ref":"torch_tensors.md","articles":[]},{"title":"Random sampling","level":"1.3.2.1.2","depth":4,"path":"torch_random_sampling.md","ref":"torch_random_sampling.md","articles":[]},{"title":"Serialization, Parallelism, Utilities","level":"1.3.2.1.3","depth":4,"path":"torch_serialization_parallelism_utilities.md","ref":"torch_serialization_parallelism_utilities.md","articles":[]},{"title":"Math operations","level":"1.3.2.1.4","depth":4,"path":"torch_math_operations.md","ref":"torch_math_operations.md","articles":[{"title":"Pointwise Ops","level":"1.3.2.1.4.1","depth":5,"path":"torch_math_operations_pointwise_ops.md","ref":"torch_math_operations_pointwise_ops.md","articles":[]},{"title":"Reduction Ops","level":"1.3.2.1.4.2","depth":5,"path":"torch_math_operations_reduction_ops.md","ref":"torch_math_operations_reduction_ops.md","articles":[]},{"title":"Comparison Ops","level":"1.3.2.1.4.3","depth":5,"path":"torch_math_operations_comparison_ops.md","ref":"torch_math_operations_comparison_ops.md","articles":[]},{"title":"Spectral Ops","level":"1.3.2.1.4.4","depth":5,"path":"torch_math_operations_spectral_ops.md","ref":"torch_math_operations_spectral_ops.md","articles":[]},{"title":"Other Operations","level":"1.3.2.1.4.5","depth":5,"path":"torch_math_operations_other_ops.md","ref":"torch_math_operations_other_ops.md","articles":[]},{"title":"BLAS and LAPACK Operations","level":"1.3.2.1.4.6","depth":5,"path":"torch_math_operations_blas_lapack_ops.md","ref":"torch_math_operations_blas_lapack_ops.md","articles":[]}]}]},"dir":"ltr"},"config":{"plugins":["github","github-buttons","-sharing","insert-logo","sharing-plus","back-to-top-button","code","copy-code-button","mathjax","pageview-count","edit-link","emphasize","alerts","auto-scroll-table","popup","hide-element","page-toc-button","tbfed-pagefooter","sitemap","advanced-emoji","expandable-chapters","splitter","search-pro"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright &copy ibooker.org.cn 2019","modify_label":"该文件修订时间： ","modify_format":"YYYY-MM-DD HH:mm:ss"},"emphasize":{},"github":{"url":"https://github.com/apachecn/pytorch-doc-zh"},"splitter":{},"search-pro":{},"search":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"auto-scroll-table":{},"popup":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"code":{"copyButtons":true},"hide-element":{"elements":[".gitbook-link"]},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"sitemap":{"hostname":"http://pytorch.apachecn.org"},"page-toc-button":{"maxTocDepth":4,"minTocSize":4},"back-to-top-button":{},"pageview-count":{},"alerts":{},"github-buttons":{"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"copy-code-button":{},"advanced-emoji":{"embedEmojis":false},"sharing":{"qq":false,"all":["qq","douban","facebook","google","linkedin","twitter","weibo","whatsapp"],"douban":false,"facebook":false,"weibo":true,"whatsapp":false,"twitter":false,"line":false,"google":false,"qzone":true},"edit-link":{"label":"编辑本页","base":"https://github.com/apachecn/pytorch-doc-zh/blob/master/docs/1.0"},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"insert-logo":{"style":"background: none; max-height: 150px; min-height: 150px","url":"http://data.apachecn.org/img/logo.jpg"},"expandable-chapters":{}},"my_links":{"sidebar":{"Home":"https://www.baidu.com"}},"theme":"default","author":"ApacheCN","my_plugins":["donate","todo","-lunr","-search","expandable-chapters-small","chapter-fold","expandable-chapters","expandable-chapters-small","back-to-top-button","ga","baidu","sitemap","tbfed-pagefooter","advanced-emoji","sectionx","page-treeview","simple-page-toc","ancre-navigation","theme-apachecn@git+https://github.com/apachecn/theme-apachecn#HEAD","pagefooter-apachecn@git+https://github.com/apachecn/gitbook-plugin-pagefooter-apachecn#HEAD"],"my_pluginsConfig":{"page-treeview":{"copyright":"Copyright &#169; aleen42","minHeaderCount":"2","minHeaderDeep":"2"},"ignores":["node_modules"],"simple-page-toc":{"maxDepth":3,"skipFirstH1":true},"page-copyright":{"wisdom":"Designer, Frontend Developer & overall web enthusiast","noPowered":false,"copyright":"Copyright &#169; 你的名字","style":"normal","timeColor":"#666","utcOffset":"8","format":"YYYY-MM-dd hh:mm:ss","signature":"你的签名","copyrightColor":"#666","description":"modified at"},"donate":{"wechat":"微信收款的二维码URL","alipay":"支付宝收款的二维码URL","title":"","button":"赏","alipayText":"支付宝打赏","wechatText":"微信打赏"},"page-toc-button":{"maxTocDepth":2,"minTocSize":2},"github-buttons":{"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"},{"user":"apachecn","width":"160","type":"follow","count":true,"size":"small"}]},"ga":{"token":"UA-102475051-10"},"baidu":{"token":"75439e2cbd22bdd813226000e9dcc12f"},"pagefooter-apachecn":{"copyright":"Copyright &copy ibooker.org.cn 2019","modify_label":"该文件修订时间： ","modify_format":"YYYY-MM-DD HH:mm:ss"}},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"Pytorch 中文文档","language":"zh-hans","gitbook":"*","description":"Pytorch 中文文档: 教程和文档"},"file":{"path":"torch_tensors.md","mtime":"2019-12-27T08:05:23.763Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2019-12-27T08:07:43.252Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-insert-logo/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-code/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-copy-code-button/toggle.js"></script>
        
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-edit-link/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-alerts/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-auto-scroll-table/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-hide-element/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-page-toc-button/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search-pro/jquery.mark.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search-pro/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

