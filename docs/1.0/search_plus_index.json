{"./":{"url":"./","title":"Introduction","keywords":"","body":"PyTorch 1.0 中文文档 & 教程 PyTorch 是一个针对深度学习, 并且使用 GPU 和 CPU 来优化的 tensor library (张量库) 1.0 中文版本 最新 英文教程 最新 英文文档 0.4 中文版本 0.3 中文版本 0.2 中文版本 欢迎任何人参与和完善：一个人可以走的很快，但是一群人却可以走的更远。 在线阅读 ApacheCN 学习资源 PyTorch 中文翻译组 | ApacheCN 713436582 目录结构 Introduction 中文教程 起步 PyTorch 深度学习: 60 分钟极速入门 什么是 PyTorch？ Autograd：自动求导 神经网络 训练分类器 可选：数据并行处理 数据加载和处理教程 用例子学习 PyTorch 迁移学习教程 混合前端的 seq2seq 模型部署 Saving and Loading Models What is torch.nn really? 图像 Torchvision 模型微调 空间变换器网络教程 使用 PyTorch 进行图像风格转换 对抗性示例生成 使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端 文本 聊天机器人教程 使用字符级别特征的 RNN 网络生成姓氏 使用字符级别特征的 RNN 网络进行姓氏分类 Deep Learning for NLP with Pytorch PyTorch 介绍 使用 PyTorch 进行深度学习 Word Embeddings: Encoding Lexical Semantics 序列模型和 LSTM 网络 Advanced: Making Dynamic Decisions and the Bi-LSTM CRF 基于注意力机制的 seq2seq 神经网络翻译 生成 DCGAN Tutorial 强化学习 Reinforcement Learning (DQN) Tutorial 扩展 PyTorch 用 numpy 和 scipy 创建扩展 Custom C++ and CUDA Extensions Extending TorchScript with Custom C++ Operators 生产性使用 Writing Distributed Applications with PyTorch 使用 Amazon AWS 进行分布式训练 ONNX 现场演示教程 在 C++ 中加载 PYTORCH 模型 其它语言中的 PyTorch 使用 PyTorch C++ 前端 中文文档 注解 自动求导机制 广播语义 CUDA 语义 Extending PyTorch Frequently Asked Questions Multiprocessing best practices Reproducibility Serialization semantics Windows FAQ 包参考 torch Tensors Random sampling Serialization, Parallelism, Utilities Math operations Pointwise Ops Reduction Ops Comparison Ops Spectral Ops Other Operations BLAS and LAPACK Operations torch.Tensor Tensor Attributes 数据类型信息 torch.sparse torch.cuda torch.Storage torch.nn torch.nn.functional torch.nn.init torch.optim Automatic differentiation package - torch.autograd Distributed communication package - torch.distributed Probability distributions - torch.distributions Torch Script 多进程包 - torch.multiprocessing torch.utils.bottleneck torch.utils.checkpoint torch.utils.cpp_extension torch.utils.data torch.utils.dlpack torch.hub torch.utils.model_zoo torch.onnx Distributed communication package (deprecated) - torch.distributed.deprecated torchvision 参考 torchvision.datasets torchvision.models torchvision.transforms torchvision.utils 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"tut_getting_started.html":{"url":"tut_getting_started.html","title":"起步","keywords":"","body":"起步 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"deep_learning_60min_blitz.html":{"url":"deep_learning_60min_blitz.html","title":"PyTorch 深度学习: 60 分钟极速入门","keywords":"","body":"PyTorch 深度学习: 60 分钟极速入门 译者：bat67 校对者：FontTian 作者：Soumith Chintala 此教程的目标： 更高层次地理解PyTorch的Tensor库以及神经网络。 训练一个小的神经网络模型用于分类图像。 本教程假设读者对numpy有基本的了解 注意:确保你安装了 torch 和 torchvision 包。 PyTorch 是什么？ Autograd：自动求导 神经网络 训练分类器 可选：数据并行 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"blitz_tensor_tutorial.html":{"url":"blitz_tensor_tutorial.html","title":"什么是 PyTorch？","keywords":"","body":"什么是PyTorch？ 译者：bat67 校对者：FontTian 作者： Soumith Chintala PyTorch是一个基于python的科学计算包，主要针对两类人群： 作为NumPy的替代品，可以利用GPU的性能进行计算 作为一个高灵活性、速度快的深度学习平台 入门 张量 Tensor（张量）类似于NumPy的ndarray，但还可以在GPU上使用来加速计算。 from __future__ import print_function import torch 创建一个没有初始化的5*3矩阵： x = torch.empty(5, 3) print(x) 输出： tensor([[2.2391e-19, 4.5869e-41, 1.4191e-17], [4.5869e-41, 0.0000e+00, 0.0000e+00], [0.0000e+00, 0.0000e+00, 0.0000e+00], [0.0000e+00, 0.0000e+00, 0.0000e+00], [0.0000e+00, 0.0000e+00, 0.0000e+00]]) 创建一个随机初始化矩阵： x = torch.rand(5, 3) print(x) 输出： tensor([[0.5307, 0.9752, 0.5376], [0.2789, 0.7219, 0.1254], [0.6700, 0.6100, 0.3484], [0.0922, 0.0779, 0.2446], [0.2967, 0.9481, 0.1311]]) 构造一个填满0且数据类型为long的矩阵: x = torch.zeros(5, 3, dtype=torch.long) print(x) 输出： tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]) 直接从数据构造张量： x = torch.tensor([5.5, 3]) print(x) 输出： tensor([5.5000, 3.0000]) 或者根据已有的tensor建立新的tensor。除非用户提供新的值，否则这些方法将重用输入张量的属性，例如dtype等： x = x.new_ones(5, 3, dtype=torch.double) # new_* methods take in sizes print(x) x = torch.randn_like(x, dtype=torch.float) # 重载 dtype! print(x) # 结果size一致 输出： tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], dtype=torch.float64) tensor([[ 1.6040, -0.6769, 0.0555], [ 0.6273, 0.7683, -0.2838], [-0.7159, -0.5566, -0.2020], [ 0.6266, 0.3566, 1.4497], [-0.8092, -0.6741, 0.0406]]) 获取张量的形状： print(x.size()) 输出： torch.Size([5, 3]) 注意： torch.Size本质上还是tuple，所以支持tuple的一切操作。 运算 一种运算有多种语法。在下面的示例中，我们将研究加法运算。 加法：形式一 y = torch.rand(5, 3) print(x + y) 输出： tensor([[ 2.5541, 0.0943, 0.9835], [ 1.4911, 1.3117, 0.5220], [-0.0078, -0.1161, 0.6687], [ 0.8176, 1.1179, 1.9194], [-0.3251, -0.2236, 0.7653]]) 加法：形式二 print(torch.add(x, y)) 输出： tensor([[ 2.5541, 0.0943, 0.9835], [ 1.4911, 1.3117, 0.5220], [-0.0078, -0.1161, 0.6687], [ 0.8176, 1.1179, 1.9194], [-0.3251, -0.2236, 0.7653]]) 加法：给定一个输出张量作为参数 result = torch.empty(5, 3) torch.add(x, y, out=result) print(result) 输出： tensor([[ 2.5541, 0.0943, 0.9835], [ 1.4911, 1.3117, 0.5220], [-0.0078, -0.1161, 0.6687], [ 0.8176, 1.1179, 1.9194], [-0.3251, -0.2236, 0.7653]]) 加法：原位/原地操作（in-place） # adds x to y y.add_(x) print(y) 输出： tensor([[ 2.5541, 0.0943, 0.9835], [ 1.4911, 1.3117, 0.5220], [-0.0078, -0.1161, 0.6687], [ 0.8176, 1.1179, 1.9194], [-0.3251, -0.2236, 0.7653]]) 注意： 任何一个in-place改变张量的操作后面都固定一个_。例如x.copy_(y)、x.t_()将更改x 也可以使用像标准的NumPy一样的各种索引操作： print(x[:, 1]) 输出： tensor([-0.6769, 0.7683, -0.5566, 0.3566, -0.6741]) 改变形状：如果想改变形状，可以使用torch.view x = torch.randn(4, 4) y = x.view(16) z = x.view(-1, 8) # the size -1 is inferred from other dimensions print(x.size(), y.size(), z.size()) 输出： torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) 如果是仅包含一个元素的tensor，可以使用.item()来得到对应的python数值 x = torch.randn(1) print(x) print(x.item()) 输出： tensor([0.0445]) 0.0445479191839695 后续阅读： 超过100种tensor的运算操作，包括转置，索引，切片，数学运算， 线性代数，随机数等，具体访问这里 桥接 NumPy 将一个Torch张量转换为一个NumPy数组是轻而易举的事情，反之亦然。 Torch张量和NumPy数组将共享它们的底层内存位置，因此当一个改变时,另外也会改变。 将torch的Tensor转化为NumPy数组 输入： a = torch.ones(5) print(a) 输出： tensor([1., 1., 1., 1., 1.]) 输入： b = a.numpy() print(b) 输出： [1. 1. 1. 1. 1.] 看NumPy数组是如何改变里面的值的： a.add_(1) print(a) print(b) 输出： tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.] 将NumPy数组转化为Torch张量 看改变NumPy数组是如何自动改变Torch张量的： import numpy as np a = np.ones(5) b = torch.from_numpy(a) np.add(a, 1, out=a) print(a) print(b) 输出： [2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64) CPU上的所有张量(CharTensor除外)都支持与Numpy的相互转换。 CUDA上的张量 张量可以使用.to方法移动到任何设备（device）上： # 当GPU可用时,我们可以运行以下代码 # 我们将使用`torch.device`来将tensor移入和移出GPU if torch.cuda.is_available(): device = torch.device(\"cuda\") # a CUDA device object y = torch.ones_like(x, device=device) # 直接在GPU上创建tensor x = x.to(device) # 或者使用`.to(\"cuda\")`方法 z = x + y print(z) print(z.to(\"cpu\", torch.double)) # `.to`也能在移动时改变dtype 输出： tensor([1.0445], device='cuda:0') tensor([1.0445], dtype=torch.float64) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"blitz_autograd_tutorial.html":{"url":"blitz_autograd_tutorial.html","title":"Autograd：自动求导","keywords":"","body":"Autograd：自动求导 译者：bat67 校对者：FontTian PyTorch中，所有神经网络的核心是 autograd 包。先简单介绍一下这个包，然后训练我们的第一个的神经网络。 autograd 包为张量上的所有操作提供了自动求导机制。它是一个在运行时定义（define-by-run）的框架，这意味着反向传播是根据代码如何运行来决定的，并且每次迭代可以是不同的. 让我们用一些简单的例子来看看吧。 张量 torch.Tensor 是这个包的核心类。如果设置它的属性 .requires_grad 为 True，那么它将会追踪对于该张量的所有操作。当完成计算后可以通过调用 .backward()，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到.grad属性. 要阻止一个张量被跟踪历史，可以调用 .detach() 方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。 为了防止跟踪历史记录（和使用内存），可以将代码块包装在 with torch.no_grad(): 中。在评估模型时特别有用，因为模型可能具有 requires_grad = True 的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。 还有一个类对于autograd的实现非常重要：Function。 Tensor 和 Function 互相连接生成了一个无圈图(acyclic graph)，它编码了完整的计算历史。每个张量都有一个 .grad_fn 属性，该属性引用了创建 Tensor 自身的Function（除非这个张量是用户手动创建的，即这个张量的 grad_fn 是 None ）。 如果需要计算导数，可以在 Tensor 上调用 .backward()。如果 Tensor 是一个标量（即它包含一个元素的数据），则不需要为 backward() 指定任何参数，但是如果它有更多的元素，则需要指定一个 gradient 参数，该参数是形状匹配的张量。 import torch 创建一个张量并设置requires_grad=True用来追踪其计算历史 x = torch.ones(2, 2, requires_grad=True) print(x) 输出： tensor([[1., 1.], [1., 1.]], requires_grad=True) 对这个张量做一次运算： y = x + 2 print(y) 输出： tensor([[3., 3.], [3., 3.]], grad_fn=) y是计算的结果，所以它有grad_fn属性。 print(y.grad_fn) 输出： 对y进行更多操作 z = y * y * 3 out = z.mean() print(z, out) 输出： tensor([[27., 27.], [27., 27.]], grad_fn=) tensor(27., grad_fn=) .requires_grad_(...) 原地改变了现有张量的 requires_grad 标志。如果没有指定的话，默认输入的这个标志是 False。 a = torch.randn(2, 2) a = ((a * 3) / (a - 1)) print(a.requires_grad) a.requires_grad_(True) print(a.requires_grad) b = (a * a).sum() print(b.grad_fn) 输出： False True 梯度 现在开始进行反向传播，因为 out 是一个标量，因此 out.backward() 和 out.backward(torch.tensor(1.)) 等价。 out.backward() 输出导数 d(out)/dx print(x.grad) 输出： tensor([[4.5000, 4.5000], [4.5000, 4.5000]]) 我们的得到的是一个数取值全部为4.5的矩阵。 让我们来调用 out 张量 “o”。 就可以得到 o = \\frac{1}{4}\\sum_i z_i，z_i = 3(x_i+2)^2 和 z_i\\bigr\\rvert_{x_i=1} = 27 因此, \\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)，因而 \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5。 数学上，若有向量值函数 \\vec{y}=f(\\vec{x})，那么 \\vec{y} 相对于 \\vec{x} 的梯度是一个雅可比矩阵： J=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right) 通常来说，torch.autograd 是计算雅可比向量积的一个“引擎”。也就是说，给定任意向量 v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T}，计算乘积 v^{T}\\cdot J。如果 v 恰好是一个标量函数 l=g\\left(\\vec{y}\\right) 的导数，即 v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}，那么根据链式法则，雅可比向量积应该是 l 对 \\vec{x} 的导数： J^{T}\\cdot v=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial y_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial y_{m}} \\end{array}\\right)=\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial x_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial x_{n}} \\end{array}\\right) （注意：行向量的 v^{T}\\cdot J也可以被视作列向量的J^{T}\\cdot v) 雅可比向量积的这一特性使得将外部梯度输入到具有非标量输出的模型中变得非常方便。 现在我们来看一个雅可比向量积的例子: x = torch.randn(3, requires_grad=True) y = x * 2 while y.data.norm() 输出： tensor([-278.6740, 935.4016, 439.6572], grad_fn=) 在这种情况下，y 不再是标量。torch.autograd 不能直接计算完整的雅可比矩阵，但是如果我们只想要雅可比向量积，只需将这个向量作为参数传给 backward： v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float) y.backward(v) print(x.grad) 输出： tensor([4.0960e+02, 4.0960e+03, 4.0960e-01]) 也可以通过将代码块包装在 with torch.no_grad(): 中，来阻止autograd跟踪设置了 .requires_grad=True 的张量的历史记录。 print(x.requires_grad) print((x ** 2).requires_grad) with torch.no_grad(): print((x ** 2).requires_grad) 输出： True True False 后续阅读： autograd 和 Function 的文档见：https://pytorch.org/docs/autograd 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"blitz_neural_networks_tutorial.html":{"url":"blitz_neural_networks_tutorial.html","title":"神经网络","keywords":"","body":"神经网络 译者：bat67 校对者：FontTian 可以使用torch.nn包来构建神经网络. 我们已经介绍了autograd，nn包则依赖于autograd包来定义模型并对它们求导。一个nn.Module包含各个层和一个forward(input)方法，该方法返回output。 例如，下面这个神经网络可以对数字进行分类： 这是一个简单的前馈神经网络（feed-forward network）。它接受一个输入，然后将它送入下一层，一层接一层的传递，最后给出输出。 一个神经网络的典型训练过程如下： 定义包含一些可学习参数（或者叫权重）的神经网络 在输入数据集上迭代 通过网络处理输入 计算损失（输出和正确答案的距离） 将梯度反向传播给网络的参数 更新网络的权重，一般使用一个简单的规则：weight = weight - learning_rate * gradient 定义网络 让我们定义这样一个网络： import torch import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net, self).__init__() # 输入图像channel：1；输出channel：6；5x5卷积核 self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # an affine operation: y = Wx + b self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # 2x2 Max pooling x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # 如果是方阵,则可以只使用一个数字进行定义 x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # 除去批处理维度的其他所有维度 num_features = 1 for s in size: num_features *= s return num_features net = Net() print(net) 输出： Net( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) 我们只需要定义 forward 函数，backward函数会在使用autograd时自动定义，backward函数用来计算导数。可以在 forward 函数中使用任何针对张量的操作和计算。 一个模型的可学习参数可以通过net.parameters()返回 params = list(net.parameters()) print(len(params)) print(params[0].size()) # conv1's .weight 输出： 10 torch.Size([6, 1, 5, 5]) 让我们尝试一个随机的32x32的输入。注意:这个网络（LeNet）的期待输入是32x32。如果使用MNIST数据集来训练这个网络，要把图片大小重新调整到32x32。 input = torch.randn(1, 1, 32, 32) out = net(input) print(out) 输出： tensor([[ 0.0399, -0.0856, 0.0668, 0.0915, 0.0453, -0.0680, -0.1024, 0.0493, -0.1043, -0.1267]], grad_fn=) 清零所有参数的梯度缓存，然后进行随机梯度的反向传播： net.zero_grad() out.backward(torch.randn(1, 10)) 注意： torch.nn只支持小批量处理（mini-batches）。整个torch.nn包只支持小批量样本的输入，不支持单个样本。 比如，nn.Conv2d 接受一个4维的张量，即nSamples x nChannels x Height x Width 如果是一个单独的样本，只需要使用input.unsqueeze(0)来添加一个“假的”批大小维度。 在继续之前，让我们回顾一下到目前为止看到的所有类。 复习： torch.Tensor - 一个多维数组，支持诸如backward()等的自动求导操作，同时也保存了张量的梯度。 nn.Module - 神经网络模块。是一种方便封装参数的方式，具有将参数移动到GPU、导出、加载等功能。 nn.Parameter - 张量的一种，当它作为一个属性分配给一个Module时，它会被自动注册为一个参数。 autograd.Function - 实现了自动求导前向和反向传播的定义，每个Tensor至少创建一个Function节点，该节点连接到创建Tensor的函数并对其历史进行编码。 目前为止，我们讨论了： 定义一个神经网络 处理输入调用backward 还剩下： 计算损失 更新网络权重 损失函数 一个损失函数接受一对(output, target)作为输入，计算一个值来估计网络的输出和目标值相差多少。 nn包中有很多不同的损失函数。nn.MSELoss是比较简单的一种，它计算输出和目标的均方误差（mean-squared error）。 例如： output = net(input) target = torch.randn(10) # 本例子中使用模拟数据 target = target.view(1, -1) # 使目标值与数据值形状一致 criterion = nn.MSELoss() loss = criterion(output, target) print(loss) 输出： tensor(1.0263, grad_fn=) 现在，如果使用loss的.grad_fn属性跟踪反向传播过程，会看到计算图如下： input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> view -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss 所以，当我们调用loss.backward()，整张图开始关于loss微分，图中所有设置了requires_grad=True的张量的.grad属性累积着梯度张量。 为了说明这一点，让我们向后跟踪几步： print(loss.grad_fn) # MSELoss print(loss.grad_fn.next_functions[0][0]) # Linear print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # ReLU 输出： 反向传播 我们只需要调用loss.backward()来反向传播权重。我们需要清零现有的梯度，否则梯度将会与已有的梯度累加。 现在，我们将调用loss.backward()，并查看conv1层的偏置（bias）在反向传播前后的梯度。 net.zero_grad() # 清零所有参数（parameter）的梯度缓存 print('conv1.bias.grad before backward') print(net.conv1.bias.grad) loss.backward() print('conv1.bias.grad after backward') print(net.conv1.bias.grad) 输出： conv1.bias.grad before backward tensor([0., 0., 0., 0., 0., 0.]) conv1.bias.grad after backward tensor([ 0.0084, 0.0019, -0.0179, -0.0212, 0.0067, -0.0096]) 现在，我们已经见到了如何使用损失函数。 稍后阅读 神经网络包包含了各种模块和损失函数，这些模块和损失函数构成了深度神经网络的构建模块。完整的文档列表见这里。 现在唯一要学习的是： 更新网络的权重 更新权重 最简单的更新规则是随机梯度下降法（SGD）: weight = weight - learning_rate * gradient 我们可以使用简单的python代码来实现: learning_rate = 0.01 for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) 然而，在使用神经网络时，可能希望使用各种不同的更新规则，如SGD、Nesterov-SGD、Adam、RMSProp等。为此，我们构建了一个较小的包torch.optim，它实现了所有的这些方法。使用它很简单： import torch.optim as optim # 创建优化器（optimizer） optimizer = optim.SGD(net.parameters(), lr=0.01) # 在训练的迭代中： optimizer.zero_grad() # 清零梯度缓存 output = net(input) loss = criterion(output, target) loss.backward() optimizer.step() # 更新参数 注意： 观察梯度缓存区是如何使用optimizer.zero_grad()手动清零的。这是因为梯度是累加的，正如前面反向传播章节叙述的那样。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"blitz_cifar10_tutorial.html":{"url":"blitz_cifar10_tutorial.html","title":"训练分类器","keywords":"","body":"训练分类器 译者：bat67 校对者：FontTian 目前为止，我们以及看到了如何定义网络，计算损失，并更新网络的权重。所以你现在可能会想, 数据应该怎么办呢？ 通常来说，当必须处理图像、文本、音频或视频数据时，可以使用python标准库将数据加载到numpy数组里。然后将这个数组转化成torch.*Tensor。 对于图片，有Pillow，OpenCV等包可以使用 对于音频，有scipy和librosa等包可以使用 对于文本，不管是原生python的或者是基于Cython的文本，可以使用NLTK和SpaCy 特别对于视觉方面，我们创建了一个包，名字叫torchvision，其中包含了针对Imagenet、CIFAR10、MNIST等常用数据集的数据加载器（data loaders），还有对图片数据变形的操作，即torchvision.datasets和torch.utils.data.DataLoader。 这提供了极大的便利，可以避免编写样板代码。 在这个教程中，我们将使用CIFAR10数据集，它有如下的分类：“飞机”，“汽车”，“鸟”，“猫”，“鹿”，“狗”，“青蛙”，“马”，“船”，“卡车”等。在CIFAR-10里面的图片数据大小是3x32x32，即三通道彩色图，图片大小是32x32像素。 训练一个图片分类器 我们将按顺序做以下步骤： 通过torchvision加载CIFAR10里面的训练和测试数据集，并对数据进行标准化 定义卷积神经网络 定义损失函数 利用训练数据训练网络 利用测试数据测试网络 1.加载并标准化CIFAR10 使用torchvision加载CIFAR10超级简单。 import torch import torchvision import torchvision.transforms as transforms torchvision数据集加载完后的输出是范围在[0, 1]之间的PILImage。我们将其标准化为范围在[-1, 1]之间的张量。 transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2) classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') 输出： Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz Files already downloaded and verified 乐趣所致，现在让我们可视化部分训练数据。 import matplotlib.pyplot as plt import numpy as np # 输出图像的函数 def imshow(img): img = img / 2 + 0.5 # unnormalize npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) plt.show() # 随机获取训练图片 dataiter = iter(trainloader) images, labels = dataiter.next() # 显示图片 imshow(torchvision.utils.make_grid(images)) # 打印图片标签 print(' '.join('%5s' % classes[labels[j]] for j in range(4))) 输出： horse horse horse car 2.定义卷积神经网络 将之前神经网络章节定义的神经网络拿过来，并将其修改成输入为3通道图像（替代原来定义的单通道图像）。 import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() 3.定义损失函数和优化器 我们使用分类的交叉熵损失和随机梯度下降（使用momentum）。 import torch.optim as optim criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) 4.训练网络 事情开始变得有趣了。我们只需要遍历我们的数据迭代器，并将输入“喂”给网络和优化函数。 for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print('Finished Training') 输出： [1, 2000] loss: 2.182 [1, 4000] loss: 1.819 [1, 6000] loss: 1.648 [1, 8000] loss: 1.569 [1, 10000] loss: 1.511 [1, 12000] loss: 1.473 [2, 2000] loss: 1.414 [2, 4000] loss: 1.365 [2, 6000] loss: 1.358 [2, 8000] loss: 1.322 [2, 10000] loss: 1.298 [2, 12000] loss: 1.282 Finished Training 5.使用测试数据测试网络 我们已经在训练集上训练了2遍网络。但是我们需要检查网络是否学到了一些东西。 我们将通过预测神经网络输出的标签来检查这个问题，并和正确样本进行（ground-truth）对比。如果预测是正确的，我们将样本添加到正确预测的列表中。 ok，第一步。让我们显示测试集中的图像来熟悉一下。 dataiter = iter(testloader) images, labels = dataiter.next() # 输出图片 imshow(torchvision.utils.make_grid(images)) print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4))) GroundTruth: cat ship ship plane ok，现在让我们看看神经网络认为上面的例子是: outputs = net(images) 输出是10个类别的量值。一个类的值越高，网络就越认为这个图像属于这个特定的类。让我们得到最高量值的下标/索引； _, predicted = torch.max(outputs, 1) print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(4))) 输出： Predicted: dog ship ship plane 结果还不错。 让我们看看网络在整个数据集上表现的怎么样。 correct = 0 total = 0 with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print('Accuracy of the network on the 10000 test images: %d %%' % ( 100 * correct / total)) 输出： Accuracy of the network on the 10000 test images: 55 % 这比随机选取（即从10个类中随机选择一个类，正确率是10%）要好很多。看来网络确实学到了一些东西。 那么哪些是表现好的类呢？哪些是表现的差的类呢？ class_correct = list(0. for i in range(10)) class_total = list(0. for i in range(10)) with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs, 1) c = (predicted == labels).squeeze() for i in range(4): label = labels[i] class_correct[label] += c[i].item() class_total[label] += 1 for i in range(10): print('Accuracy of %5s : %2d %%' % ( classes[i], 100 * class_correct[i] / class_total[i])) 输出： Accuracy of plane : 70 % Accuracy of car : 70 % Accuracy of bird : 28 % Accuracy of cat : 25 % Accuracy of deer : 37 % Accuracy of dog : 60 % Accuracy of frog : 66 % Accuracy of horse : 62 % Accuracy of ship : 69 % Accuracy of truck : 61 % ok，接下来呢？ 怎么在GPU上运行神经网络呢？ 在GPU上训练 与将一个张量传递给GPU一样，可以这样将神经网络转移到GPU上。 如果我们有cuda可用的话，让我们首先定义第一个设备为可见cuda设备： device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Assuming that we are on a CUDA machine, this should print a CUDA device: print(device) 输出： cuda:0 本节的其余部分假设device是CUDA。 然后这些方法将递归遍历所有模块，并将它们的参数和缓冲区转换为CUDA张量： net.to(device) 请记住，我们不得不将输入和目标在每一步都送入GPU： inputs, labels = inputs.to(device), labels.to(device) 为什么我们感受不到与CPU相比的巨大加速？因为我们的网络实在是太小了。 尝试一下：加宽你的网络（注意第一个nn.Conv2d的第二个参数和第二个nn.Conv2d的第一个参数要相同），看看能获得多少加速。 已实现的目标： 在更高层次上理解PyTorch的Tensor库和神经网络 训练一个小的神经网络做图片分类 在多GPU上训练 如果希望使用您所有GPU获得更大的加速，请查看Optional: Data Parallelism。 接下来要做什么？ Train neural nets to play video games Train a state-of-the-art ResNet network on imagenet Train a face generator using Generative Adversarial Networks Train a word-level language model using Recurrent LSTM networks More examples More tutorials Discuss PyTorch on the Forums Chat with other users on Slack 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"blitz_data_parallel_tutorial.html":{"url":"blitz_data_parallel_tutorial.html","title":"可选：数据并行处理","keywords":"","body":"可选: 数据并行处理 作者: Sung Kim Jenny Kang 译者: bat67 校对者: FontTian 片刻 在这个教程里，我们将学习如何使用数据并行（DataParallel）来使用多GPU。 PyTorch非常容易的就可以使用GPU，可以用如下方式把一个模型放到GPU上: device = torch.device(\"cuda: 0\") model.to(device) 然后可以复制所有的张量到GPU上: mytensor = my_tensor.to(device) 请注意，调用my_tensor.to(device)返回一个GPU上的my_tensor副本，而不是重写my_tensor。我们需要把它赋值给一个新的张量并在GPU上使用这个张量。 在多GPU上执行前向和反向传播是自然而然的事。然而，PyTorch默认将只是用一个GPU。你可以使用DataParallel让模型并行运行来轻易的让你的操作在多个GPU上运行。 model = nn.DataParallel(model) 这是这篇教程背后的核心，我们接下来将更详细的介绍它。 导入和参数 导入PyTorch模块和定义参数。 import torch import torch.nn as nn from torch.utils.data import Dataset, DataLoader # Parameters 和 DataLoaders input_size = 5 output_size = 2 batch_size = 30 data_size = 100 设备（Device）: device = torch.device(\"cuda: 0\" if torch.cuda.is_available() else \"cpu\") 虚拟数据集 要制作一个虚拟（随机）数据集，只需实现__getitem__。 class RandomDataset(Dataset): def __init__(self, size, length): self.len = length self.data = torch.randn(length, size) def __getitem__(self, index): return self.data[index] def __len__(self): return self.len rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size), batch_size=batch_size, shuffle=True) 简单模型 作为演示，我们的模型只接受一个输入，执行一个线性操作，然后得到结果。然而，你能在任何模型（CNN，RNN，Capsule Net等）上使用DataParallel。 我们在模型内部放置了一条打印语句来检测输入和输出向量的大小。请注意批等级为0时打印的内容。 class Model(nn.Module): # Our model def __init__(self, input_size, output_size): super(Model, self).__init__() self.fc = nn.Linear(input_size, output_size) def forward(self, input): output = self.fc(input) print(\"\\tIn Model: input size\", input.size(), \"output size\", output.size()) return output 创建一个模型和数据并行 这是本教程的核心部分。首先，我们需要创建一个模型实例和检测我们是否有多个GPU。如果我们有多个GPU，我们使用nn.DataParallel来包装我们的模型。然后通过model.to(device)把模型放到GPU上。 model = Model(input_size, output_size) if torch.cuda.device_count() > 1: print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\") # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs model = nn.DataParallel(model) model.to(device) 输出: Let's use 2 GPUs! 运行模型 现在我们可以看输入和输出张量的大小。 for data in rand_loader: input = data.to(device) output = model(input) print(\"Outside: input size\", input.size(), \"output_size\", output.size()) 输出: In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 结果 当我们对30个输入和输出进行批处理时，我们和期望的一样得到30个输入和30个输出，但是若有多个GPU，会得到如下的结果。 2个GPU 若有2个GPU，将看到: Let's use 2 GPUs! In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 3个GPU 若有3个GPU，将看到: Let's use 3 GPUs! In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 8个GPU 若有8个GPU，将看到: Let's use 8 GPUs! In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 总结 DataParallel自动的划分数据，并将作业发送到多个GPU上的多个模型。DataParallel会在每个模型完成作业后，收集与合并结果然后返回给你。 更多信息，请参考: https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"data_loading_tutorial.html":{"url":"data_loading_tutorial.html","title":"数据加载和处理教程","keywords":"","body":"数据加载和处理教程 译者：yportne13 作者：Sasank Chilamkurthy 在解决机器学习问题的时候，人们花了大量精力准备数据。pytorch提供了许多工具来让载入数据更简单并尽量让你的代码的可读性更高。在这篇教程中，我们将从一个容易处理的数据集中学习如何加载和预处理/增强数据。 在运行这个教程前请先确保你已安装以下的包: scikit-image: 图形接口以及变换 pandas: 便于处理csv文件 from __future__ import print_function, division import os import torch import pandas as pd from skimage import io, transform import numpy as np import matplotlib.pyplot as plt from torch.utils.data import Dataset, DataLoader from torchvision import transforms, utils # Ignore warnings import warnings warnings.filterwarnings(\"ignore\") plt.ion() # interactive mode 我们要处理的是一个面部姿态的数据集。也就是按如下方式标注的人脸: 每张脸标注了68个不同的特征点。 注意 从这里下载数据集并把它放置在 ‘data/faces/’路径下。这个数据集实际上是对ImageNet中的人脸图像使用表现出色的DLIB姿势估计模型(dlib’s pose estimation) 生成的。 数据集是按如下规则打包成的csv文件: image_name,part_0_x,part_0_y,part_1_x,part_1_y,part_2_x, ... ,part_67_x,part_67_y 0805personali01.jpg,27,83,27,98, ... 84,134 1084239450_e76e00b7e7.jpg,70,236,71,257, ... ,128,312 快速读取csv并将标注点数据写入（N，2）数组中，其中N是特征点的数量。 landmarks_frame = pd.read_csv('data/faces/face_landmarks.csv') n = 65 img_name = landmarks_frame.iloc[n, 0] landmarks = landmarks_frame.iloc[n, 1:].as_matrix() landmarks = landmarks.astype('float').reshape(-1, 2) print('Image name: {}'.format(img_name)) print('Landmarks shape: {}'.format(landmarks.shape)) print('First 4 Landmarks: {}'.format(landmarks[:4])) 输出: Image name: person-7.jpg Landmarks shape: (68, 2) First 4 Landmarks: [[32\\. 65.] [33\\. 76.] [34\\. 86.] [34\\. 97.]] 写一个简单的辅助函数来展示一张图片和它对应的标注点作为例子。 def show_landmarks(image, landmarks): \"\"\"Show image with landmarks\"\"\" plt.imshow(image) plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r') plt.pause(0.001) # pause a bit so that plots are updated plt.figure() show_landmarks(io.imread(os.path.join('data/faces/', img_name)), landmarks) plt.show() 数据集类 Dataset class torch.utils.data.Dataset 是一个代表数据集的抽象类。你自定的数据集类应该继承自 Dataset 类并重新实现以下方法: __len__ 实现 len(dataset) 返还数据集的尺寸。 __getitem__ 用来获取一些索引数据，例如 使用dataset[i] 获得第i个样本。 让我们来为我们的数据集创建一个类。我们将在 __init__ 中读取csv的文件内容，在 __getitem__中读取图片。这么做是为了节省内存空间。只有在需要用到图片的时候才读取它而不是一开始就把图片全部存进内存里。 我们的数据样本将按这样一个字典 {'image': image, 'landmarks': landmarks}组织。 我们的数据集类将添加一个可选参数 transform 以方便对样本进行预处理。下一节我们会看到什么时候需要用到 transform 参数。 class FaceLandmarksDataset(Dataset): \"\"\"Face Landmarks dataset.\"\"\" def __init__(self, csv_file, root_dir, transform=None): \"\"\" Args: csv_file (string): Path to the csv file with annotations. root_dir (string): Directory with all the images. transform (callable, optional): Optional transform to be applied on a sample. \"\"\" self.landmarks_frame = pd.read_csv(csv_file) self.root_dir = root_dir self.transform = transform def __len__(self): return len(self.landmarks_frame) def __getitem__(self, idx): img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, 0]) image = io.imread(img_name) landmarks = self.landmarks_frame.iloc[idx, 1:].as_matrix() landmarks = landmarks.astype('float').reshape(-1, 2) sample = {'image': image, 'landmarks': landmarks} if self.transform: sample = self.transform(sample) return sample 让我们实例化这个类并创建几个数据。我们将会打印出前四个例子的尺寸并展示标注的特征点。 face_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv', root_dir='data/faces/') fig = plt.figure() for i in range(len(face_dataset)): sample = face_dataset[i] print(i, sample['image'].shape, sample['landmarks'].shape) ax = plt.subplot(1, 4, i + 1) plt.tight_layout() ax.set_title('Sample #{}'.format(i)) ax.axis('off') show_landmarks(**sample) if i == 3: plt.show() break 输出: 0 (324, 215, 3) (68, 2) 1 (500, 333, 3) (68, 2) 2 (250, 258, 3) (68, 2) 3 (434, 290, 3) (68, 2) 转换 Transforms 通过上面的例子我们会发现图片并不是同样的尺寸。绝大多数神经网络都假定图片的尺寸相同。因此我们需要做一些预处理。让我们创建三个转换: Rescale: 缩放图片 RandomCrop: 对图片进行随机裁剪。这是一种数据增强操作 ToTensor: 把 numpy 格式图片转为 torch 格式图片 (我们需要交换坐标轴). 我们会把它们写成可调用的类的形式而不是简单的函数，这样就不需要每次调用时传递一遍参数。我们只需要实现 __call__ 方法，必要的时候实现 __init__ 方法。我们可以这样调用这些转换: tsfm = Transform(params) transformed_sample = tsfm(sample) 观察下面这些转换是如何应用在图像和标签上的。 class Rescale(object): \"\"\"Rescale the image in a sample to a given size. Args: output_size (tuple or int): Desired output size. If tuple, output is matched to output_size. If int, smaller of image edges is matched to output_size keeping aspect ratio the same. \"\"\" def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) self.output_size = output_size def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] h, w = image.shape[:2] if isinstance(self.output_size, int): if h > w: new_h, new_w = self.output_size * h / w, self.output_size else: new_h, new_w = self.output_size, self.output_size * w / h else: new_h, new_w = self.output_size new_h, new_w = int(new_h), int(new_w) img = transform.resize(image, (new_h, new_w)) # h and w are swapped for landmarks because for images, # x and y axes are axis 1 and 0 respectively landmarks = landmarks * [new_w / w, new_h / h] return {'image': img, 'landmarks': landmarks} class RandomCrop(object): \"\"\"Crop randomly the image in a sample. Args: output_size (tuple or int): Desired output size. If int, square crop is made. \"\"\" def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) if isinstance(output_size, int): self.output_size = (output_size, output_size) else: assert len(output_size) == 2 self.output_size = output_size def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] h, w = image.shape[:2] new_h, new_w = self.output_size top = np.random.randint(0, h - new_h) left = np.random.randint(0, w - new_w) image = image[top: top + new_h, left: left + new_w] landmarks = landmarks - [left, top] return {'image': image, 'landmarks': landmarks} class ToTensor(object): \"\"\"Convert ndarrays in sample to Tensors.\"\"\" def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] # swap color axis because # numpy image: H x W x C # torch image: C X H X W image = image.transpose((2, 0, 1)) return {'image': torch.from_numpy(image), 'landmarks': torch.from_numpy(landmarks)} 组合转换 Compose transforms 接下来我们把这些转换应用到一个例子上。 我们想要把图像的短边调整为256，然后随机裁剪 (randomcrop) 为224大小的正方形。也就是说，我们打算组合一个 Rescale 和 RandomCrop 的变换。 我们可以调用一个简单的类 torchvision.transforms.Compose 来实现这一操作。 scale = Rescale(256) crop = RandomCrop(128) composed = transforms.Compose([Rescale(256), RandomCrop(224)]) # Apply each of the above transforms on sample. fig = plt.figure() sample = face_dataset[65] for i, tsfrm in enumerate([scale, crop, composed]): transformed_sample = tsfrm(sample) ax = plt.subplot(1, 3, i + 1) plt.tight_layout() ax.set_title(type(tsfrm).__name__) show_landmarks(**transformed_sample) plt.show() 迭代数据集 Iterating through the dataset 让我们把这些整合起来以创建一个带组合转换的数据集。 总结一下，每次这个数据集被采样时: 及时地从文件中读取图片 对读取的图片应用转换 由于其中一步操作是随机的 (randomcrop) , 数据被增强了 我们可以像之前那样使用 for i in range 循环来对所有创建的数据集执行同样的操作。 transformed_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv', root_dir='data/faces/', transform=transforms.Compose([ Rescale(256), RandomCrop(224), ToTensor() ])) for i in range(len(transformed_dataset)): sample = transformed_dataset[i] print(i, sample['image'].size(), sample['landmarks'].size()) if i == 3: break 输出: 0 torch.Size([3, 224, 224]) torch.Size([68, 2]) 1 torch.Size([3, 224, 224]) torch.Size([68, 2]) 2 torch.Size([3, 224, 224]) torch.Size([68, 2]) 3 torch.Size([3, 224, 224]) torch.Size([68, 2]) 但是，对所有数据集简单的使用 for 循环牺牲了许多功能，尤其是: 批处理数据（Batching the data） 打乱数据（Shuffling the data） 使用多线程 multiprocessing 并行加载数据。 torch.utils.data.DataLoader 这个迭代器提供了以上所有功能。 下面使用的参数必须是清楚的。 一个值得关注的参数是 collate_fn. 你可以通过 collate_fn 来决定如何对数据进行批处理。 但是绝大多数情况下默认值就能运行良好。 dataloader = DataLoader(transformed_dataset, batch_size=4, shuffle=True, num_workers=4) # Helper function to show a batch def show_landmarks_batch(sample_batched): \"\"\"Show image with landmarks for a batch of samples.\"\"\" images_batch, landmarks_batch = \\ sample_batched['image'], sample_batched['landmarks'] batch_size = len(images_batch) im_size = images_batch.size(2) grid = utils.make_grid(images_batch) plt.imshow(grid.numpy().transpose((1, 2, 0))) for i in range(batch_size): plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size, landmarks_batch[i, :, 1].numpy(), s=10, marker='.', c='r') plt.title('Batch from dataloader') for i_batch, sample_batched in enumerate(dataloader): print(i_batch, sample_batched['image'].size(), sample_batched['landmarks'].size()) # observe 4th batch and stop. if i_batch == 3: plt.figure() show_landmarks_batch(sample_batched) plt.axis('off') plt.ioff() plt.show() break 输出: 0 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 1 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 2 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 3 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 后记: torchvision 在这篇教程中我们学习了如何构造和使用数据集类 (datasets), 转换 (transforms) 和数据加载器 (dataloader)。 torchvision 包提供了常用的数据集类 (datasets) 和转换 (transforms)。 你可能不需要自己构造这些类。 torchvision 中还有一个更常用的数据集类 ImageFolder. 它假定了数据集是以如下方式构造的: root/ants/xxx.png root/ants/xxy.jpeg root/ants/xxz.png . . . root/bees/123.jpg root/bees/nsdf3.png root/bees/asd932_.png 其中 ‘ants’, ‘bees’ 等是分类标签。 在 PIL.Image 中你也可以使用类似的转换 (transforms) 例如 RandomHorizontalFlip, Scale。利用这些你可以按如下的方式创建一个数据加载器 (dataloader) : import torch from torchvision import transforms, datasets data_transform = transforms.Compose([ transforms.RandomSizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ]) hymenoptera_dataset = datasets.ImageFolder(root='hymenoptera_data/train', transform=data_transform) dataset_loader = torch.utils.data.DataLoader(hymenoptera_dataset, batch_size=4, shuffle=True, num_workers=4) 带训练部分的例程可以参考这里 Transfer Learning Tutorial. 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"pytorch_with_examples.html":{"url":"pytorch_with_examples.html","title":"用例子学习 PyTorch","keywords":"","body":"用例子学习 PyTorch 译者：bat67 最新版会在译者仓库首先同步。 作者：Justin Johnson 这个教程通过自洽的示例介绍了PyTorch的基本概念。 PyTorch主要是提供了两个核心的功能特性： 一个类似于numpy的n维张量，但是可以在GPU上运行 搭建和训练神经网络时的自动微分/求导机制 我们将使用全连接的ReLU网络作为运行示例。该网络将有一个单一的隐藏层，并将使用梯度下降训练，通过最小化网络输出和真正结果的欧几里得距离，来拟合随机生成的数据。 目录 用例子学习 PyTorch 目录 张量 热身：NumPy PyTorch：张量 自动求导 PyTorch：张量和自动求导 PyTorch：定义新的自动求导函数 TensorFlow：静态图 nn模块 PyTorch：nn PyTorch：optim PyTorch：自定义nn模块 PyTorch：控制流和权重共享 Examples Tensors Autograd nn module 张量 热身：NumPy 在介绍PyTorch之前，我们将首先使用NumPy实现网络。 NumPy提供了一个n维数组对象和许多用于操作这些数组的函数。NumPy是用于科学计算的通用框架；它对计算图、深度学习和梯度一无所知。然而，我们可以很容易地使用NumPy，手动实现网络的前向和反向传播，来拟合随机数据： # 可运行代码见本文件夹中的 two_layer_net_numpy.py import numpy as np # N是批大小；D_in是输入维度 # H是隐藏层维度；D_out是输出维度 N, D_in, H, D_out = 64, 1000, 100, 10 # 产生随机输入和输出数据 x = np.random.randn(N, D_in) y = np.random.randn(N, D_out) # 随机初始化权重 w1 = np.random.randn(D_in, H) w2 = np.random.randn(H, D_out) learning_rate = 1e-6 for t in range(500): # 前向传播：计算预测值y h = x.dot(w1) h_relu = np.maximum(h, 0) y_pred = h_relu.dot(w2) # 计算并显示loss（损失） loss = np.square(y_pred - y).sum() print(t, loss) # 反向传播，计算w1、w2对loss的梯度 grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.T.dot(grad_y_pred) grad_h_relu = grad_y_pred.dot(w2.T) grad_h = grad_h_relu.copy() grad_h[h PyTorch：张量 NumPy是一个很棒的框架，但是它不支持GPU以加速运算。现代深度神经网络，GPU常常提供50倍以上的加速)，所以NumPy不能满足当代深度学习的需求。 我们先介绍PyTorch最基础的概念：张量（Tensor）。逻辑上，PyTorch的tensor和NumPy array是一样的：tensor是一个n维数组，PyTorch提供了很多函数操作这些tensor。任何希望使用NumPy执行的计算也可以使用PyTorch的tensor来完成；可以认为它们是科学计算的通用工具。 和NumPy不同的是，PyTorch可以利用GPU加速。要在GPU上运行PyTorch张量，在构造张量使用device参数把tensor建立在GPU上。 这里我们利用PyTorch的tensor在随机数据上训练一个两层的网络。和前面NumPy的例子类似，我们使用PyTorch的tensor，手动在网络中实现前向传播和反向传播： # 可运行代码见本文件夹中的 two_layer_net_tensor.py import torch device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # N是批大小； D_in 是输入维度； # H 是隐藏层维度； D_out 是输出维度 N, D_in, H, D_out = 64, 1000, 100, 10 # 产生随机输入和输出数据 x = torch.randn(N, D_in, device=device) y = torch.randn(N, D_out, device=device) # 随机初始化权重 w1 = torch.randn(D_in, H, device=device) w2 = torch.randn(H, D_out, device=device) learning_rate = 1e-6 for t in range(500): # 前向传播：计算预测值y h = x.mm(w1) h_relu = h.clamp(min=0) y_pred = h_relu.mm(w2) # 计算并输出loss；loss是存储在PyTorch的tensor中的标量，维度是()（零维标量）； # 我们使用loss.item()得到tensor中的纯python数值。 loss = (y_pred - y).pow(2).sum() print(t, loss.item()) # 反向传播，计算w1、w2对loss的梯度 grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.t().mm(grad_y_pred) grad_h_relu = grad_y_pred.mm(w2.t()) grad_h = grad_h_relu.clone() grad_h[h 自动求导 PyTorch：张量和自动求导 在上面的例子里，需要我们手动实现神经网络的前向和后向传播。对于简单的两层网络，手动实现前向、后向传播不是什么难事，但是对于大型的复杂网络就比较麻烦了。 庆幸的是，我们可以使用自动微分来自动完成神经网络中反向传播的计算。PyTorch中autograd包提供的正是这个功能。当使用autograd时，网络前向传播将定义一个计算图；图中的节点是tensor，边是函数，这些函数是输出tensor到输入tensor的映射。这张计算图使得在网络中反向传播时梯度的计算十分简单。 这听起来复杂，但是实际操作很简单。如果我们想计算某些的tensor的梯度，我们只需要在建立这个tensor时加入这么一句：requires_grad=True。这个tensor上的任何PyTorch的操作都将构造一个计算图，从而允许我们稍后在图中执行反向传播。如果这个tensorx的requires_grad=True，那么反向传播之后x.grad将会是另一个张量，其为x关于某个标量值的梯度。 有时可能希望防止PyTorch在requires_grad=True的张量执行某些操作时构建计算图；例如，在训练神经网络时，我们通常不希望通过权重更新步骤进行反向传播。在这种情况下，我们可以使用torch.no_grad()上下文管理器来防止构造计算图。 下面我们使用PyTorch的Tensors和autograd来实现我们的两层的神经网络；我们不再需要手动执行网络的反向传播： # 可运行代码见本文件夹中的 two_layer_net_autograd.py import torch device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # N是批大小；D_in是输入维度； # H是隐藏层维度；D_out是输出维度 N, D_in, H, D_out = 64, 1000, 100, 10 # 产生随机输入和输出数据 x = torch.randn(N, D_in, device=device) y = torch.randn(N, D_out, device=device) # 产生随机权重tensor，将requires_grad设置为True意味着我们希望在反向传播时候计算这些值的梯度 w1 = torch.randn(D_in, H, device=device, requires_grad=True) w2 = torch.randn(H, D_out, device=device, requires_grad=True) learning_rate = 1e-6 for t in range(500): # 前向传播：使用tensor的操作计算预测值y。 # 由于w1和w2有requires_grad=True，涉及这些张量的操作将让PyTorch构建计算图， # 从而允许自动计算梯度。由于我们不再手工实现反向传播，所以不需要保留中间值的引用。 y_pred = x.mm(w1).clamp(min=0).mm(w2) # 计算并输出loss，loss是一个形状为()的张量，loss.item()是这个张量对应的python数值 loss = (y_pred - y).pow(2).sum() print(t, loss.item()) # 使用autograd计算反向传播。这个调用将计算loss对所有requires_grad=True的tensor的梯度。 # 这次调用后，w1.grad和w2.grad将分别是loss对w1和w2的梯度张量。 loss.backward() # 使用梯度下降更新权重。对于这一步，我们只想对w1和w2的值进行原地改变；不想为更新阶段构建计算图， # 所以我们使用torch.no_grad()上下文管理器防止PyTorch为更新构建计算图 with torch.no_grad(): w1 -= learning_rate * w1.grad w2 -= learning_rate * w2.grad # 反向传播之后手动置零梯度 w1.grad.zero_() w2.grad.zero_() PyTorch：定义新的自动求导函数 在底层，每一个原始的自动求导运算实际上是两个在Tensor上运行的函数。其中，forward函数计算从输入Tensors获得的输出Tensors。而backward函数接收输出Tensors对于某个标量值的梯度，并且计算输入Tensors相对于该相同标量值的梯度。 在PyTorch中，我们可以很容易地通过定义torch.autograd.Function的子类并实现forward和backward函数，来定义自己的自动求导运算。之后我们就可以使用这个新的自动梯度运算符了。然后，我们可以通过构造一个实例并像调用函数一样，传入包含输入数据的tensor调用它，这样来使用新的自动求导运算。 这个例子中，我们自定义一个自动求导函数来展示ReLU的非线性。并用它实现我们的两层网络： # 可运行代码见本文件夹中的 two_layer_net_custom_function.py import torch class MyReLU(torch.autograd.Function): \"\"\" 我们可以通过建立torch.autograd的子类来实现我们自定义的autograd函数， 并完成张量的正向和反向传播。 \"\"\" @staticmethod def forward(ctx, x): \"\"\" 在正向传播中，我们接收到一个上下文对象和一个包含输入的张量； 我们必须返回一个包含输出的张量， 并且我们可以使用上下文对象来缓存对象，以便在反向传播中使用。 \"\"\" ctx.save_for_backward(x) return x.clamp(min=0) @staticmethod def backward(ctx, grad_output): \"\"\" 在反向传播中，我们接收到上下文对象和一个张量， 其包含了相对于正向传播过程中产生的输出的损失的梯度。 我们可以从上下文对象中检索缓存的数据， 并且必须计算并返回与正向传播的输入相关的损失的梯度。 \"\"\" x, = ctx.saved_tensors grad_x = grad_output.clone() grad_x[x TensorFlow：静态图 PyTorch自动求导看起来非常像TensorFlow：这两个框架中，我们都定义计算图，使用自动微分来计算梯度。两者最大的不同就是TensorFlow的计算图是静态的，而PyTorch使用动态的计算图。 在TensorFlow中，我们定义计算图一次，然后重复执行这个相同的图，可能会提供不同的输入数据。而在PyTorch中，每一个前向通道定义一个新的计算图。 静态图的好处在于你可以预先对图进行优化。例如，一个框架可能要融合一些图的运算来提升效率，或者产生一个策略来将图分布到多个GPU或机器上。如果重复使用相同的图，那么在重复运行同一个图时，，前期潜在的代价高昂的预先优化的消耗就会被分摊开。 静态图和动态图的一个区别是控制流。对于一些模型，我们希望对每个数据点执行不同的计算。例如，一个递归神经网络可能对于每个数据点执行不同的时间步数，这个展开（unrolling）可以作为一个循环来实现。对于一个静态图，循环结构要作为图的一部分。因此，TensorFlow提供了运算符（例如tf.scan）来把循环嵌入到图当中。对于动态图来说，情况更加简单：既然我们为每个例子即时创建图，我们可以使用普通的命令式控制流来为每个输入执行不同的计算。 为了与上面的PyTorch自动梯度实例做对比，我们使用TensorFlow来拟合一个简单的2层网络： # 可运行代码见本文件夹中的 tf_two_layer_net.py import tensorflow as tf import numpy as np # 首先我们建立计算图（computational graph） # N是批大小；D是输入维度； # H是隐藏层维度；D_out是输出维度。 N, D_in, H, D_out = 64, 1000, 100, 10 # 为输入和目标数据创建placeholder； # 当执行计算图时，他们将会被真实的数据填充 x = tf.placeholder(tf.float32, shape=(None, D_in)) y = tf.placeholder(tf.float32, shape=(None, D_out)) # 为权重创建Variable并用随机数据初始化 # TensorFlow的Variable在执行计算图时不会改变 w1 = tf.Variable(tf.random_normal((D_in, H))) w2 = tf.Variable(tf.random_normal((H, D_out))) # 前向传播：使用TensorFlow的张量运算计算预测值y。 # 注意这段代码实际上不执行任何数值运算； # 它只是建立了我们稍后将执行的计算图。 h = tf.matmul(x, w1) h_relu = tf.maximum(h, tf.zeros(1)) y_pred = tf.matmul(h_relu, w2) # 使用TensorFlow的张量运算损失（loss） loss = tf.reduce_sum((y - y_pred) ** 2.0) # 计算loss对于w1和w2的导数 grad_w1, grad_w2 = tf.gradients(loss, [w1, w2]) # 使用梯度下降更新权重。为了实际更新权重，我们需要在执行计算图时计算new_w1和new_w2。 # 注意，在TensorFlow中，更新权重值的行为是计算图的一部分; # 但在PyTorch中，这发生在计算图形之外。 learning_rate = 1e-6 new_w1 = w1.assign(w1 - learning_rate * grad_w1) new_w2 = w2.assign(w2 - learning_rate * grad_w2) # 现在我们搭建好了计算图，所以我们开始一个TensorFlow的会话（session）来实际执行计算图。 with tf.Session() as sess: # 运行一次计算图来初始化Variable w1和w2 sess.run(tf.global_variables_initializer()) # 创建numpy数组来存储输入x和目标y的实际数据 x_value = np.random.randn(N, D_in) y_value = np.random.randn(N, D_out) for _ in range(500): # 多次运行计算图。每次执行时，我们都用feed_dict参数， # 将x_value绑定到x，将y_value绑定到y， # 每次执行图形时我们都要计算损失、new_w1和new_w2； # 这些张量的值以numpy数组的形式返回。 loss_value, _, _ = sess.run([loss, new_w1, new_w2], feed_dict={x: x_value, y: y_value}) print(loss_value) nn模块 PyTorch：nn 计算图和autograd是十分强大的工具，可以定义复杂的操作并自动求导；然而对于大规模的网络，autograd太过于底层。 在构建神经网络时，我们经常考虑将计算安排成层，其中一些具有可学习的参数，它们将在学习过程中进行优化。 TensorFlow里，有类似Keras，TensorFlow-Slim和TFLearn这种封装了底层计算图的高度抽象的接口，这使得构建网络十分方便。 在PyTorch中，包nn完成了同样的功能。nn包中定义一组大致等价于层的模块。一个模块接受输入的tesnor，计算输出的tensor，而且还保存了一些内部状态比如需要学习的tensor的参数等。nn包中也定义了一组损失函数（loss functions），用来训练神经网络。 这个例子中，我们用nn包实现两层的网络： # 可运行代码见本文件夹中的 two_layer_net_nn.py import torch device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # N是批大小；D是输入维度 # H是隐藏层维度；D_out是输出维度 N, D_in, H, D_out = 64, 1000, 100, 10 # 产生输入和输出随机张量 x = torch.randn(N, D_in, device=device) y = torch.randn(N, D_out, device=device) # 使用nn包将我们的模型定义为一系列的层。 # nn.Sequential是包含其他模块的模块，并按顺序应用这些模块来产生其输出。 # 每个线性模块使用线性函数从输入计算输出，并保存其内部的权重和偏差张量。 # 在构造模型之后，我们使用.to()方法将其移动到所需的设备。 model = torch.nn.Sequential( torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out), ).to(device) # nn包还包含常用的损失函数的定义； # 在这种情况下，我们将使用平均平方误差(MSE)作为我们的损失函数。 # 设置reduction='sum'，表示我们计算的是平方误差的“和”，而不是平均值; # 这是为了与前面我们手工计算损失的例子保持一致， # 但是在实践中，通过设置reduction='elementwise_mean'来使用均方误差作为损失更为常见。 loss_fn = torch.nn.MSELoss(reduction='sum') learning_rate = 1e-4 for t in range(500): # 前向传播：通过向模型传入x计算预测的y。 # 模块对象重载了__call__运算符，所以可以像函数那样调用它们。 # 这么做相当于向模块传入了一个张量，然后它返回了一个输出张量。 y_pred = model(x) # 计算并打印损失。我们传递包含y的预测值和真实值的张量，损失函数返回包含损失的张量。 loss = loss_fn(y_pred, y) print(t, loss.item()) # 反向传播之前清零梯度 model.zero_grad() # 反向传播：计算模型的损失对所有可学习参数的导数（梯度）。 # 在内部，每个模块的参数存储在requires_grad=True的张量中， # 因此这个调用将计算模型中所有可学习参数的梯度。 loss.backward() # 使用梯度下降更新权重。 # 每个参数都是张量，所以我们可以像我们以前那样可以得到它的数值和梯度 with torch.no_grad(): for param in model.parameters(): param.data -= learning_rate * param.grad PyTorch：optim 到目前为止，我们已经通过手动改变包含可学习参数的张量来更新模型的权重。对于随机梯度下降(SGD/stochastic gradient descent)等简单的优化算法来说，这不是一个很大的负担，但在实践中，我们经常使用AdaGrad、RMSProp、Adam等更复杂的优化器来训练神经网络。 # 可运行代码见本文件夹中的 two_layer_net_optim.py import torch # N是批大小；D是输入维度 # H是隐藏层维度；D_out是输出维度 N, D_in, H, D_out = 64, 1000, 100, 10 # 产生随机输入和输出张量 x = torch.randn(N, D_in) y = torch.randn(N, D_out) # 使用nn包定义模型和损失函数 model = torch.nn.Sequential( torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out), ) loss_fn = torch.nn.MSELoss(reduction='sum') # 使用optim包定义优化器（Optimizer）。Optimizer将会为我们更新模型的权重。 # 这里我们使用Adam优化方法；optim包还包含了许多别的优化算法。 # Adam构造函数的第一个参数告诉优化器应该更新哪些张量。 learning_rate = 1e-4 optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) for t in range(500): # 前向传播：通过像模型输入x计算预测的y y_pred = model(x) # 计算并打印loss loss = loss_fn(y_pred, y) print(t, loss.item()) # 在反向传播之前，使用optimizer将它要更新的所有张量的梯度清零(这些张量是模型可学习的权重) optimizer.zero_grad() # 反向传播：根据模型的参数计算loss的梯度 loss.backward() # 调用Optimizer的step函数使它所有参数更新 optimizer.step() PyTorch：自定义nn模块 有时候需要指定比现有模块序列更复杂的模型；对于这些情况，可以通过继承nn.Module并定义forward函数，这个forward函数可以使用其他模块或者其他的自动求导运算来接收输入tensor，产生输出tensor。 在这个例子中，我们用自定义Module的子类构建两层网络： # 可运行代码见本文件夹中的 two_layer_net_module.py import torch class TwoLayerNet(torch.nn.Module): def __init__(self, D_in, H, D_out): \"\"\" 在构造函数中，我们实例化了两个nn.Linear模块，并将它们作为成员变量。 \"\"\" super(TwoLayerNet, self).__init__() self.linear1 = torch.nn.Linear(D_in, H) self.linear2 = torch.nn.Linear(H, D_out) def forward(self, x): \"\"\" 在前向传播的函数中，我们接收一个输入的张量，也必须返回一个输出张量。 我们可以使用构造函数中定义的模块以及张量上的任意的（可微分的）操作。 \"\"\" h_relu = self.linear1(x).clamp(min=0) y_pred = self.linear2(h_relu) return y_pred # N是批大小； D_in 是输入维度； # H 是隐藏层维度； D_out 是输出维度 N, D_in, H, D_out = 64, 1000, 100, 10 # 产生输入和输出的随机张量 x = torch.randn(N, D_in) y = torch.randn(N, D_out) # 通过实例化上面定义的类来构建我们的模型。 model = TwoLayerNet(D_in, H, D_out) # 构造损失函数和优化器。 # SGD构造函数中对model.parameters()的调用， # 将包含模型的一部分，即两个nn.Linear模块的可学习参数。 loss_fn = torch.nn.MSELoss(reduction='sum') optimizer = torch.optim.SGD(model.parameters(), lr=1e-4) for t in range(500): # 前向传播：通过向模型传递x计算预测值y y_pred = model(x) #计算并输出loss loss = loss_fn(y_pred, y) print(t, loss.item()) # 清零梯度，反向传播，更新权重 optimizer.zero_grad() loss.backward() optimizer.step() PyTorch：控制流和权重共享 作为动态图和权重共享的一个例子，我们实现了一个非常奇怪的模型：一个全连接的ReLU网络，在每一次前向传播时，它的隐藏层的层数为随机1到4之间的数，这样可以多次重用相同的权重来计算。 因为这个模型可以使用普通的Python流控制来实现循环，并且我们可以通过在定义转发时多次重用同一个模块来实现最内层之间的权重共享。 我们利用Mudule的子类很容易实现这个模型： # 可运行代码见本文件夹中的 dynamic_net.py import random import torch class DynamicNet(torch.nn.Module): def __init__(self, D_in, H, D_out): \"\"\" 在构造函数中，我们构造了三个nn.Linear实例，它们将在前向传播时被使用。 \"\"\" super(DynamicNet, self).__init__() self.input_linear = torch.nn.Linear(D_in, H) self.middle_linear = torch.nn.Linear(H, H) self.output_linear = torch.nn.Linear(H, D_out) def forward(self, x): \"\"\" 对于模型的前向传播，我们随机选择0、1、2、3， 并重用了多次计算隐藏层的middle_linear模块。 由于每个前向传播构建一个动态计算图， 我们可以在定义模型的前向传播时使用常规Python控制流运算符，如循环或条件语句。 在这里，我们还看到，在定义计算图形时多次重用同一个模块是完全安全的。 这是Lua Torch的一大改进，因为Lua Torch中每个模块只能使用一次。 \"\"\" h_relu = self.input_linear(x).clamp(min=0) for _ in range(random.randint(0, 3)): h_relu = self.middle_linear(h_relu).clamp(min=0) y_pred = self.output_linear(h_relu) return y_pred # N是批大小；D是输入维度 # H是隐藏层维度；D_out是输出维度 N, D_in, H, D_out = 64, 1000, 100, 10 # 产生输入和输出随机张量 x = torch.randn(N, D_in) y = torch.randn(N, D_out) # 实例化上面定义的类来构造我们的模型 model = DynamicNet(D_in, H, D_out) # 构造我们的损失函数（loss function）和优化器（Optimizer）。 # 用平凡的随机梯度下降训练这个奇怪的模型是困难的，所以我们使用了momentum方法。 criterion = torch.nn.MSELoss(reduction='sum') optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9) for t in range(500): # 前向传播：通过向模型传入x计算预测的y。 y_pred = model(x) # 计算并打印损失 loss = criterion(y_pred, y) print(t, loss.item()) # 清零梯度，反向传播，更新权重 optimizer.zero_grad() loss.backward() optimizer.step() Examples You can browse the above examples here. Tensors Warm-up: numpy PyTorch: Tensors Autograd PyTorch: Tensors and autograd PyTorch: Defining New autograd Functions TensorFlow: Static Graphs nn module PyTorch: nn PyTorch: optim PyTorch: Custom nn Modules PyTorch: Control Flow + Weight Sharing 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"transfer_learning_tutorial.html":{"url":"transfer_learning_tutorial.html","title":"迁移学习教程","keywords":"","body":"迁移学习教程 译者：片刻 校对者：cluster 作者: Sasank Chilamkurthy 在本教程中，您将学习如何使用迁移学习来训练您的网络。您可以在 cs231n 笔记 上阅读更多关于迁移学习的信息 引用这些笔记： 在实践中，很少有人从头开始训练整个卷积网络（随机初始化），因为拥有足够大小的数据集是相对罕见的。相反，通常在非常大的数据集（例如 ImageNet，其包含具有1000个类别的120万个图像）上预先训练 ConvNet，然后使用 ConvNet 对感兴趣的任务进行初始化或用作固定特征提取器。 如下是两个主要的迁移学习场景： Finetuning the convnet: 我们使用预训练网络初始化网络，而不是随机初始化，就像在imagenet 1000数据集上训练的网络一样。其余训练看起来像往常一样。(此微调过程对应引用中所说的初始化) ConvNet as fixed feature extractor: 在这里，我们将冻结除最终完全连接层之外的所有网络的权重。最后一个全连接层被替换为具有随机权重的新层，并且仅训练该层。(此步对应引用中的固定特征提取器) # License: BSD # Author: Sasank Chilamkurthy from __future__ import print_function, division import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler import numpy as np import torchvision from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import time import os import copy plt.ion() # interactive mode 加载数据 我们将使用 torchvision 和 torch.utils.data 包来加载数据。 我们今天要解决的问题是训练一个模型来对 蚂蚁 和 蜜蜂 进行分类。我们有大约120个训练图像，每个图像用于 蚂蚁 和 蜜蜂。每个类有75个验证图像。通常，如果从头开始训练，这是一个非常小的数据集。由于我们正在使用迁移学习，我们应该能够合理地泛化。 该数据集是 imagenet 的一个非常小的子集。 注意 从 此处 下载数据并将其解压缩到当前目录。 # Data augmentation and normalization for training # Just normalization for validation data_transforms = { 'train': transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), 'val': transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } data_dir = 'data/hymenoptera_data' image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']} dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val']} dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']} class_names = image_datasets['train'].classes device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") 可视化一些图像 让我们可视化一些训练图像，以便了解数据增强。 def imshow(inp, title=None): \"\"\"Imshow for Tensor.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated # Get a batch of training data inputs, classes = next(iter(dataloaders['train'])) # Make a grid from batch out = torchvision.utils.make_grid(inputs) imshow(out, title=[class_names[x] for x in classes]) 训练模型 现在, 让我们编写一个通用函数来训练模型. 这里, 我们将会举例说明: 调度学习率 保存最佳的学习模型 下面函数中, scheduler 参数是 torch.optim.lr_scheduler 中的 LR scheduler 对象. def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): print('Epoch {}/{}'.format(epoch, num_epochs - 1)) print('-' * 10) # Each epoch has a training and validation phase for phase in ['train', 'val']: if phase == 'train': scheduler.step() model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == 'train'): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # backward + optimize only if in training phase if phase == 'train': loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print('{} Loss: {:.4f} Acc: {:.4f}'.format( phase, epoch_loss, epoch_acc)) # deep copy the model if phase == 'val' and epoch_acc > best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) print() time_elapsed = time.time() - since print('Training complete in {:.0f}m {:.0f}s'.format( time_elapsed // 60, time_elapsed % 60)) print('Best val Acc: {:4f}'.format(best_acc)) # load best model weights model.load_state_dict(best_model_wts) return model 可视化模型预测 用于显示少量图像预测的通用功能 def visualize_model(model, num_images=6): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): for i, (inputs, labels) in enumerate(dataloaders['val']): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis('off') ax.set_title('predicted: {}'.format(class_names[preds[j]])) imshow(inputs.cpu().data[j]) if images_so_far == num_images: model.train(mode=was_training) return model.train(mode=was_training) 微调卷积网络 加载预训练模型并重置最终的全连接层。 model_ft = models.resnet18(pretrained=True) num_ftrs = model_ft.fc.in_features model_ft.fc = nn.Linear(num_ftrs, 2) model_ft = model_ft.to(device) criterion = nn.CrossEntropyLoss() # Observe that all parameters are being optimized optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) 训练和评估 CPU上需要大约15-25分钟。但是在GPU上，它只需不到一分钟。 model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25) Out: Epoch 0/24 ---------- train Loss: 0.6022 Acc: 0.6844 val Loss: 0.1765 Acc: 0.9412 Epoch 1/24 ---------- train Loss: 0.4156 Acc: 0.8238 val Loss: 0.2380 Acc: 0.9216 Epoch 2/24 ---------- train Loss: 0.5010 Acc: 0.7951 val Loss: 0.2571 Acc: 0.8954 Epoch 3/24 ---------- train Loss: 0.7152 Acc: 0.7705 val Loss: 0.2060 Acc: 0.9346 Epoch 4/24 ---------- train Loss: 0.5779 Acc: 0.8033 val Loss: 0.4542 Acc: 0.8889 Epoch 5/24 ---------- train Loss: 0.5653 Acc: 0.7951 val Loss: 0.3167 Acc: 0.8824 Epoch 6/24 ---------- train Loss: 0.4948 Acc: 0.8074 val Loss: 0.3238 Acc: 0.8758 Epoch 7/24 ---------- train Loss: 0.3712 Acc: 0.8361 val Loss: 0.2284 Acc: 0.9020 Epoch 8/24 ---------- train Loss: 0.2982 Acc: 0.8730 val Loss: 0.3488 Acc: 0.8497 Epoch 9/24 ---------- train Loss: 0.2491 Acc: 0.8934 val Loss: 0.2405 Acc: 0.8889 Epoch 10/24 ---------- train Loss: 0.3498 Acc: 0.8238 val Loss: 0.2435 Acc: 0.8889 Epoch 11/24 ---------- train Loss: 0.3042 Acc: 0.8648 val Loss: 0.3021 Acc: 0.8627 Epoch 12/24 ---------- train Loss: 0.2500 Acc: 0.8852 val Loss: 0.2340 Acc: 0.8954 Epoch 13/24 ---------- train Loss: 0.3246 Acc: 0.8730 val Loss: 0.2236 Acc: 0.9020 Epoch 14/24 ---------- train Loss: 0.2976 Acc: 0.8566 val Loss: 0.2928 Acc: 0.8562 Epoch 15/24 ---------- train Loss: 0.2733 Acc: 0.8934 val Loss: 0.2370 Acc: 0.8954 Epoch 16/24 ---------- train Loss: 0.3502 Acc: 0.8361 val Loss: 0.2792 Acc: 0.8824 Epoch 17/24 ---------- train Loss: 0.2215 Acc: 0.8975 val Loss: 0.2790 Acc: 0.8497 Epoch 18/24 ---------- train Loss: 0.3929 Acc: 0.8484 val Loss: 0.2648 Acc: 0.8824 Epoch 19/24 ---------- train Loss: 0.3227 Acc: 0.8607 val Loss: 0.2643 Acc: 0.8693 Epoch 20/24 ---------- train Loss: 0.3816 Acc: 0.8484 val Loss: 0.2395 Acc: 0.9085 Epoch 21/24 ---------- train Loss: 0.2904 Acc: 0.8975 val Loss: 0.2399 Acc: 0.8889 Epoch 22/24 ---------- train Loss: 0.3375 Acc: 0.8648 val Loss: 0.2380 Acc: 0.9020 Epoch 23/24 ---------- train Loss: 0.2107 Acc: 0.9139 val Loss: 0.2251 Acc: 0.9085 Epoch 24/24 ---------- train Loss: 0.3243 Acc: 0.8525 val Loss: 0.2545 Acc: 0.8824 Training complete in 1m 7s Best val Acc: 0.941176 visualize_model(model_ft) ConvNet 作为固定特征提取器 在这里，我们需要冻结除最后一层之外的所有网络。我们需要设置 requires_grad == False 冻结参数，以便在 backward() 中不计算梯度。 您可以在 此处 的文档中阅读更多相关信息。 model_conv = torchvision.models.resnet18(pretrained=True) for param in model_conv.parameters(): param.requires_grad = False # Parameters of newly constructed modules have requires_grad=True by default num_ftrs = model_conv.fc.in_features model_conv.fc = nn.Linear(num_ftrs, 2) model_conv = model_conv.to(device) criterion = nn.CrossEntropyLoss() # Observe that only parameters of final layer are being optimized as # opposed to before. optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) 训练和评估 在CPU上，与前一个场景相比，这将花费大约一半的时间。这是预期的，因为不需要为大多数网络计算梯度。但是，前向传递需要计算梯度。 model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25) Out: Epoch 0/24 ---------- train Loss: 0.5666 Acc: 0.6967 val Loss: 0.2794 Acc: 0.8824 Epoch 1/24 ---------- train Loss: 0.5590 Acc: 0.7582 val Loss: 0.1473 Acc: 0.9477 Epoch 2/24 ---------- train Loss: 0.4187 Acc: 0.8156 val Loss: 0.3534 Acc: 0.8693 Epoch 3/24 ---------- train Loss: 0.5248 Acc: 0.7459 val Loss: 0.1848 Acc: 0.9477 Epoch 4/24 ---------- train Loss: 0.4315 Acc: 0.8115 val Loss: 0.1640 Acc: 0.9477 Epoch 5/24 ---------- train Loss: 0.3948 Acc: 0.8238 val Loss: 0.1609 Acc: 0.9542 Epoch 6/24 ---------- train Loss: 0.3359 Acc: 0.8648 val Loss: 0.1734 Acc: 0.9608 Epoch 7/24 ---------- train Loss: 0.3681 Acc: 0.8443 val Loss: 0.1715 Acc: 0.9477 Epoch 8/24 ---------- train Loss: 0.4034 Acc: 0.8361 val Loss: 0.1602 Acc: 0.9477 Epoch 9/24 ---------- train Loss: 0.2983 Acc: 0.8811 val Loss: 0.1561 Acc: 0.9542 Epoch 10/24 ---------- train Loss: 0.4516 Acc: 0.7992 val Loss: 0.1660 Acc: 0.9477 Epoch 11/24 ---------- train Loss: 0.3516 Acc: 0.8484 val Loss: 0.1551 Acc: 0.9542 Epoch 12/24 ---------- train Loss: 0.3592 Acc: 0.8238 val Loss: 0.1525 Acc: 0.9477 Epoch 13/24 ---------- train Loss: 0.2982 Acc: 0.8648 val Loss: 0.1772 Acc: 0.9542 Epoch 14/24 ---------- train Loss: 0.3352 Acc: 0.8484 val Loss: 0.1583 Acc: 0.9542 Epoch 15/24 ---------- train Loss: 0.2981 Acc: 0.8770 val Loss: 0.2133 Acc: 0.9412 Epoch 16/24 ---------- train Loss: 0.2778 Acc: 0.8811 val Loss: 0.1934 Acc: 0.9542 Epoch 17/24 ---------- train Loss: 0.3678 Acc: 0.8156 val Loss: 0.1846 Acc: 0.9477 Epoch 18/24 ---------- train Loss: 0.3520 Acc: 0.8197 val Loss: 0.1577 Acc: 0.9542 Epoch 19/24 ---------- train Loss: 0.3342 Acc: 0.8402 val Loss: 0.1734 Acc: 0.9542 Epoch 20/24 ---------- train Loss: 0.3649 Acc: 0.8361 val Loss: 0.1554 Acc: 0.9412 Epoch 21/24 ---------- train Loss: 0.2948 Acc: 0.8566 val Loss: 0.1878 Acc: 0.9542 Epoch 22/24 ---------- train Loss: 0.3047 Acc: 0.8811 val Loss: 0.1760 Acc: 0.9477 Epoch 23/24 ---------- train Loss: 0.3363 Acc: 0.8648 val Loss: 0.1660 Acc: 0.9542 Epoch 24/24 ---------- train Loss: 0.2745 Acc: 0.8770 val Loss: 0.1853 Acc: 0.9542 Training complete in 0m 34s Best val Acc: 0.960784 visualize_model(model_conv) plt.ioff() plt.show() 脚本总运行时间: (1分54.087秒) Download Python source code: transfer_learning_tutorial.pyDownload Jupyter notebook: transfer_learning_tutorial.ipynb 由Sphinx-Gallery生成的图库 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"deploy_seq2seq_hybrid_frontend_tutorial.html":{"url":"deploy_seq2seq_hybrid_frontend_tutorial.html","title":"混合前端的 seq2seq 模型部署","keywords":"","body":"混合前端的seq2seq模型部署 译者：cangyunye 校对者：FontTian 作者: Matthew Inkawhich 本教程将介绍如何是seq2seq模型转换为PyTorch可用的前端混合Torch脚本。 我们要转换的模型是来自于聊天机器人教程 Chatbot tutorial. 你可以把这个教程当做Chatbot tutorial的第二篇章,并且部署你的预训练模型，或者你也可以依据本文使用我们采取的预训练模型。就后者而言，你可以从原始的Chatbot tutorial参考更详细的数据预处理，模型理论和定义以及模型训练。 什么是混合前端（Hybrid Frontend）? 在一个基于深度学习项目的研发阶段, 使用像PyTorch这样即时eager、命令式的界面进行交互能带来很大便利。 这使用户能够在使用Python数据结构、控制流操作、打印语句和调试实用程序时通过熟悉的、惯用的Python脚本编写。尽管即时性界面对于研究和试验应用程序是一个有用的工具，但是对于生产环境中部署模型时，使用基于图形graph-based的模型表示将更加适用的。 一个延迟的图型展示意味着可以优化，比如无序执行操作，以及针对高度优化的硬件架构的能力。 此外，基于图形的表示支持框架无关的模型导出。PyTorch提供了将即时模式的代码增量转换为Torch脚本的机制，Torch脚本是一个在Python中的静态可分析和可优化的子集，Torch使用它来在Python运行时独立进行深度学习。 在Torch中的torch.jit模块可以找到将即时模式的PyTorch程序转换为Torch脚本的API。 这个模块有两个核心模式用于将即时模式模型转换为Torch脚本图形表示: 跟踪tracing 以及 脚本化scripting。torch.jit.trace 函数接受一个模块或者一个函数和一组示例的输入，然后通过函数或模块运行输入示例，同时跟跟踪遇到的计算步骤，然后输出一个可以展示跟踪流程的基于图形的函数。跟踪Tracing对于不涉及依赖于数据的控制流的直接的模块和函数非常有用，就比如标准的卷积神经网络。然而，如果一个有数据依赖的if语句和循环的函数被跟踪，则只记录示例输入沿执行路径调用的操作。换句话说，控制流本身并没有被捕获。要将带有数据依赖控制流的模块和函数进行转化，已提供了一个脚本化机制。脚本显式地将模块或函数代码转换为Torch脚本，包括所有可能的控制流路径。 如需使用脚本模式script mode， 要确定继承了 torch.jit.ScriptModule基本类 (取代torch.nn.Module) 并且增加 torch.jit.script 装饰器到你的Python函数或者 torch.jit.script_method 装饰器到你的模块方法。使用脚本化的一个警告是，它只支持Python的一个受限子集。要获取与支持的特性相关的所有详细信息，请参考 Torch Script language reference。为了达到最大的灵活性，可以组合Torch脚本的模式来表示整个程序，并且可以增量地应用这些技术。 致谢 本篇教程灵感来自如下资源： Yuan-Kuei Wu’s pytorch-chatbot implementation: https://github.com/ywk991112/pytorch-chatbot Sean Robertson’s practical-pytorch seq2seq-translation example: https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation FloydHub’s Cornell Movie Corpus preprocessing code: https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus 预备环境 首先，我们应该要导入所需的模块以及设置一些常量。如果你想使用自己的模型，需要保证MAX_LENGTH常量设置正确。提醒一下，这个常量定义了在训练过程中允许的最大句子长度以及模型能够产生的最大句子长度输出。 from __future__ import absolute_import from __future__ import division from __future__ import print_function from __future__ import unicode_literals import torch import torch.nn as nn import torch.nn.functional as F import re import os import unicodedata import numpy as np device = torch.device(\"cpu\") MAX_LENGTH = 10 # Maximum sentence length # Default word tokens PAD_token = 0 # Used for padding short sentences SOS_token = 1 # Start-of-sentence token EOS_token = 2 # End-of-sentence token 模型概览 正如前文所言，我们使用的sequence-to-sequence (seq2seq) 模型。这种类型的模型用于输入是可变长度序列的情况，我们的输出也是一个可变长度序列它不一定是一对一输入映射。seq2seq 模型由两个递归神经网络(RNNs)组成：编码器 encoder和解码器decoder. 图片来源: https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/ 编码器(Encoder) 编码器RNN在输入语句中每次迭代一个标记(例如单词)，每次步骤输出一个“输出”向量和一个“隐藏状态”向量。”隐藏状态“向量在之后则传递到下一个步骤，同时记录输出向量。编码器将序列中每个坐标代表的文本转换为高维空间中的一组坐标，解码器将使用这些坐标为给定的任务生成有意义的输出。 解码器(Decoder) 解码器RNN以逐个令牌的方式生成响应语句。它使用来自于编码器的文本向量和内部隐藏状态来生成序列中的下一个单词。它继续生成单词，直到输出表示句子结束的EOS语句。我们在解码器中使用专注机制attention mechanism来帮助它在输入的某些部分生成输出时\"保持专注\"。对于我们的模型，我们实现了 Luong et al等人的“全局关注Global attention”模块，并将其作为解码模型中的子模块。 数据处理 尽管我们的模型在概念上处理标记序列，但在现实中，它们与所有机器学习模型一样处理数字。在这种情况下，在训练之前建立的模型词汇表中的每个单词都映射到一个整数索引。我们使用Voc对象来包含从单词到索引的映射，以及词汇表中的单词总数。我们将在运行模型之前加载对象。 此外，为了能够进行评估，我们必须提供一个处理字符串输入的工具。normalizeString函数将字符串中的所有字符转换为小写，并删除所有非字母字符。indexesFromSentence函数接受一个单词的句子并返回相应的单词索引序列。 class Voc: def __init__(self, name): self.name = name self.trimmed = False self.word2index = {} self.word2count = {} self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"} self.num_words = 3 # Count SOS, EOS, PAD def addSentence(self, sentence): for word in sentence.split(' '): self.addWord(word) def addWord(self, word): if word not in self.word2index: self.word2index[word] = self.num_words self.word2count[word] = 1 self.index2word[self.num_words] = word self.num_words += 1 else: self.word2count[word] += 1 # Remove words below a certain count threshold def trim(self, min_count): if self.trimmed: return self.trimmed = True keep_words = [] for k, v in self.word2count.items(): if v >= min_count: keep_words.append(k) print('keep_words {} / {} = {:.4f}'.format( len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index) )) # Reinitialize dictionaries self.word2index = {} self.word2count = {} self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"} self.num_words = 3 # Count default tokens for word in keep_words: self.addWord(word) # Lowercase and remove non-letter characters def normalizeString(s): s = s.lower() s = re.sub(r\"([.!?])\", r\" \\1\", s) s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) return s # Takes string sentence, returns sentence of word indexes def indexesFromSentence(voc, sentence): return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token] 编码器定义 我们通过torch.nn.GRU模块实现编码器的RNN。本模块接受一批语句(嵌入单词的向量)的输入，它在内部遍历这些句子，每次一个标记，计算隐藏状态。我们将这个模块初始化为双向的，这意味着我们有两个独立的GRUs:一个按时间顺序遍历序列，另一个按相反顺序遍历序列。我们最终返回这两个GRUs输出的和。由于我们的模型是使用批处理进行训练的，所以我们的EncoderRNN模型的forward函数需要一个填充的输入批处理。为了批量处理可变长度的句子，我们通过MAX_LENGTH令牌允许一个句子中支持的最大长度，并且批处理中所有小于MAX_LENGTH令牌的句子都使用我们专用的PAD_token令牌填充在最后。要使用带有PyTorch RNN模块的批量填充，我们必须把转发forward密令在调用torch.nn.utils.rnn.pack_padded_sequence和torch.nn.utils.rnn.pad_packed_sequence数据转换时进行打包。注意，forward函数还接受一个input_length列表，其中包含批处理中每个句子的长度。该输入在填充时通过torch.nn.utils.rnn.pack_padded_sequence使用。 混合前端笔记: 由于编码器的转发函数forward不包含任何依赖于数据的控制流，因此我们将使用跟踪tracing将其转换为脚本模式script mode。在跟踪模块时，我们可以保持模块定义不变。在运行评估之前，我们将在本文末尾初始化所有模型。 class EncoderRNN(nn.Module): def __init__(self, hidden_size, embedding, n_layers=1, dropout=0): super(EncoderRNN, self).__init__() self.n_layers = n_layers self.hidden_size = hidden_size self.embedding = embedding # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size' # because our input size is a word embedding with number of features == hidden_size self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout), bidirectional=True) def forward(self, input_seq, input_lengths, hidden=None): # Convert word indexes to embeddings embedded = self.embedding(input_seq) # Pack padded batch of sequences for RNN module packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths) # Forward pass through GRU outputs, hidden = self.gru(packed, hidden) # Unpack padding outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs) # Sum bidirectional GRU outputs outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Return output and final hidden state return outputs, hidden 解码专注模块定义 接下来，我们将定义我们的注意力模块(Attn)。请注意，此模块将用作解码器模型中的子模块。Luong等人考虑了各种“分数函数”score functions，它们取当前解码器RNN输出和整个编码器输出，并返回关注点“能值”engergies。这个关注能值张量attension energies tensor与编码器输出的大小相同，两者最终相乘，得到一个加权张量，其最大值表示在特定时间步长解码的查询语句最重要的部分。 # Luong attention layer class Attn(torch.nn.Module): def __init__(self, method, hidden_size): super(Attn, self).__init__() self.method = method if self.method not in ['dot', 'general', 'concat']: raise ValueError(self.method, \"is not an appropriate attention method.\") self.hidden_size = hidden_size if self.method == 'general': self.attn = torch.nn.Linear(self.hidden_size, hidden_size) elif self.method == 'concat': self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size) self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size)) def dot_score(self, hidden, encoder_output): return torch.sum(hidden * encoder_output, dim=2) def general_score(self, hidden, encoder_output): energy = self.attn(encoder_output) return torch.sum(hidden * energy, dim=2) def concat_score(self, hidden, encoder_output): energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh() return torch.sum(self.v * energy, dim=2) def forward(self, hidden, encoder_outputs): # Calculate the attention weights (energies) based on the given method if self.method == 'general': attn_energies = self.general_score(hidden, encoder_outputs) elif self.method == 'concat': attn_energies = self.concat_score(hidden, encoder_outputs) elif self.method == 'dot': attn_energies = self.dot_score(hidden, encoder_outputs) # Transpose max_length and batch_size dimensions attn_energies = attn_energies.t() # Return the softmax normalized probability scores (with added dimension) return F.softmax(attn_energies, dim=1).unsqueeze(1) 解码器定义 类似于EncoderRNN，我们使用torch.nn.GRU模块作为我们的解码器RNN。然而，这一次我们使用单向GRU。需要注意的是，与编码器不同，我们将向解码器RNN每次提供一个单词。我们首先得到当前单词的嵌入并应用抛出功能dropout。接下来，我们将嵌入和最后的隐藏状态转发给GRU，得到当前的GRU输出和隐藏状态。然后，我们使用Attn模块作为一个层来获得专注权重，我们将其乘以编码器的输出来获得我们的参与编码器输出。我们使用这个参与编码器输出作为文本context张量，它表示一个加权和，表示编码器输出的哪些部分需要注意。在这里，我们使用线性层linear layer和softmax normalization规范化来选择输出序列中的下一个单词。 混合前端笔记: 与EncoderRNN类似，此模块不包含任何依赖于数据的控制流。因此，在初始化该模型并加载其参数之后，我们可以再次使用跟踪tracing将其转换为Torch脚本。 class LuongAttnDecoderRNN(nn.Module): def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1): super(LuongAttnDecoderRNN, self).__init__() # Keep for reference self.attn_model = attn_model self.hidden_size = hidden_size self.output_size = output_size self.n_layers = n_layers self.dropout = dropout # Define layers self.embedding = embedding self.embedding_dropout = nn.Dropout(dropout) self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout)) self.concat = nn.Linear(hidden_size * 2, hidden_size) self.out = nn.Linear(hidden_size, output_size) self.attn = Attn(attn_model, hidden_size) def forward(self, input_step, last_hidden, encoder_outputs): # Note: we run this one step (word) at a time # Get embedding of current input word embedded = self.embedding(input_step) embedded = self.embedding_dropout(embedded) # Forward through unidirectional GRU rnn_output, hidden = self.gru(embedded, last_hidden) # Calculate attention weights from the current GRU output attn_weights = self.attn(rnn_output, encoder_outputs) # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # Concatenate weighted context vector and GRU output using Luong eq. 5 rnn_output = rnn_output.squeeze(0) context = context.squeeze(1) concat_input = torch.cat((rnn_output, context), 1) concat_output = torch.tanh(self.concat(concat_input)) # Predict next word using Luong eq. 6 output = self.out(concat_output) output = F.softmax(output, dim=1) # Return output and final hidden state return output, hidden 评估定义 贪婪搜索解码器(GreedySearchDecoder) 在聊天机器人教程中，我们使用GreedySearchDecoder模块来简化实际的解码过程。该模块将训练好的编码器和解码器模型作为属性，驱动输入语句(词索引向量)的编码过程，并一次一个词(词索引)迭代地解码输出响应序列 对输入序列进行编码很简单:只需将整个序列张量及其对应的长度向量转发给编码器。需要注意的是，这个模块一次只处理一个输入序列，而不是成批的序列。因此，当常数1用于声明张量大小时，它对应于批处理大小为1。要解码给定的解码器输出，我们必须通过解码器模型迭代地向前运行，该解码器模型输出softmax分数，该分数对应于每个单词在解码序列中是正确的下一个单词的概率。我们将decoder_input初始化为一个包含SOS_token的张量。在每次通过解码器之后，我们贪婪地将softmax概率最高的单词追加到decoded_words列表中。我们还使用这个单词作为下一个迭代的decoder_input。如果``decoded_words列表的长度达到MAX_LENGTH，或者预测的单词是EOS_token，那么解码过程将终止。 混合前端注释: 该模块的forward方法涉及到在每次解码一个单词的输出序列时，遍历/([0,max/_length]/)的范围。因此，我们应该使用脚本将这个模块转换为Torch脚本。与我们可以跟踪的编码器和解码器模型不同，我们必须对GreedySearchDecoder模块进行一些必要的更改，以便在不出错的情况下初始化对象。换句话说，我们必须确保我们的模块遵守脚本机制的规则，并且不使用Torch脚本包含的Python子集之外的任何语言特性。 为了了解可能需要的一些操作，我们将回顾聊天机器人教程中的GreedySearchDecoder实现与下面单元中使用的实现之间的区别。请注意，用红色突出显示的行是从原始实现中删除的行，而用绿色突出显示的行是新的。 变更事项: nn.Module -> torch.jit.ScriptModule 为了在模块上使用PyTorch的脚本化机制, 模型需要从 torch.jit.ScriptModule继承。 将 decoder_n_layers 追加到结构参数 这种变化源于这样一个事实，即我们传递给这个模块的编码器和解码器模型将是TracedModule(非模块)的子模块。因此，我们无法使用decoder.n_layers访问解码器的层数。相反，我们对此进行计划，并在模块构建过程中传入此值。 将新属性作为常量保存 在最初的实现中， 我们可以在GreedySearchDecoder的forward方法中自由地使用来自周围(全局)范围的变量. 然而，现在我们正在使用脚本，我们没有这种自由，因为脚本处理的设想4是我们不一定要保留Python对象，尤其是在导出时。 一个简单的解决方案是将全局作用域中的这些值作为属性存储到构造函数中的模块中， 并将它们添加到一个名为__constants__的特殊列表中，以便在forward方法中构造图形时将它们用作文本值。这种用法的一个例子在第19行，取代使用 device 和 SOS_token 全局值，我们使用常量属性 self._device 和 self._SOS_token。 将 torch.jit.script_method 装饰器添加到 forward 方法 添加这个装饰器可以让JIT编译器知道它所装饰的函数应该是脚本化的。 强制 forward 方法的参数类型 默认情况下，Torch脚本函数的所有参数都假定为张量。如果需要传递不同类型的参数，可以使用PEP 3107中引入的函数类型注释。 此外，还可以使用MyPy-style类型的注释声明不同类型的参数(参见(see doc))。 变更decoder_input的初始化 在原有实现中，我们用torch.LongTensor([[SOS_token]])初始化了 decoder_input 的张量。 当脚本编写时,我们不允许像这样以一种文字方式初始化张量。 取而代之的是，我们可以用一个显式的torch函数，比如torch.ones来初始化我们的张量。这种情况下，我们可以很方便的复制标量 decoder_input 和通过将1乘以我们存在常量中的SOS_token的值 self._SOS_token得到的张量。 class GreedySearchDecoder(torch.jit.ScriptModule): def __init__(self, encoder, decoder, decoder_n_layers): super(GreedySearchDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self._device = device self._SOS_token = SOS_token self._decoder_n_layers = decoder_n_layers __constants__ = ['_device', '_SOS_token', '_decoder_n_layers'] @torch.jit.script_method def forward(self, input_seq : torch.Tensor, input_length : torch.Tensor, max_length : int): # Forward input through encoder model encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length) # Prepare encoder's final hidden layer to be first hidden input to the decoder decoder_hidden = encoder_hidden[:self._decoder_n_layers] # Initialize decoder input with SOS_token decoder_input = torch.ones(1, 1, device=self._device, dtype=torch.long) * self._SOS_token # Initialize tensors to append decoded words to all_tokens = torch.zeros([0], device=self._device, dtype=torch.long) all_scores = torch.zeros([0], device=self._device) # Iteratively decode one word token at a time for _ in range(max_length): # Forward pass through decoder decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs) # Obtain most likely word token and its softmax score decoder_scores, decoder_input = torch.max(decoder_output, dim=1) # Record token and score all_tokens = torch.cat((all_tokens, decoder_input), dim=0) all_scores = torch.cat((all_scores, decoder_scores), dim=0) # Prepare current token to be next decoder input (add a dimension) decoder_input = torch.unsqueeze(decoder_input, 0) # Return collections of word tokens and scores return all_tokens, all_scores 输入评估 接下来，我们定义一些函数来计算输入。求值函数evaluate接受一个规范化字符串语句，将其处理为其对应的单词索引张量(批处理大小为1)，并将该张量传递给一个名为searcher的GreedySearchDecoder实例，以处理编码/解码过程。检索器返回输出的单词索引向量和一个分数张量，该张量对应于每个解码的单词标记的softmax分数。最后一步是使用voc.index2word将每个单词索引转换回其字符串表示形式。 我们还定义了两个函数来计算输入语句。evaluateInput函数提示用户输入，并计算输入。它持续请求另一次输入，直到用户输入“q”或“quit”。 evaluateExample函数只接受一个字符串输入语句作为参数，对其进行规范化、计算并输出响应。 def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH): ### Format input sentence as a batch # words -> indexes indexes_batch = [indexesFromSentence(voc, sentence)] # Create lengths tensor lengths = torch.tensor([len(indexes) for indexes in indexes_batch]) # Transpose dimensions of batch to match models' expectations input_batch = torch.LongTensor(indexes_batch).transpose(0, 1) # Use appropriate device input_batch = input_batch.to(device) lengths = lengths.to(device) # Decode sentence with searcher tokens, scores = searcher(input_batch, lengths, max_length) # indexes -> words decoded_words = [voc.index2word[token.item()] for token in tokens] return decoded_words # Evaluate inputs from user input (stdin) def evaluateInput(encoder, decoder, searcher, voc): input_sentence = '' while(1): try: # Get input sentence input_sentence = input('> ') # Check if it is quit case if input_sentence == 'q' or input_sentence == 'quit': break # Normalize sentence input_sentence = normalizeString(input_sentence) # Evaluate sentence output_words = evaluate(encoder, decoder, searcher, voc, input_sentence) # Format and print response sentence output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')] print('Bot:', ' '.join(output_words)) except KeyError: print(\"Error: Encountered unknown word.\") # Normalize input sentence and call evaluate() def evaluateExample(sentence, encoder, decoder, searcher, voc): print(\"> \" + sentence) # Normalize sentence input_sentence = normalizeString(sentence) # Evaluate sentence output_words = evaluate(encoder, decoder, searcher, voc, input_sentence) output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')] print('Bot:', ' '.join(output_words)) 加载预训练参数 好的，是时候加载我们的模型了 使用托管模型 托管模型使用步骤: 下载模型 here. 设置loadFilename变量作为下载的检查点文件的路径 将checkpoint = torch.load(loadFilename) 行取消注释，表示托管模型在CPU上训练。 使用自己的模型 加载自己的预训练模型设计步骤: 将loadFilename变量设置为希望加载的检查点文件的路径。注意，如果您遵循从chatbot tutorial中保存模型的协议，这会涉及更改model_name、encoder_n_layers、decoder_n_layers、hidden_size和checkpoint_iter(因为这些值在模型路径中使用到)。 如果你在CPU上训练，确保你在 checkpoint = torch.load(loadFilename) 行打开了检查点。如果你在GPU 上训练，并且在CPU运行这篇教程，解除checkpoint = torch.load(loadFilename, map_location=torch.device('cpu')) 的注释。 混合前端的注释: 请注意，我们像往常一样初始化并将参数加载到编码器和解码器模型中。另外，在跟踪模型之前，我们必须调用.to(device)来设置模型的设备选项，调用.eval()来设置抛出层dropout layer为test mode。TracedModule对象不继承to或eval方法 save_dir = os.path.join(\"data\", \"save\") corpus_name = \"cornell movie-dialogs corpus\" # Configure models model_name = 'cb_model' attn_model = 'dot' #attn_model = 'general' #attn_model = 'concat' hidden_size = 500 encoder_n_layers = 2 decoder_n_layers = 2 dropout = 0.1 batch_size = 64 # If you're loading your own model # Set checkpoint to load from checkpoint_iter = 4000 # loadFilename = os.path.join(save_dir, model_name, corpus_name, # '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size), # '{}_checkpoint.tar'.format(checkpoint_iter)) # If you're loading the hosted model loadFilename = 'data/4000_checkpoint.tar' # Load model # Force CPU device options (to match tensors in this tutorial) checkpoint = torch.load(loadFilename, map_location=torch.device('cpu')) encoder_sd = checkpoint['en'] decoder_sd = checkpoint['de'] encoder_optimizer_sd = checkpoint['en_opt'] decoder_optimizer_sd = checkpoint['de_opt'] embedding_sd = checkpoint['embedding'] voc = Voc(corpus_name) voc.__dict__ = checkpoint['voc_dict'] print('Building encoder and decoder ...') # Initialize word embeddings embedding = nn.Embedding(voc.num_words, hidden_size) embedding.load_state_dict(embedding_sd) # Initialize encoder & decoder models encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout) decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout) # Load trained model params encoder.load_state_dict(encoder_sd) decoder.load_state_dict(decoder_sd) # Use appropriate device encoder = encoder.to(device) decoder = decoder.to(device) # Set dropout layers to eval mode encoder.eval() decoder.eval() print('Models built and ready to go!') Out: Building encoder and decoder ... Models built and ready to go! 模型转换为 Torch 脚本 编码器 正如前文所述，要将编码器模型转换为Torch脚本，我们需要使用跟踪Tracing。跟踪任何需要通过模型的forward方法运行一个示例输入，以及跟踪数据相遇时的图形计算。编码器模型接收一个输入序列和一个长度相关的张量。因此，我们创建一个输入序列test_seq，配置合适的大小(MAX_LENGTH,1) 包含适当范围内的数值 [0,voc.num\\_words] 以及搭配的类型(int64)。我们还创建了test_seq_length标量，该标量实际包含与test_seq中单词数量对应的值。下一步是使用torch.jit.trace函数来跟踪模型。注意，我们传递的第一个参数是要跟踪的模块，第二个参数是模块forward方法的参数元组。 解码器 我们对解码器的跟踪过程与对编码器的跟踪过程相同。请注意，我们对traced_encoder的一组随机输入调用forward，以获得解码器所需的输出。这不是必需的，因为我们也可以简单地生成一个形状、类型和值范围正确的张量。这种方法是可行的，因为在我们的例子中，我们对张量的值没有任何约束，因为我们没有任何操作可能导致超出范围的输入出错。 贪婪搜索解码器(GreedySearchDecoder) 回想一下，由于存在依赖于数据的控制流，我们为搜索器模块编写了脚本。在脚本化的情况下，我们通过添加修饰符并确保实现符合脚本规则来预先完成转换工作。我们初始化脚本搜索器的方式与初始化未脚本化变量的方式相同。 ### Convert encoder model # Create artificial inputs test_seq = torch.LongTensor(MAX_LENGTH, 1).random_(0, voc.num_words) test_seq_length = torch.LongTensor([test_seq.size()[0]]) # Trace the model traced_encoder = torch.jit.trace(encoder, (test_seq, test_seq_length)) ### Convert decoder model # Create and generate artificial inputs test_encoder_outputs, test_encoder_hidden = traced_encoder(test_seq, test_seq_length) test_decoder_hidden = test_encoder_hidden[:decoder.n_layers] test_decoder_input = torch.LongTensor(1, 1).random_(0, voc.num_words) # Trace the model traced_decoder = torch.jit.trace(decoder, (test_decoder_input, test_decoder_hidden, test_encoder_outputs)) ### Initialize searcher module scripted_searcher = GreedySearchDecoder(traced_encoder, traced_decoder, decoder.n_layers) 图形打印 现在我们的模型是Torch脚本形式的，我们可以打印每个模型的图形，以确保适当地捕获计算图形。因为scripted_searcher包含traced_encoder和traced_decoder，所以这些图将以内联方式打印 print('scripted_searcher graph:\\n', scripted_searcher.graph) Out: scripted_searcher graph: graph(%input_seq : Tensor %input_length : Tensor %max_length : int %3 : Tensor %4 : Tensor %5 : Tensor %6 : Tensor %7 : Tensor %8 : Tensor %9 : Tensor %10 : Tensor %11 : Tensor %12 : Tensor %13 : Tensor %14 : Tensor %15 : Tensor %16 : Tensor %17 : Tensor %18 : Tensor %19 : Tensor %118 : Tensor %119 : Tensor %120 : Tensor %121 : Tensor %122 : Tensor %123 : Tensor %124 : Tensor %125 : Tensor %126 : Tensor %127 : Tensor %128 : Tensor %129 : Tensor %130 : Tensor) { %58 : int = prim::Constant[value=9223372036854775807](), scope: EncoderRNN %53 : float = prim::Constant[value=0](), scope: EncoderRNN %43 : float = prim::Constant[value=0.1](), scope: EncoderRNN/GRU[gru] %42 : int = prim::Constant[value=2](), scope: EncoderRNN/GRU[gru] %41 : bool = prim::Constant[value=1](), scope: EncoderRNN/GRU[gru] %36 : int = prim::Constant[value=6](), scope: EncoderRNN/GRU[gru] %34 : int = prim::Constant[value=500](), scope: EncoderRNN/GRU[gru] %25 : int = prim::Constant[value=4](), scope: EncoderRNN %24 : Device = prim::Constant[value=\"cpu\"](), scope: EncoderRNN %21 : bool = prim::Constant[value=0](), scope: EncoderRNN/Embedding[embedding] %20 : int = prim::Constant[value=-1](), scope: EncoderRNN/Embedding[embedding] %90 : int = prim::Constant[value=0]() %94 : int = prim::Constant[value=1]() %input.7 : Float(10, 1, 500) = aten::embedding(%3, %input_seq, %20, %21, %21), scope: EncoderRNN/Embedding[embedding] %lengths : Long(1) = aten::to(%input_length, %24, %25, %21, %21), scope: EncoderRNN %input.1 : Float(10, 500), %batch_sizes : Long(10) = aten::_pack_padded_sequence(%input.7, %lengths, %21), scope: EncoderRNN %35 : int[] = prim::ListConstruct(%25, %94, %34), scope: EncoderRNN/GRU[gru] %hx : Float(4, 1, 500) = aten::zeros(%35, %36, %90, %24), scope: EncoderRNN/GRU[gru] %40 : Tensor[] = prim::ListConstruct(%4, %5, %6, %7, %8, %9, %10, %11, %12, %13, %14, %15, %16, %17, %18, %19), scope: EncoderRNN/GRU[gru] %46 : Float(10, 1000), %encoder_hidden : Float(4, 1, 500) = aten::gru(%input.1, %batch_sizes, %hx, %40, %41, %42, %43, %21, %41), scope: EncoderRNN/GRU[gru] %49 : int = aten::size(%batch_sizes, %90), scope: EncoderRNN %max_seq_length : Long() = prim::NumToTensor(%49), scope: EncoderRNN %51 : int = prim::Int(%max_seq_length), scope: EncoderRNN %outputs : Float(10, 1, 1000), %55 : Long(1) = aten::_pad_packed_sequence(%46, %batch_sizes, %21, %53, %51), scope: EncoderRNN %60 : Float(10, 1, 1000) = aten::slice(%outputs, %90, %90, %58, %94), scope: EncoderRNN %65 : Float(10, 1, 1000) = aten::slice(%60, %94, %90, %58, %94), scope: EncoderRNN %70 : Float(10, 1!, 500) = aten::slice(%65, %42, %90, %34, %94), scope: EncoderRNN %75 : Float(10, 1, 1000) = aten::slice(%outputs, %90, %90, %58, %94), scope: EncoderRNN %80 : Float(10, 1, 1000) = aten::slice(%75, %94, %90, %58, %94), scope: EncoderRNN %85 : Float(10, 1!, 500) = aten::slice(%80, %42, %34, %58, %94), scope: EncoderRNN %encoder_outputs : Float(10, 1, 500) = aten::add(%70, %85, %94), scope: EncoderRNN %decoder_hidden.1 : Tensor = aten::slice(%encoder_hidden, %90, %90, %42, %94) %98 : int[] = prim::ListConstruct(%94, %94) %100 : Tensor = aten::ones(%98, %25, %90, %24) %decoder_input.1 : Tensor = aten::mul(%100, %94) %103 : int[] = prim::ListConstruct(%90) %all_tokens.1 : Tensor = aten::zeros(%103, %25, %90, %24) %108 : int[] = prim::ListConstruct(%90) %all_scores.1 : Tensor = aten::zeros(%108, %36, %90, %24) %all_scores : Tensor, %all_tokens : Tensor, %decoder_hidden : Tensor, %decoder_input : Tensor = prim::Loop(%max_length, %41, %all_scores.1, %all_tokens.1, %decoder_hidden.1, %decoder_input.1) block0(%114 : int, %188 : Tensor, %184 : Tensor, %116 : Tensor, %115 : Tensor) { %input.2 : Float(1, 1, 500) = aten::embedding(%118, %115, %20, %21, %21), scope: LuongAttnDecoderRNN/Embedding[embedding] %input.3 : Float(1, 1, 500) = aten::dropout(%input.2, %43, %21), scope: LuongAttnDecoderRNN/Dropout[embedding_dropout] %138 : Tensor[] = prim::ListConstruct(%119, %120, %121, %122, %123, %124, %125, %126), scope: LuongAttnDecoderRNN/GRU[gru] %hidden : Float(1, 1, 500), %decoder_hidden.2 : Float(2, 1, 500) = aten::gru(%input.3, %116, %138, %41, %42, %43, %21, %21, %21), scope: LuongAttnDecoderRNN/GRU[gru] %147 : Float(10, 1, 500) = aten::mul(%hidden, %encoder_outputs), scope: LuongAttnDecoderRNN/Attn[attn] %149 : int[] = prim::ListConstruct(%42), scope: LuongAttnDecoderRNN/Attn[attn] %attn_energies : Float(10, 1) = aten::sum(%147, %149, %21), scope: LuongAttnDecoderRNN/Attn[attn] %input.4 : Float(1!, 10) = aten::t(%attn_energies), scope: LuongAttnDecoderRNN/Attn[attn] %154 : Float(1, 10) = aten::softmax(%input.4, %94), scope: LuongAttnDecoderRNN/Attn[attn] %attn_weights : Float(1, 1, 10) = aten::unsqueeze(%154, %94), scope: LuongAttnDecoderRNN/Attn[attn] %159 : Float(1!, 10, 500) = aten::transpose(%encoder_outputs, %90, %94), scope: LuongAttnDecoderRNN %context.1 : Float(1, 1, 500) = aten::bmm(%attn_weights, %159), scope: LuongAttnDecoderRNN %rnn_output : Float(1, 500) = aten::squeeze(%hidden, %90), scope: LuongAttnDecoderRNN %context : Float(1, 500) = aten::squeeze(%context.1, %94), scope: LuongAttnDecoderRNN %165 : Tensor[] = prim::ListConstruct(%rnn_output, %context), scope: LuongAttnDecoderRNN %input.5 : Float(1, 1000) = aten::cat(%165, %94), scope: LuongAttnDecoderRNN %168 : Float(1000!, 500!) = aten::t(%127), scope: LuongAttnDecoderRNN/Linear[concat] %171 : Float(1, 500) = aten::addmm(%128, %input.5, %168, %94, %94), scope: LuongAttnDecoderRNN/Linear[concat] %input.6 : Float(1, 500) = aten::tanh(%171), scope: LuongAttnDecoderRNN %173 : Float(500!, 7826!) = aten::t(%129), scope: LuongAttnDecoderRNN/Linear[out] %input : Float(1, 7826) = aten::addmm(%130, %input.6, %173, %94, %94), scope: LuongAttnDecoderRNN/Linear[out] %decoder_output : Float(1, 7826) = aten::softmax(%input, %94), scope: LuongAttnDecoderRNN %decoder_scores : Tensor, %decoder_input.2 : Tensor = aten::max(%decoder_output, %94, %21) %186 : Tensor[] = prim::ListConstruct(%184, %decoder_input.2) %all_tokens.2 : Tensor = aten::cat(%186, %90) %190 : Tensor[] = prim::ListConstruct(%188, %decoder_scores) %all_scores.2 : Tensor = aten::cat(%190, %90) %decoder_input.3 : Tensor = aten::unsqueeze(%decoder_input.2, %90) -> (%41, %all_scores.2, %all_tokens.2, %decoder_hidden.2, %decoder_input.3) } %198 : (Tensor, Tensor) = prim::TupleConstruct(%all_tokens, %all_scores) return (%198); } 运行结果评估 最后，我们将使用Torch脚本模型对聊天机器人模型进行评估。如果转换正确，模型的行为将与它们在即时模式表示中的行为完全相同。 默认情况下，我们计算一些常见的查询语句。如果您想自己与机器人聊天，取消对evaluateInput行的注释并让它旋转。 # Evaluate examples sentences = [\"hello\", \"what's up?\", \"who are you?\", \"where am I?\", \"where are you from?\"] for s in sentences: evaluateExample(s, traced_encoder, traced_decoder, scripted_searcher, voc) # Evaluate your input #evaluateInput(traced_encoder, traced_decoder, scripted_searcher, voc) Out: > hello Bot: hello . > what's up? Bot: i m going to get my car . > who are you? Bot: i m the owner . > where am I? Bot: in the house . > where are you from? Bot: south america . 保存模型 现在我们已经成功地将模型转换为Torch脚本，接下来将对其进行序列化，以便在非python部署环境中使用。为此，我们只需保存scripted_searcher模块，因为这是用于对聊天机器人模型运行推理的面向用户的接口。保存脚本模块时，使用script_module.save(PATH)代替torch.save(model, PATH)。 scripted_searcher.save(\"scripted_chatbot.pth\") 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"saving_loading_models.html":{"url":"saving_loading_models.html","title":"Saving and Loading Models","keywords":"","body":"保存和加载模型 译者 bruce1408 作者: Matthew Inkawhich 本文提供有关Pytorch模型保存和加载的各种用例的解决方案。您可以随意阅读整个文档，或者只是跳转到所需用例的代码部分。 当保存和加载模型时，有三个核心功能需要熟悉： torch.save: 将序列化对象保存到磁盘。 此函数使用 Python 的pickle模块进行序列化。使用此函数可以保存如模型、tensor、字典等各种对象。 torch.load: 使用 pickle的 unpickling 功能将pickle对象文件反序列化到内存。 此功能还可以有助于设备加载数据(详见 Saving & Loading Model Across Devices). torch.nn.Module.load_state_dict: 使用反序列化函数 state_dict 来加载模型的参数字典。更多有关 state_dict 的信息，请参考What is a state_dict?. 内容: 什么是状态字典? 保存和加载推断模型 保存 和 加载 Checkpoint 在一个文件中保存多个模型 使用在不同模型参数下的热启动模式 Saving & Loading Model Across Devices 什么是 状态字典? 在Pytorch中，torch.nn.Module 模型的可学习参数(即权重和偏差)包含在模型的 parameters 中，(使用model.parameters()可以进行访问)。 state_dict 仅仅是python字典对象，它将每一层映射到其参数张量。注意，只有具有可学习参数的层(如卷积层、线性层等)的模型才具有 state_dict 这一项。优化目标 torch.optim 也有 state_dict 属性，它包含有关优化器的状态信息，以及使用的超参数。 因为 state_dict 的对象是python字典，所以他们可以很容易的保存、更新、更改和恢复，为Pytorch模型和优化器添加了大量模块。 示例: 让我们从 简单模型训练一个分类器中了解一下 state_dict 的使用。 # Define model class TheModelClass(nn.Module): def __init__(self): super(TheModelClass, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x # Initialize model model = TheModelClass() # Initialize optimizer optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) # Print model's state_dict print(\"Model's state_dict:\") for param_tensor in model.state_dict(): print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size()) # Print optimizer's state_dict print(\"Optimizer's state_dict:\") for var_name in optimizer.state_dict(): print(var_name, \"\\t\", optimizer.state_dict()[var_name]) 输出: Model's state_dict: conv1.weight torch.Size([6, 3, 5, 5]) conv1.bias torch.Size([6]) conv2.weight torch.Size([16, 6, 5, 5]) conv2.bias torch.Size([16]) fc1.weight torch.Size([120, 400]) fc1.bias torch.Size([120]) fc2.weight torch.Size([84, 120]) fc2.bias torch.Size([84]) fc3.weight torch.Size([10, 84]) fc3.bias torch.Size([10]) Optimizer's state_dict: state {} param_groups [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [4675713712, 4675713784, 4675714000, 4675714072, 4675714216, 4675714288, 4675714432, 4675714504, 4675714648, 4675714720]}] 保存和加载推断模型 保存/加载 state_dict (推荐使用) 保存: torch.save(model.state_dict(), PATH) 加载: model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH)) model.eval() 当保存好模型用来推断的时候，只需要保存模型学习到的参数，使用 torch.save() 函数来保存模型 state_dict ,它会给模型恢复提供最大的灵活性，这就是为什么要推荐它来保存的原因。 在 Pytorch 中最常见的模型保存使用 ‘.pt’ 或者是 ‘.pth’ 作为模型文件扩展名。 请记住，在运行推理之前，务必调用 model.eval() 去设置 dropout 和 batch normalization 层为评估模式。如果不这么做，可能导致模型推断结果不一致。 注意 请注意 load_state_dict() 函数只接受字典对象，而不是保存对象的路径。这就意味着在你传给 load_state_dict() 函数之前，你必须反序列化你保存的 state_dict。例如，你无法通过 model.load_state_dict(PATH)来加载模型。 保存/加载完整模型 保存: torch.save(model, PATH) 加载: # Model class must be defined somewhere model = torch.load(PATH) model.eval() 此部分保存/加载过程使用最直观的语法并涉及最少量的代码。以Pythonpickle模块的方式来保存模型。这种方法的缺点是序列化数据受限于某种特殊的类而且需要确切的字典结构。这是因为pickle无法保存模型类本身。相反，它保存包含类的文件的路径，该文件在加载时使用。因此，当在其他项目使用或者重构之后，您的代码可能会以各种方式中断。 在 Pytorch 中最常见的模型保存使用 ‘.pt’ 或者是 ‘.pth’ 作为模型文件扩展名。 请记住，在运行推理之前，务必调用 model.eval() 去设置 dropout 和 batch normalization 层为评估模式。如果不这么做，可能导致模型推断结果不一致。 保存 和 加载 Checkpoint 用于推理/继续训练 保存: torch.save({ 'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'loss': loss, ... }, PATH) 加载: model = TheModelClass(*args, **kwargs) optimizer = TheOptimizerClass(*args, **kwargs) checkpoint = torch.load(PATH) model.load_state_dict(checkpoint['model_state_dict']) optimizer.load_state_dict(checkpoint['optimizer_state_dict']) epoch = checkpoint['epoch'] loss = checkpoint['loss'] model.eval() # - or - model.train() 当保存成 checkpoint 的时候，可用于推理或者是恢复训练，您保存的不仅仅是模型的 state_dict 。 保存优化器的 state_dict 也很重要, 因为它包含作为模型训练更新的缓冲区和参数。你也许想保存其他项目，比如最新记录的训练损失，外部的 torch.nn.Embedding 层等等。 要保存多个组件，请在字典中组织它们并使用 torch.save() 来序列化字典。 Pytorch 中常见的保存checkpoint 是使用 .tar 文件扩展名。 要加载项目，首先需要初始化模型和优化器，然后使用 torch.load() 来加载本地字典。 这里，您可以非常容易的通过简单查询字典来访问您所保存的项目。 请记住在运行推理之前，务必调用 model.eval() 去设置 dropout 和 batch normalization 为评估。如果不这样做，有可能得到不一致的推断结果。如果你想要恢复训练，请调用 model.train() 以确保这些层处于训练模式。 在一个文件中保存多个模型 保存: torch.save({ 'modelA_state_dict': modelA.state_dict(), 'modelB_state_dict': modelB.state_dict(), 'optimizerA_state_dict': optimizerA.state_dict(), 'optimizerB_state_dict': optimizerB.state_dict(), ... }, PATH) 加载: modelA = TheModelAClass(*args, **kwargs) modelB = TheModelBClass(*args, **kwargs) optimizerA = TheOptimizerAClass(*args, **kwargs) optimizerB = TheOptimizerBClass(*args, **kwargs) checkpoint = torch.load(PATH) modelA.load_state_dict(checkpoint['modelA_state_dict']) modelB.load_state_dict(checkpoint['modelB_state_dict']) optimizerA.load_state_dict(checkpoint['optimizerA_state_dict']) optimizerB.load_state_dict(checkpoint['optimizerB_state_dict']) modelA.eval() modelB.eval() # - or - modelA.train() modelB.train() 当保存一个模型由多个 torch.nn.Modules组成时，例如GAN(对抗生成网络), sequence-to-sequence (序列到序列模型), 或者是多个模型融合, 您可以采用与保存常规检查点相同的方法。换句话说，保存每个模型的 state_dict 的字典和相对应的优化器。如前所述，您可以通过简单地将它们附加到字典的方式来保存任何其他项目，这样有助于您恢复训练。 Pytorch 中常见的保存checkpoint 是使用 .tar 文件扩展名。 要加载项目，首先需要初始化模型和优化器，然后使用 torch.load() 来加载本地字典。 这里，您可以非常容易的通过简单查询字典来访问您所保存的项目。 请记住在运行推理之前，务必调用 model.eval() 去设置 dropout 和 batch normalization 为评估。如果不这样做，有可能得到不一致的推断结果。如果你想要恢复训练，请调用 model.train() 以确保这些层处于训练模式。 使用在不同模型参数下的热启动模式 保存: torch.save(modelA.state_dict(), PATH) 加载: modelB = TheModelBClass(*args, **kwargs) modelB.load_state_dict(torch.load(PATH), strict=False) 在迁移学习或训练新的复杂模型时， 部分加载模型或加载部分模型是常见的情况。利用训练好的参数，有助于热启动训练过程，并希望帮助您的模型比从头开始训练更快地收敛 无论是从缺少某些键的 state_dict 加载还是从键数多于加载模型的 state_dict , 您可以通过在load_state_dict()函数中将strict参数设置为 False 来忽略非匹配键的函数。 如果要将参数从一个层加载到另一个层，但是某些键不匹配，主要修改正在加载的 state_dict 中的参数键的名称以匹配要在加载到模型中的键即可。 通过设备保存/加载模型 保存到 GPU, 加载到 CPU 保存: torch.save(model.state_dict(), PATH) 加载: device = torch.device('cpu') model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH, map_location=device)) 当从CPU上加载模型在GPU上训练时, 将 torch.device('cpu') 传递给 torch.load() 函数中的 map_location参数.在这种情况下，使用map_location 参数将张量下的存储器动态的重新映射到CPU设备。 保存到 GPU, 加载到 GPU 保存: torch.save(model.state_dict(), PATH) 加载: device = torch.device(\"cuda\") model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH)) model.to(device) # Make sure to call input = input.to(device) on any input tensors that you feed to the model 当在GPU上训练并把模型保存在GPU，只需要使用 model.to(torch.device('cuda'))，将初始化的 model 转换为CUDA优化模型。另外，请务必在所有模型输入上使用 .to(torch.device('cuda')) 函数来为模型准备数据。请注意，调用 my_tensor.to(device) 会在GPU上返回my_tensor 的副本。因此，请记住手动覆盖张量：my_tensor= my_tensor.to(torch.device('cuda'))。 保存到 CPU, 加载到 GPU 保存: torch.save(model.state_dict(), PATH) 加载: device = torch.device(\"cuda\") model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\")) # Choose whatever GPU device number you want model.to(device) # Make sure to call input = input.to(device) on any input tensors that you feed to the model 在CPU上训练好并保存的模型加载到GPU时， 将torch.load() 函数中的 map_location 参数设置为 cuda:device_id。这会将模型加载到指定的GPU设备。接下来，请务必调用 model.to(torch.device('cuda')) 将模型的参数张量转换为 CUDA 张量。最后，确保在所有模型输入上使用 .to(torch.device('cuda')) 函数来为CUDA优化模型。请注意， 调用 my_tensor.to(device) 会在GPU上返回 my_tensor 的新副本。 它不会覆盖 my_tensor。因此， 请手动覆盖张量 my_tensor = my_tensor.to(torch.device('cuda'))。 保存 torch.nn.DataParallel 模型 保存: torch.save(model.module.state_dict(), PATH) 加载: # Load to whatever device you want torch.nn.DataParallel 是一个模型封装，支持并行GPU使用。要一般性的保存 DataParallel 模型, 请保存 model.module.state_dict()。这样，您就可以非常灵活地以任何方式加载模型到您想要的设备中。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"nn_tutorial.html":{"url":"nn_tutorial.html","title":"What is torch.nn really?","keywords":"","body":"torch.nn 到底是什么？ 译者：lhc741 作者：Jeremy Howard，fast.ai。感谢Rachel Thomas和Francisco Ingham的帮助和支持。 我们推荐使用notebook来运行这个教程，而不是脚本，点击这里下载notebook(.ipynb)文件。 Pytorch提供了torch.nn、torch.optim、Dataset和DataLoader这些设计优雅的模块和类以帮助使用者创建和训练神经网络。 为了最大化利用这些模块和类的功能，并使用它们做出适用于你所研究问题的模型，你需要真正理解他们是如何工作的。 为了做到这一点，我们首先基于MNIST数据集训练一个没有任何特征的简单神经网络。 最开始我们只会用到PyTorch中最基本的tensor功能，然后我们将会逐渐的从torch.nn，torch.optim，Dataset，DataLoader中选择一个特征加入到模型中，来展示新加入的特征会对模型产生什么样的效果，以及它是如何使模型变得更简洁或更灵活。 在这个教程中，我们假设你已经安装好了PyTorch，并且已经熟悉了基本的tensor运算。(如果你熟悉Numpy的数组运算，你将会发现这里用到的PyTorch tensor运算和numpy几乎是一样的) MNIST数据安装 我们将要使用经典的MNIST数据集，这个数据集由手写数字（0到9）的黑白图片组成。 我们将使用pathlib来处理文件路径的相关操作（python3中的一个标准库），使用request来下载数据集。 我们只会在用到相关库的时候进行引用，这样你就可以明确在每个操作中用到了哪些库。 from pathlib import Path import requests DATA_PATH = Path(\"data\") PATH = DATA_PATH / \"mnist\" PATH.mkdir(parents=True, exist_ok=True) URL = \"http://deeplearning.net/data/mnist/\" FILENAME = \"mnist.pkl.gz\" if not (PATH / FILENAME).exists(): content = requests.get(URL + FILENAME).content (PATH / FILENAME).open(\"wb\").write(content)` 该数据集采用numpy数组格式，并已使用pickle存储，pickle是一个用来把数据序列化为python特定格式的库。 import pickle import gzip with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\") 每一幅图像都是28 x 28的，并被拉平成长度为784(=28x28)的一行。 我们以其中一个为例展示一下，首先需要将这个一行的数据重新变形为一个2d的数据。 from matplotlib import pyplot import numpy as np pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\") print(x_train.shape) 输出： (50000, 784) PyTorch使用torch.tensor，而不是numpy数组，所以我们需要将数据转换。 import torch x_train, y_train, x_valid, y_valid = map( torch.tensor, (x_train, y_train, x_valid, y_valid) ) n, c = x_train.shape x_train, x_train.shape, y_train.min(), y_train.max() print(x_train, y_train) print(x_train.shape) print(y_train.min(), y_train.max()) 输出： tensor([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]) tensor([5, 0, 4, ..., 8, 4, 8]) torch.Size([50000, 784]) tensor(0) tensor(9) 神经网络从零开始（不使用torch.nn） 我们先来建立一个只使用PyTorch张量运算的模型。 我们假设你已经熟悉神经网络的基础。（如果你还不熟悉，可以访问course.fast.ai进行学习）。 PyTorch提供创建随机数填充或全零填充张量的方法，我们使用该方法初始化一个简单线性模型的权重和偏置。 这两个都是普通的张量，但它们有一个特殊的附加条件：设置需要计算梯度的参数为True。这样PyTorch就会记录所有与这个张量相关的运算，使其能在反向传播阶段自动计算梯度。 对于weights而言，由于我们希望初始化张量过程中存在梯度，所以我们在初始化之后设置requires_grad。（注意：尾缀为_的方法在PyTorch中表示这个操作会被立即被执行。） 注意： 我们以Xavier初始化方法（每个元素都除以1/sqrt(n)）为例来对权重进行初始化。 import math weights = torch.randn(784, 10) / math.sqrt(784) weights.requires_grad_() bias = torch.zeros(10, requires_grad=True) 多亏了PyTorch具有自动梯度计算功能，我们可以使用Python中任何标准函数（或者可调用对象）来创建模型！ 因此，让我们编写一个普通的矩阵乘法和广播加法建立一个简单的线性模型。 我们还需要一个激活函数，所以我们编写并使用一个log_softmax函数。 请记住：尽管Pytorch提供了许多预先编写好的损失函数、激活函数等等，你仍然可以使用纯python轻松实现你自己的函数。 Pytorch甚至可以自动地为你的函数创建快速的GPU代码或向量化的CPU代码。 def log_softmax(x): return x - x.exp().sum(-1).log().unsqueeze(-1) def model(xb): return log_softmax(xb @ weights + bias) 在上面的一段代码中，@表示点积运算符。我们将调用我们的函数计算一个批次的数据（本例中为64幅图像）。 这是一次模型前向传递的过程。 请注意，因为我们使用了随机数来初始化权重，所以在这个阶段我们的预测值并不会比随机的更好。 bs = 64 # 一批数据个数 xb = x_train[0:bs] # 从x获取一小批数据 preds = model(xb) # 预测值 preds[0], preds.shape print(preds[0], preds.shape) 输出： tensor([-2.4513, -2.5024, -2.0599, -3.1052, -3.2918, -2.2665, -1.9007, -2.2588, -2.0149, -2.0287], grad_fn=) torch.Size([64, 10]) 可以从上面的结果不难看出，张量preds不仅包括了张量值，还包括了梯度函数。这个梯度函数我们可以在后面的反向传播阶段用到。 下面我们来实现一个负的对数似然函数（Negative log-likehood）作为损失函数（同样也使用纯python实现）： def nll(input, target): return -input[range(target.shape[0]), target].mean() loss_func = nll 让我们查看下随机模型的损失值，这样我们就可以确认在执行反向传播的步骤后，模型的预测效果是否有了改进。 yb = y_train[0:bs] print(loss_func(preds, yb)) 输出： tensor(2.3620, grad_fn=) 我们再来实现一个用来计算模型准确率的函数。对于每次预测，我们规定如果预测结果中概率最大的数字和图片实际对应的数字是相同的，那么这次预测就是正确的。 def accuracy(out, yb): preds = torch.argmax(out, dim=1) return (preds == yb).float().mean() 我们先来看一下被随机初始化的模型的准确率，这样我们就可以看到损失值降低的时候准确率是否提高了。 print(accuracy(preds, yb)) 输出： tensor(0.0938) 现在我们可以运行一个完整的训练步骤了，每次迭代，我们会进行以下几个操作： 从全部数据中选择一小批数据（大小为bs） 使用模型进行预测 计算当前预测的损失值 使用loss.backward()更新模型中的梯度，在这个例子中，更新的是weights和bias 现在，我们来利用计算出的梯度对权值和偏置项进行更新，因为我们不希望这一步的操作被用于下一次迭代的梯度计算，所以我们在torch.no_grad()这个上下文管理器中完成。想要了解更多PyTorch Autograd记录操作现，可以点击这里。 接下来，我们将梯度设置为0，来为下一次循环做准备。否则我们的梯度将会记录所有已经执行过的运算（如，loss.backward()会将梯度变化值直接与变量已有值进行累加，而不是替换变量原有的值）。 小贴士 您可以使用标准python调试器对PyTorch代码进行单步调试，从而在每一步检查不同的变量值。取消下面的set_trace()来尝试该功能。 from IPython.core.debugger import set_trace lr = 0.5 # 学习率 epochs = 2 # 训练的轮数 for epoch in range(epochs): for i in range((n - 1) // bs + 1): # set_trace() start_i = i * bs end_i = start_i + bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] pred = model(xb) loss = loss_func(pred, yb) loss.backward() with torch.no_grad(): weights -= weights.grad * lr bias -= bias.grad * lr weights.grad.zero_() bias.grad.zero_() 目前为止，我们已经从零开始完成了建立和训练一个最小的神经网络（因为我们建立的logistic回归模型不包含隐层）！ 现在，我们来看一下模型的损失值和准确率，并于我们之前输出的值进行比较。结果正如我们预期的，损失值下降，准确率提高。 print(loss_func(model(xb), yb), accuracy(model(xb), yb)) 输出： tensor(0.0822, grad_fn=) tensor(1.) torch.nn.functional的使用 现在，我们要对前面的代码进行重构，使代码在完成相同功能的同时，用PyTorch的nn来使代码变得更加简洁和灵活。 从现在开始，接下来的每一步我们都会使代码变得更短，更好理解或更灵活。 要进行的第一步也是最简单的一步，是使用torch.nn.functional（通过会在引用时用F表示）中的函数替换我们自己的激活函数和损失函数使代码变得更短。 这个模块包含了torch.nn库中的所有函数（这个库的其它部分是各种类），所以在这个模块中还会找到其它便于建立神经网络的函数，比如池化函数。（模块中还包含卷积函数，线性函数等等，不过在后面的内容中我们会看到，这些操作使用库中的其它部分会更好。） 如果你使用负对数似然损失和对数柔性最大值(softmax)激活函数，PyTorch有一个结合了这两个函数的简单函数F.cross_entropy供你使用，这样我们就可以删掉模型中的激活函数。 import torch.nn.functional as F loss_func = F.cross_entropy def model(xb): return xb @ weights + bias 注意在model函数中我们不再调用log_softmax。现在我们来确认一下损失值和准确率与之前相同。 print(loss_func(model(xb), yb), accuracy(model(xb), yb)) 输出： tensor(0.0822, grad_fn=) tensor(1.) 使用nn.Module进行重构 接下来，我们将会用到nn.Model和nn.Parameter来完成一个更加清晰简洁的训练循环。我们继承nn.Module(它是一个能够跟踪状态的类)。在这个例子中，我们想要新建一个类，实现存储权重，偏置和前向传播步骤中所有用到方法。nn.Module包含了许多属性和方法（比如.parameters()和.zero_grad()），我们会在后面用到。 注意 nn.Module（M大写）是一个PyTorch中特有的概念，它是一个会经常用到的类。不要和Python中module（m小写）混淆，module是一个可以被引入的Python代码文件。 from torch import nn class Mnist_Logistic(nn.Module): def __init__(self): super().__init__() self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784)) self.bias = nn.Parameter(torch.zeros(10)) def forward(self, xb): return xb @ self.weights + self.bias 既然现在我们要使用一个对象而不是函数，我们要先对模型进行实例化。 model = Mnist_Logistic() 现在我们可以像之前那样计算损失值了。注意nn.Module对象的使用方式很像函数（例如它们是可调用的），但是PyTorch将会自动调用我们的forward函数 print(loss_func(model(xb), yb)) 输出： tensor(2.2082, grad_fn=) 之前在每个训练循环中，我们通过变量名对每个变量的值进行更新，并手动的将每个变量的梯度置为0，像这样： with torch.no_grad(): weights -= weights.grad * lr bias -= bias.grad * lr weights.grad.zero_() bias.grad.zero_() 现在我们可以利用model.parameters()和model.zero_grad()（这两个都是PyTorch定义在nn.Module中的）使这些步骤变得更加简洁并且更不容易忘记更新部分参数，尤其是模型很复杂的情况： with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr model.zero_grad() 下面我们把训练循环封装进fit函数中，这样就能在后面再次运行。 def fit(): for epoch in range(epochs): for i in range((n - 1) // bs + 1): start_i = i * bs end_i = start_i + bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] pred = model(xb) loss = loss_func(pred, yb) loss.backward() with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr model.zero_grad() fit() 我们来再次检查一下我们的损失值是否下降。 print(loss_func(model(xb), yb)) 输出： tensor(0.0812, grad_fn=) 使用nn.Linear进行重构 我们继续对代码进行重构。我们将用PyTorch中的nn.Linear代替手动定义和初始化self.weights和self.bias以及计算xb @ self.weights + self.bias, 因为nn.Linear可以完成这些操作。 PyTorch中预设了很多类型的神经网络层，使用它们可以极大的简化我们的代码，通常还会带来速度上的提升。 class Mnist_Logistic(nn.Module): def __init__(self): super().__init__() self.lin = nn.Linear(784, 10) def forward(self, xb): return self.lin(xb) 我们初始化模型并像之前那样计算损失值： model = Mnist_Logistic() print(loss_func(model(xb), yb)) 输出： tensor(2.2731, grad_fn=) 我们仍然可以像之前那样使用fit函数： fit() print(loss_func(model(xb), yb)) 输出： tensor(0.0820, grad_fn=) 使用optim进行重构 PyTorch还有一个包含很多优化算法的包————torch.optim。我们可以使用优化器中的step方法执行前向传播过程中的步骤来替换手动更新每个参数。 这个方法将允许我们替换之前手动编写的优化步骤： with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr model.zero_grad() 替换后如下： opt.step() opt.zero_grad() （optim.zero_grad()将梯度重置为0，我们需要在计算下一次梯度之前调用它） from torch import optim 我们将建立模型和优化器的步骤定义为一个小函数方便将来复用。 def get_model(): model = Mnist_Logistic() return model, optim.SGD(model.parameters(), lr=lr) model, opt = get_model() print(loss_func(model(xb), yb)) for epoch in range(epochs): for i in range((n - 1) // bs + 1): start_i = i * bs end_i = start_i + bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() print(loss_func(model(xb), yb)) 输出： tensor(2.3785, grad_fn=) tensor(0.0802, grad_fn=) 使用Dataset进行重构 Pytorch包含一个Dataset抽象类。Dataset可以是任何东西，但它始终包含一个__len__函数（通过Python中的标准函数len调用）和一个用来索引到内容中的__getitem__函数。 这篇教程以创建Dataset的自定义子类FacialLandmarkDataset为例进行介绍。 PyTorch中的TensorDataset是一个封装了张量的Dataset。通过定义长度和索引的方式，是我们可以对张量的第一维进行迭代，索引和切片。这将使我们在训练中，获取同一行中的自变量和因变量更加容易。 from torch.utils.data import TensorDataset 可以把x_train和y_train中的数据合并成一个简单的TensorDataset，这样就可以方便的进行迭代和切片操作。 train_ds = TensorDataset(x_train, y_train) 之前，我们不得不分别对x和y的值进行迭代循环。 xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] 现在我们可以将这两步合二为一了。 xb,yb = train_ds[i*bs : i*bs+bs] model, opt = get_model() for epoch in range(epochs): for i in range((n - 1) // bs + 1): xb, yb = train_ds[i * bs: i * bs + bs] pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() print(loss_func(model(xb), yb)) 输出： tensor(0.0817, grad_fn=) 使用DataLoader进行重构 PyTorch的DataLoader负责批量数据管理，你可以使用任意的Dataset创建一个DataLoader。DataLoader使得对批量数据的迭代更容易。DataLoader自动的为我们提供每一小批量的数据来代替切片的方式train_ds[i*bs : i*bs+bs]。 from torch.utils.data import DataLoader train_ds = TensorDataset(x_train, y_train) train_dl = DataLoader(train_ds, batch_size=bs) 之前我们像下面这样按批(xb,yb)对数据进行迭代： for i in range((n-1)//bs + 1): xb,yb = train_ds[i*bs : i*bs+bs] pred = model(xb) 现在我们的循环变得更加简洁，因为使用了data loader来自动获取数据。 for xb,yb in train_dl: pred = model(xb) model, opt = get_model() for epoch in range(epochs): for xb, yb in train_dl: pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() print(loss_func(model(xb), yb)) 输出： tensor(0.0817, grad_fn=) 多亏PyTorch中的nn.Module，nn.Parameter，Dataset和DataLoader，我们的训练代码变得非常简洁易懂。下面我们来试着增加一些用于提高模型效率所必需的的基本特征。 增加验证集 在第一部分，我们仅仅是试着为我们的训练集构建一个合理的训练步骤，但实际上，我们始终应该有一个验证集来确认模型是否过拟合。 打乱训练数据的顺序通常是避免不同批数据中存在相关性和过拟合的重要步骤。另一方面，无论是否打乱顺序计算出的验证集损失值都是一样的。鉴于打乱顺序还会消耗额外的时间，所以打乱验证集数据是没有任何意义的。 我们在验证集上用到的每批数据的数量是训练集的两倍，这是因为在验证集上不需要进行反向传播，这样就会占用较小的内存（因为它并不需要储存梯度）。我们利用了这一点，使用了更大的batchsize，更快的计算出了损失值。 train_ds = TensorDataset(x_train, y_train) train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True) valid_ds = TensorDataset(x_valid, y_valid) valid_dl = DataLoader(valid_ds, batch_size=bs * 2) 我们将会在每轮(epoch)结束后计算并输出验证集上的损失值。 （注意：在训练前我们总是会调用model.train()函数，在推断之前调用model.eval()函数，因为这些会被nn.BatchNorm2d，nn.Dropout等层使用，确保在不同阶段的准确性。） model, opt = get_model() for epoch in range(epochs): model.train() for xb, yb in train_dl: pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() model.eval() with torch.no_grad(): valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl) print(epoch, valid_loss / len(valid_dl)) 输出： 0 tensor(0.2999) 1 tensor(0.2742) 编写fit()和get_data()函数 现在我们来重构一下我们自己的函数。 我们在计算训练集和验证集上的损失值时执行了差不多的过程两次，因此我们将这部分代码提炼成一个函数loss_batch，用来计算每个批的损失值。 我们为训练集传递一个优化器参数来执行反向传播。对于验证集我们不传优化器参数，这样就不会执行反向传播。 def loss_batch(model, loss_func, xb, yb, opt=None): loss = loss_func(model(xb), yb) if opt is not None: loss.backward() opt.step() opt.zero_grad() return loss.item(), len(xb) fit执行了训练模型的必要操作，并在每轮(epoch)结束后计算模型在训练集和测试集上的损失。 import numpy as np def fit(epochs, model, loss_func, opt, train_dl, valid_dl): for epoch in range(epochs): model.train() for xb, yb in train_dl: loss_batch(model, loss_func, xb, yb, opt) model.eval() with torch.no_grad(): losses, nums = zip( *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl] ) val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums) print(epoch, val_loss) get_data返回训练集和验证集需要使用到的dataloaders。 def get_data(train_ds, valid_ds, bs): return ( DataLoader(train_ds, batch_size=bs, shuffle=True), DataLoader(valid_ds, batch_size=bs * 2), ) 现在，我们只需要三行代码就可以获取数据、拟合模型了。 train_dl, valid_dl = get_data(train_ds, valid_ds, bs) model, opt = get_model() fit(epochs, model, loss_func, opt, train_dl, valid_dl) 输出： 0 0.2961075816631317 1 0.28558296990394594 现在你能用这三行代码训练各种各样的模型。我们来看一下能否用它们来训练一个卷积神经网络（CNN）吧！ 应用到卷积神经网络 我们现在将要创建一个包含三个卷积层的神经网络。因为前面章节中没有一个函数涉及到模型的具体形式，所以我们不需要对它们进行任何修改就可以训练一个卷积神经网络。 我们将会使用PyTorch中预先定义好的Conv2d类作为我们的卷积层。我们定义一个有三个卷积层的卷积神经网络。每个卷积层之后会执行ReLu。在最后，我们会执行一个平均池化操作。 （注意：view是PyTorch版的numpy reshape） class Mnist_CNN(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1) self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1) self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1) def forward(self, xb): xb = xb.view(-1, 1, 28, 28) xb = F.relu(self.conv1(xb)) xb = F.relu(self.conv2(xb)) xb = F.relu(self.conv3(xb)) xb = F.avg_pool2d(xb, 4) return xb.view(-1, xb.size(1)) lr = 0.1 Momentum是随机梯度下降的一个变型，它将前面步骤的更新也考虑在内，通常能够加快训练速度。 model = Mnist_CNN() opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9) fit(epochs, model, loss_func, opt, train_dl, valid_dl) 输出： 0 0.3829730714321136 1 0.2258522843360901 nn.Sequential torch.nn中还有另一个类可以方便的用来简化我们的代码：Sequential。一个Sequential对象可以序列化运行它包含的模块。这是一个更简单的搭建神经网络的方式。 想要充分利用这一优势，我们要能够使用给定的函数轻松的定义一个自定义层。比如说，PyTorch中没有view层，我们需要为我们的网络定义一个。 Lambda函数将会创建一个层，并在后面使用Sequential定义神经网络的时候用到。 class Lambda(nn.Module): def __init__(self, func): super().__init__() self.func = func def forward(self, x): return self.func(x) def preprocess(x): return x.view(-1, 1, 28, 28) 使用Sequential创建模型非常简单： model = nn.Sequential( Lambda(preprocess), nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.AvgPool2d(4), Lambda(lambda x: x.view(x.size(0), -1)), ) opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9) fit(epochs, model, loss_func, opt, train_dl, valid_dl) 输出： 0 0.32739396529197695 1 0.25574398956298827 对DataLoader进行封装 我们的卷积神经网络已经非常简洁了，但是它只能运行在MNIST数据集上，原因如下： 它假定输入是长度为28*28的向量 它假定卷积神经网络最终输出是大小为4*4的网格（因为这是平均值池化操作时我们使用的核大小） 让我们摆脱这两种假定，这样我们的模型就可以运行在任意的2d单通道图像上。 首先，我们可以删除最初的Lambda层，并将数据预处理放在一个生成器中。 def preprocess(x, y): return x.view(-1, 1, 28, 28), y class WrappedDataLoader: def __init__(self, dl, func): self.dl = dl self.func = func def __len__(self): return len(self.dl) def __iter__(self): batches = iter(self.dl) for b in batches: yield (self.func(*b)) train_dl, valid_dl = get_data(train_ds, valid_ds, bs) train_dl = WrappedDataLoader(train_dl, preprocess) valid_dl = WrappedDataLoader(valid_dl, preprocess) 接下来，我们用nn.AdaptiveAvgPool2d替换nn.AvgPool2d，这个函数允许我们定义期望输出张量的大小，而不是定义已有输入的大小。 这样我们的模型就可以处理任意大小的输入了。 model = nn.Sequential( nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d(1), Lambda(lambda x: x.view(x.size(0), -1)), ) opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9) 我们来试一下新的模型： fit(epochs, model, loss_func, opt, train_dl, valid_dl) 输出： 0 0.32888883714675904 1 0.31000419993400574 使用你的GPU 如果你有幸拥有支持CUDA的GPU（你可以租一个，大部分云服务提供商的价格使0.5$/每小时），那你可以用GPU来加速你的代码。 首先检查一下的GPU是否可以被PyTorch调用： print(torch.cuda.is_available()) 输出： True 接下来，新建一个设备对象： dev = torch.device( \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") 然后更新一下preprocess函数将批运算移到GPU上计算 def preprocess(x, y): return x.view(-1, 1, 28, 28).to(dev), y.to(dev) train_dl, valid_dl = get_data(train_ds, valid_ds, bs) train_dl = WrappedDataLoader(train_dl, preprocess) valid_dl = WrappedDataLoader(valid_dl, preprocess) 最后，我们可以把模型移动到GPU上。 model.to(dev) opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9) 你现在应该能发现模型运算变快了。 fit(epochs, model, loss_func, opt, train_dl, valid_dl) 输出： 0 0.21190375366210937 1 0.18018000435829162 总结 现在我们有一个用PyTorch构建的通用数据管道和训练循环可以用来训练很多类型的模型。 想要知道训练一个模型有多么简单，可以参照mnist_sample这个例子。 当然了，你可能还想要在模型中加入很多其它的东西，比如数据扩充，超参数调整，训练监控，迁移学习等。这些特性可以在fastai库中获取到，该库使用与本教程中介绍的相同设计方法开发的，为想要扩展模型的学习者提供了合理的后续步骤。 在教程的开始部分，我们说了要通过例子对torch.nn，torch.optim，Dataset和DataLoader进行说明。 现在我们来总结一下，我们都讲了些什么： torch.nn Module：创建一个可调用的，其表现类似于函数，但又可以包含状态（比如神经网络层的权重）的对象。该对象知道它包含的Parameter（s），并可以将梯度置为0，以及对梯度进行循环以更新权重等。 Parameter：是一个对张量的封装，它告诉Module在反向传播阶段更新权重。只有设置了requires_grad属性的张量会被更新。 functional：一个包含了梯度函数、损失函数等以及一些无状态的层，如卷积层和线性层的模块（通常使用F作为导入的别名）。 torch.optim：包含了优化器，比如在反向阶段更新Parameter中权重的SGD。 Dataset：一个抽象接口，包含了__len__和__getitem__，还包含了PyTorch提供的类，如TensorDataset。 DataLoader：接受任意的Dataset并生成一个迭代器可以批量返回数据。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"tut_image.html":{"url":"tut_image.html","title":"图像","keywords":"","body":"图像 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"finetuning_torchvision_models_tutorial.html":{"url":"finetuning_torchvision_models_tutorial.html","title":"Torchvision 模型微调","keywords":"","body":"Torchvision模型微调 译者：ZHHAYO 作者: Nathan Inkawhich 在本教程中，我们将深入探讨如何微调和特征提取torchvision 模型，所有这些模型都已经预先在1000类的magenet数据集上训练完成。本程将深入介绍如何使用几个现代的CNN架构，并将为微调任意的PyTorch模型建立一个直觉。 由于每个模型架构是有差异的，因此没有可以在所有场景中使用的样板微调代码。 然而，研究人员必须查看现有架构并对每个模型进行自定义调整。 在本文档中，我们将执行两种类型的迁移学习：微调和特征提取。 在微调中，我们从一个预训练模型开始，然后为我们的新任务更新所有的模型参数，实质上就是重新训练整个模型。 在特征提取中，我们从预训练模型开始，只更新产生预测的最后一层的权重。它被称为特征提取是因为我们使用预训练的CNN作为固定的特征提取器，并且仅改变输出层。 有关迁移学习的更多技术信息，请参阅here和here. 通常，这两种迁移学习方法都遵循以下几个步骤： 初始化预训练模型 重组最后一层，使其具有与新数据集类别数相同的输出数 为优化算法定义我们想要在训练期间更新的参数 运行训练步骤 from __future__ import print_function from __future__ import division import torch import torch.nn as nn import torch.optim as optim import numpy as np import torchvision from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import time import os import copy print(\"PyTorch Version: \",torch.__version__) print(\"Torchvision Version: \",torchvision.__version__) 输出: PyTorch Version: 1.0.0.dev20190117 Torchvision Version: 0.2.1 输入 以下为运行时需要更改的所有参数。 我们将使用的数据集hymenoptera_data可在此处下载。 该数据集包含两类：蜜蜂和蚂蚁，其结构使得我们可以使用 ImageFolder 数据集，不需要编写我们自己的自定义数据集。下载数据并设置 data_dir 为数据集的根目录。model_name是您要使用的模型名称，必须从此列表中选择： [resnet, alexnet, vgg, squeezenet, densenet, inception] 其他输入如下：num_classes为数据集的类别数，batch_size是训练的batch大小，可以根据您机器的计算能力进行调整，num_epochsis是我们想要运行的训练epoch数，feature_extractis 是定义我们选择微调还是特征提取的布尔值。 如果feature_extract = False，将微调模型，并更新所有模型参数。 如果feature_extract = True，则仅更新最后一层的参数，其他参数保持不变。 # Top level data directory. Here we assume the format of the directory conforms # to the ImageFolder structure data_dir = \"./data/hymenoptera_data\" # Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception] model_name = \"squeezenet\" # Number of classes in the dataset num_classes = 2 # Batch size for training (change depending on how much memory you have) batch_size = 8 # Number of epochs to train for num_epochs = 15 # Flag for feature extracting. When False, we finetune the whole model, # when True we only update the reshaped layer params feature_extract = True 辅助函数 在编写调整模型的代码之前，我们先定义一些辅助函数。 模型训练和验证代码 train_model函数处理给定模型的训练和验证。 作为输入，它需要PyTorch模型，数据加载器字典，损失函数，优化器，用于训练和验证epoch数，以及当模型是初始模型时的布尔标志。 is_inception 标志用于容纳 Inception v3 模型，因为该体系结构使用辅助输出，并且整体模型损失涉及辅助输出和最终输出，如此处所述。 这个函数训练指定数量的epoch,并且在每个epoch之后运行完整的验证步骤。 它还跟踪最佳性能的模型（从验证准确率方面），并在训练结束时返回性能最好的模型。 在每个epoch之后，打印训练和验证正确率。 def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False): since = time.time() val_acc_history = [] best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): print('Epoch {}/{}'.format(epoch, num_epochs - 1)) print('-' * 10) # Each epoch has a training and validation phase for phase in ['train', 'val']: if phase == 'train': model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == 'train'): # Get model outputs and calculate loss # Special case for inception because in training it has an auxiliary output. In train # mode we calculate the loss by summing the final output and the auxiliary output # but in testing we only consider the final output. if is_inception and phase == 'train': # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958 outputs, aux_outputs = model(inputs) loss1 = criterion(outputs, labels) loss2 = criterion(aux_outputs, labels) loss = loss1 + 0.4*loss2 else: outputs = model(inputs) loss = criterion(outputs, labels) _, preds = torch.max(outputs, 1) # backward + optimize only if in training phase if phase == 'train': loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) epoch_loss = running_loss / len(dataloaders[phase].dataset) epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset) print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc)) # deep copy the model if phase == 'val' and epoch_acc > best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) if phase == 'val': val_acc_history.append(epoch_acc) print() time_elapsed = time.time() - since print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) print('Best val Acc: {:4f}'.format(best_acc)) # load best model weights model.load_state_dict(best_model_wts) return model, val_acc_history 设置模型参数的.requires_grad属性 当我们进行特征提取时，此辅助函数将模型中参数的 .requires_grad 属性设置为False。默认情况下，当我们加载一个预训练模型时，所有参数都是 .requires_grad = True，如果我们从头开始训练或微调，这种设置就没问题。 但是，如果我们要运行特征提取并且只想为新初始化的层计算梯度，那么我们希望所有其他参数不需要梯度变化。这将在稍后更能理解。 def set_parameter_requires_grad(model, feature_extracting): if feature_extracting: for param in model.parameters(): param.requires_grad = False 初始化和重塑网络 现在来到最有趣的部分。在这里我们对每个网络进行重塑。请注意，这不是一个自动过程，并且对每个模型都是唯一的。 回想一下，CNN模型的最后一层（通常是FC层）与数据集中的输出类的数量具有相同的节点数。 由于所有模型都已在Imagenet上预先训练，因此它们都具有大小为1000的输出层，每个类一个节点。 这里的目标是将最后一层重塑为与之前具有相同数量的输入，并且具有与数据集中的类别数相同的输出数。 在以下部分中，我们将讨论如何更改每个模型的体系结构。 但首先，有一个关于微调和特征提取之间差异的重要细节。 当进行特征提取时，我们只想更新最后一层的参数，换句话说，我们只想更新我们正在重塑层的参数。 因此，我们不需要计算不需要改变的参数的梯度，因此为了提高效率，我们将其它层的.requires_grad属性设置为False。 这很重要，因为默认情况下，此属性设置为True。 然后，当我们初始化新层时，默认情况下新参数.requires_grad = True，因此只更新新层的参数。 当我们进行微调时，我们可以将所有.required_grad设置为默认值True。 最后，请注意inception_v3的输入大小为（299,299），而所有其他模型都输入为（224,224）。 Resnet 论文Deep Residual Learning for Image Recognition介绍了Resnet模型。有几种不同尺寸的变体，包括Resnet18，Resnet34，Resnet50，Resnet101和Resnet152，所有这些模型都可以从torchvision模型中获得。因为我们的数据集很小，只有两个类，所以我们使用Resnet18。 当我们打印这个模型时，我们看到最后一层是全连接层，如下所示： (fc): Linear(in_features=512, out_features=1000, bias=True) 因此，我们必须将model.fc重新初始化为具有512个输入特征和2个输出特征的线性层： model.fc = nn.Linear(512, num_classes) Alexnet Alexnet在论文ImageNet Classification with Deep Convolutional Neural Networks中被介绍，是ImageNet数据集上第一个非常成功的CNN。当我们打印模型架构时，我们看到模型输出为分类器的第6层 (classifier): Sequential( ... (6): Linear(in_features=4096, out_features=1000, bias=True) ) 要在我们的数据集中使用这个模型，我们将此图层重新初始化为 model.classifier[6] = nn.Linear(4096,num_classes) VGG VGG在论文Very Deep Convolutional Networks for Large-Scale Image Recognition中被引入。Torchvision提供了8种不同长度的VGG版本，其中一些版本具有批标准化层。这里我们使用VGG-11进行批标准化。输出层与Alexnet类似，即 (classifier): Sequential( ... (6): Linear(in_features=4096, out_features=1000, bias=True) ) 因此，我们使用相同的方法来修改输出层 model.classifier[6] = nn.Linear(4096,num_classes) Squeezenet 论文SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and 描述了Squeeznet架构，使用了与此处显示的任何其他模型不同的输出结构。Torchvision的Squeezenet有两个版本，我们使用1.0版本。输出来自1x1卷积层，它是分类器的第一层： (classifier): Sequential( (0): Dropout(p=0.5) (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1)) (2): ReLU(inplace) (3): AvgPool2d(kernel_size=13, stride=1, padding=0) ) 为了修改网络，我们重新初始化Conv2d层，使输出特征图深度为2 model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1)) Densenet 论文Densely Connected Convolutional Networks引入了Densenet模型。 Torchvision有四种Densenet变型，但在这里我们只使用Densenet-121。 输出层是一个具有1024个输入特征的线性层： (classifier): Linear(in_features=1024, out_features=1000, bias=True) 为了重塑这个网络，我们将分类器的线性层重新初始化为 model.classifier = nn.Linear(1024, num_classes) Inception v3 最后，Inception v3首先在论文 Rethinking the Inception Architecture for Computer Vision中描述。该网络的独特之处在于它在训练时有两个输出层。第二个输出称为辅助输出，包含在网络的AuxLogits部分中。主输出是网络末端的线性层。注意，测试时我们只考虑主输出。 加载模型的辅助输出和主输出打印为： (AuxLogits): InceptionAux( ... (fc): Linear(in_features=768, out_features=1000, bias=True) ) ... (fc): Linear(in_features=2048, out_features=1000, bias=True) 要微调这个模型，我们必须重塑这两个层。 可以通过以下方式完成 model.AuxLogits.fc = nn.Linear(768, num_classes) model.fc = nn.Linear(2048, num_classes) 请注意，许多模型具有相似的输出结构，但每个模型的处理方式略有不同。 另外，请查看重塑网络的模型体系结构，并确保输出特征数与数据集中的类别数相同。 def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True): # Initialize these variables which will be set in this if statement. Each of these # variables is model specific. model_ft = None input_size = 0 if model_name == \"resnet\": \"\"\" Resnet18 \"\"\" model_ft = models.resnet18(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) num_ftrs = model_ft.fc.in_features model_ft.fc = nn.Linear(num_ftrs, num_classes) input_size = 224 elif model_name == \"alexnet\": \"\"\" Alexnet \"\"\" model_ft = models.alexnet(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) num_ftrs = model_ft.classifier[6].in_features model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes) input_size = 224 elif model_name == \"vgg\": \"\"\" VGG11_bn \"\"\" model_ft = models.vgg11_bn(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) num_ftrs = model_ft.classifier[6].in_features model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes) input_size = 224 elif model_name == \"squeezenet\": \"\"\" Squeezenet \"\"\" model_ft = models.squeezenet1_0(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1)) model_ft.num_classes = num_classes input_size = 224 elif model_name == \"densenet\": \"\"\" Densenet \"\"\" model_ft = models.densenet121(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) num_ftrs = model_ft.classifier.in_features model_ft.classifier = nn.Linear(num_ftrs, num_classes) input_size = 224 elif model_name == \"inception\": \"\"\" Inception v3 Be careful, expects (299,299) sized images and has auxiliary output \"\"\" model_ft = models.inception_v3(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) # Handle the auxilary net num_ftrs = model_ft.AuxLogits.fc.in_features model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes) # Handle the primary net num_ftrs = model_ft.fc.in_features model_ft.fc = nn.Linear(num_ftrs,num_classes) input_size = 299 else: print(\"Invalid model name, exiting...\") exit() return model_ft, input_size # Initialize the model for this run model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True) # Print the model we just instantiated print(model_ft) 输出: SqueezeNet( (features): Sequential( (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2)) (1): ReLU(inplace) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True) (3): Fire( (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace) (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace) (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace) ) (4): Fire( (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace) (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace) (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace) ) (5): Fire( (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace) (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace) (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace) ) (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True) (7): Fire( (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace) (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace) (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace) ) (8): Fire( (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace) (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace) (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace) ) (9): Fire( (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace) (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace) (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace) ) (10): Fire( (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace) (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace) (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace) ) (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True) (12): Fire( (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace) (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace) (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace) ) ) (classifier): Sequential( (0): Dropout(p=0.5) (1): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1)) (2): ReLU(inplace) (3): AdaptiveAvgPool2d(output_size=(1, 1)) ) ) Load Data 现在我们知道输入尺寸大小必须是什么，我们可以初始化数据转换，图像数据集和数据加载器。请注意，模型是使用硬编码标准化值进行预先训练的，如here所述。 # Data augmentation and normalization for training # Just normalization for validation data_transforms = { 'train': transforms.Compose([ transforms.RandomResizedCrop(input_size), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), 'val': transforms.Compose([ transforms.Resize(input_size), transforms.CenterCrop(input_size), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } print(\"Initializing Datasets and Dataloaders...\") # Create training and validation datasets image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']} # Create training and validation dataloaders dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']} # Detect if we have a GPU available device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") 输出: Initializing Datasets and Dataloaders... 创建优化器 现在模型结构是正确的，微调和特征提取的最后一步是创建一个只更新所需参数的优化器。 回想一下，在加载预训练模型之后，但在重塑之前，如果feature_extract = True，我们手动将所有参数的.requires_grad属性设置为False。然后重新初始化默认为.requires_grad = True的网络层参数。所以现在我们知道应该优化所有具有 .requires_grad = True的参数。接下来，我们列出这些参数并将此列表输入到SGD算法构造器。 要验证这一点，可以查看要学习的参数。微调时，此列表应该很长并包含所有模型参数。但是，当进行特征提取时，此列表应该很短并且仅包括重塑层的权重和偏差。 # Send the model to GPU model_ft = model_ft.to(device) # Gather the parameters to be optimized/updated in this run. If we are # finetuning we will be updating all parameters. However, if we are # doing feature extract method, we will only update the parameters # that we have just initialized, i.e. the parameters with requires_grad # is True. params_to_update = model_ft.parameters() print(\"Params to learn:\") if feature_extract: params_to_update = [] for name,param in model_ft.named_parameters(): if param.requires_grad == True: params_to_update.append(param) print(\"\\t\",name) else: for name,param in model_ft.named_parameters(): if param.requires_grad == True: print(\"\\t\",name) # Observe that all parameters are being optimized optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9) 输出: Params to learn: classifier.1.weight classifier.1.bias 运行训练和验证 最后一步是为模型设置损失，然后对设定的epoch数运行训练和验证函数。请注意，取决于epoch的数量，此步骤在CPU上可能需要执行一段时间。 此外，默认的学习率对所有模型都不是最佳的，因此为了获得最大精度，有必要分别调整每个模型。 # Setup the loss fxn criterion = nn.CrossEntropyLoss() # Train and evaluate model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\")) 输出: Epoch 0/14 ---------- train Loss: 0.5981 Acc: 0.7131 val Loss: 0.3849 Acc: 0.8889 Epoch 1/14 ---------- train Loss: 0.3282 Acc: 0.8402 val Loss: 0.3023 Acc: 0.9085 Epoch 2/14 ---------- train Loss: 0.2248 Acc: 0.9139 val Loss: 0.3363 Acc: 0.8758 Epoch 3/14 ---------- train Loss: 0.1924 Acc: 0.9057 val Loss: 0.2833 Acc: 0.9150 Epoch 4/14 ---------- train Loss: 0.1359 Acc: 0.9344 val Loss: 0.3221 Acc: 0.9150 Epoch 5/14 ---------- train Loss: 0.1583 Acc: 0.9426 val Loss: 0.3069 Acc: 0.9412 Epoch 6/14 ---------- train Loss: 0.1918 Acc: 0.9344 val Loss: 0.3139 Acc: 0.9150 Epoch 7/14 ---------- train Loss: 0.1950 Acc: 0.9262 val Loss: 0.2431 Acc: 0.9281 Epoch 8/14 ---------- train Loss: 0.1534 Acc: 0.9344 val Loss: 0.2680 Acc: 0.9281 Epoch 9/14 ---------- train Loss: 0.1796 Acc: 0.9262 val Loss: 0.2573 Acc: 0.9346 Epoch 10/14 ---------- train Loss: 0.1181 Acc: 0.9549 val Loss: 0.2987 Acc: 0.9216 Epoch 11/14 ---------- train Loss: 0.1401 Acc: 0.9262 val Loss: 0.2977 Acc: 0.9281 Epoch 12/14 ---------- train Loss: 0.1463 Acc: 0.9221 val Loss: 0.3645 Acc: 0.8889 Epoch 13/14 ---------- train Loss: 0.1388 Acc: 0.9508 val Loss: 0.3617 Acc: 0.9281 Epoch 14/14 ---------- train Loss: 0.1799 Acc: 0.9180 val Loss: 0.3371 Acc: 0.9216 Training complete in 0m 19s Best val Acc: 0.941176 与从头开始训练模型比较 只是为了好玩，看看如果我们不使用迁移学习，模型将如何学习。微调与特征提取的性能在很大程度上取决于数据集，但一般而言，两种迁移学习方法相对于从头开始训练模型，在训练时间和总体准确性方面产生了良好的结果。 # Initialize the non-pretrained version of the model used for this run scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False) scratch_model = scratch_model.to(device) scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9) scratch_criterion = nn.CrossEntropyLoss() _,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\")) # Plot the training curves of validation accuracy vs. number # of training epochs for the transfer learning method and # the model trained from scratch ohist = [] shist = [] ohist = [h.cpu().numpy() for h in hist] shist = [h.cpu().numpy() for h in scratch_hist] plt.title(\"Validation Accuracy vs. Number of Training Epochs\") plt.xlabel(\"Training Epochs\") plt.ylabel(\"Validation Accuracy\") plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\") plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\") plt.ylim((0,1.)) plt.xticks(np.arange(1, num_epochs+1, 1.0)) plt.legend() plt.show() 输出: Epoch 0/14 ---------- train Loss: 0.7935 Acc: 0.5246 val Loss: 0.6931 Acc: 0.4575 Epoch 1/14 ---------- train Loss: 0.6931 Acc: 0.5082 val Loss: 0.6931 Acc: 0.4575 Epoch 2/14 ---------- train Loss: 0.6931 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Epoch 3/14 ---------- train Loss: 0.6931 Acc: 0.5123 val Loss: 0.6931 Acc: 0.4575 Epoch 4/14 ---------- train Loss: 0.6931 Acc: 0.5123 val Loss: 0.6931 Acc: 0.4575 Epoch 5/14 ---------- train Loss: 0.6931 Acc: 0.4918 val Loss: 0.6931 Acc: 0.4575 Epoch 6/14 ---------- train Loss: 0.6931 Acc: 0.5000 val Loss: 0.6931 Acc: 0.4575 Epoch 7/14 ---------- train Loss: 0.6931 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Epoch 8/14 ---------- train Loss: 0.6931 Acc: 0.5164 val Loss: 0.6931 Acc: 0.4575 Epoch 9/14 ---------- train Loss: 0.6931 Acc: 0.5000 val Loss: 0.6931 Acc: 0.4575 Epoch 10/14 ---------- train Loss: 0.6931 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Epoch 11/14 ---------- train Loss: 0.6931 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Epoch 12/14 ---------- train Loss: 0.6931 Acc: 0.4918 val Loss: 0.6931 Acc: 0.4575 Epoch 13/14 ---------- train Loss: 0.6931 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Epoch 14/14 ---------- train Loss: 0.6931 Acc: 0.5000 val Loss: 0.6931 Acc: 0.4575 Training complete in 0m 29s Best val Acc: 0.457516 最后的想法及下一步 尝试运行其他模型，看看可以得到多好的正确率。另外，请注意特征提取花费的时间较少，因为在后向传播中我们不需要计算大部分的梯度。还有很多地方可以尝试。 你可以： 在更难的数据集上运行此代码，查看迁移学习的更多好处。 在新的领域（比如NLP，音频等）中，使用此处描述的方法，使用迁移学习更新不同的模型。 一旦您对一个模型感到满意， 可以将其导出为ONNX模型，或使用混合前端跟踪它以获得更快的速度和优化的机会。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"spatial_transformer_tutorial.html":{"url":"spatial_transformer_tutorial.html","title":"空间变换器网络教程","keywords":"","body":"空间变换器网络教程 译者：冯宝宝 作者: Ghassen HAMROUNI 在本教程中，您将学习如何使用称为空间变换器网络的视觉注意机制来扩充您的网络。你可以在 DeepMind paper阅读有关空间变换器网络的更多内容。 空间变换器网络是对任何空间变换的差异化关注的概括。空间变换器网络（简称STN）允许神经网络学习如何在输入图像上执行空间变换，以增强模型的几何不变性。例如，它可以裁剪感兴趣的区域，缩放并校正图像的方向。它可能是一种有用的机制，因为CNN对于旋转和缩放以及更一般的仿射变换并不是不变的。关于STN的最棒的事情之一是能够简单地将其插入任何现有的CNN，只需很少的修改。 # License: BSD # 作者: Ghassen Hamrouni from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim import torchvision from torchvision import datasets, transforms import matplotlib.pyplot as plt import numpy as np plt.ion() # 交互模式 加载数据 在这篇文章中，我们尝试了经典的MNIST数据集。使用标准卷积网络增强空间变换器网络。 device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Training dataset train_loader = torch.utils.data.DataLoader( datasets.MNIST(root='.', train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=64, shuffle=True, num_workers=4) # Test dataset test_loader = torch.utils.data.DataLoader( datasets.MNIST(root='.', train=False, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=64, shuffle=True, num_workers=4) 输出: Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz Extracting ./MNIST/raw/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz Processing... Done! 空间变换器网络叙述 空间变换器网络归结为三个主要组成部分： 本地网络（Localisation Network）是常规CNN，其对变换参数进行回归。不会从该数据集中明确地学习转换，而是网络自动学习增强全局准确性的空间变换。 网格生成器( Grid Genator)在输入图像中生成与输出图像中的每个像素相对应的坐标网格。 采样器（Sampler）使用变换的参数并将其应用于输入图像。 笔记 我们使用最新版本的Pytorch，它应该包含affine_grid和grid_sample模块。 class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 10, kernel_size=5) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(320, 50) self.fc2 = nn.Linear(50, 10) # Spatial transformer localization-network self.localization = nn.Sequential( nn.Conv2d(1, 8, kernel_size=7), nn.MaxPool2d(2, stride=2), nn.ReLU(True), nn.Conv2d(8, 10, kernel_size=5), nn.MaxPool2d(2, stride=2), nn.ReLU(True) ) # Regressor for the 3 * 2 affine matrix self.fc_loc = nn.Sequential( nn.Linear(10 * 3 * 3, 32), nn.ReLU(True), nn.Linear(32, 3 * 2) ) # Initialize the weights/bias with identity transformation self.fc_loc[2].weight.data.zero_() self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float)) # Spatial transformer network forward function def stn(self, x): xs = self.localization(x) xs = xs.view(-1, 10 * 3 * 3) theta = self.fc_loc(xs) theta = theta.view(-1, 2, 3) grid = F.affine_grid(theta, x.size()) x = F.grid_sample(x, grid) return x def forward(self, x): # transform the input x = self.stn(x) # Perform the usual forward pass x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(-1, 320) x = F.relu(self.fc1(x)) x = F.dropout(x, training=self.training) x = self.fc2(x) return F.log_softmax(x, dim=1) model = Net().to(device) 训练模型 现在我们使用SGD（随机梯度下降）算法来训练模型。网络正在以有监督的方式学习分类任务。同时，该模型以端到端的方式自动学习STN。 optimizer = optim.SGD(model.parameters(), lr=0.01) def train(epoch): model.train() for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() optimizer.step() if batch_idx % 500 == 0: print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) # # A simple test procedure to measure STN the performances on MNIST. # def test(): with torch.no_grad(): model.eval() test_loss = 0 correct = 0 for data, target in test_loader: data, target = data.to(device), target.to(device) output = model(data) # sum up batch loss test_loss += F.nll_loss(output, target, size_average=False).item() # get the index of the max log-probability pred = output.max(1, keepdim=True)[1] correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n' .format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) 可视化STN结果 现在，我们将检查我们学习的视觉注意机制的结果。我们定义了一个小辅助函数，以便在训练时可视化变换。 def convert_image_np(inp): \"\"\"Convert a Tensor to numpy image.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) return inp # We want to visualize the output of the spatial transformers layer # after the training, we visualize a batch of input images and # the corresponding transformed batch using STN. def visualize_stn(): with torch.no_grad(): # Get a batch of training data data = next(iter(test_loader))[0].to(device) input_tensor = data.cpu() transformed_input_tensor = model.stn(data).cpu() in_grid = convert_image_np( torchvision.utils.make_grid(input_tensor)) out_grid = convert_image_np( torchvision.utils.make_grid(transformed_input_tensor)) # Plot the results side-by-side f, axarr = plt.subplots(1, 2) axarr[0].imshow(in_grid) axarr[0].set_title('Dataset Images') axarr[1].imshow(out_grid) axarr[1].set_title('Transformed Images') for epoch in range(1, 20 + 1): train(epoch) test() # Visualize the STN transformation on some input batch visualize_stn() plt.ioff() plt.show() 输出： Train Epoch: 1 [0/60000 (0%)] Loss: 2.336866 Train Epoch: 1 [32000/60000 (53%)] Loss: 0.841600 Test set: Average loss: 0.2624, Accuracy: 9212/10000 (92%) Train Epoch: 2 [0/60000 (0%)] Loss: 0.527656 Train Epoch: 2 [32000/60000 (53%)] Loss: 0.428908 Test set: Average loss: 0.1176, Accuracy: 9632/10000 (96%) Train Epoch: 3 [0/60000 (0%)] Loss: 0.305364 Train Epoch: 3 [32000/60000 (53%)] Loss: 0.263615 Test set: Average loss: 0.1099, Accuracy: 9677/10000 (97%) Train Epoch: 4 [0/60000 (0%)] Loss: 0.169776 Train Epoch: 4 [32000/60000 (53%)] Loss: 0.408683 Test set: Average loss: 0.0861, Accuracy: 9734/10000 (97%) Train Epoch: 5 [0/60000 (0%)] Loss: 0.286635 Train Epoch: 5 [32000/60000 (53%)] Loss: 0.122162 Test set: Average loss: 0.0817, Accuracy: 9743/10000 (97%) Train Epoch: 6 [0/60000 (0%)] Loss: 0.331074 Train Epoch: 6 [32000/60000 (53%)] Loss: 0.126413 Test set: Average loss: 0.0589, Accuracy: 9822/10000 (98%) Train Epoch: 7 [0/60000 (0%)] Loss: 0.109780 Train Epoch: 7 [32000/60000 (53%)] Loss: 0.172199 Test set: Average loss: 0.0629, Accuracy: 9814/10000 (98%) Train Epoch: 8 [0/60000 (0%)] Loss: 0.078934 Train Epoch: 8 [32000/60000 (53%)] Loss: 0.156452 Test set: Average loss: 0.0563, Accuracy: 9839/10000 (98%) Train Epoch: 9 [0/60000 (0%)] Loss: 0.063500 Train Epoch: 9 [32000/60000 (53%)] Loss: 0.186023 Test set: Average loss: 0.0713, Accuracy: 9799/10000 (98%) Train Epoch: 10 [0/60000 (0%)] Loss: 0.199808 Train Epoch: 10 [32000/60000 (53%)] Loss: 0.083502 Test set: Average loss: 0.0528, Accuracy: 9850/10000 (98%) Train Epoch: 11 [0/60000 (0%)] Loss: 0.092909 Train Epoch: 11 [32000/60000 (53%)] Loss: 0.204410 Test set: Average loss: 0.0471, Accuracy: 9857/10000 (99%) Train Epoch: 12 [0/60000 (0%)] Loss: 0.078322 Train Epoch: 12 [32000/60000 (53%)] Loss: 0.041391 Test set: Average loss: 0.0634, Accuracy: 9796/10000 (98%) Train Epoch: 13 [0/60000 (0%)] Loss: 0.061228 Train Epoch: 13 [32000/60000 (53%)] Loss: 0.137952 Test set: Average loss: 0.0654, Accuracy: 9802/10000 (98%) Train Epoch: 14 [0/60000 (0%)] Loss: 0.068635 Train Epoch: 14 [32000/60000 (53%)] Loss: 0.084583 Test set: Average loss: 0.0515, Accuracy: 9853/10000 (99%) Train Epoch: 15 [0/60000 (0%)] Loss: 0.263158 Train Epoch: 15 [32000/60000 (53%)] Loss: 0.127036 Test set: Average loss: 0.0493, Accuracy: 9851/10000 (99%) Train Epoch: 16 [0/60000 (0%)] Loss: 0.083642 Train Epoch: 16 [32000/60000 (53%)] Loss: 0.028274 Test set: Average loss: 0.0461, Accuracy: 9867/10000 (99%) Train Epoch: 17 [0/60000 (0%)] Loss: 0.076734 Train Epoch: 17 [32000/60000 (53%)] Loss: 0.034796 Test set: Average loss: 0.0409, Accuracy: 9864/10000 (99%) Train Epoch: 18 [0/60000 (0%)] Loss: 0.122501 Train Epoch: 18 [32000/60000 (53%)] Loss: 0.152187 Test set: Average loss: 0.0474, Accuracy: 9860/10000 (99%) Train Epoch: 19 [0/60000 (0%)] Loss: 0.050512 Train Epoch: 19 [32000/60000 (53%)] Loss: 0.270055 Test set: Average loss: 0.0416, Accuracy: 9878/10000 (99%) Train Epoch: 20 [0/60000 (0%)] Loss: 0.073357 Train Epoch: 20 [32000/60000 (53%)] Loss: 0.017542 Test set: Average loss: 0.0713, Accuracy: 9816/10000 (98%) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"neural_style_tutorial.html":{"url":"neural_style_tutorial.html","title":"使用 PyTorch 进行图像风格转换","keywords":"","body":"使用PyTorch进行图像风格转换 译者：bdqfork 作者: Alexis Jacq 简介 本教程主要讲解如何实现由Leon A. Gatys，Alexander S. Ecker和Matthias Bethge提出的 Neural-Style 算法。Neural-Style或者叫Neural-Transfer，可以让你使用一种新的风格将指定的图片进行重构。这个算法使用三张图片，一张输入图片，一张内容图片和一张风格图片，并将输入的图片变得与内容图片相似，且拥有风格图片的优美风格。 基本原理 原理很简单：我们定义两个间距，一个用于内容D_C，另一个用于风格D_S。D_C测量两张图片内容的不同，而D_S用来测量两张图片风格的不同。然后，我们输入第三张图片，并改变这张图片，使其与内容图片的内容间距和风格图片的风格间距最小化。现在，我们可以导入必要的包，开始图像风格转换。 导包并选择设备 下面是一张实现图像风格转换所需包的清单。 torch, torch.nn, numpy (使用PyTorch进行风格转换必不可少的包) torch.optim (高效的梯度下降) PIL, PIL.Image, matplotlib.pyplot (加载和展示图片) torchvision.transforms (将PIL图片转换成张量) torchvision.models (训练或加载预训练模型) copy (对模型进行深度拷贝；系统包) from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from PIL import Image import matplotlib.pyplot as plt import torchvision.transforms as transforms import torchvision.models as models import copy 下一步，我们选择用哪一个设备来运行神经网络，导入内容和风格图片。在大量图片上运行图像风格算法需要很长时间，在GPU上运行可以加速。我们可以使用torch.cuda.is_available()来判断是否有可用的GPU。下一步，我们在整个教程中使用 torch.device 。 .to(device) 方法也被用来将张量或者模型移动到指定设备。 device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") 加载图片 现在我们将导入风格和内容图片。原始的PIL图片的值介于0到255之间，但是当转换成torch张量时，它们的值被转换成0到1之间。图片也需要被重设成相同的维度。一个重要的细节是，注意torch库中的神经网络用来训练的张量的值为0到1之间。如果你尝试将0到255的张量图片加载到神经网络，然后激活的特征映射将不能侦测到目标内容和风格。然而，Caffe库中的预训练网络用来训练的张量值为0到255之间的图片。 注意 这是一个下载本教程需要用到的图片的链接： picasso.jpg 和 dancing.jpg。下载这两张图片并且将它们添加到你当前工作目录中的 images 文件夹。 # desired size of the output image imsize = 512 if torch.cuda.is_available() else 128 # use small size if no gpu loader = transforms.Compose([ transforms.Resize(imsize), # scale imported image transforms.ToTensor()]) # transform it into a torch tensor def image_loader(image_name): image = Image.open(image_name) # fake batch dimension required to fit network's input dimensions image = loader(image).unsqueeze(0) return image.to(device, torch.float) style_img = image_loader(\"./data/images/neural-style/picasso.jpg\") content_img = image_loader(\"./data/images/neural-style/dancing.jpg\") assert style_img.size() == content_img.size(), \\ \"we need to import style and content images of the same size\" 现在，让我们创建一个方法，通过重新将图片转换成PIL格式来展示，并使用plt.imshow展示它的拷贝。我们将尝试展示内容和风格图片来确保它们被正确的导入。 unloader = transforms.ToPILImage() # reconvert into PIL image plt.ion() def imshow(tensor, title=None): image = tensor.cpu().clone() # we clone the tensor to not do changes on it image = image.squeeze(0) # remove the fake batch dimension image = unloader(image) plt.imshow(image) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated plt.figure() imshow(style_img, title='Style Image') plt.figure() imshow(content_img, title='Content Image') 损失函数 内容损失 内容损失是一个表示一层内容间距的加权版本。这个方法使用网络中的L层的特征映射F_XL，该网络处理输入X并返回在图片X和内容图片C之间的加权内容间距W_CL*D_C^L(X,C)。该方法必须知道内容图片（F_CL）的特征映射来计算内容间距。我们使用一个以F_CL作为构造参数输入的torch模型来实现这个方法。间距||F_XL-F_CL||^2是两个特征映射集合之间的平均方差，可以使用nn.MSELoss来计算。 我们将直接添加这个内容损失模型到被用来计算内容间距的卷积层之后。这样每一次输入图片到网络中时，内容损失都会在目标层被计算。而且因为自动求导的缘故，所有的梯度都会被计算。现在，为了使内容损失层透明化，我们必须定义一个forward方法来计算内容损失，同时返回该层的输入。计算的损失作为模型的参数被保存。 class ContentLoss(nn.Module): def __init__(self, target,): super(ContentLoss, self).__init__() # we 'detach' the target content from the tree used # to dynamically compute the gradient: this is a stated value, # not a variable. Otherwise the forward method of the criterion # will throw an error. self.target = target.detach() def forward(self, input): self.loss = F.mse_loss(input, self.target) return input 注意 重要细节：尽管这个模型的名称被命名为 ContentLoss, 它不是一个真实的PyTorch损失方法。如果你想要定义你的内容损失为PyTorch Loss方法，你必须创建一个PyTorch自动求导方法来手动的在backward方法中重计算/实现梯度. 风格损失 风格损失模型与内容损失模型的实现方法类似。它要作为一个网络中的透明层，来计算相应层的风格损失。为了计算风格损失，我们需要计算Gram矩阵G_XL。Gram矩阵是将给定矩阵和它的转置矩阵的乘积。在这个应用中，给定的矩阵是L层特征映射F_XL的重塑版本。F_XL被重塑成F̂_XL，一个KxN的矩阵，其中K是L层特征映射的数量，N是任何向量化特征映射F_XL^K的长度。例如，第一行的F̂_XL与第一个向量化的F_XL^1。 最后，Gram矩阵必须通过将每一个元素除以矩阵中所有元素的数量进行标准化。标准化是为了消除拥有很大的N维度F̂_XL在Gram矩阵中产生的很大的值。这些很大的值将在梯度下降的时候，对第一层（在池化层之前）产生很大的影响。风格特征往往在网络中更深的层，所以标准化步骤是很重要的。 def gram_matrix(input): a, b, c, d = input.size() # a=batch size(=1) # b=number of feature maps # (c,d)=dimensions of a f. map (N=c*d) features = input.view(a * b, c * d) # resise F_XL into \\hat F_XL G = torch.mm(features, features.t()) # compute the gram product # we 'normalize' the values of the gram matrix # by dividing by the number of element in each feature maps. return G.div(a * b * c * d) 现在风格损失模型看起来和内容损失模型很像。风格间距也用G_XL和G_SL之间的均方差来计算。 class StyleLoss(nn.Module): def __init__(self, target_feature): super(StyleLoss, self).__init__() self.target = gram_matrix(target_feature).detach() def forward(self, input): G = gram_matrix(input) self.loss = F.mse_loss(G, self.target) return input 导入模型 现在我们需要导入预训练的神经网络。我们将使用19层的VGG网络，就像论文中使用的一样。 PyTorch的VGG模型实现被分为了两个字Sequential模型：features（包含卷积层和池化层）和classifier（包含全连接层）。我们将使用features模型，因为我们需要每一层卷积层的输出来计算内容和风格损失。在训练的时候有些层会有和评估不一样的行为，所以我们必须用.eval()将网络设置成评估模式。 cnn = models.vgg19(pretrained=True).features.to(device).eval() 此外，VGG网络通过使用mean=[0.485, 0.456, 0.406]和std=[0.229, 0.224, 0.225]参数来标准化图片的每一个通道，并在图片上进行训练。因此，我们将在把图片输入神经网络之前，先使用这些参数对图片进行标准化。 cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device) cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device) # create a module to normalize input image so we can easily put it in a # nn.Sequential class Normalization(nn.Module): def __init__(self, mean, std): super(Normalization, self).__init__() # .view the mean and std to make them [C x 1 x 1] so that they can # directly work with image Tensor of shape [B x C x H x W]. # B is batch size. C is number of channels. H is height and W is width. self.mean = torch.tensor(mean).view(-1, 1, 1) self.std = torch.tensor(std).view(-1, 1, 1) def forward(self, img): # normalize img return (img - self.mean) / self.std 一个Sequential模型包含一个顺序排列的子模型序列。例如，vff19.features包含一个以正确的深度顺序排列的序列（Conv2d, ReLU, MaxPool2d, Conv2d, ReLU…）。我们需要将我们自己的内容损失和风格损失层在感知到卷积层之后立即添加进去。因此，我们必须创建一个新的Sequential模型，并正确的插入内容损失和风格损失模型。 # desired depth layers to compute style/content losses : content_layers_default = ['conv_4'] style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5'] def get_style_model_and_losses(cnn, normalization_mean, normalization_std, style_img, content_img, content_layers=content_layers_default, style_layers=style_layers_default): cnn = copy.deepcopy(cnn) # normalization module normalization = Normalization(normalization_mean, normalization_std).to(device) # just in order to have an iterable access to or list of content/syle # losses content_losses = [] style_losses = [] # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential # to put in modules that are supposed to be activated sequentially model = nn.Sequential(normalization) i = 0 # increment every time we see a conv for layer in cnn.children(): if isinstance(layer, nn.Conv2d): i += 1 name = 'conv_{}'.format(i) elif isinstance(layer, nn.ReLU): name = 'relu_{}'.format(i) # The in-place version doesn't play very nicely with the ContentLoss # and StyleLoss we insert below. So we replace with out-of-place # ones here. layer = nn.ReLU(inplace=False) elif isinstance(layer, nn.MaxPool2d): name = 'pool_{}'.format(i) elif isinstance(layer, nn.BatchNorm2d): name = 'bn_{}'.format(i) else: raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__)) model.add_module(name, layer) if name in content_layers: # add content loss: target = model(content_img).detach() content_loss = ContentLoss(target) model.add_module(\"content_loss_{}\".format(i), content_loss) content_losses.append(content_loss) if name in style_layers: # add style loss: target_feature = model(style_img).detach() style_loss = StyleLoss(target_feature) model.add_module(\"style_loss_{}\".format(i), style_loss) style_losses.append(style_loss) # now we trim off the layers after the last content and style losses for i in range(len(model) - 1, -1, -1): if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss): break model = model[:(i + 1)] return model, style_losses, content_losses 下一步，我们选择输入图片。你可以使用内容图片的副本或者白噪声。 input_img = content_img.clone() # if you want to use white noise instead uncomment the below line: # input_img = torch.randn(content_img.data.size(), device=device) # add the original input image to the figure: plt.figure() imshow(input_img, title='Input Image') 梯度下降 和算法的作者Leon Gatys的在 这里建议的一样，我们将使用L-BFGS算法来进行我们的梯度下降。与训练一般网络不同，我们训练输入图片是为了最小化内容/风格损失。我们要创建一个PyTorch的L-BFGS优化器optim.LBFGS，并传入我们的图片到其中，作为张量去优化。 def get_input_optimizer(input_img): # this line to show that input is a parameter that requires a gradient optimizer = optim.LBFGS([input_img.requires_grad_()]) return optimizer 最后，我们必须定义一个方法来展示图像风格转换。对于每一次的网络迭代，都将更新过的输入传入其中并计算损失。我们要运行每一个损失模型的backward方法来计算它们的梯度。优化器需要一个“关闭”方法，它重新估计模型并且返回损失。 我们还有最后一个问题要解决。神经网络可能会尝试使张量图片的值超过0到1之间来优化输入。我们可以通过在每次网络运行的时候将输入的值矫正到0到1之间来解决这个问题。 def run_style_transfer(cnn, normalization_mean, normalization_std, content_img, style_img, input_img, num_steps=300, style_weight=1000000, content_weight=1): \"\"\"Run the style transfer.\"\"\" print('Building the style transfer model..') model, style_losses, content_losses = get_style_model_and_losses(cnn, normalization_mean, normalization_std, style_img, content_img) optimizer = get_input_optimizer(input_img) print('Optimizing..') run = [0] while run[0] 最后，我们可以运行这个算法。 output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std, content_img, style_img, input_img) plt.figure() imshow(output, title='Output Image') # sphinx_gallery_thumbnail_number = 4 plt.ioff() plt.show() 输出: Building the style transfer model.. Optimizing.. run [50]: Style Loss : 4.169304 Content Loss: 4.235329 run [100]: Style Loss : 1.145476 Content Loss: 3.039176 run [150]: Style Loss : 0.716769 Content Loss: 2.663749 run [200]: Style Loss : 0.476047 Content Loss: 2.500893 run [250]: Style Loss : 0.347092 Content Loss: 2.410895 run [300]: Style Loss : 0.263698 Content Loss: 2.358449 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"fgsm_tutorial.html":{"url":"fgsm_tutorial.html","title":"对抗性示例生成","keywords":"","body":"对抗性示例生成 译者：cangyunye 作者: Nathan Inkawhich 如果你正在阅读这篇文章，希望你能理解一些机器学习模型是多么有效。现在的研究正在不断推动ML模型变得更快、更准确和更高效。然而，在设计和训练模型中经常会忽视的是安全性和健壮性方面，特别是在面对欺骗模型的对手时。 本教程将提高您对ML模型安全漏洞的认识，并将深入探讨对抗性机器学习这一热门话题。您可能会惊讶地发现，在图像中添加细微的干扰会导致模型性能的巨大差异。鉴于这是一个教程，我们将通过一个图像分类器上的示例来探索这个主题。具体来说，我们将使用第一个也是最流行的攻击方法之一，快速梯度符号攻击Fast Gradient Sign Attack(FGSM)，以欺骗一个MNIST分类器。 威胁模型 就上下文而言，有许多类型的对抗性攻击，每一类攻击都有不同的目标和对攻击者知识的假设。然而，总的目标是在输入数据中添加最少的扰动，以导致所需的错误分类。攻击者的知识有几种假设，其中两种是:白盒和黑盒。白盒攻击假定攻击者具有对模型的全部知识和访问权，包括体系结构、输入、输出和权重。黑盒攻击假设攻击者只访问模型的输入和输出，对底层架构或权重一无所知。目标也有几种类型，包括错误分类和源/目标错误分类。错误分类的目标意味着对手只希望输出分类是错误的，而不关心新的分类是什么。源/目标错误分类意味着对手想要更改原来属于特定源类的图像，以便将其分类为特定的目标类。 在这种情况下，FGSM攻击是一种以错误分类为目标的白盒攻击。有了这些背景信息，我们现在可以详细讨论攻击。 快速梯度符号攻击 到目前为止，最早也是最流行的对抗性攻击之一被称为快速梯度符号攻击(FGSM)，由Goodfellow等人在解释和利用对抗性示例( Explaining and Harnessing Adversarial Examples)时介绍到。这种攻击非常强大，而且直观。它被设计用来攻击神经网络，利用他们学习的方式，梯度gradients。这个想法很简单，比起根据后向传播梯度来调整权重使损失最小化，这种攻击是根据相同的反向传播梯度调整输入数据来最大化损失。换句话说，攻击使用了输入数据相关的梯度损失方式，通过调整输入数据，使损失最大化。 在我们深入代码之前，让我们看看著名的FGSM panda示例并提取一些符号。 从图像中看，\\mathbf{x} 是一个正确分类为“熊猫”(panda)的原始输入图像， y 是对 \\mathbf{x} 的真实表征标签ground truth label, \\mathbf{\\theta} 表示模型参数， 而 J(\\mathbf{\\theta}, \\mathbf{x}, y) 是用来训练网络的损失函数。 这种攻击将梯度后向传播到输入数据来计算 \\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)。然后将输入数据通过一小步 \\epsilon 或 如图中的0.007 ) 在(i.e. sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)) 方向上调整，使损失最大化。结果将得到受到干扰的图像， x'，尽管图片还是“熊猫”，但它一杯目标网络错误分类为“长臂猿”(gibbon)了 希望看到现在的你，已经明确了解了本教程的动机，那么，让我们开始实现它吧。 from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms import numpy as np import matplotlib.pyplot as plt 实现 在本节中，我们将讨论本教程的输入参数，定义受攻击的模型，然后编写攻击代码并运行一些测试。 输入 本教程只有三个输入，定义如下: epsilons - 要用于运行的epsilon值的列表。在列表中保留0是很重要的，因为它代表了原始测试集上的模型性能。而且，直觉上我们认为，epsilon越大，扰动越明显，但在降低模型精度方面攻击越有效。因为这里的数据范围是 [0,1]，所以取值不应该超过1。 pretrained_model - 表示使用 pytorch/examples/mnist进行训练的预训练MNIST模型的路径。为了简单起见，在这里 下载预先训练的模型。 use_cuda - 如果需要和可用，使用CUDA的布尔标志。注意，带有CUDA的GPU对于本教程来说并不重要，因为CPU不会占用太多时间。 epsilons = [0, .05, .1, .15, .2, .25, .3] pretrained_model = \"data/lenet_mnist_model.pth\" use_cuda=True 受攻模型 如前所述，受攻模型与pytorch/examples/mnist中的MNIST模型相同。您可以培训并保存自己的MNIST模型，也可以下载并使用提供的模型。这里的Net定义和测试dataloader是从MNIST示例中复制的。本节的目的是定义模型和加载数据，然后初始化模型并加载预先训练的权重。 # LeNet Model definition class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 10, kernel_size=5) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(320, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(-1, 320) x = F.relu(self.fc1(x)) x = F.dropout(x, training=self.training) x = self.fc2(x) return F.log_softmax(x, dim=1) # MNIST Test dataset and dataloader declaration test_loader = torch.utils.data.DataLoader( datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([ transforms.ToTensor(), ])), batch_size=1, shuffle=True) # Define what device we are using print(\"CUDA Available: \",torch.cuda.is_available()) device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\") # Initialize the network model = Net().to(device) # Load the pretrained model model.load_state_dict(torch.load(pretrained_model, map_location='cpu')) # Set the model in evaluation mode. In this case this is for the Dropout layers model.eval() Out: Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz Processing... Done! CUDA Available: True FGSM 攻击方式 现在，我们可以定义一个通过打乱原始输入来生成对抗性示例的函数。 fgsm_attack 函数有3个输入, image 是原始图像 x ， epsilon 是像素级干扰量 \\epsilon，data_grad 是关于输入图像 \\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y) 的损失。然后该函数创建干扰图像如下 perturbed\\_image = image + epsilon*sign(data\\_grad) = x + \\epsilon * sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)) 最后，为了保持数据的原始范围，将扰动后的图像截取范围在 [0,1]。 # FGSM attack code def fgsm_attack(image, epsilon, data_grad): # Collect the element-wise sign of the data gradient sign_data_grad = data_grad.sign() # Create the perturbed image by adjusting each pixel of the input image perturbed_image = image + epsilon*sign_data_grad # Adding clipping to maintain [0,1] range perturbed_image = torch.clamp(perturbed_image, 0, 1) # Return the perturbed image return perturbed_image 功能验证 最后，本教程的核心结果来自测试test函数。对这个测试函数的每次调用都在MNIST测试集上执行一个完整的测试步骤，然后给出一个最终准确性报告。但是，注意这个函数也接受一个epsilon输入。这是因为测试test函数报告了一个模型的准确性，该模型正受到强度为\\epsilon的对手的攻击。更具体地说，对于测试集中的每个样本，该函数计算和输入数据 data\\_grad 相关的损失梯度，用fgsm_attack perturbed\\_data 创建一个干扰图像，然后检查干扰的例子是否是对抗性的。除了检测模型的准确性外，函数还需要保存和返回一些成功性的示例以便日后查看。 def test( model, device, test_loader, epsilon ): # Accuracy counter correct = 0 adv_examples = [] # Loop over all examples in test set for data, target in test_loader: # Send the data and label to the device data, target = data.to(device), target.to(device) # Set requires_grad attribute of tensor. Important for Attack data.requires_grad = True # Forward pass the data through the model output = model(data) init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability # If the initial prediction is wrong, dont bother attacking, just move on if init_pred.item() != target.item(): continue # Calculate the loss loss = F.nll_loss(output, target) # Zero all existing gradients model.zero_grad() # Calculate gradients of model in backward pass loss.backward() # Collect datagrad data_grad = data.grad.data # Call FGSM Attack perturbed_data = fgsm_attack(data, epsilon, data_grad) # Re-classify the perturbed image output = model(perturbed_data) # Check for success final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability if final_pred.item() == target.item(): correct += 1 # Special case for saving 0 epsilon examples if (epsilon == 0) and (len(adv_examples) 启动攻击 实现的最后一部分是运行攻击操作。在这里，我们对输入中的每个epsilon值运行一个完整的测试步骤。对于每个epsilon，我们也保存最后的精度和一些将在接下来的部分中绘制的成功的对抗性例子。请注意，随着epsilon值的增加，打印出来的精度是如何降低的。另外，注意\\epsilon=0用例表示原始未受攻击的测试准确性。 accuracies = [] examples = [] # Run test for each epsilon for eps in epsilons: acc, ex = test(model, device, test_loader, eps) accuracies.append(acc) examples.append(ex) Out: Epsilon: 0 Test Accuracy = 9810 / 10000 = 0.981 Epsilon: 0.05 Test Accuracy = 9426 / 10000 = 0.9426 Epsilon: 0.1 Test Accuracy = 8510 / 10000 = 0.851 Epsilon: 0.15 Test Accuracy = 6826 / 10000 = 0.6826 Epsilon: 0.2 Test Accuracy = 4301 / 10000 = 0.4301 Epsilon: 0.25 Test Accuracy = 2082 / 10000 = 0.2082 Epsilon: 0.3 Test Accuracy = 869 / 10000 = 0.0869 结果 准确性 vs Epsilon 第一个结果是相对于epsilon的精确度。正如前面提到的，随着epsilon的增加，我们预期测试的准确性会降低。这是因为更大的epsilon意味着我们在使损失最大化的方向上迈出了更大的一步。注意，即使epsilon值是线性间隔的，曲线的趋势却不是线性的。比如说，精度在\\epsilon=0.05 只比\\epsilon=0小约4%，但是这个 \\epsilon=0.2精度却比 \\epsilon=0.15小了25%。 另外，需要注意的是，在 \\epsilon=0.25 和 \\epsilon=0.3之间做了10次分类的分类器，模型的精度会达到随机精度。 plt.figure(figsize=(5,5)) plt.plot(epsilons, accuracies, \"*-\") plt.yticks(np.arange(0, 1.1, step=0.1)) plt.xticks(np.arange(0, .35, step=0.05)) plt.title(\"Accuracy vs Epsilon\") plt.xlabel(\"Epsilon\") plt.ylabel(\"Accuracy\") plt.show() 对抗性用例样本 并没有什么尽善尽美之事，在这里，随着epsilon的增加，测试精度降低，但扰动变得更容易察觉。实际上，攻击者必须考虑准确性下降和可感知性之间的权衡。在这里，我们展示了在每个值上成功的对抗性例子。图中的每一行都显示不同的epsilon值。第一行是\\epsilon=0的例子，它表示原始的无扰动的纯净图像。每个图像的标题显示“原始分类->干扰分类（adversarial classification）”。请注意，在\\epsilon=0.15和\\epsilon=0.3处开始出现明显的扰动。然而，在所有情况下，尽管添加了躁动因素（干扰），人类仍然能够识别正确的类。 # Plot several examples of adversarial samples at each epsilon cnt = 0 plt.figure(figsize=(8,10)) for i in range(len(epsilons)): for j in range(len(examples[i])): cnt += 1 plt.subplot(len(epsilons),len(examples[0]),cnt) plt.xticks([], []) plt.yticks([], []) if j == 0: plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14) orig,adv,ex = examples[i][j] plt.title(\"{} -> {}\".format(orig, adv)) plt.imshow(ex, cmap=\"gray\") plt.tight_layout() plt.show() 接下来的方向 希望本教程对您来说，能够提供一些关于对抗性机器学习主题的见解。从这里开始有很多可能的方向。这种攻击代表了对抗性攻击研究的开始，并且自从有了许多关于如何攻击和保护ML模型不受对手攻击的后续想法以来。事实上，在NIPS 2017年有一场对抗性的攻防竞赛，本文描述了很多比赛中使用的方法:对抗性的攻防及竞赛（Adversarial Attacks and Defences Competition）。在防御方面的工作也引入了使机器学习模型在一般情况下更健壮*robust*的想法，这是一种自然扰动和反向精心设计的输入。 另一个研究方向是不同领域的对抗性攻击和防御。对抗性研究并不局限于图像领域，就比如这种语音到文本模型speech-to-text models的攻击。当然，了解更多关于对抗性机器学习的最好方法是多动手。首先，尝试实现一个不同于NIPS 2017比赛的攻击，看看它与FGSM有什么不同，然后，尝试设计保护模型，使其免于自己的攻击。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"super_resolution_with_caffe2.html":{"url":"super_resolution_with_caffe2.html","title":"使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端","keywords":"","body":"使用ONNX将模型从PyTorch传输到Caffe2和Mobile(移动端) 译者：冯宝宝 在本教程中，我们将介绍如何使用ONNX将PyTorch中定义的模型转换为ONNX格式，然后将其加载到Caffe2中。一旦进入Caffe2，我们就可以运行模型来仔细检查它是否正确导出，然后我们展示了如何使用Caffe2功能（如移动导出器）在移动设备上执行模型。 在本教程中，你需要安装onnx和Caffe2。您可以使用pip install onnx获取onnx的二进制版本。 注意: 本教程需要PyTorch master分支，可以按照 这里说明进行安装。 # 一些包的导入 import io import numpy as np from torch import nn import torch.utils.model_zoo as model_zoo import torch.onnx 超分辨率是一种提高图像，视频分辨率的方法，广泛用于图像处理或视频剪辑。在本教程中，我们将首先使用带有虚拟输入的小型超分辨率模型。 首先，让我们在PyTorch中创建一个SuperResolution模型。这个模型 直接来自PyTorch的例子，没有修改： # PyTorch中定义的Super Resolution模型 import torch.nn as nn import torch.nn.init as init class SuperResolutionNet(nn.Module): def __init__(self, upscale_factor, inplace=False): super(SuperResolutionNet, self).__init__() self.relu = nn.ReLU(inplace=inplace) self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2)) self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1)) self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1)) self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1)) self.pixel_shuffle = nn.PixelShuffle(upscale_factor) self._initialize_weights() def forward(self, x): x = self.relu(self.conv1(x)) x = self.relu(self.conv2(x)) x = self.relu(self.conv3(x)) x = self.pixel_shuffle(self.conv4(x)) return x def _initialize_weights(self): init.orthogonal_(self.conv1.weight, init.calculate_gain('relu')) init.orthogonal_(self.conv2.weight, init.calculate_gain('relu')) init.orthogonal_(self.conv3.weight, init.calculate_gain('relu')) init.orthogonal_(self.conv4.weight) # 使用上面模型定义，创建super-resolution模型 torch_model = SuperResolutionNet(upscale_factor=3) 通常，你现在会训练这个模型; 但是，对于本教程我们将下载一些预先训练的权重。请注意，此模型未经过充分训练来获得良好的准确性，此处仅用于演示目的。 # 加载预先训练好的模型权重 del_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth' batch_size = 1 # just a random number # 使用预训练的权重初始化模型 map_location = lambda storage, loc: storage if torch.cuda.is_available(): map_location = None torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location)) # 将训练模式设置为falsesince we will only run the forward pass. torch_model.train(False) 在PyTorch中导出模型通过跟踪工作。要导出模型，请调用torch.onnx._export（）函数。这将执行模型，记录运算符用于计算输出的轨迹。因为_export运行模型，我们需要提供输入张量x。这个张量的值并不重要; 它可以是图像或随机张量，只要它是正确的大小。 要了解有关PyTorch导出界面的更多详细信息，请查看torch.onnx documentation文档。 # 输入模型 x = torch.randn(batch_size, 1, 224, 224, requires_grad=True) # 导出模型 torch_out = torch.onnx._export(torch_model, # model being run x, # model input (or a tuple for multiple inputs) \"super_resolution.onnx\", # where to save the model (can be a file or file-like object) export_params=True) # store the trained parameter weights inside the model file torch_out 是执行模型后的输出。通常您可以忽略此输出，但在这里我们将使用它来验证我们导出的模型在Caffe2中运行时计算相同的值。 现在让我们采用ONNX表示并在Caffe2中使用它。这部分通常可以在一个单独的进程中或在另一台机器上完成，但我们将继续在同一个进程中，以便我们可以验证Caffe2和PyTorch是否为网络计算相同的值： import onnx import caffe2.python.onnx.backend as onnx_caffe2_backend # Load the ONNX ModelProto object. model is a standard Python protobuf object model = onnx.load(\"super_resolution.onnx\") # prepare the caffe2 backend for executing the model this converts the ONNX model into a # Caffe2 NetDef that can execute it. Other ONNX backends, like one for CNTK will be # availiable soon. prepared_backend = onnx_caffe2_backend.prepare(model) # run the model in Caffe2 # Construct a map from input names to Tensor data. # The graph of the model itself contains inputs for all weight parameters, after the input image. # Since the weights are already embedded, we just need to pass the input image. # Set the first input. W = {model.graph.input[0].name: x.data.numpy()} # Run the Caffe2 net: c2_out = prepared_backend.run(W)[0] # Verify the numerical correctness upto 3 decimal places np.testing.assert_almost_equal(torch_out.data.cpu().numpy(), c2_out, decimal=3) print(\"Exported model has been executed on Caffe2 backend, and the result looks good!\") 我们应该看到PyTorch和Caffe2的输出在数字上匹配最多3位小数。作为旁注，如果它们不匹配则存在Caffe2和PyTorch中的运算符以不同方式实现的问题，请在这种情况下与我们联系。 使用ONNX转换SRResNET 使用与上述相同的过程，我们参考本文中提出的超分辨率转移了一个有趣的新模型“SRResNet”（感谢Twitter上的作者为本教程的目的提供了代码和预训练参数）。可在此处找到模型定义和预训练模型。下面是SRResNet模型输入，输出的样子。 在移动设备上运行模型 到目前为止，我们已经从PyTorch导出了一个模型，并展示了如何加载它并在Caffe2中运行它。现在模型已加载到Caffe2中，我们可以将其转换为适合在移动设备上运行的格式移动设备上运行的格式。 我们将使用Caffe2的mobile_exporter生成可在移动设备上运行的两个模型protobufs。 第一个用于使用正确的权重初始化网络，第二个实际运行执行模型。在本教程的其余部分，我们将继续使用小型超分辨率模型。 # extract the workspace and the model proto from the internal representation c2_workspace = prepared_backend.workspace c2_model = prepared_backend.predict_net # 现在导入caffe2mobile_exporter from caffe2.python.predictor import mobile_exporter # call the Export to get the predict_net, init_net. These nets are needed for running things on mobile init_net, predict_net = mobile_exporter.Export(c2_workspace, c2_model, c2_model.external_input) # 我们还将init_net和predict_net保存到我们稍后将用于在移动设备上运行它们的文件中 with open('init_net.pb', \"wb\") as fopen: fopen.write(init_net.SerializeToString()) with open('predict_net.pb', \"wb\") as fopen: fopen.write(predict_net.SerializeToString()) init_net具有模型参数和嵌入在其中的模型输入，predict_net将用于指导运行时的init_net执行。 在本教程中，我们将使用上面生成的init_net和predict_net，并在正常的Caffe2后端和移动设备中运行它们，并验证两次运行中生成的输出高分辨率猫咪图像是否相同。 在本教程中，我们将使用广泛使用的著名猫咪图像，如下所示： # Some standard imports from caffe2.proto import caffe2_pb2 from caffe2.python import core, net_drawer, net_printer, visualize, workspace, utils import numpy as np import os import subprocess from PIL import Image from matplotlib import pyplot from skimage import io, transform 首先，让我们加载图像，使用标准的skimage python库对其进行预处理。 请注意，此预处理是处理用于训练/测试神经网络的数据的标准做法。 # 加载图像 img_in = io.imread(\"./_static/img/cat.jpg\") # 设置图片分辨率为 224x224 img = transform.resize(img_in, [224, 224]) # 保存好设置的图片作为模型的输入 io.imsave(\"./_static/img/cat_224x224.jpg\", img) 现在，作为下一步，让我们拍摄调整大小的猫图像并在Caffe2后端运行超分辨率模型并保存输出图像。 这里的图像处理步骤已经从PyTorch实现的超分辨率模型中采用。 # 加载设置好的图片并更改为YCbCr的格式 img = Image.open(\"./_static/img/cat_224x224.jpg\") img_ycbcr = img.convert('YCbCr') img_y, img_cb, img_cr = img_ycbcr.split() # Let's run the mobile nets that we generated above so that caffe2 workspace is properly initialized workspace.RunNetOnce(init_net) workspace.RunNetOnce(predict_net) # Caffe2 has a nice net_printer to be able to inspect what the net looks like and identify # what our input and output blob names are. print(net_printer.to_string(predict_net)) 备注： YCbCr 从上面的输出中，我们可以看到输入名为“9”，输出名为“27”（我们将数字作为blob名称有点奇怪，但这是因为跟踪JIT为模型生成了编号条目）。有点问题 后续校正。 # Now, let's also pass in the resized cat image for processing by the model. workspace.FeedBlob(\"9\", np.array(img_y)[np.newaxis, np.newaxis, :, :].astype(np.float32)) # run the predict_net to get the model output workspace.RunNetOnce(predict_net) # Now let's get the model output blob img_out = workspace.FetchBlob(\"27\") 我们已经完成了在纯Caffe2后端运行我们的移动网络，现在，让我们在Android设备上执行该模型并获取模型输出。 注意：对于Android开发，需要adb shell，否则教程的以下部分将无法运行。 在我们在移动设备上运行模型的第一步中，我们把基于移动设备的本机速度测试基准二进制文件推送到adb。这个二进制文件可以在移动设备上执行模型，也可以导出我们稍后可以检索的模型输出。二进制文件可在此处获得。要构建二进制文件，请按照此处的说明执行build_android.sh脚本。 注意： 你需要已经安装了ANDROID_NDK,并且设置环境变量ANDROID_NDK=path to ndk root。 # let's first push a bunch of stuff to adb, specify the path for the binary CAFFE2_MOBILE_BINARY = ('caffe2/binaries/speed_benchmark') # we had saved our init_net and proto_net in steps above, we use them now. # Push the binary and the model protos os.system('adb push ' + CAFFE2_MOBILE_BINARY + ' /data/local/tmp/') os.system('adb push init_net.pb /data/local/tmp') os.system('adb push predict_net.pb /data/local/tmp') # Let's serialize the input image blob to a blob proto and then send it to mobile for execution. with open(\"input.blobproto\", \"wb\") as fid: fid.write(workspace.SerializeBlob(\"9\")) # push the input image blob to adb os.system('adb push input.blobproto /data/local/tmp/') # Now we run the net on mobile, look at the speed_benchmark --help for what various options mean os.system( 'adb shell /data/local/tmp/speed_benchmark ' # binary to execute '--init_net=/data/local/tmp/super_resolution_mobile_init.pb ' # mobile init_net '--net=/data/local/tmp/super_resolution_mobile_predict.pb ' # mobile predict_net '--input=9 ' # name of our input image blob '--input_file=/data/local/tmp/input.blobproto ' # serialized input image '--output_folder=/data/local/tmp ' # destination folder for saving mobile output '--output=27,9 ' # output blobs we are interested in '--iter=1 ' # number of net iterations to execute '--caffe2_log_level=0 ' ) # get the model output from adb and save to a file os.system('adb pull /data/local/tmp/27 ./output.blobproto') # We can recover the output content and post-process the model using same steps as we followed earlier blob_proto = caffe2_pb2.BlobProto() blob_proto.ParseFromString(open('./output.blobproto').read()) img_out = utils.Caffe2TensorToNumpyArray(blob_proto.tensor) img_out_y = Image.fromarray(np.uint8((img_out[0,0]).clip(0, 255)), mode='L') final_img = Image.merge( \"YCbCr\", [ img_out_y, img_cb.resize(img_out_y.size, Image.BICUBIC), img_cr.resize(img_out_y.size, Image.BICUBIC), ]).convert(\"RGB\") final_img.save(\"./_static/img/cat_superres_mobile.jpg\") 现在，您可以比较图像cat_superres.jpg（来自纯caffe2后端执行的模型输出）和cat_superres_mobile.jpg（来自移动执行的模型输出），并看到两个图像看起来相同。 如果它们看起来不一样，那么在移动设备上执行会出现问题，在这种情况下，请联系Caffe2社区。你应该期望看到输出图像如下所示： 使用上述步骤，您可以轻松地在移动设备上部署模型。 另外，有关caffe2移动后端的更多信息，请查看caffe2-android-demo。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"tut_text.html":{"url":"tut_text.html","title":"文本","keywords":"","body":"文本 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"chatbot_tutorial.html":{"url":"chatbot_tutorial.html","title":"聊天机器人教程","keywords":"","body":"聊天机器人教程 作者: Matthew Inkawhich 译者: 毛毛虫 校验: 片刻 在本教程中，我们探索了一个好玩和有趣的循环序列到序列的模型用例。我们将用 Cornell Movie-Dialogs Corpus 处的电影剧本来训练一个简单的聊天机器人。 在人工智能研究领域中对话模型是一个非常热门的话题。聊天机器人可以在各种设置中找到，包括客户服务应用和在线帮助。这些机器人通常由基于检索的模型提供支持，这些输出是某些形式问题预先定义的响应。在像公司IT服务台这样高度受限制的领域中，这些模型可能足够了，但是，对于更一般的用例它们不够健壮。教一台机器与多领域的人进行有意义的对话是一个远未解决的研究问题。最近，深度学习热潮已经允许强大的生成模型，如谷歌的神经对话模型 Neural Conversational Model，这标志着向多领域生成对话模型迈出了一大步。 在本教程中，我们将在PyTorch中实现这种模型。 > hello? Bot: hello . > where am I? Bot: you re in a hospital . > who are you? Bot: i m a lawyer . > how are you doing? Bot: i m fine . > are you my friend? Bot: no . > you're under arrest Bot: i m trying to help you ! > i'm just kidding Bot: i m sorry . > where are you from? Bot: san francisco . > it's time for me to leave Bot: i know . > goodbye Bot: goodbye . 教程要点 对 Cornell Movie-Dialogs Corpus 数据集的加载和预处理 用 Luong attention mechanism(s) 实现一个sequence-to-sequence模型 使用小批量数据联合训练解码器和编码器模型 实现贪婪搜索解码模块 与训练好的聊天机器人互动 鸣谢 本教程借鉴以下源码： Yuan-Kuei Wu’s pytorch-chatbot implementation: https://github.com/ywk991112/pytorch-chatbot Sean Robertson’s practical-pytorch seq2seq-translation example: https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation FloydHub’s Cornell Movie Corpus preprocessing code: https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus 准备工作 首先，下载数据文件 here 并将其放入当前目录下的data/文件夹下 之后，让我们引入一些必须的包。 from __future__ import absolute_import from __future__ import division from __future__ import print_function from __future__ import unicode_literals import torch from torch.jit import script, trace import torch.nn as nn from torch import optim import torch.nn.functional as F import csv import random import re import os import unicodedata import codecs from io import open import itertools import math USE_CUDA = torch.cuda.is_available() device = torch.device(\"cuda\" if USE_CUDA else \"cpu\") 加载和预处理数据 下一步就是格式化处理我们的数据文件并加载到我们可以使用的结构中 Cornell Movie-Dialogs Corpus 是一个丰富的电影角色对话数据集： 10,292 对电影角色的220,579 次对话 617部电影中的9,035电影角色 总共304,713中语调 这个数据集庞大而多样，在语言形式、时间段、情感上等都有很大的变化。我们希望这种多样性使我们的模型能够适应多种形式的输入和查询。 首先，我们通过数据文件的某些行来查看原始数据的格式 corpus_name = \"cornell movie-dialogs corpus\" corpus = os.path.join(\"data\", corpus_name) def printLines(file, n=10): with open(file, 'rb') as datafile: lines = datafile.readlines() for line in lines[:n]: print(line) printLines(os.path.join(corpus, \"movie_lines.txt\")) 输出: b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n' b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n' b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n' b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n' b\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n\" b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n' b\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n\" b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n' b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding. You know how sometimes you just become this \"persona\"? And you don\\'t know how to quit?\\n' b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n' 创建格式化数据文件 为了方便起见，我们将创建一个格式良好的数据文件，其中每一行包含一个由 tab 制表符分隔的查询语句和响应语句对。 以下函数便于解析原始 movie_lines.txt 数据文件。 loadLines 将文件的每一行拆分为字段(lineID, characterID, movieID, character, text)组合的字典 loadConversations 根据 movie_conversations.txt 将 loadLines 中的每一行数据进行归类 extractSentencePairs 从对话中提取一对句子 # 将文件的每一行拆分为字段字典 # line = { # 'L183198': { # 'lineID': 'L183198', # 'characterID': 'u5022', # 'movieID': 'm333', # 'character': 'FRANKIE', # 'text': \"Well we'd sure like to help you.\\n\" # }, {...} # } def loadLines(fileName, fields): lines = {} with open(fileName, 'r', encoding='iso-8859-1') as f: for line in f: values = line.split(\" +++$+++ \") # Extract fields lineObj = {} for i, field in enumerate(fields): lineObj[field] = values[i] lines[lineObj['lineID']] = lineObj return lines # 将 `loadLines` 中的行字段分组为基于 *movie_conversations.txt* 的对话 # [{ # 'character1ID': 'u0', # 'character2ID': 'u2', # 'movieID': 'm0', # 'utteranceIDs': \"['L194', 'L195', 'L196', 'L197']\\n\", # 'lines': [{ # 'lineID': 'L194', # 'characterID': 'u0', # 'movieID': 'm0', # 'character': 'BIANCA', # 'text': 'Can we make this quick? Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad. Again.\\n' # }, { # 'lineID': 'L195', # 'characterID': 'u2', # 'movieID': 'm0', # 'character': 'CAMERON', # 'text': \"Well, I thought we'd start with pronunciation, if that's okay with you.\\n\" # }, { # 'lineID': 'L196', # 'characterID': 'u0', # 'movieID': 'm0', # 'character': 'BIANCA', # 'text': 'Not the hacking and gagging and spitting part. Please.\\n' # }, { # 'lineID': 'L197', # 'characterID': 'u2', # 'movieID': 'm0', # 'character': 'CAMERON', # 'text': \"Okay... then how 'bout we try out some French cuisine. Saturday? Night?\\n\" # }] # }, {...}] def loadConversations(fileName, lines, fields): conversations = [] with open(fileName, 'r', encoding='iso-8859-1') as f: for line in f: values = line.split(\" +++$+++ \") # Extract fields convObj = {} for i, field in enumerate(fields): convObj[field] = values[i] # Convert string to list (convObj[\"utteranceIDs\"] == \"['L598485', 'L598486', ...]\") lineIds = eval(convObj[\"utteranceIDs\"]) # Reassemble lines convObj[\"lines\"] = [] for lineId in lineIds: convObj[\"lines\"].append(lines[lineId]) conversations.append(convObj) return conversations # 从对话中提取一对句子 def extractSentencePairs(conversations): qa_pairs = [] for conversation in conversations: # Iterate over all the lines of the conversation for i in range(len(conversation[\"lines\"]) - 1): # We ignore the last line (no answer for it) inputLine = conversation[\"lines\"][i][\"text\"].strip() targetLine = conversation[\"lines\"][i+1][\"text\"].strip() # Filter wrong samples (if one of the lists is empty) if inputLine and targetLine: qa_pairs.append([inputLine, targetLine]) return qa_pairs 现在我们将调用这些函数来创建文件，我们命名为 formatted_movie_lines.txt. # Define path to new file datafile = os.path.join(corpus, \"formatted_movie_lines.txt\") delimiter = '\\t' # Unescape the delimiter delimiter = str(codecs.decode(delimiter, \"unicode_escape\")) # Initialize lines dict, conversations list, and field ids lines = {} conversations = [] MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"] MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"] # Load lines and process conversations print(\"\\nProcessing corpus...\") lines = loadLines(os.path.join(corpus, \"movie_lines.txt\"), MOVIE_LINES_FIELDS) print(\"\\nLoading conversations...\") conversations = loadConversations(os.path.join(corpus, \"movie_conversations.txt\"), lines, MOVIE_CONVERSATIONS_FIELDS) # Write new csv file print(\"\\nWriting newly formatted file...\") with open(datafile, 'w', encoding='utf-8') as outputfile: writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n') for pair in extractSentencePairs(conversations): writer.writerow(pair) # Print a sample of lines print(\"\\nSample lines from file:\") printLines(datafile) 输出: Processing corpus... Loading conversations... Writing newly formatted file... Sample lines from file: b\"Can we make this quick? Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad. Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\" b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part. Please.\\n\" b\"Not the hacking and gagging and spitting part. Please.\\tOkay... then how 'bout we try out some French cuisine. Saturday? Night?\\n\" b\"You're asking me out. That's so cute. What's your name again?\\tForget it.\\n\" b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\n\" b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser. My sister. I can't date until she does.\\n\" b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser. My sister. I can't date until she does.\\tSeems like she could get a date easy enough...\\n\" b'Why?\\tUnsolved mystery. She used to be really popular when she started high school, then it was just like she got sick of it or something.\\n' b\"Unsolved mystery. She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\n\" b'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\n' 加载和清洗数据 我们下一个任务是创建词汇表并将查询/响应句子对（对话）加载到内存。 注意我们正在处理词序，这些词序没有映射到离散数值空间。因此，我们必须通过数据集中的单词来创建一个索引。 为此我们创建了一个Voc类,它会存储从单词到索引的映射、索引到单词的反向映射、每个单词的计数和总单词量。这个类提供向词汇表中添加单词的方法(addWord)、添加所有单词到句子中的方法 (addSentence) 和清洗不常见的单词方法(trim)。更多的数据清洗在后面进行。 # Default word tokens PAD_token = 0 # Used for padding short sentences SOS_token = 1 # Start-of-sentence token EOS_token = 2 # End-of-sentence token class Voc: def __init__(self, name): self.name = name self.trimmed = False self.word2index = {} self.word2count = {} self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"} self.num_words = 3 # Count SOS, EOS, PAD def addSentence(self, sentence): for word in sentence.split(' '): self.addWord(word) def addWord(self, word): if word not in self.word2index: self.word2index[word] = self.num_words self.word2count[word] = 1 self.index2word[self.num_words] = word self.num_words += 1 else: self.word2count[word] += 1 # 删除低于特定计数阈值的单词 def trim(self, min_count): if self.trimmed: return self.trimmed = True keep_words = [] for k, v in self.word2count.items(): if v >= min_count: keep_words.append(k) print('keep_words {} / {} = {:.4f}'.format( len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index) )) # Reinitialize dictionaries self.word2index = {} self.word2count = {} self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"} self.num_words = 3 # Count default tokens for word in keep_words: self.addWord(word) 现在我们可以组装词汇表和查询/响应语句对。在使用数据之前，我们必须做一些预处理。 首先，我们必须使用unicodeToAscii将unicode字符串转换为ASCII。然后，我们应该将所有字母转换为小写字母并清洗掉除基本标点之外的所有非字母字符 (normalizeString)。最后，为了帮助训练收敛，我们将过滤掉长度大于MAX_LENGTH 的句子 (filterPairs)。 MAX_LENGTH = 10 # Maximum sentence length to consider # Turn a Unicode string to plain ASCII, thanks to # https://stackoverflow.com/a/518232/2809427 def unicodeToAscii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' ) # 初始化Voc对象 和 格式化pairs对话存放到list中 def readVocs(datafile, corpus_name): print(\"Reading lines...\") # Read the file and split into lines lines = open(datafile, encoding='utf-8').read().strip().split('\\n') # Split every line into pairs and normalize pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines] voc = Voc(corpus_name) return voc, pairs # 如果对 'p' 中的两个句子都低于 MAX_LENGTH 阈值，则返回True def filterPair(p): # Input sequences need to preserve the last word for EOS token return len(p[0].split(' ')) 输出： Start preparing training data ... Reading lines... Read 221282 sentence pairs Trimmed to 64271 sentence pairs Counting words... Counted words: 18008 pairs: ['there .', 'where ?'] ['you have my word . as a gentleman', 'you re sweet .'] ['hi .', 'looks like things worked out tonight huh ?'] ['you know chastity ?', 'i believe we share an art instructor'] ['have fun tonight ?', 'tons'] ['well no . . .', 'then that s all you had to say .'] ['then that s all you had to say .', 'but'] ['but', 'you always been this selfish ?'] ['do you listen to this crap ?', 'what crap ?'] ['what good stuff ?', 'the real you .'] 另一种有利于让训练更快收敛的策略是去除词汇表中很少使用的单词。减少特征空间也会降低模型学习目标函数的难度。我们通过以下两个步骤完成这个操作: 使用 voc.trim 函数去除 MIN_COUNT 阈值以下单词 。 如果句子中包含词频过小的单词，那么整个句子也被过滤掉。 MIN_COUNT = 3 # Minimum word count threshold for trimming def trimRareWords(voc, pairs, MIN_COUNT): # Trim words used under the MIN_COUNT from the voc voc.trim(MIN_COUNT) # Filter out pairs with trimmed words keep_pairs = [] for pair in pairs: input_sentence = pair[0] output_sentence = pair[1] keep_input = True keep_output = True # Check input sentence for word in input_sentence.split(' '): if word not in voc.word2index: keep_input = False break # Check output sentence for word in output_sentence.split(' '): if word not in voc.word2index: keep_output = False break # Only keep pairs that do not contain trimmed word(s) in their input or output sentence if keep_input and keep_output: keep_pairs.append(pair) print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs))) return keep_pairs # Trim voc and pairs pairs = trimRareWords(voc, pairs, MIN_COUNT) 输出: keep_words 7823 / 18005 = 0.4345 Trimmed from 64271 pairs to 53165, 0.8272 of total 为模型准备数据 尽管我们已经投入了大量精力来准备和清洗我们的数据变成一个很好的词汇对象和一系列的句子对，但我们的模型最终希望以numerical torch 张量作为输入。 可以在 seq2seq translation tutorial 中找到为模型准备处理数据的一种方法。 在该教程中，我们使用batch size 大小为1，这意味着我们所要做的就是将句子对中的单词转换为词汇表中的相应索引，并将其提供给模型。 但是，如果你想要加速训练或者想要利用GPU并行计算能力，则需要使用小批量 mini-batches 来训练。 使用小批量 mini-batches 也意味着我们必须注意批量处理中句子长度的变化。 为了容纳同一批次中不同大小的句子，我们将使我们的批量输入张量大小 (max_length，batch_size)，其中短于 max_length 的句子在 EOS_token 之后进行零填充（zero padded）。 如果我们简单地通过将单词转换为索引 indicesFromSentence 和零填充 zero-pad 将我们的英文句子转换为张量，我们的张量将具有大小 (batch_size，max_length)，并且索引第一维将在所有时间步骤中返回完整序列。 但是，我们需要沿着时间对我们批量数据进行索引并且包括批量数据中所有序列。 因此，我们将输入批处理大小转换为 (max_length，batch_size)，以便跨第一维的索引返回批处理中所有句子的时间步长。 我们在 zeroPadding 函数中隐式处理这个转置。 inputvar 函数处理将句子转换为张量的过程，最终创建正确大小的零填充张量。它还返回批处理中每个序列的长度张量 (tensor of lengths)，长度张量稍后将传递给我们的解码器。 outputvar 函数执行与 inputvar 类似的函数，但他不返回长度张量，而是返回二进制 mask tensor 和最大目标句子长度。二进制 mask tensor 的大小与输出目标张量的大小相同，但作为 PAD_token 的每个元素都是0而其他元素都是1。 batch2traindata 只需要取一批句子对，并使用上述函数返回输入张量和目标张量。 def indexesFromSentence(voc, sentence): return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token] # zip 对数据进行合并了，相当于行列转置了 def zeroPadding(l, fillvalue=PAD_token): return list(itertools.zip_longest(*l, fillvalue=fillvalue)) # 记录 PAD_token的位置为0， 其他的为1 def binaryMatrix(l, value=PAD_token): m = [] for i, seq in enumerate(l): m.append([]) for token in seq: if token == PAD_token: m[i].append(0) else: m[i].append(1) return m # 返回填充前（加入结束index EOS_token做标记）的长度 和 填充后的输入序列张量 def inputVar(l, voc): indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l] lengths = torch.tensor([len(indexes) for indexes in indexes_batch]) padList = zeroPadding(indexes_batch) padVar = torch.LongTensor(padList) return padVar, lengths # 返回填充前（加入结束index EOS_token做标记）最长的一个长度 和 填充后的输入序列张量, 和 填充后的标记 mask def outputVar(l, voc): indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l] max_target_len = max([len(indexes) for indexes in indexes_batch]) padList = zeroPadding(indexes_batch) mask = binaryMatrix(padList) mask = torch.ByteTensor(mask) padVar = torch.LongTensor(padList) return padVar, mask, max_target_len # Returns all items for a given batch of pairs def batch2TrainData(voc, pair_batch): pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True) input_batch, output_batch = [], [] for pair in pair_batch: input_batch.append(pair[0]) output_batch.append(pair[1]) inp, lengths = inputVar(input_batch, voc) output, mask, max_target_len = outputVar(output_batch, voc) return inp, lengths, output, mask, max_target_len # Example for validation small_batch_size = 5 batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)]) input_variable, lengths, target_variable, mask, max_target_len = batches print(\"input_variable:\", input_variable) print(\"lengths:\", lengths) print(\"target_variable:\", target_variable) print(\"mask:\", mask) print(\"max_target_len:\", max_target_len) 输出： input_variable: tensor([[ 614, 281, 77, 387, 965], [ 83, 25, 53, 25, 6430], [ 11, 697, 5046, 920, 4], [1054, 50, 14, 174, 2], [ 11, 7, 7, 6, 0], [ 7, 1825, 6, 2, 0], [ 14, 234, 2, 0, 0], [5401, 36, 0, 0, 0], [ 4, 4, 0, 0, 0], [ 2, 2, 0, 0, 0]]) lengths: tensor([10, 10, 7, 6, 4]) target_variable: tensor([[ 25, 7, 7, 601, 45], [ 356, 697, 53, 4, 410], [ 7, 2182, 1231, 2, 218], [ 4, 4, 5240, 0, 492], [ 2, 2, 6, 0, 227], [ 0, 0, 2, 0, 4], [ 0, 0, 0, 0, 2]]) mask: tensor([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 0, 1], [1, 1, 1, 0, 1], [0, 0, 1, 0, 1], [0, 0, 0, 0, 1]], dtype=torch.uint8) max_target_len: 7 定义模型 Seq2Seq模型 我们聊天机器人的大脑是序列到序列（seq2seq）模型。 seq2seq模型的目标是将可变长度序列作为输入，并使用固定大小的模型将可变长度序列作为输出返回。 Sutskever et al. 发现通过一起使用两个独立的RNN，我们可以完成这项任务。 第一个RNN充当编码器，其将可变长度输入序列编码为固定长度上下文向量。 理论上，该上下文向量（RNN的最终隐藏层）将包含关于输入到机器人的查询语句的语义信息。 第二个RNN是一个解码器，它接收输入文字和上下文矢量，并返回序列中下一句文字的概率和在下一次迭代中使用的隐藏状态。 图片来源: https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/ 编码器 编码器RNN每次迭代中输入一个语句输出一个token（例如，一个单词），同时在这时间内输出“输出”向量和“隐藏状态”向量。 然后将隐藏状态向量传递到下一步，并记录输出向量。 编码器将其在序列中的每一点处看到的上下文转换为高维空间中的一系列点，解码器将使用这些点为给定任务生成有意义的输出。 我们的编码器的核心是由 Cho et al. 等人发明的多层门循环单元。 在2014年，我们将使用GRU的双向变体，这意味着基本上有两个独立的RNN：一个以正常的顺序输入输入序列，另一个以相反的顺序输入输入序列。 每个网络的输出在每个时间步骤求和。 使用双向GRU将为我们提供编码过去和未来上下文的优势。 双向RNN： 图片来源: https://colah.github.io/posts/2015-09-NN-Types-FP/ 注意:embedding层用于在任意大小的特征空间中对我们的单词索引进行编码。 对于我们的模型，此图层会将每个单词映射到大小为hidden_size的特征空间。 训练后，这些值会被编码成和他们相似的有意义词语。 最后，如果将填充的一批序列传递给RNN模块，我们必须分别使用torch.nn.utils.rnn.pack_padded_sequence和torch.nn.utils.rnn.pad_packed_sequence在RNN传递时分别进行填充和反填充。 计算图: 将单词索引转换为词嵌入 embeddings。 为RNN模块打包填充批次序列。 通过GRU进行前向传播。 反填充。 对双向GRU输出求和。 返回输出和最终隐藏状态。 输入: input_seq：一批输入句子; shape =（max_length，batch_size） input_lengths：一批次中每个句子对应的句子长度列表;shape=(batch_size) hidden:隐藏状态; shape =(n_layers x num_directions，batch_size，hidden_size) 输出: outputs：GRU最后一个隐藏层的输出特征（双向输出之和）; shape =（max_length，batch_size，hidden_size） hidden：从GRU更新隐藏状态; shape =（n_layers x num_directions，batch_size，hidden_size） class EncoderRNN(nn.Module): def __init__(self, hidden_size, embedding, n_layers=1, dropout=0): super(EncoderRNN, self).__init__() self.n_layers = n_layers self.hidden_size = hidden_size self.embedding = embedding # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size' # because our input size is a word embedding with number of features == hidden_size self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout), bidirectional=True) def forward(self, input_seq, input_lengths, hidden=None): # Convert word indexes to embeddings embedded = self.embedding(input_seq) # Pack padded batch of sequences for RNN module packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths) # Forward pass through GRU outputs, hidden = self.gru(packed, hidden) # Unpack padding outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs) # Sum bidirectional GRU outputs outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Return output and final hidden state return outputs, hidden 解码器 解码器RNN以token-by-token的方式生成响应语句。 它使用编码器的上下文向量和内部隐藏状态来生成序列中的下一个单词。 它持续生成单词，直到输出是EOS_token，这个表示句子的结尾。 一个vanilla seq2seq解码器的常见问题是，如果我们只依赖于上下文向量来编码整个输入序列的含义，那么我们很可能会丢失信息。尤其是在处理长输入序列时，这极大地限制了我们的解码器的能力。 为了解决这个问题，,Bahdanau et al. 等人创建了一种“attention mechanism”，允许解码器关注输入序列的某些部分，而不是在每一步都使用完全固定的上下文。 在一个高的层级中，用解码器的当前隐藏状态和编码器输出来计算注意力。 输出注意力的权重与输入序列具有相同的大小，允许我们将它们乘以编码器输出，给出一个加权和，表示要注意的编码器输出部分。 Sean Robertson 的图片很好地描述了这一点： Luong et al. 通过创造“Global attention”，改善了Bahdanau et al. 的基础工作。 关键的区别在于，对于“Global attention”，我们考虑所有编码器的隐藏状态，而不是Bahdanau等人的“Local attention”，它只考虑当前步中编码器的隐藏状态。 另一个区别在于，通过“Global attention”，我们仅使用当前步的解码器的隐藏状态来计算注意力权重（或者能量）。 Bahdanau等人的注意力计算需要知道前一步中解码器的状态。 此外，Luong等人提供各种方法来计算编码器输出和解码器输出之间的注意权重（能量），称之为“score functions”： 其中 h_t = 当前目标解码器状态，\\bar{h}_s = 所有编码器状态。 总体而言，Global attention机制可以通过下图进行总结。 请注意，我们将“Attention Layer”用一个名为 Attn 的 nn.Module 来单独实现。 该模块的输出是经过softmax标准化后权重张量的大小（batch_size，1，max_length）。 # Luong attention layer class Attn(torch.nn.Module): def __init__(self, method, hidden_size): super(Attn, self).__init__() self.method = method if self.method not in ['dot', 'general', 'concat']: raise ValueError(self.method, \"is not an appropriate attention method.\") self.hidden_size = hidden_size if self.method == 'general': self.attn = torch.nn.Linear(self.hidden_size, hidden_size) elif self.method == 'concat': self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size) self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size)) def dot_score(self, hidden, encoder_output): return torch.sum(hidden * encoder_output, dim=2) def general_score(self, hidden, encoder_output): energy = self.attn(encoder_output) return torch.sum(hidden * energy, dim=2) def concat_score(self, hidden, encoder_output): energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh() return torch.sum(self.v * energy, dim=2) def forward(self, hidden, encoder_outputs): # Calculate the attention weights (energies) based on the given method if self.method == 'general': attn_energies = self.general_score(hidden, encoder_outputs) elif self.method == 'concat': attn_energies = self.concat_score(hidden, encoder_outputs) elif self.method == 'dot': attn_energies = self.dot_score(hidden, encoder_outputs) # Transpose max_length and batch_size dimensions attn_energies = attn_energies.t() # Return the softmax normalized probability scores (with added dimension) return F.softmax(attn_energies, dim=1).unsqueeze(1) 现在我们已经定义了注意力子模块，我们可以实现真实的解码器模型。 对于解码器，我们将每次手动进行一批次的输入。 这意味着我们的词嵌入张量和GRU输出都将具有相同大小（1，batch_size，hidden_size）。 计算图: 获取当前输入的词嵌入 通过单向GRU进行前向传播 通过2输出的当前GRU计算注意力权重 将注意力权重乘以编码器输出以获得新的“weighted sum”上下文向量 使用Luong eq.5连接加权上下文向量和GRU输出 使用Luong eq.6预测下一个单词（没有softmax） 返回输出和最终隐藏状态 输入: input_step：每一步输入序列批次（一个单词）; shape =（1，batch_size） last_hidden：GRU的最终隐藏层; shape =（n_layers x num_directions，batch_size，hidden_size） encoder_outputs：编码器模型的输出; shape =（max_length，batch_size，hidden_size） 输出: output: 一个softmax标准化后的张量， 代表了每个单词在解码序列中是下一个输出单词的概率; shape =（batch_size，voc.num_words） hidden: GRU的最终隐藏状态; shape =（n_layers x num_directions，batch_size，hidden_size） class LuongAttnDecoderRNN(nn.Module): def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1): super(LuongAttnDecoderRNN, self).__init__() # Keep for reference self.attn_model = attn_model self.hidden_size = hidden_size self.output_size = output_size self.n_layers = n_layers self.dropout = dropout # Define layers self.embedding = embedding self.embedding_dropout = nn.Dropout(dropout) self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout)) self.concat = nn.Linear(hidden_size * 2, hidden_size) self.out = nn.Linear(hidden_size, output_size) self.attn = Attn(attn_model, hidden_size) def forward(self, input_step, last_hidden, encoder_outputs): # Note: we run this one step (word) at a time # Get embedding of current input word embedded = self.embedding(input_step) embedded = self.embedding_dropout(embedded) # Forward through unidirectional GRU rnn_output, hidden = self.gru(embedded, last_hidden) # Calculate attention weights from the current GRU output attn_weights = self.attn(rnn_output, encoder_outputs) # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # Concatenate weighted context vector and GRU output using Luong eq. 5 rnn_output = rnn_output.squeeze(0) context = context.squeeze(1) concat_input = torch.cat((rnn_output, context), 1) concat_output = torch.tanh(self.concat(concat_input)) # Predict next word using Luong eq. 6 output = self.out(concat_output) output = F.softmax(output, dim=1) # Return output and final hidden state return output, hidden 定义训练步骤 Masked 损失 由于我们处理的是批量填充序列，因此在计算损失时我们不能简单地考虑张量的所有元素。 我们定义maskNLLLoss可以根据解码器的输出张量、描述目标张量填充的binary mask张量来计算损失。 该损失函数计算与mask tensor中的1对应的元素的平均负对数似然。 def maskNLLLoss(inp, target, mask): nTotal = mask.sum() crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1)) loss = crossEntropy.masked_select(mask).mean() loss = loss.to(device) return loss, nTotal.item() 单次训练迭代 train 函数包含单次训练迭代的算法（单批输入）。 我们将使用一些巧妙的技巧来帮助融合： 第一个技巧是使用 teacher forcing。 这意味着在一些概率是由teacher_forcing_ratio设置，我们使用当前目标单词作为解码器的下一个输入，而不是使用解码器的当前推测。 该技巧充当解码器的 training wheels，有助于更有效的训练。 然而，teacher forcing 可能导致推导中的模型不稳定，因为解码器可能没有足够的机会在训练期间真正地制作自己的输出序列。 因此，我们必须注意我们如何设置teacher_forcing_ratio，同时不要被快速的收敛所迷惑。 我们实现的第二个技巧是梯度裁剪(gradient clipping)。 这是一种用于对抗“爆炸梯度（exploding gradient）”问题的常用技术。 本质上，通过将梯度剪切或阈值化到最大值，我们可以防止在损失函数中梯度以指数方式增长并发生溢出（NaN）或者越过梯度陡峭的悬崖。 图片来源: Goodfellow et al. Deep Learning. 2016. https://www.deeplearningbook.org/ Sequence of Operations: 操作顺序: 通过编码器前向计算整个批次输入。 将解码器输入初始化为SOS_token，将隐藏状态初始化为编码器的最终隐藏状态。 通过解码器一次一步地前向计算输入一批序列。 如果teacher forcing算法：将下一个解码器输入设置为当前目标; 否则：将下一个解码器输入设置为当前解码器输出。 计算并累积损失。 执行反向传播。 裁剪梯度。 更新编码器和解码器模型参数。 注意: PyTorch的RNN模块（RNN，LSTM，GRU）可以像任何其他非重复层一样使用，只需将整个输入序列（或一批序列）传递给它们。 我们在编码器中使用GRU层就是这样的。 实际情况是，在计算中有一个迭代过程循环计算隐藏状态的每一步。 或者，你每次只运行一个模块。 在这种情况下，我们在训练过程中手动循环遍历序列就像我们必须为解码器模型做的那样。 只要你正确的维护这些模型的模块，就可以非常简单的实现顺序模型。 def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH): # Zero gradients encoder_optimizer.zero_grad() decoder_optimizer.zero_grad() # Set device options input_variable = input_variable.to(device) lengths = lengths.to(device) target_variable = target_variable.to(device) mask = mask.to(device) # Initialize variables loss = 0 print_losses = [] n_totals = 0 # Forward pass through encoder encoder_outputs, encoder_hidden = encoder(input_variable, lengths) # Create initial decoder input (start with SOS tokens for each sentence) decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]]) decoder_input = decoder_input.to(device) # Set initial decoder hidden state to the encoder's final hidden state decoder_hidden = encoder_hidden[:decoder.n_layers] # Determine if we are using teacher forcing this iteration use_teacher_forcing = True if random.random() 训练迭代 现在终于将完整的训练步骤与数据结合在一起了。 给定传递的模型，优化器，数据等，trainIters函数负责运行n_iterations的训练。这个功能不言自明，因为我们通过train函数的完成了繁重工作。 需要注意的一点是，当我们保存模型时，我们会保存一个包含编码器和解码器state_dicts（参数）、优化器的state_dicts、损失、迭代等的压缩包。以这种方式保存模型将为我们checkpoint,提供最大的灵活性。 加载checkpoint后，我们将能够使用模型参数进行推理，或者我们可以在我们中断的地方继续训练。 def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename): # Load batches for each iteration training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_iteration)] # Initializations print('Initializing ...') start_iteration = 1 print_loss = 0 if loadFilename: start_iteration = checkpoint['iteration'] + 1 # Training loop print(\"Training...\") for iteration in range(start_iteration, n_iteration + 1): training_batch = training_batches[iteration - 1] # Extract fields from batch input_variable, lengths, target_variable, mask, max_target_len = training_batch # Run a training iteration with batch loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip) print_loss += loss # Print progress if iteration % print_every == 0: print_loss_avg = print_loss / print_every print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg)) print_loss = 0 # Save checkpoint if (iteration % save_every == 0): directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size)) if not os.path.exists(directory): os.makedirs(directory) torch.save({ 'iteration': iteration, 'en': encoder.state_dict(), 'de': decoder.state_dict(), 'en_opt': encoder_optimizer.state_dict(), 'de_opt': decoder_optimizer.state_dict(), 'loss': loss, 'voc_dict': voc.__dict__, 'embedding': embedding.state_dict() }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint'))) 评估定义 在训练模型后，我们希望能够自己与机器人交谈。 首先，我们必须定义我们希望模型如何解码编码输入。 贪婪解码 贪婪解码是我们在不使用 teacher forcing时在训练期间使用的解码方法。 换句话说，对于每一步，我们只需从具有最高softmax值的decoder_output中选择单词。 该解码方法在单步长级别上是最佳的。 为了便于贪婪解码操作，我们定义了一个GreedySearchDecoder类。 当运行时，类的实例化对象输入序列（input_seq）的大小是（input_seq length，1），标量输入（input_length）长度的张量和max_length来约束响应句子长度。 使用以下计算图来评估输入句子： 计算图: 通过编码器模型前向计算。 准备编码器的最终隐藏层，作为解码器的第一个隐藏输入。 将解码器的第一个输入初始化为SOS_token。 将初始化张量追加到解码后的单词中。 一次迭代解码一个单词token： 通过解码器进行前向计算。 获得最可能的单词token及其softmax分数。 记录token和分数。 准备当前token作为下一个解码器的输入。 返回收集到的单词 tokens 和 分数。 class GreedySearchDecoder(nn.Module): def __init__(self, encoder, decoder): super(GreedySearchDecoder, self).__init__() self.encoder = encoder self.decoder = decoder def forward(self, input_seq, input_length, max_length): # Forward input through encoder model encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length) # Prepare encoder's final hidden layer to be first hidden input to the decoder decoder_hidden = encoder_hidden[:decoder.n_layers] # Initialize decoder input with SOS_token decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token # Initialize tensors to append decoded words to all_tokens = torch.zeros([0], device=device, dtype=torch.long) all_scores = torch.zeros([0], device=device) # Iteratively decode one word token at a time for _ in range(max_length): # Forward pass through decoder decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs) # Obtain most likely word token and its softmax score decoder_scores, decoder_input = torch.max(decoder_output, dim=1) # Record token and score all_tokens = torch.cat((all_tokens, decoder_input), dim=0) all_scores = torch.cat((all_scores, decoder_scores), dim=0) # Prepare current token to be next decoder input (add a dimension) decoder_input = torch.unsqueeze(decoder_input, 0) # Return collections of word tokens and scores return all_tokens, all_scores 评估我们的文本 现在我们已经定义了解码方法，我们可以编写用于评估字符串输入句子的函数。 evaluate函数管理输入句子的低层级处理过程。我们首先使用batch_size == 1将句子格式化为输入批量的单词索引。我们通过将句子的单词转换为相应的索引，并通过转换维度来为我们的模型准备张量。我们还创建了一个 lengths 张量，其中包含输入句子的长度。在这种情况下，lengths 是标量因为我们一次只评估一个句子（batch_size == 1）。接下来，我们使用我们的GreedySearchDecoder实例化后的对象（searcher）获得解码响应句子的张量。最后，我们将响应的索引转换为单词并返回已解码单词的列表。 evaluateInput充当聊天机器人的用户接口。调用时，将生成一个输入文本字段，我们可以在其中输入查询语句。在输入我们的输入句子并按Enter后，我们的文本以与训练数据相同的方式标准化，并最终被输入到评估函数以获得解码的输出句子。我们循环这个过程，这样我们可以继续与我们的机器人聊天直到我们输入“q”或“quit”。 最后，如果输入的句子包含一个不在词汇表中的单词，我们会通过打印错误消息并提示用户输入另一个句子来优雅地处理。 def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH): ### Format input sentence as a batch # words -> indexes indexes_batch = [indexesFromSentence(voc, sentence)] # Create lengths tensor lengths = torch.tensor([len(indexes) for indexes in indexes_batch]) # Transpose dimensions of batch to match models' expectations input_batch = torch.LongTensor(indexes_batch).transpose(0, 1) # Use appropriate device input_batch = input_batch.to(device) lengths = lengths.to(device) # Decode sentence with searcher tokens, scores = searcher(input_batch, lengths, max_length) # indexes -> words decoded_words = [voc.index2word[token.item()] for token in tokens] return decoded_words def evaluateInput(encoder, decoder, searcher, voc): input_sentence = '' while(1): try: # Get input sentence input_sentence = input('> ') # Check if it is quit case if input_sentence == 'q' or input_sentence == 'quit': break # Normalize sentence input_sentence = normalizeString(input_sentence) # Evaluate sentence output_words = evaluate(encoder, decoder, searcher, voc, input_sentence) # Format and print response sentence output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')] print('Bot:', ' '.join(output_words)) except KeyError: print(\"Error: Encountered unknown word.\") 运行模型 最后，是时候运行我们的模型了！ 无论我们是否想要训练或测试聊天机器人模型，我们都必须初始化各个编码器和解码器模型。 在接下来的部分中，我们设置所需要的配置，选择从头开始或设置检查点以从中加载，并构建和初始化模型。 您可以随意使用不同的配置来优化性能。 # Configure models model_name = 'cb_model' attn_model = 'dot' #attn_model = 'general' #attn_model = 'concat' hidden_size = 500 encoder_n_layers = 2 decoder_n_layers = 2 dropout = 0.1 batch_size = 64 # Set checkpoint to load from; set to None if starting from scratch loadFilename = None checkpoint_iter = 4000 #loadFilename = os.path.join(save_dir, model_name, corpus_name, # '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size), # '{}_checkpoint.tar'.format(checkpoint_iter)) # Load model if a loadFilename is provided if loadFilename: # If loading on same machine the model was trained on checkpoint = torch.load(loadFilename) # If loading a model trained on GPU to CPU #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu')) encoder_sd = checkpoint['en'] decoder_sd = checkpoint['de'] encoder_optimizer_sd = checkpoint['en_opt'] decoder_optimizer_sd = checkpoint['de_opt'] embedding_sd = checkpoint['embedding'] voc.__dict__ = checkpoint['voc_dict'] print('Building encoder and decoder ...') # Initialize word embeddings embedding = nn.Embedding(voc.num_words, hidden_size) if loadFilename: embedding.load_state_dict(embedding_sd) # Initialize encoder & decoder models encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout) decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout) if loadFilename: encoder.load_state_dict(encoder_sd) decoder.load_state_dict(decoder_sd) # Use appropriate device encoder = encoder.to(device) decoder = decoder.to(device) print('Models built and ready to go!') 输出: Building encoder and decoder ... Models built and ready to go! 执行训练 如果要训练模型，请运行以下部分。 首先我们设置训练参数，然后初始化我们的优化器，最后我们调用trainIters函数来运行我们的训练迭代。 # Configure training/optimization clip = 50.0 teacher_forcing_ratio = 1.0 learning_rate = 0.0001 decoder_learning_ratio = 5.0 n_iteration = 4000 print_every = 1 save_every = 500 # Ensure dropout layers are in train mode encoder.train() decoder.train() # Initialize optimizers print('Building optimizers ...') encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate) decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio) if loadFilename: encoder_optimizer.load_state_dict(encoder_optimizer_sd) decoder_optimizer.load_state_dict(decoder_optimizer_sd) # Run training iterations print(\"Starting Training!\") trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename) 输出: Building optimizers ... Starting Training! Initializing ... Training... Iteration: 1; Percent complete: 0.0%; Average loss: 8.9717 Iteration: 2; Percent complete: 0.1%; Average loss: 8.8521 Iteration: 3; Percent complete: 0.1%; Average loss: 8.6360 Iteration: 4; Percent complete: 0.1%; Average loss: 8.4234 Iteration: 5; Percent complete: 0.1%; Average loss: 7.9403 Iteration: 6; Percent complete: 0.1%; Average loss: 7.3892 Iteration: 7; Percent complete: 0.2%; Average loss: 7.0589 Iteration: 8; Percent complete: 0.2%; Average loss: 7.0130 Iteration: 9; Percent complete: 0.2%; Average loss: 6.7383 Iteration: 10; Percent complete: 0.2%; Average loss: 6.5343 ... Iteration: 3991; Percent complete: 99.8%; Average loss: 2.6607 Iteration: 3992; Percent complete: 99.8%; Average loss: 2.6188 Iteration: 3993; Percent complete: 99.8%; Average loss: 2.8319 Iteration: 3994; Percent complete: 99.9%; Average loss: 2.5817 Iteration: 3995; Percent complete: 99.9%; Average loss: 2.4979 Iteration: 3996; Percent complete: 99.9%; Average loss: 2.7317 Iteration: 3997; Percent complete: 99.9%; Average loss: 2.5969 Iteration: 3998; Percent complete: 100.0%; Average loss: 2.2275 Iteration: 3999; Percent complete: 100.0%; Average loss: 2.7124 Iteration: 4000; Percent complete: 100.0%; Average loss: 2.5975 运行评估 To chat with your model, run the following block. 运行以下部分来与你的模型聊天 # Set dropout layers to eval mode encoder.eval() decoder.eval() # Initialize search module searcher = GreedySearchDecoder(encoder, decoder) # Begin chatting (uncomment and run the following line to begin) # evaluateInput(encoder, decoder, searcher, voc) 结论 伙计们，这就是这一切。 恭喜，您现在知道构建生成聊天机器人模型的基础知识！ 如果您有兴趣，可以尝试通过调整模型和训练参数以及自定义训练模型的数据来定制聊天机器人的行为。 查看其他教程，了解PyTorch中更酷的深度学习应用程序！ 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"char_rnn_generation_tutorial.html":{"url":"char_rnn_generation_tutorial.html","title":"使用字符级别特征的 RNN 网络生成姓氏","keywords":"","body":"使用字符级别特征的RNN网络生成名字 译者：hhxx2015 校对者：hijkzzz 作者: Sean Robertson 在上一个 例子 中我们使用RNN网络对名字所属的语言进行分类。 这一次我们会反过来根据语言生成名字。 > python sample.py Russian RUS Rovakov Uantov Shavakov > python sample.py German GER Gerren Ereng Rosher > python sample.py Spanish SPA Salla Parer Allan > python sample.py Chinese CHI Chan Hang Iun 我们仍使用只有几层线性层的小型RNN。 最大的区别在于，这里不是在读取一个名字的所有字母后预测类别，而是输入一个类别之后在每一时刻输出一个字母。 循环预测字符以形成语言通常也被称为“语言模型”。（也可以将字符换成单词或更高级的结构进行这一过程） 阅读建议: 我默认你已经安装好了PyTorch，熟悉Python语言，理解“张量”的概念： https://pytorch.org/ PyTorch安装指南 Deep Learning with PyTorch: A 60 Minute Blitz PyTorch入门 Learning PyTorch with Examples 一些PyTorch的例子 PyTorch for Former Torch Users Lua Torch 用户参考 事先学习并了解RNN的工作原理对理解这个例子十分有帮助: The Unreasonable Effectiveness of Recurrent Neural Networks 展示了很多实际的例子 Understanding LSTM Networks 是关于LSTM的，但也提供有关RNN的说明 准备数据 点击这里下载数据 并将其解压到当前文件夹。 有关此过程的更多详细信息，请参阅上一个教程。 简而言之，有一些纯文本文件data/names/[Language].txt，它们的每行都有一个名字。 我们按行将文本按行切分得到一个数组，将Unicode编码转化为ASCII编码，最终得到{language: [names ...]}格式存储的字典变量。 from __future__ import unicode_literals, print_function, division from io import open import glob import os import unicodedata import string all_letters = string.ascii_letters + \" .,;'-\" n_letters = len(all_letters) + 1 # Plus EOS marker def findFiles(path): return glob.glob(path) # Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427 def unicodeToAscii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in all_letters ) # Read a file and split into lines def readLines(filename): lines = open(filename, encoding='utf-8').read().strip().split('\\n') return [unicodeToAscii(line) for line in lines] # Build the category_lines dictionary, a list of lines per category category_lines = {} all_categories = [] for filename in findFiles('data/names/*.txt'): category = os.path.splitext(os.path.basename(filename))[0] all_categories.append(category) lines = readLines(filename) category_lines[category] = lines n_categories = len(all_categories) if n_categories == 0: raise RuntimeError('Data not found. Make sure that you downloaded data ' 'from https://download.pytorch.org/tutorial/data.zip and extract it to ' 'the current directory.') print('# categories:', n_categories, all_categories) print(unicodeToAscii(\"O'Néàl\")) Out: # categories: 18 ['Italian', 'German', 'Portuguese', 'Chinese', 'Greek', 'Polish', 'French', 'English', 'Spanish', 'Arabic', 'Czech', 'Russian', 'Irish', 'Dutch', 'Scottish', 'Vietnamese', 'Korean', 'Japanese'] O'Neal 构造神经网络 这个神经网络比 上一个RNN教程中的网络增加了额外的类别张量参数，该参数与其他输入连接在一起。 类别可以像字母一样组成one-hot向量构成张量输入。 我们将输出作为下一个字母是什么的可能性。采样过程中，当前输出可能性最高的字母作为下一时刻输入字母。 在组合隐藏状态和输出之后我增加了第二个linear层o2o，使模型的性能更好。当然还有一个dropout层，参考这篇论文随机将输入部分替换为0给出的参数（dropout=0.1）来模糊处理输入防止过拟合。 我们将它添加到网络的末端，故意添加一些混乱使采样特征增加。 import torch import torch.nn as nn class RNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(RNN, self).__init__() self.hidden_size = hidden_size self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size) self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size) self.o2o = nn.Linear(hidden_size + output_size, output_size) self.dropout = nn.Dropout(0.1) self.softmax = nn.LogSoftmax(dim=1) def forward(self, category, input, hidden): input_combined = torch.cat((category, input, hidden), 1) hidden = self.i2h(input_combined) output = self.i2o(input_combined) output_combined = torch.cat((hidden, output), 1) output = self.o2o(output_combined) output = self.dropout(output) output = self.softmax(output) return output, hidden def initHidden(self): return torch.zeros(1, self.hidden_size) 训练 训练准备 首先，构造一个可以随机获取成对训练数据(category, line)的函数。 import random # Random item from a list def randomChoice(l): return l[random.randint(0, len(l) - 1)] # Get a random category and random line from that category def randomTrainingPair(): category = randomChoice(all_categories) line = randomChoice(category_lines[category]) return category, line 对于每个时间步长（即，对于要训练单词中的每个字母），网络的输入将是“（类别，当前字母，隐藏状态）”，输出将是“（下一个字母，下一个隐藏状态）”。 因此，对于每个训练集，我们将需要类别、一组输入字母和一组输出/目标字母。 在每一个时间序列，我们使用当前字母预测下一个字母，所以训练用的字母对来自于一个单词。例如 对于 \"ABCD\"，我们将创建（“A”，“B”），（“B”，“C”），（“C”，“D”），（“D”，“EOS”））。 类别张量是一个尺寸的one-hot 向量 训练时，我们在每一个时间序列都将其提供给神经网络。这是一种选择策略，也可选择将其作为初始隐藏状态的一部分，或者其他什么结构。 # One-hot vector for category9 def categoryTensor(category): li = all_categories.index(category) tensor = torch.zeros(1, n_categories) tensor[0][li] = 1 return tensor # One-hot matrix of first to last letters (not including EOS) for input def inputTensor(line): tensor = torch.zeros(len(line), 1, n_letters) for li in range(len(line)): letter = line[li] tensor[li][0][all_letters.find(letter)] = 1 return tensor # LongTensor of second letter to end (EOS) for target def targetTensor(line): letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))] letter_indexes.append(n_letters - 1) # EOS return torch.LongTensor(letter_indexes) 为了方便训练，我们将创建一个randomTrainingExample函数，该函数随机获取（类别，行）的对并将它们转换为所需要的（类别，输入，目标）格式张量。 # Make category, input, and target tensors from a random category, line pair def randomTrainingExample(): category, line = randomTrainingPair() category_tensor = categoryTensor(category) input_line_tensor = inputTensor(line) target_line_tensor = targetTensor(line) return category_tensor, input_line_tensor, target_line_tensor 训练神经网络 和只使用最后一个时刻输出的分类任务相比，这次我们每一个时间序列都会进行一次预测，所以每一个时间序列我们都会计算损失。 autograd的神奇之处在于您可以在每一步中简单地累加这些损失，并在最后反向传播。 criterion = nn.NLLLoss() learning_rate = 0.0005 def train(category_tensor, input_line_tensor, target_line_tensor): target_line_tensor.unsqueeze_(-1) hidden = rnn.initHidden() rnn.zero_grad() loss = 0 for i in range(input_line_tensor.size(0)): output, hidden = rnn(category_tensor, input_line_tensor[i], hidden) l = criterion(output, target_line_tensor[i]) loss += l loss.backward() for p in rnn.parameters(): p.data.add_(-learning_rate, p.grad.data) return output, loss.item() / input_line_tensor.size(0) 为了跟踪训练耗费的时间，我添加一个timeSince（timestamp）函数，它返回一个人类可读的字符串： import time import math def timeSince(since): now = time.time() s = now - since m = math.floor(s / 60) s -= m * 60 return '%dm %ds' % (m, s) 训练过程和平时一样。多次运行train，等待几分钟，每print_every次打印当前时间和损失。在 all_losses 中保留每plot_every次的平均损失，以便稍后进行绘图。 rnn = RNN(n_letters, 128, n_letters) n_iters = 100000 print_every = 5000 plot_every = 500 all_losses = [] total_loss = 0 # Reset every plot_every iters start = time.time() for iter in range(1, n_iters + 1): output, loss = train(*randomTrainingExample()) total_loss += loss if iter % print_every == 0: print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss)) if iter % plot_every == 0: all_losses.append(total_loss / plot_every) total_loss = 0 Out: 0m 21s (5000 5%) 2.5152 0m 43s (10000 10%) 2.7758 1m 4s (15000 15%) 2.2884 1m 25s (20000 20%) 3.2404 1m 47s (25000 25%) 2.7298 2m 8s (30000 30%) 3.4301 2m 29s (35000 35%) 2.2306 2m 51s (40000 40%) 2.5628 3m 12s (45000 45%) 1.7700 3m 34s (50000 50%) 2.4657 3m 55s (55000 55%) 2.1909 4m 16s (60000 60%) 2.1004 4m 38s (65000 65%) 2.3524 4m 59s (70000 70%) 2.3339 5m 21s (75000 75%) 2.3936 5m 42s (80000 80%) 2.1886 6m 3s (85000 85%) 2.0739 6m 25s (90000 90%) 2.5451 6m 46s (95000 95%) 1.5104 7m 7s (100000 100%) 2.4600 损失数据作图 从all_losses得到历史损失记录，反映了神经网络的学习情况： import matplotlib.pyplot as plt import matplotlib.ticker as ticker plt.figure() plt.plot(all_losses) 网络采样 我们每次给网络提供一个字母并预测下一个字母是什么，将预测到的字母继续输入，直到得到EOS字符结束循环。 用输入类别、起始字母和空隐藏状态创建输入张量。 用起始字母构建一个字符串变量 output_name 得到最大输出长度， 将当前字母传入神经网络 从前一层得到下一个字母和下一个隐藏状态 如果字母是EOS，在这里停止 如果是一个普通的字母，添加到output_name变量并继续循环 返回最终得到的名字单词 另一种策略是，不必给网络一个起始字母，而是在训练中提供一个“字符串开始”的标记，并让网络自己选择起始的字母。 max_length = 20 # Sample from a category and starting letter def sample(category, start_letter='A'): with torch.no_grad(): # no need to track history in sampling category_tensor = categoryTensor(category) input = inputTensor(start_letter) hidden = rnn.initHidden() output_name = start_letter for i in range(max_length): output, hidden = rnn(category_tensor, input[0], hidden) topv, topi = output.topk(1) topi = topi[0][0] if topi == n_letters - 1: break else: letter = all_letters[topi] output_name += letter input = inputTensor(letter) return output_name # Get multiple samples from one category and multiple starting letters def samples(category, start_letters='ABC'): for start_letter in start_letters: print(sample(category, start_letter)) samples('Russian', 'RUS') samples('German', 'GER') samples('Spanish', 'SPA') samples('Chinese', 'CHI') Out: Rovanik Uakilovev Shaveri Garter Eren Romer Santa Parera Artera Chan Ha Iua 练习 尝试其它 （类别->行） 格式的数据集，比如: 系列小说 -> 角色名称 词性 -> 单词 国家 -> 城市 尝试“start of sentence” 标记，使采样的开始过程不需要指定起始字母 通过更大和更复杂的网络获得更好的结果 尝试 nn.LSTM 和 nn.GRU 层 组合这些 RNN构造更复杂的神经网络 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"char_rnn_classification_tutorial.html":{"url":"char_rnn_classification_tutorial.html","title":"使用字符级别特征的 RNN 网络进行姓氏分类","keywords":"","body":"使用字符级别特征的RNN网络进行名字分类 译者：hhxx2015 校对者：hijkzzz 作者: Sean Robertson 我们将构建和训练字符级RNN来对单词进行分类。 字符级RNN将单词作为一系列字符读取，在每一步输出预测和“隐藏状态”，将其先前的隐藏状态输入至下一时刻。 我们将最终时刻输出作为预测结果，即表示该词属于哪个类。 具体来说，我们将在18种语言构成的几千个名字的数据集上训练模型，根据一个名字的拼写预测它是哪种语言的名字： $ python predict.py Hinton (-0.47) Scottish (-1.52) English (-3.57) Irish $ python predict.py Schmidhuber (-0.19) German (-2.48) Czech (-2.68) Dutch 推荐阅读: 我默认你已经安装好了PyTorch，熟悉Python语言，理解“张量”的概念： https://pytorch.org/ PyTorch安装指南 Deep Learning with PyTorch: A 60 Minute Blitz PyTorch入门 Learning PyTorch with Examples 一些PyTorch的例子 PyTorch for Former Torch Users Lua Torch 用户参考 事先学习并了解RNN的工作原理对理解这个例子十分有帮助: The Unreasonable Effectiveness of Recurrent Neural Networks 展示了一些现实生活中的例子 Understanding LSTM Networks 是关于LSTM的，但也提供有关RNN的一般信息 准备数据 点击这里下载数据 并将其解压到当前文件夹。 在\"data/names\"文件夹下是名称为\"[language].txt\"的18个文本文件。每个文件的每一行都有一个名字，它们几乎都是罗马化的文本（但是我们仍需要将其从Unicode转换为ASCII编码） 我们最终会得到一个语言对应名字列表的字典，{language: [names ...]} 通用变量“category”和“line”（例子中的语言和名字单词）用于以后的可扩展性。 from __future__ import unicode_literals, print_function, division from io import open import glob import os def findFiles(path): return glob.glob(path) print(findFiles('data/names/*.txt')) import unicodedata import string all_letters = string.ascii_letters + \" .,;'\" n_letters = len(all_letters) # Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427 def unicodeToAscii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in all_letters ) print(unicodeToAscii('Ślusàrski')) # Build the category_lines dictionary, a list of names per language category_lines = {} all_categories = [] # Read a file and split into lines def readLines(filename): lines = open(filename, encoding='utf-8').read().strip().split('\\n') return [unicodeToAscii(line) for line in lines] for filename in findFiles('data/names/*.txt'): category = os.path.splitext(os.path.basename(filename))[0] all_categories.append(category) lines = readLines(filename) category_lines[category] = lines n_categories = len(all_categories) 输出: ['data/names/Italian.txt', 'data/names/German.txt', 'data/names/Portuguese.txt', 'data/names/Chinese.txt', 'data/names/Greek.txt', 'data/names/Polish.txt', 'data/names/French.txt', 'data/names/English.txt', 'data/names/Spanish.txt', 'data/names/Arabic.txt', 'data/names/Czech.txt', 'data/names/Russian.txt', 'data/names/Irish.txt', 'data/names/Dutch.txt', 'data/names/Scottish.txt', 'data/names/Vietnamese.txt', 'data/names/Korean.txt', 'data/names/Japanese.txt'] Slusarski 现在我们有了category_lines，一个字典变量存储每一种语言及其对应的每一行文本(名字)列表的映射关系。 变量all_categories是全部语言种类的列表， 变量n_categories 是语言种类的数量，后续会使用 print(category_lines['Italian'][:5]) 输出: ['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni'] 单词转化为张量 现在我们已经加载了所有的名字，我们需要将它们转换为张量来使用它们。 我们使用大小为的“one-hot 向量”表示一个字母。 一个one-hot向量所有位置都填充为0，并在其表示的字母的位置表示为1，例如\"b\" = .（字母b的编号是2，第二个位置是1，其他位置是0） 我们使用一个的2D矩阵表示一个单词 额外的1维是batch的维度，PyTorch默认所有的数据都是成batch处理的。我们这里只设置了batch的大小为1。 import torch # 从所有的字母中得到某个letter的索引编号, 例如 \"a\" = 0 def letterToIndex(letter): return all_letters.find(letter) # Just for demonstration, turn a letter into a Tensor def letterToTensor(letter): tensor = torch.zeros(1, n_letters) tensor[0][letterToIndex(letter)] = 1 return tensor # Turn a line into a , # or an array of one-hot letter vectors def lineToTensor(line): tensor = torch.zeros(len(line), 1, n_letters) for li, letter in enumerate(line): tensor[li][0][letterToIndex(letter)] = 1 return tensor print(letterToTensor('J')) print(lineToTensor('Jones').size()) 输出: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) torch.Size([5, 1, 57]) 构造神经网络 在autograd之前，要在Torch中构建一个可以复制之前时刻层参数的循环神经网络。 layer的隐藏状态和梯度将交给计算图自己处理。 这意味着你可以像实现的常规的 feed-forward 层一样，以很纯粹的方式实现RNN。 这个RNN组件 (几乎是从这里复制的 the PyTorch for Torch users tutorial) 仅使用两层 linear 层对输入和隐藏层做处理, 在最后添加一层 LogSoftmax 层预测最终输出。 import torch.nn as nn class RNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(RNN, self).__init__() self.hidden_size = hidden_size self.i2h = nn.Linear(input_size + hidden_size, hidden_size) self.i2o = nn.Linear(input_size + hidden_size, output_size) self.softmax = nn.LogSoftmax(dim=1) def forward(self, input, hidden): combined = torch.cat((input, hidden), 1) hidden = self.i2h(combined) output = self.i2o(combined) output = self.softmax(output) return output, hidden def initHidden(self): return torch.zeros(1, self.hidden_size) n_hidden = 128 rnn = RNN(n_letters, n_hidden, n_categories) 要运行此网络的一个步骤，我们需要传递一个输入（在我们的例子中，是当前字母的Tensor）和一个先前隐藏的状态（我们首先将其初始化为零）。 我们将返回输出（每种语言的概率）和下一个隐藏状态（为我们下一步保留使用）。 input = letterToTensor('A') hidden =torch.zeros(1, n_hidden) output, next_hidden = rnn(input, hidden) 为了提高效率，我们不希望为每一步都创建一个新的Tensor，因此我们将使用lineToTensor函数而不是letterToTensor函数，并使用切片方法。 这一步可以通过预先计算批量的张量进一步优化。 input = lineToTensor('Albert') hidden = torch.zeros(1, n_hidden) output, next_hidden = rnn(input[0], hidden) print(output) 输出: tensor([[-2.8857, -2.9005, -2.8386, -2.9397, -2.8594, -2.8785, -2.9361, -2.8270, -2.9602, -2.8583, -2.9244, -2.9112, -2.8545, -2.8715, -2.8328, -2.8233, -2.9685, -2.9780]], grad_fn=) 可以看到输出是一个的张量，其中每一条代表这个单词属于某一类的可能性（越高可能性越大） 训练 训练前的准备 进行训练步骤之前我们需要构建一些辅助函数。 第一个是当我们知道输出结果对应每种类别的可能性时，解析神经网络的输出。 我们可以使用 Tensor.topk函数得到最大值在结果中的位置索引 def categoryFromOutput(output): top_n, top_i = output.topk(1) category_i = top_i[0].item() return all_categories[category_i], category_i print(categoryFromOutput(output)) 输出: ('Vietnamese', 15) 我们还需要一种快速获取训练示例（得到一个名字及其所属的语言类别）的方法： import random def randomChoice(l): return l[random.randint(0, len(l) - 1)] def randomTrainingExample(): category = randomChoice(all_categories) line = randomChoice(category_lines[category]) category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long) line_tensor = lineToTensor(line) return category, line, category_tensor, line_tensor for i in range(10): category, line, category_tensor, line_tensor = randomTrainingExample() print('category =', category, '/ line =', line) 输出: category = Russian / line = Minkin category = French / line = Masson category = German / line = Hasek category = Dutch / line = Kloeten category = Scottish / line = Allan category = Italian / line = Agostini category = Japanese / line = Fumihiko category = Polish / line = Gajos category = Scottish / line = Duncan category = Arabic / line = Gerges 训练神经网络 现在，训练过程只需要向神经网络输入大量的数据，让它做出预测，并将对错反馈给它。 nn.LogSoftmax作为最后一层layer时，nn.NLLLoss作为损失函数是合适的。 criterion = nn.NLLLoss() 训练过程的每次循环将会发生： 构建输入和目标张量 构建0初始化的隐藏状态 读入每一个字母 将当前隐藏状态传递给下一字母 比较最终结果和目标 反向传播 返回结果和损失 learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn def train(category_tensor, line_tensor): hidden = rnn.initHidden() rnn.zero_grad() for i in range(line_tensor.size()[0]): output, hidden = rnn(line_tensor[i], hidden) loss = criterion(output, category_tensor) loss.backward() # Add parameters' gradients to their values, multiplied by learning rate for p in rnn.parameters(): p.data.add_(-learning_rate, p.grad.data) return output, loss.item() 现在我们只需要准备一些例子来运行程序。 由于train函数同时返回输出和损失，我们可以打印其输出结果并跟踪其损失画图。 由于有1000个示例，我们每print_every次打印样例，并求平均损失。 import time import math n_iters = 100000 print_every = 5000 plot_every = 1000 # Keep track of losses for plotting current_loss = 0 all_losses = [] def timeSince(since): now = time.time() s = now - since m = math.floor(s / 60) s -= m * 60 return '%dm %ds' % (m, s) start = time.time() for iter in range(1, n_iters + 1): category, line, category_tensor, line_tensor = randomTrainingExample() output, loss = train(category_tensor, line_tensor) current_loss += loss # Print iter number, loss, name and guess if iter % print_every == 0: guess, guess_i = categoryFromOutput(output) correct = '✓' if guess == category else '✗ (%s)' % category print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct)) # Add current loss avg to list of losses if iter % plot_every == 0: all_losses.append(current_loss / plot_every) current_loss = 0 输出: 5000 5% (0m 11s) 2.0318 Jaeger / German ✓ 10000 10% (0m 18s) 2.1296 Sokolofsky / Russian ✗ (Polish) 15000 15% (0m 26s) 1.2620 Jo / Korean ✓ 20000 20% (0m 34s) 1.9295 Livson / Scottish ✗ (Russian) 25000 25% (0m 41s) 1.2325 Fortier / French ✓ 30000 30% (0m 49s) 2.5714 Purdes / Dutch ✗ (Czech) 35000 35% (0m 56s) 2.3312 Bayer / Arabic ✗ (German) 40000 40% (1m 4s) 2.3792 Mitchell / Dutch ✗ (Scottish) 45000 45% (1m 12s) 1.3536 Maes / Dutch ✓ 50000 50% (1m 20s) 2.6095 Sai / Chinese ✗ (Vietnamese) 55000 55% (1m 28s) 0.5883 Cheung / Chinese ✓ 60000 60% (1m 35s) 1.5788 William / Irish ✓ 65000 65% (1m 43s) 2.5809 Mulder / Scottish ✗ (Dutch) 70000 70% (1m 51s) 1.3440 Bruce / German ✗ (Scottish) 75000 75% (1m 58s) 1.1839 Romero / Italian ✗ (Spanish) 80000 80% (2m 6s) 2.6453 Reyes / Portuguese ✗ (Spanish) 85000 85% (2m 14s) 0.0290 Mcmillan / Scottish ✓ 90000 90% (2m 22s) 0.7337 Riagan / Irish ✓ 95000 95% (2m 30s) 2.6208 Maneates / Dutch ✗ (Greek) 100000 100% (2m 37s) 0.5170 Szwarc / Polish ✓ 画出结果 从all_losses得到历史损失记录，反映了神经网络的学习情况： import matplotlib.pyplot as plt import matplotlib.ticker as ticker plt.figure() plt.plot(all_losses) 评价结果 为了了解网络在不同类别上的表现，我们将创建一个混淆矩阵，显示每种语言（行）和神经网络将其预测为哪种语言（列）。 为了计算混淆矩阵，使用evaluate()函数处理了一批数据，evaluate()函数与去掉反向传播的train()函数大体相同。 # Keep track of correct guesses in a confusion matrix confusion = torch.zeros(n_categories, n_categories) n_confusion = 10000 # Just return an output given a line def evaluate(line_tensor): hidden = rnn.initHidden() for i in range(line_tensor.size()[0]): output, hidden = rnn(line_tensor[i], hidden) return output # Go through a bunch of examples and record which are correctly guessed for i in range(n_confusion): category, line, category_tensor, line_tensor = randomTrainingExample() output = evaluate(line_tensor) guess, guess_i = categoryFromOutput(output) category_i = all_categories.index(category) confusion[category_i][guess_i] += 1 # Normalize by dividing every row by its sum for i in range(n_categories): confusion[i] = confusion[i] / confusion[i].sum() # Set up plot fig = plt.figure() ax = fig.add_subplot(111) cax = ax.matshow(confusion.numpy()) fig.colorbar(cax) # Set up axes ax.set_xticklabels([''] + all_categories, rotation=90) ax.set_yticklabels([''] + all_categories) # Force label at every tick ax.xaxis.set_major_locator(ticker.MultipleLocator(1)) ax.yaxis.set_major_locator(ticker.MultipleLocator(1)) # sphinx_gallery_thumbnail_number = 2 plt.show() 你可以从主轴线以外挑出亮的点，显示模型预测错了哪些语言，例如汉语预测为了韩语，西班牙预测为了意大利。 看上去在希腊语上效果很好，在英语上表现欠佳。（可能是因为英语与其他语言的重叠较多）。 处理用户输入 def predict(input_line, n_predictions=3): print('\\n> %s' % input_line) with torch.no_grad(): output = evaluate(lineToTensor(input_line)) # Get top N categories topv, topi = output.topk(n_predictions, 1, True) predictions = [] for i in range(n_predictions): value = topv[0][i].item() category_index = topi[0][i].item() print('(%.2f) %s' % (value, all_categories[category_index])) predictions.append([value, all_categories[category_index]]) predict('Dovesky') predict('Jackson') predict('Satoshi') 输出: > Dovesky (-0.74) Russian (-0.77) Czech (-3.31) English > Jackson (-0.80) Scottish (-1.69) English (-1.84) Russian > Satoshi (-1.16) Japanese (-1.89) Arabic (-1.90) Polish 最终版的脚本 in the Practical PyTorch repo 将上述代码拆分为几个文件： data.py (读取文件) model.py (构造RNN网络) train.py (运行训练过程) predict.py (在命令行中和参数一起运行predict()函数) server.py (使用bottle.py构建JSON API的预测服务) 运行 train.py 来训练和保存网络 将predict.py和一个名字的单词一起运行查看预测结果 : $ python predict.py Hazaki (-0.42) Japanese (-1.39) Polish (-3.51) Czech 运行 server.py 并访问http://localhost:5533/Yourname 得到JSON格式的预测输出 练习 尝试其它 （类别->行） 格式的数据集，比如: 任何单词 -> 语言 姓名 -> 性别 角色姓名 -> 作者 页面标题 -> blog 或 subreddit 通过更大和更复杂的网络获得更好的结果 增加更多linear层 尝试 nn.LSTM 和 nn.GRU 层 组合这些 RNN构造更复杂的神经网络 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"deep_learning_nlp_tutorial.html":{"url":"deep_learning_nlp_tutorial.html","title":"Deep Learning for NLP with Pytorch","keywords":"","body":"在深度学习和 NLP 中使用 Pytorch 译者 bruce1408 校对者：FontTian 作者: Robert Guthrie 本文带您进入pytorch框架进行深度学习编程的核心思想。Pytorch的很多概念(比如计算图抽象和自动求导)并非它所独有的,和其他深度学习框架相关。 我写这篇教程是专门针对那些从未用任何深度学习框架(例如：Tensorflow, Theano, Keras, Dynet)编写代码而从事NLP领域的人。我假设你已经知道NLP领域要解决的核心问题：词性标注、语言模型等等。我也认为你通过AI这本书中所讲的知识熟悉了神经网络达到了入门的级别。通常这些课程都会介绍反向传播算法和前馈神经网络，并指出它们是线性组合和非线性组合构成的链。本文在假设你已经有了这些知识的情况下，教你如何开始写深度学习代码。 注意这篇文章主要关于models，而不是数据。对于所有的模型，我只创建一些数据维度较小的测试示例以便你可以看到权重在训练过程中如何变化。如果你想要尝试一些真实数据，您有能力删除本示例中的模型并重新训练他们。 Introduction to PyTorch Deep Learning with PyTorch Word Embeddings: Encoding Lexical Semantics Sequence Models and Long-Short Term Memory Networks Advanced: Making Dynamic Decisions and the Bi-LSTM CRF 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"nlp_pytorch_tutorial.html":{"url":"nlp_pytorch_tutorial.html","title":"PyTorch 介绍","keywords":"","body":"PyTorch 介绍 译者：YAOKE7 校对者：FontTian Torch张量库介绍 深度学习的所有计算都是在张量上进行的,其中张量是一个可以被超过二维索引的矩阵的一般表示形式。稍后我们将详细讨论这意味着什么。首先，我们先来看一下我们可以用张量来干什么。 # 作者: Robert Guthrie import torch import torch.autograd as autograd import torch.nn as nn import torch.nn.functional as F import torch.optim as optim torch.manual_seed(1) 创建张量 张量可以在Python list形式下通过torch.Tensor()函数创建。 # 利用给定数据创建一个torch.Tensor对象.这是一个一维向量 V_data = [1., 2., 3.] V = torch.Tensor(V_data) print(V) # 创建一个矩阵 M_data = [[1., 2., 3.], [4., 5., 6]] M = torch.Tensor(M_data) print(M) # 创建2x2x2形式的三维张量. T_data = [[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]]] T = torch.Tensor(T_data) print(T) 输出 tensor([1., 2., 3.]) tensor([[1., 2., 3.], [4., 5., 6.]]) tensor([[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]]]) 什么是三维张量？让我们这样想象。如果你有一个向量,那么对这个向量索引就会得到一个标量。如果你有一个矩阵，对这个矩阵索引那么就会得到一个向量。如果你有一个三维张量，那么对其索引就会得到一个矩阵! 针对术语的说明：当我在本教程内使用“tensor”，它针对的是所有torch.Tensor对象。矩阵和向量是特殊的torch.Tensors，他们的维度分别是1和2。当我说到三维张量，我会简洁的使用“3D tensor”。 # 索引V得到一个标量（0维张量） print(V[0]) # 从向量V中获取一个数字 print(V[0].item()) # 索引M得到一个向量 print(M[0]) # 索引T得到一个矩阵 print(T[0]) 输出： tensor(1.) 1.0 tensor([1., 2., 3.]) tensor([[1., 2.], [3., 4.]]) 你也可以创建其他数据类型的tensors。默认的数据类型为浮点型。可以使用torch.LongTensor()来创建一个整数类型的张量。你可以在文件中寻找更多的数据类型，但是浮点型和长整形是最常用的。 你可以使用torch.randn()创建一个张量。这个张量拥有随机数据和需要指定的维度。 x = torch.randn((3, 4, 5)) print(x) 输出： tensor([[[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002], [-0.6092, -0.9798, -1.6091, -0.7121, 0.3037], [-0.7773, -0.2515, -0.2223, 1.6871, 0.2284], [ 0.4676, -0.6970, -1.1608, 0.6995, 0.1991]], [[ 0.8657, 0.2444, -0.6629, 0.8073, 1.1017], [-0.1759, -2.2456, -1.4465, 0.0612, -0.6177], [-0.7981, -0.1316, 1.8793, -0.0721, 0.1578], [-0.7735, 0.1991, 0.0457, 0.1530, -0.4757]], [[-0.1110, 0.2927, -0.1578, -0.0288, 0.4533], [ 1.1422, 0.2486, -1.7754, -0.0255, -1.0233], [-0.5962, -1.0055, 0.4285, 1.4761, -1.7869], [ 1.6103, -0.7040, -0.1853, -0.9962, -0.8313]]]) 张量操作 你可以以你想要的方式操作张量。 x = torch.Tensor([1., 2., 3.]) y = torch.Tensor([4., 5., 6.]) z = x + y print(z) 输出： tensor([5., 7., 9.]) 可以查阅 文档 获取大量可用操作的完整列表,这些操作不仅局限于数学操作范围。 接下来一个很有帮助的操作就是连接。 # 默认情况下, 它沿着第一个行进行连接 (连接行) x_1 = torch.randn(2, 5) y_1 = torch.randn(3, 5) z_1 = torch.cat([x_1, y_1]) print(z_1) # 连接列： x_2 = torch.randn(2, 3) y_2 = torch.randn(2, 5) # 第二个参数指定了沿着哪条轴连接 z_2 = torch.cat([x_2, y_2], 1) print(z_2) # 如果你的tensors是不兼容的，torch会报错。取消注释来查看错误。 # torch.cat([x_1, x_2]) 输出： tensor([[-0.8029, 0.2366, 0.2857, 0.6898, -0.6331], [ 0.8795, -0.6842, 0.4533, 0.2912, -0.8317], [-0.5525, 0.6355, -0.3968, -0.6571, -1.6428], [ 0.9803, -0.0421, -0.8206, 0.3133, -1.1352], [ 0.3773, -0.2824, -2.5667, -1.4303, 0.5009]]) tensor([[ 0.5438, -0.4057, 1.1341, -0.1473, 0.6272, 1.0935, 0.0939, 1.2381], [-1.1115, 0.3501, -0.7703, -1.3459, 0.5119, -0.6933, -0.1668, -0.9999]]) 重构张量 使用.view()去重构张量。这是一个高频方法，因为许多神经网络的神经元对输入格式有明确的要求。你通常需要先将数据重构再输入到神经元中。 x = torch.randn(2, 3, 4) print(x) print(x.view(2, 12)) # 重构为2行12列 # 同上。如果维度为-1,那么它的大小可以根据数据推断出来 print(x.view(2, -1)) 输出： tensor([[[ 0.4175, -0.2127, -0.8400, -0.4200], [-0.6240, -0.9773, 0.8748, 0.9873], [-0.0594, -2.4919, 0.2423, 0.2883]], [[-0.1095, 0.3126, 1.5038, 0.5038], [ 0.6223, -0.4481, -0.2856, 0.3880], [-1.1435, -0.6512, -0.1032, 0.6937]]]) tensor([[ 0.4175, -0.2127, -0.8400, -0.4200, -0.6240, -0.9773, 0.8748, 0.9873, -0.0594, -2.4919, 0.2423, 0.2883], [-0.1095, 0.3126, 1.5038, 0.5038, 0.6223, -0.4481, -0.2856, 0.3880, -1.1435, -0.6512, -0.1032, 0.6937]]) tensor([[ 0.4175, -0.2127, -0.8400, -0.4200, -0.6240, -0.9773, 0.8748, 0.9873, -0.0594, -2.4919, 0.2423, 0.2883], [-0.1095, 0.3126, 1.5038, 0.5038, 0.6223, -0.4481, -0.2856, 0.3880, -1.1435, -0.6512, -0.1032, 0.6937]]) 计算图和自动求导 计算图的思想对于有效率的深度学习编程是很重要的，因为它可以使你不必去自己写反向梯度传播。计算图只是简单地说明了如何将数据组合在一起以输出结果。因为图完全指定了操作所包含的参数，因此它包含了足够的信息去求导。这可能听起来很模糊，所以让我们看看使用Pytorch的基本类：requires_grad。 首先，从程序员的角度来思考。我们在上面刚刚创建的torch.Tensor对象中存储了什么？显然，是数据和结构，也很可能是其他的东西。但是当我们将两个张量相加，我们得到了一个输出张量。这个输出所能体现出的只有数据和结构，并不能体现出是由两个张量加之和得到的（因为它可能是从一个文件中读取的, 也可能是其他操作的结果等）。 如果requires_grad=True，张量对象可以一直跟踪它是如何创建的。让我们在实际中来看。 # 张量对象带有“requires_grad”标记 x =torch.Tensor([1., 2., 3], requires_grad=True) # 通过requires_grad=True，您也可以做之前所有的操作。 y = torch.Tensor([4., 5., 6], requires_grad=True) z = x + y print(z) # 但是z还有一些额外的东西. print(z.grad_fn) 输出： tensor([5., 7., 9.], grad_fn=) 既然变量知道怎么创建的它们。z知道它并非是从文件读取的，也不是乘法或指数或其他运算的结果。如果你继续跟踪 z.grad_fn，你会从中找到x和y的痕迹。 但是它如何帮助我们计算梯度? # 我们来将z中所有项作和运算 s = z.sum() print(s) print(s.grad_fn) 输出： tensor(21., grad_fn=) 那么这个计算和对x的第一个分量的导数等于多少? 在数学上,我们求 \\frac{\\partial s}{\\partial x_0} s是被作为张量z的和创建的。张量z是x+y的和 s = \\overbrace{x_0 + y_0}^\\text{$z_0$} + \\overbrace{x_1 + y_1}^\\text{$z_1$} + \\overbrace{x_2 + y_2}^\\text{$z_2$} 并且s包含了足够的信息去决定我们需要的导数为1! 当然它掩盖了如何计算导数的挑战。这是因为s携带了足够多的信息所以导数可以被计算。现实中，Pytorch 程序的开发人员用程序指令sum()和 + 操作以知道如何计算它们的梯度并且运行反向传播算法。深入讨论此算法超出了本教程的范围. 让我们用Pytorch计算梯度，发现我们是对的:(注意如果你运行这个模块很多次，它的梯度会上升，这是因为Pytorch累积梯度渐变为.grad属性，而且对于很多模型它是很方便的.) # 在任意变量上使用 .backward()将会运行反向,从它开始. s.backward() print(x.grad) 输出： tensor([1., 1., 1.]) 作为一个成功的深度学习程序员了解下面的模块如何运行是至关重要的。 x = torch.randn((2, 2)) y = torch.randn((2, 2)) #用户创建的张量在默认情况下“requires_grad=False” print(x.requires_grad, y.requires_grad) z = x + y # 你不能通过z反向传播。 print(z.grad_fn) # “.requires_grad_( ... )”改变了“requires_grad”属性 # 如果没有指定，标记默认为True x = x.requires_grad_() y = y.requires_grad_() #正如我们在上面看到的一样，z包含足够的信息计算梯度。 z = x + y print(z.grad_fn) # 如果任何操作的输入部分带有“requires_grad=True”那么输出就会变为： print(z.requires_grad) # 现在z有关于x,y的历史信息 # 我们可以获取它的值，将其从历史中分离出来吗？ new_z = z.detach() # new_z有足够的信息反向传播至x和y吗？ # 答案是没有 print(new_z.grad_fn) # 怎么会这样？ “z.detach()”函数返回了一个与“z”相同存储的张量 #但是没有携带历史的计算信息。 # 它对于自己是如何计算得来的不知道任何事情。 # 从本质上讲，我们已经把这个变量从过去的历史中分离出来了。 输出： False False None True None 您也可以通过.requires_grad``=True by wrapping the code block in ``with torch.no_grad():停止跟踪张量的历史记录中的自动求导。 print(x.requires_grad) print((x ** 2).requires_grad) with torch.no_grad(): print((x ** 2).requires_grad) 输出： True True False 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"nlp_deep_learning_tutorial.html":{"url":"nlp_deep_learning_tutorial.html","title":"使用 PyTorch 进行深度学习","keywords":"","body":"使用PyTorch进行深度学习 译者：bdqfork 校对者：FontTian 作者: Robert Guthrie 深度学习构建模块：仿射变换, 非线性函数以及目标函数 深度学习表现为使用更巧妙的方法将线性函数和非线性函数进行组合。非线性函数的引入使得训练出来的模型更加强大。在本节中，我们将学习这些核心组件，建立目标函数，并理解模型是如何构建的。 仿射变换 深度学习的核心组件之一是仿射变换，仿射变换是一个关于矩阵A和向量x，b的f(x)函数，如下所示： 需要训练的参数就是该公式中的A和b。 PyTorch以及大多数的深度学习框架所做的事情都与传统的线性代数有些不同。它的映射输入是行而不是列。也就是说，下面代码输出的第i行是输入的第i行进行A变换，并加上偏移项的结果。看下面的例子： # Author: Robert Guthrie import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim torch.manual_seed(1) lin = nn.Linear(5, 3) # maps from R^5 to R^3, parameters A, b # data is 2x5. A maps from 5 to 3... can we map \"data\" under A? data = torch.randn(2, 5) print(lin(data)) # yes 输出: tensor([[ 0.1755, -0.3268, -0.5069], [-0.6602, 0.2260, 0.1089]], grad_fn=) 非线性函数 首先，注意以下这个例子，它将解释为什么我们需要非线性函数。假设我们有两个仿射变换 f(x) = Ax + b 和 g(x) = Cx + d 。那么 f(g(x)) 又是什么呢？ AC 是一个矩阵，Ad + b是一个向量，可以看出，两个仿射变换的组合还是一个仿射变换。 由此可以看出，使用以上方法将多个仿射变换组合成的长链式的神经网络，相对于单个仿射变换并没有性能上的提升。 但是如果我们在两个仿射变换之间引入非线性，那么结果就大不一样了，我们可以构建出一个高性能的模型。 最常用的核心的非线性函数有：tanh(x)，σ(x)，ReLU(x)。你可能会想：“为什么是这些函数？明明有其他更多的非线性函数。”这些函数常用的原因是它们拥有可以容易计算的梯度，而计算梯度是学习的本质。例如 注意：尽管你可能在AI课程的介绍中学习了一些神经网络，在这些神经网络中σ(x)是默认非线性的，但是通常在实际使用的过程中都会避开它们。这是因为当参数的绝对值增长时，梯度会很快消失。小梯度意味着很难学习。因此大部分人默认选择tanh或者ReLU。 # In pytorch, most non-linearities are in torch.functional (we have it imported as F) # Note that non-linearites typically don't have parameters like affine maps do. # That is, they don't have weights that are updated during training. data = torch.randn(2, 2) print(data) print(F.relu(data)) 输出: tensor([[-0.5404, -2.2102], [ 2.1130, -0.0040]]) tensor([[0.0000, 0.0000], [2.1130, 0.0000]]) Softmax和概率 Softmax(x)也是一个非线性函数，但它的特殊之处在于，它通常是神经网络的最后一个操作。这是因为它接受实数向量，并且返回一个概率分布。它的定义如下。设x为实数向量（正、负，无论什么，没有约束）。然后Softmax(x)的第i个分量是： 很明显，输出的是一个概率分布：每一个元素都非负且和为1。 你也可以认为这只是一个对输入的元素进行的求幂运算符，使所有的内容都非负，然后除以规范化常量。 # Softmax is also in torch.nn.functional data = torch.randn(5) print(data) print(F.softmax(data, dim=0)) print(F.softmax(data, dim=0).sum()) # Sums to 1 because it is a distribution! print(F.log_softmax(data, dim=0)) # theres also log_softmax 输出: tensor([ 1.3800, -1.3505, 0.3455, 0.5046, 1.8213]) tensor([0.2948, 0.0192, 0.1048, 0.1228, 0.4584]) tensor(1.) tensor([-1.2214, -3.9519, -2.2560, -2.0969, -0.7801]) 目标函数 目标函数正是神经网络通过训练来最小化的函数（因此，它常常被称作损失函数或者成本函数）。这需要首先选择一个训练数据实例，通过神经网络运行它并计算输出的损失。然后通过损失函数的导数来更新模型的参数。因此直观来讲，如果它的结果是错误的，而模型完全信任他，那么损失将会很高。反之，当模型信任计算结果而结果正确时，损失会很低。 在你的训练实例中最小化损失函数的目的是使你的网络拥有很好的泛化能力，可以在开发数据集，测试数据集以及实际生产中拥有很小的损失。损失函数的一个例子是负对数似然损失函数，这个函数经常在多级分类中出现。在监督多级分类中，这意味着训练网络最小化正确输出的负对数概率（等效的于最大化正确输出的对数概率）。 优化和训练 那么，我们该怎么计算函数实例的损失函数呢？我们应该做什么呢？我们在之前了解到TensorFlow中的Tensor知道如何计算梯度以及计算梯度相关的东西。由于我们的损失正是一个Tensor，因此我们可以使用所有与梯度有关的参数来计算梯度。然后我们可以进行标准梯度更新。设 θ为我们的参数，L(θ)为损失函数，η一个正的学习率。然后： 目前，有大量的算法和积极的研究试图做一些除了这种普通的梯度更新以外的事情。许多人尝试去基于训练时发生的事情来改变学习率。但是，你不需要担心这些特殊的算法到底在干什么，除非你真的很感兴趣。Torch提供了大量的算法在torch.optim包中，且全部都是透明的。在语法上使用复杂的算法和使用最简单的梯度更新一样简单。但是尝试不同的更新算法和在更新算法中使用不同的参数（例如不同的初始学习率）对于优化你的网络的性能很重要。通常，仅仅将普通的SGD替换成一个例如Adam或者RMSProp优化器都可以显著的提升性能。 使用PyTorch创建网络组件 在我们继续关注NLP之前，让我们先使用PyTorch构建一个只用仿射变换和非线性函数组成的网络示例。我们也将了解如何计算损失函数，使用PyTorch内置的负对数似然函数，以及通过反向传播更新参数。 所有的网络组件应该继承nn.Module并覆盖forward()方法。继承nn.Module提供给了一些方法给你的组件。例如，它可以跟踪可训练的参数，你可以通过.to(device)方法在CPU和GPU之间交换它们。.to(device)方法中的device可以是CPU设备torch.device(\"cpu\")或者CUDA设备torch.device(\"cuda:0\")。 让我们写一个神经网络的示例，它接受一些稀疏的BOW(词袋模式)表示，然后输出分布在两个标签上的概率：“English”和“Spanish”。这个模型只是一个逻辑回归。 示例: 基于逻辑回归与词袋模式的文本分类器 我们的模型将会把BOW表示映射成标签上的对数概率。我们为词汇中的每个词指定一个索引。例如，我们所有的词汇是两个单词“hello”和\"world\"，用0和1表示。句子“hello hello hello hello”的表示是 [4,0] 对于“hello world world hello”, 则表示成 [2,2] 通常表示成 [Count(hello),Count(world)] 用x来表示这个BOW向量。网络的输出是: 也就是说，我们数据传入一个仿射变换然后做对数归一化logsoftmax。 data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"), (\"Give it to me\".split(), \"ENGLISH\"), (\"No creo que sea una buena idea\".split(), \"SPANISH\"), (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")] test_data = [(\"Yo creo que si\".split(), \"SPANISH\"), (\"it is lost on me\".split(), \"ENGLISH\")] # word_to_ix maps each word in the vocab to a unique integer, which will be its # index into the Bag of words vector word_to_ix = {} for sent, _ in data + test_data: for word in sent: if word not in word_to_ix: word_to_ix[word] = len(word_to_ix) print(word_to_ix) VOCAB_SIZE = len(word_to_ix) NUM_LABELS = 2 class BoWClassifier(nn.Module): # inheriting from nn.Module! def __init__(self, num_labels, vocab_size): # calls the init function of nn.Module. Dont get confused by syntax, # just always do it in an nn.Module super(BoWClassifier, self).__init__() # Define the parameters that you will need. In this case, we need A and b, # the parameters of the affine mapping. # Torch defines nn.Linear(), which provides the affine map. # Make sure you understand why the input dimension is vocab_size # and the output is num_labels! self.linear = nn.Linear(vocab_size, num_labels) # NOTE! The non-linearity log softmax does not have parameters! So we don't need # to worry about that here def forward(self, bow_vec): # Pass the input through the linear layer, # then pass that through log_softmax. # Many non-linearities and other functions are in torch.nn.functional return F.log_softmax(self.linear(bow_vec), dim=1) def make_bow_vector(sentence, word_to_ix): vec = torch.zeros(len(word_to_ix)) for word in sentence: vec[word_to_ix[word]] += 1 return vec.view(1, -1) def make_target(label, label_to_ix): return torch.LongTensor([label_to_ix[label]]) model = BoWClassifier(NUM_LABELS, VOCAB_SIZE) # the model knows its parameters. The first output below is A, the second is b. # Whenever you assign a component to a class variable in the __init__ function # of a module, which was done with the line # self.linear = nn.Linear(...) # Then through some Python magic from the PyTorch devs, your module # (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters for param in model.parameters(): print(param) # To run the model, pass in a BoW vector # Here we don't need to train, so the code is wrapped in torch.no_grad() with torch.no_grad(): sample = data[0] bow_vector = make_bow_vector(sample[0], word_to_ix) log_probs = model(bow_vector) print(log_probs) 输出: {'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25} Parameter containing: tensor([[ 0.1194, 0.0609, -0.1268, 0.1274, 0.1191, 0.1739, -0.1099, -0.0323, -0.0038, 0.0286, -0.1488, -0.1392, 0.1067, -0.0460, 0.0958, 0.0112, 0.0644, 0.0431, 0.0713, 0.0972, -0.1816, 0.0987, -0.1379, -0.1480, 0.0119, -0.0334], [ 0.1152, -0.1136, -0.1743, 0.1427, -0.0291, 0.1103, 0.0630, -0.1471, 0.0394, 0.0471, -0.1313, -0.0931, 0.0669, 0.0351, -0.0834, -0.0594, 0.1796, -0.0363, 0.1106, 0.0849, -0.1268, -0.1668, 0.1882, 0.0102, 0.1344, 0.0406]], requires_grad=True) Parameter containing: tensor([0.0631, 0.1465], requires_grad=True) tensor([[-0.5378, -0.8771]]) 上面的哪一个值对应的是ENGLISH的对数概率，哪一个是SPANISH的对数概率？我们还没有定义，但是如果我必须要定义我们想要训练的东西。 label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1} 让我们来训练吧! 我们将实例传入来获取对数概率，计算损失函数，计算损失函数的梯度，然后使用一个梯度步长来更新参数。在PyTorch的nn包里提供了损失函数。nn.NLLLoss()是我们想要的负对数似然损失函数。torch.optim中也定义了优化方法。这里，我们只使用SGD。 注意，因为NLLLoss的输入是一个对数概率的向量以及目标标签。它不会为我们计算对数概率。这也是为什么我们最后一层网络是log_softmax的原因。损失函数nn.CrossEntropyLoss()除了对结果额外计算了logsoftmax之外和NLLLoss()没什么区别。 # Run on test data before we train, just to see a before-and-after with torch.no_grad(): for instance, label in test_data: bow_vec = make_bow_vector(instance, word_to_ix) log_probs = model(bow_vec) print(log_probs) # Print the matrix column corresponding to \"creo\" print(next(model.parameters())[:, word_to_ix[\"creo\"]]) loss_function = nn.NLLLoss() optimizer = optim.SGD(model.parameters(), lr=0.1) # Usually you want to pass over the training data several times. # 100 is much bigger than on a real data set, but real datasets have more than # two instances. Usually, somewhere between 5 and 30 epochs is reasonable. for epoch in range(100): for instance, label in data: # Step 1\\. Remember that PyTorch accumulates gradients. # We need to clear them out before each instance model.zero_grad() # Step 2\\. Make our BOW vector and also we must wrap the target in a # Tensor as an integer. For example, if the target is SPANISH, then # we wrap the integer 0\\. The loss function then knows that the 0th # element of the log probabilities is the log probability # corresponding to SPANISH bow_vec = make_bow_vector(instance, word_to_ix) target = make_target(label, label_to_ix) # Step 3\\. Run our forward pass. log_probs = model(bow_vec) # Step 4\\. Compute the loss, gradients, and update the parameters by # calling optimizer.step() loss = loss_function(log_probs, target) loss.backward() optimizer.step() with torch.no_grad(): for instance, label in test_data: bow_vec = make_bow_vector(instance, word_to_ix) log_probs = model(bow_vec) print(log_probs) # Index corresponding to Spanish goes up, English goes down! print(next(model.parameters())[:, word_to_ix[\"creo\"]]) 输出: tensor([[-0.9297, -0.5020]]) tensor([[-0.6388, -0.7506]]) tensor([-0.1488, -0.1313], grad_fn=) tensor([[-0.2093, -1.6669]]) tensor([[-2.5330, -0.0828]]) tensor([ 0.2803, -0.5605], grad_fn=) 我们得到了正确的结果！你可以看到Spanish的对数概率比第一个例子中的高的多，English的对数概率在第二个测试数据中更高，结果也应该是这样。 现在你了解了如何创建一个PyTorch组件，将数据传入并进行梯度更新。现在我们已经可以开始进行深度学习上的自然语言处理了。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"nlp_word_embeddings_tutorial.html":{"url":"nlp_word_embeddings_tutorial.html","title":"Word Embeddings: Encoding Lexical Semantics","keywords":"","body":"词嵌入：编码形式的词汇语义 译者：巩子惠 词嵌入是一种由真实数字组成的稠密向量，每个向量都代表了单词表里的一个单词。 在自然语言处理中，总会遇到这样的情况：特征全是单词！但是，如何在电脑上表述一个单词呢？你在电脑上存储的单词的ascii码，但是它仅仅代表单词怎么拼写，没有说明单词的内在含义(你也许能够从词缀中了解它的词性，或者从大小写中得到一些属性，但仅此而已)。 更重要的是，你能把这些ascii码字符组合成什么含义？当代表词汇表、输入数据是维的情况下，我们往往想从神经网络中得到数据密集的结果，但是结果只有很少的几个维度（例如，预测的数据只有几个标签时）。我们如何从大的数据维度空间中得到稍小一点的维度空间？ 放弃使用ascii码字符的形式表示单词，换用one-hot encoding会怎么样了？好吧，这个单词就能这样表示： 其中，1 表示的独有位置，其他位置全是0。其他的词都类似，在另外不一样的位置有一个1代表它，其他位置也都是0。 这种表达除了占用巨大的空间外，还有个很大的缺陷。 它只是简单的把词看做一个单独个体，认为它们之间毫无联系。 我们真正想要的是能够表达单词之间一些相似的含义。为什么要这样做呢？来看下面的例子： 假如我们正在搭建一个语言模型，训练数据有下面一些句子： The mathematician ran to the store. The physicist ran to the store. The mathematician solved the open problem. 现在又得到一个没见过的新句子: The physicist solved the open problem. 我们的模型可能在这个句子上表现的还不错，但是，如果利用了下面两个事实，模型会表现更佳： 我们发现数学家和物理学家在句子里有相同的作用，所以在某种程度上，他们有语义的联系。 当看见物理学家在新句子中的作用时，我们发现数学家也有起着相同的作用。 然后我们就推测，物理学家在上面的句子里也类似于数学家吗？ 这就是我们所指的相似性理念： 指的是语义相似，而不是简单的拼写相似。 这就是一种通过连接我们发现的和没发现的一些内容相似点、用于解决语言数据稀疏性的技术。 这个例子依赖于一个基本的语言假设： 那些在相似语句中出现的单词，在语义上也是相互关联的。 这就叫做 distributional hypothesis（分布式假设）。 Getting Dense Word Embeddings（密集词嵌入） 我们如何解决这个问题呢？也就是，怎么编码单词中的语义相似性？ 也许我们会想到一些语义属性。 举个例子，我们发现数学家和物理学家都能跑， 所以也许可以给含有“能跑”语义属性的单词打高分，考虑一下其他的属性，想象一下你可能会在这些属性上给普通的单词打什么分。 如果每个属性都表示一个维度，那我们也许可以用一个向量表示一个单词，就像这样： 那么，我们就这可以通过下面的方法得到这些单词之间的相似性： 尽管通常情况下需要进行长度归一化： 是两个向量的夹角。 这就意味着，完全相似的单词相似度为1。完全不相似的单词相似度为-1。 你可以把本章开头介绍的one-hot稀疏向量看做是我们新定义向量的一种特殊形式，那里的单词相似度为0， 现在我们给每个单词一些独特的语义属性。 这些向量数据密集，也就是说它们数字通常都非零。 但是新的这些向量存在一个严重的问题： 你可以想到数千种不同的语义属性，它们可能都与决定相似性有关，而且，到底如何设置不同属性的值呢？深度学习的中心思想是用神经网络来学习特征的表示，而不是程序员去设计它们。 所以为什么不把词嵌入只当做模型参数，而是通过训练来更新呢？ 这就才是我们要确切做的事。我们将用神经网络做一些潜在语义属性，但是原则上，学习才是关键。 注意，词嵌入可能无法解释。就是说，尽管使用我们上面手动制作的向量，能够发现数学家和物理学家都喜欢喝咖啡的相似性， 如果我们允许神经网络来学习词嵌入，那么就会发现数学家和物理学家在第二维度有个较大的值，它所代表的含义很不清晰。 它们在一些潜在语义上是相似的，但是对我们来说无法解释。 总结一下，词嵌入是单词语义的表示，有效地编码语义信息可能与手头的任务有关。你也可以嵌入其他的东西：语音标签，解析树，其他任何东西！特征嵌入是这个领域的核心思想。 Pytorch中的词嵌入 在我们举例或练习之前，这里有一份关于如何在Pytorch和常见的深度学习中使用词嵌入的简要介绍。 与制作one-hot向量时对每个单词定义一个特殊的索引类似，当我们使用词向量时也需要为每个单词定义一个索引。这些索引将是查询表的关键点。意思就是，词嵌入被被存储在一个的向量中，其中是词嵌入的维度。词被被分配的索引i，表示在向量的第i行存储它的嵌入。在所有的代码中，从单词到索引的映射是一个叫word_to_ix的字典。 能使用词嵌入的模块是torch.nn.Embedding，这里面有两个参数：词汇表的大小和词嵌入的维度。 索引这张表时，你必须使用torch.LongTensor（因为索引是整数，不是浮点数）。 # 作者: Robert Guthrie import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim torch.manual_seed(1) word_to_ix = {\"hello\": 0, \"world\": 1} embeds = nn.Embedding(2, 5) # 2 words in vocab, 5 dimensional embeddings lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long) hello_embed = embeds(lookup_tensor) print(hello_embed) 输出： tensor([[ 0.6614, 0.2669, 0.0617, 0.6213, -0.4519]], grad_fn=) 例子： N-Gram语言模型 回想一下，在n-gram语言模型中,给定一个单词序列向量，我们要计算的是 是单词序列的第i个单词。 在本例中，我们将在训练样例上计算损失函数，并且用反向传播算法更新参数。 CONTEXT_SIZE = 2 EMBEDDING_DIM = 10 # 我们用莎士比亚的十四行诗 Sonnet 2 test_sentence = \"\"\"When forty winters shall besiege thy brow, And dig deep trenches in thy beauty's field, Thy youth's proud livery so gazed on now, Will be a totter'd weed of small worth held: Then being asked, where all thy beauty lies, Where all the treasure of thy lusty days; To say, within thine own deep sunken eyes, Were an all-eating shame, and thriftless praise. How much more praise deserv'd thy beauty's use, If thou couldst answer 'This fair child of mine Shall sum my count, and make my old excuse,' Proving his beauty by succession thine! This were to be new made when thou art old, And see thy blood warm when thou feel'st it cold.\"\"\".split() # 应该对输入变量进行标记，但暂时忽略。 # 创建一系列的元组，每个元组都是([ word_i-2, word_i-1 ], target word)的形式。 trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2]) for i in range(len(test_sentence) - 2)] # 输出前3行，先看下是什么样子。 print(trigrams[:3]) vocab = set(test_sentence) word_to_ix = {word: i for i, word in enumerate(vocab)} class NGramLanguageModeler(nn.Module): def __init__(self, vocab_size, embedding_dim, context_size): super(NGramLanguageModeler, self).__init__() self.embeddings = nn.Embedding(vocab_size, embedding_dim) self.linear1 = nn.Linear(context_size * embedding_dim, 128) self.linear2 = nn.Linear(128, vocab_size) def forward(self, inputs): embeds = self.embeddings(inputs).view((1, -1)) out = F.relu(self.linear1(embeds)) out = self.linear2(out) log_probs = F.log_softmax(out, dim=1) return log_probs losses = [] loss_function = nn.NLLLoss() model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE) optimizer = optim.SGD(model.parameters(), lr=0.001) for epoch in range(10): total_loss = 0 for context, target in trigrams: # 步骤 1\\. 准备好进入模型的数据 (例如将单词转换成整数索引,并将其封装在变量中) context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long) # 步骤 2\\. 回调torch累乘梯度 # 在传入一个新实例之前，需要把旧实例的梯度置零。 model.zero_grad() # 步骤 3\\. 继续运行代码，得到单词的log概率值。 log_probs = model(context_idxs) # 步骤 4\\. 计算损失函数（再次注意，Torch需要将目标单词封装在变量里）。 loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long)) # 步骤 5\\. 反向传播更新梯度 loss.backward() optimizer.step() # 通过调tensor.item()得到单个Python数值。 total_loss += loss.item() losses.append(total_loss) print(losses) # 用训练数据每次迭代，损失函数都会下降。 输出： [(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')] [518.6343855857849, 516.0739576816559, 513.5321269035339, 511.0085496902466, 508.5003893375397, 506.0077188014984, 503.52977323532104, 501.06553316116333, 498.6121823787689, 496.16915798187256] 练习：计算连续词袋模型的词向量 连续词袋模型（CBOW）在NLP深度学习中使用很频繁。它是一个模型，尝试通过目标词前后几个单词的文本，来预测目标词。这有别于语言模型，因为CBOW不是序列的，也不必是概率性的。 CBOW常用于快速地训练词向量，得到的嵌入用来初始化一些复杂模型的嵌入。通常情况下，这被称为预训练嵌入。 它几乎总能帮忙把模型性能提升几个百分点。 CBOW模型如下所示： 给定一个单词 ，代表两边的滑窗距，如和，并将所有的上下文词统称为 ，CBOW试图最小化 其中是单词的嵌入。 在Pytorch中，通过填充下面的类来实现这个模型，有两条需要注意： 考虑下你需要定义哪些参数。 确保你知道每步操作后的结构，如果想重构，请使用.view() CONTEXT_SIZE = 2 # 左右各两个词 raw_text = \"\"\"We are about to study the idea of a computational process. Computational processes are abstract beings that inhabit computers. As they evolve, processes manipulate other abstract things called data. The evolution of a process is directed by a pattern of rules called a program. People create programs to direct processes. In effect, we conjure the spirits of the computer with our spells.\"\"\".split() # 通过对`raw_text`使用set()函数，我们进行去重操作 vocab = set(raw_text) vocab_size = len(vocab) word_to_ix = {word: i for i, word in enumerate(vocab)} data = [] for i in range(2, len(raw_text) - 2): context = [raw_text[i - 2], raw_text[i - 1], raw_text[i + 1], raw_text[i + 2]] target = raw_text[i] data.append((context, target)) print(data[:5]) class CBOW(nn.Module): def __init__(self): pass def forward(self, inputs): pass # 创建模型并且训练。这里有些函数帮你在使用模块之前制作数据。 def make_context_vector(context, word_to_ix): idxs = [word_to_ix[w] for w in context] return torch.tensor(idxs, dtype=torch.long) make_context_vector(data[0][0], word_to_ix) # example 输出： [(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')] 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"nlp_sequence_models_tutorial.html":{"url":"nlp_sequence_models_tutorial.html","title":"序列模型和 LSTM 网络","keywords":"","body":"序列模型和LSTM网络(长短记忆网络) 译者：ETCartman 校对者：FontTian 之前我们已经学过了许多的前馈网络. 所谓前馈网络, 就是网络中不会保存状态. 然而有时 这并不是我们想要的效果. 在自然语言处理 (NLP, Natural Language Processing) 中, 序列模型是一个核心的概念. 所谓序列模型, 即输入依赖于时间信息的模型. 一个典型的序列模型是隐马尔科夫模型 (HMM, Hidden Markov Model). 另一个序列模型的例子是条件随机场 (CRF, Conditional Random Field). 循环神经网络是指可以保存某种状态的神经网络. 比如说, 神经网络中上个时刻的输出可以作为下个 时刻的输入的一部分, 以此信息就可以通过序列在网络中一直往后传递. 对于LSTM (Long-Short Term Memory) 来说, 序列中的每个元素都有一个相应的隐状态 $h_t$, 该隐状态 原则上可以包含序列当前结点之前的任一节点的信息. 我们可以使用隐藏状态来预测语言模型 中的单词, 词性标签以及其他各种各样的东西. Pytorch中的LSTM 在正式学习之前，有几个点要说明一下，Pytorch中LSTM的输入形式是一个3D的Tensor，每一个维度都有重要的意义，第一个维度就是序列本身，第二个维度是mini-batch中实例的索引，第三个维度是输入元素的索引，我们之前没有接触过mini-batch，所以我们就先忽略它并假设第二维的维度是1。 如果要用\"The cow jumped\"这个句子来运行一个序列模型，那么就应该把它整理成如下的形式： \\begin{split}\\begin{bmatrix} \\overbrace{q_\\text{The}}^\\text{row vector} \\\\ q_\\text{cow} \\\\ q_\\text{jumped} \\end{bmatrix}\\end{split} 除了有一个额外的大小为1的第二维度. 此外, 你还可以向网络逐个输入序列, 在这种情况下, 第一个轴的大小也是1. 来看一个简单的例子. # 作者: Robert Guthrie import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim torch.manual_seed(1) lstm = nn.LSTM(3, 3) # 输入维度为3维，输出维度为3维 inputs = [torch.randn(1, 3) for _ in range(5)] # 生成一个长度为5的序列 # 初始化隐藏状态. hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3)) for i in inputs: # 将序列中的元素逐个输入到LSTM. # 经过每步操作,hidden 的值包含了隐藏状态的信息. out, hidden = lstm(i.view(1, 1, -1), hidden) # 另外我们可以对一整个序列进行训练. # LSTM第一个返回的第一个值是所有时刻的隐藏状态 # 第二个返回值是最后一个时刻的隐藏状态 #(所以\"out\"的最后一个和\"hidden\"是一样的) # 之所以这样设计: # 通过\"out\"你能取得任何一个时刻的隐藏状态，而\"hidden\"的值是用来进行序列的反向传播运算, 具体方式就是将它作为参数传入后面的 LSTM 网络. # 增加额外的第二个维度. inputs = torch.cat(inputs).view(len(inputs), 1, -1) hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3)) # 清空隐藏状态. out, hidden = lstm(inputs, hidden) print(out) print(hidden) 输出: tensor([[[-0.0187, 0.1713, -0.2944]], [[-0.3521, 0.1026, -0.2971]], [[-0.3191, 0.0781, -0.1957]], [[-0.1634, 0.0941, -0.1637]], [[-0.3368, 0.0959, -0.0538]]], grad_fn=) (tensor([[[-0.3368, 0.0959, -0.0538]]], grad_fn=), tensor([[[-0.9825, 0.4715, -0.0633]]], grad_fn=)) 例子:用LSTM来进行词性标注 在这部分, 我们将会使用一个 LSTM 网络来进行词性标注. 在这里我们不会用到维特比算法, 前向-后向算法或者任何类似的算法,而是将这部分内容作为一个 (有挑战) 的练习留给读者, 希望读者在了解了这部分的内容后能够实现如何将维特比算法应用到 LSTM 网络中来. 该模型如下:输入的句子是w1,...,wM​对应的词性为y_1, ...,y_M​ ，用\\hat{y}_i​表示对单词w_i​词性的预测，标签的集合定义为T。 这是一个结构预测模型, 我们的输出是一个序列\\hat{y}_1,...,\\hat{y}_M, 其中\\hat{y}_i\\in T. 在进行预测时, 需将句子每个词输入到一个 LSTM 网络中. 将时刻i​的隐藏状态标记为h_i​,同样地, 对每个标签赋一个独一无二的索引 (类似 word embeddings 部分 word_to_ix 的设置). 然后就得到了\\hat{y}_i​的预测规则。 \\hat{y}^i=argmaxj (logSoftmax(Ahi+b))j​ 即先对隐状态进行一个仿射变换, 然后计算一个对数 softmax, 最后得到的预测标签即为对数 softmax 中最大的值对应的标签. 注意, 这也意味着 A 空间的维度是 |T|​. 准备数据: def prepare_sequence(seq, to_ix): idxs = [to_ix[w] for w in seq] return torch.tensor(idxs, dtype=torch.long) training_data = [ (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]), (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"]) ] word_to_ix = {} for sent, tags in training_data: for word in sent: if word not in word_to_ix: word_to_ix[word] = len(word_to_ix) print(word_to_ix) tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2} # 实际中通常使用更大的维度如32维, 64维. # 这里我们使用小的维度, 为了方便查看训练过程中权重的变化. EMBEDDING_DIM = 6 HIDDEN_DIM = 6 输出: {'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8} 创建模型: class LSTMTagger(nn.Module): def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size): super(LSTMTagger, self).__init__() self.hidden_dim = hidden_dim self.word_embeddings = nn.Embedding(vocab_size, embedding_dim) # LSTM以word_embeddings作为输入, 输出维度为 hidden_dim 的隐藏状态值 self.lstm = nn.LSTM(embedding_dim, hidden_dim) # 线性层将隐藏状态空间映射到标注空间 self.hidden2tag = nn.Linear(hidden_dim, tagset_size) self.hidden = self.init_hidden() def init_hidden(self): # 一开始并没有隐藏状态所以我们要先初始化一个 # 关于维度为什么这么设计请参考Pytoch相关文档 # 各个维度的含义是 (num_layers, minibatch_size, hidden_dim) return (torch.zeros(1, 1, self.hidden_dim), torch.zeros(1, 1, self.hidden_dim)) def forward(self, sentence): embeds = self.word_embeddings(sentence) lstm_out, self.hidden = self.lstm( embeds.view(len(sentence), 1, -1), self.hidden) tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1)) tag_scores = F.log_softmax(tag_space, dim=1) return tag_scores 训练模型: model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix)) loss_function = nn.NLLLoss() optimizer = optim.SGD(model.parameters(), lr=0.1) # 查看训练前的分数 # 注意: 输出的 i,j 元素的值表示单词 i 的 j 标签的得分 # 这里我们不需要训练不需要求导，所以使用torch.no_grad() with torch.no_grad(): inputs = prepare_sequence(training_data[0][0], word_to_ix) tag_scores = model(inputs) print(tag_scores) for epoch in range(300): # 实际情况下你不会训练300个周期, 此例中我们只是随便设了一个值 for sentence, tags in training_data: # 第一步: 请记住Pytorch会累加梯度. # 我们需要在训练每个实例前清空梯度 model.zero_grad() # 此外还需要清空 LSTM 的隐状态, # 将其从上个实例的历史中分离出来. model.hidden = model.init_hidden() # 准备网络输入, 将其变为词索引的 Tensor 类型数据 sentence_in = prepare_sequence(sentence, word_to_ix) targets = prepare_sequence(tags, tag_to_ix) # 第三步: 前向传播. tag_scores = model(sentence_in) # 第四步: 计算损失和梯度值, 通过调用 optimizer.step() 来更新梯度 loss = loss_function(tag_scores, targets) loss.backward() optimizer.step() # 查看训练后的得分 with torch.no_grad(): inputs = prepare_sequence(training_data[0][0], word_to_ix) tag_scores = model(inputs) # 句子是 \"the dog ate the apple\", i,j 表示对于单词 i, 标签 j 的得分. # 我们采用得分最高的标签作为预测的标签. 从下面的输出我们可以看到, 预测得 # 到的结果是0 1 2 0 1. 因为 索引是从0开始的, 因此第一个值0表示第一行的 # 最大值, 第二个值1表示第二行的最大值, 以此类推. 所以最后的结果是 DET # NOUN VERB DET NOUN, 整个序列都是正确的! print(tag_scores) 输出: tensor([[-1.1389, -1.2024, -0.9693], [-1.1065, -1.2200, -0.9834], [-1.1286, -1.2093, -0.9726], [-1.1190, -1.1960, -0.9916], [-1.0137, -1.2642, -1.0366]]) tensor([[-0.0858, -2.9355, -3.5374], [-5.2313, -0.0234, -4.0314], [-3.9098, -4.1279, -0.0368], [-0.0187, -4.7809, -4.5960], [-5.8170, -0.0183, -4.1879]]) 练习:使用字符级特征来增强LSTM词性标注器 在上面的例子中, 每个词都有一个词嵌入, 作为序列模型的输入. 接下来让我们使用每个的单词的 字符级别的表达来增强词嵌入。 我们期望这个操作对结果能有显著提升, 因为像词缀这样的字符级 信息对于词性有很大的影响。比如说, 像包含词缀 -ly 的单词基本上都是被标注为副词. 具体操作如下. 用c_w​的字符级表达, 同之前一样, 我们使用x_w​来表示词嵌入. 序列模型的输入就变成了x_w和c_w​的拼接. 因此, 如果 的维度x_w​是5, 的维度c_w​是3, 那么我们的 LSTM 网络的输入维度大小就是8. 为了得到字符级别的表达, 将单词的每个字符输入一个 LSTM 网络, 而c_w则为这个 LSTM 网络最后的隐状态。 一些提示： 新模型中需要两个 LSTM, 一个跟之前一样, 用来输出词性标注的得分, 另外一个新增加的用来 获取每个单词的字符级别表达。 为了在字符级别上运行序列模型, 你需要用嵌入的字符来作为字符 LSTM 的输入。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"nlp_advanced_tutorial.html":{"url":"nlp_advanced_tutorial.html","title":"Advanced: Making Dynamic Decisions and the Bi-LSTM CRF","keywords":"","body":"高级：制定动态决策和Bi-LSTM CRF 作者：PyTorch 译者：ApacheCN 校对者：enningxie 动态与静态深度学习工具包 Pytorch是一种 动态 神经网络套件。另一个动态套件的例子是 Dynet （我之所以提到这一点，因为与Pytorch和Dynet一起使用是相似的。如果你在Dynet中看到一个例子，它可能会帮助你在Pytorch中实现它）。相反的是 静态 工具包，其中包括Theano，Keras，TensorFlow等。核心区别如下： 在静态工具包中，您可以定义一次计算图，对其进行编译，然后将实例流式传输给它。 在动态工具包中，为每个实例定义计算图。它永远不会被编译并且是即时执行的。 在没有很多经验的情况下，很难理解其中的差异。一个例子是假设我们想要构建一个深层组成解析器。假设我们的模型大致涉及以下步骤： 我们自下而上建造树 标记根节点（句子的单词） 从那里，使用神经网络和单词的嵌入来找到形成组成部分的组合。每当你形成一个新的成分时，使用某种技术来嵌入成分。在这种情况下，我们的网络架构将完全取决于输入句子。在“绿猫划伤墙”一句中，在模型中的某个点上，我们想要结合跨度 (i,j,r) = (1, 3, \\text{NP})（即，NP组成部分跨越单词1到单词3，在这种情况下是“绿猫” ）。 然而，另一句话可能是“某处，大肥猫划伤了墙”。在这句话中，我们希望在某个时刻形成组成 (2, 4, NP)。我们想要形成的成分将取决于实例。如果我们只编译计算图一次，就像在静态工具包中那样，编写这个逻辑将是非常困难或不可能的。但是，在动态工具包中，不仅有1个预定义的计算图。每个实例都可以有一个新的计算图，所以这个问题就消失了。 动态工具包还具有易于调试和代码更接近宿主语言的优点（我的意思是Pytorch和Dynet看起来更像是比Keras或Theano更实际的Python代码）。 Bi-LSTM条件随机场讨论 对于本节，我们将看到用于命名实体识别的Bi-LSTM条件随机场的完整复杂示例。上面的LSTM标记符通常足以用于词性标注，但是像CRF这样的序列模型对于NER上的强大性能非常重要。假设熟悉CRF。虽然这个名字听起来很可怕，但所有模型都是CRF，但是LSTM提供了这些功能。这是一个高级模型，比本教程中的任何早期模型复杂得多。如果你想跳过它，那很好。要查看您是否准备好，请查看是否可以： 在步骤i中为标记k写出维特比变量的递归。 修改上述重复以计算转发变量。 再次修改上面的重复计算以计算日志空间中的转发变量（提示：log-sum-exp） 如果你可以做这三件事，你应该能够理解下面的代码。回想一下，CRF计算条件概率。设 y 为标签序列，x 为字的输入序列。然后我们计算 P(y|x)=\\frac{\\exp{(\\text {Score}(x，y）})} {\\sum_ {y'} \\exp {(\\text {Score}(x，y')})} 通过定义一些对数电位 \\log\\psi_i(x,y) 来确定得分 \\text {Score}(x，y)= \\sum_i\\log\\psi_i(x，y) 为了使分区功能易于处理，电位必须仅查看局部特征。 在Bi-LSTM CRF中，我们定义了两种潜力：发射和过渡。索引 i 处的单词的发射电位来自时间步长 i 处的Bi-LSTM的隐藏状态。转换分数存储在 |T|x|T| 矩阵 \\textbf{P} 中，其中 T 是标记集。在我的实现中，\\textbf{P}_{j,k} 是从标签 转换到标签 j 的分数。所以： \\begin{align} \\text{Score}(x,y) &= \\sum_i \\log \\psi_\\text{EMIT}(y_i \\rightarrow x_i) + \\log \\psi_\\text{TRANS}(y_{i-1} \\rightarrow y_i)\\\\ &= \\sum_i h_i[y_i] + \\textbf{P}_{y_i, y_{i-1}}\\\\ \\end{align} 在第二个表达式中，我们将标记视为分配了唯一的非负索引。 如果上面的讨论过于简短，你可以查看这个从迈克尔柯林斯那里写的关于CRF的文章。 实施说明 下面的示例实现了日志空间中的前向算法来计算分区函数，以及用于解码的维特比算法。反向传播将自动为我们计算梯度。我们不需要手工做任何事情。 实施未优化。如果您了解发生了什么，您可能会很快发现在前向算法中迭代下一个标记可能是在一个大的操作中完成的。我想编码更具可读性。如果您想进行相关更改，可以将此标记器用于实际任务。 # Author: Robert Guthrie import torch import torch.autograd as autograd import torch.nn as nn import torch.optim as optim torch.manual_seed(1) 帮助程序的功能是使代码更具可读性。 def argmax(vec): # return the argmax as a python int _, idx = torch.max(vec, 1) return idx.item() def prepare_sequence(seq, to_ix): idxs = [to_ix[w] for w in seq] return torch.tensor(idxs, dtype=torch.long) # Compute log sum exp in a numerically stable way for the forward algorithm def log_sum_exp(vec): max_score = vec[0, argmax(vec)] max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1]) return max_score + \\ torch.log(torch.sum(torch.exp(vec - max_score_broadcast))) 创建模型 class BiLSTM_CRF(nn.Module): def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim): super(BiLSTM_CRF, self).__init__() self.embedding_dim = embedding_dim self.hidden_dim = hidden_dim self.vocab_size = vocab_size self.tag_to_ix = tag_to_ix self.tagset_size = len(tag_to_ix) self.word_embeds = nn.Embedding(vocab_size, embedding_dim) self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True) # Maps the output of the LSTM into tag space. self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size) # Matrix of transition parameters. Entry i,j is the score of # transitioning *to* i *from* j. self.transitions = nn.Parameter( torch.randn(self.tagset_size, self.tagset_size)) # These two statements enforce the constraint that we never transfer # to the start tag and we never transfer from the stop tag self.transitions.data[tag_to_ix[START_TAG], :] = -10000 self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000 self.hidden = self.init_hidden() def init_hidden(self): return (torch.randn(2, 1, self.hidden_dim // 2), torch.randn(2, 1, self.hidden_dim // 2)) def _forward_alg(self, feats): # Do the forward algorithm to compute the partition function init_alphas = torch.full((1, self.tagset_size), -10000.) # START_TAG has all of the score. init_alphas[0][self.tag_to_ix[START_TAG]] = 0. # Wrap in a variable so that we will get automatic backprop forward_var = init_alphas # Iterate through the sentence for feat in feats: alphas_t = [] # The forward tensors at this timestep for next_tag in range(self.tagset_size): # broadcast the emission score: it is the same regardless of # the previous tag emit_score = feat[next_tag].view( 1, -1).expand(1, self.tagset_size) # the ith entry of trans_score is the score of transitioning to # next_tag from i trans_score = self.transitions[next_tag].view(1, -1) # The ith entry of next_tag_var is the value for the # edge (i -> next_tag) before we do log-sum-exp next_tag_var = forward_var + trans_score + emit_score # The forward variable for this tag is log-sum-exp of all the # scores. alphas_t.append(log_sum_exp(next_tag_var).view(1)) forward_var = torch.cat(alphas_t).view(1, -1) terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]] alpha = log_sum_exp(terminal_var) return alpha def _get_lstm_features(self, sentence): self.hidden = self.init_hidden() embeds = self.word_embeds(sentence).view(len(sentence), 1, -1) lstm_out, self.hidden = self.lstm(embeds, self.hidden) lstm_out = lstm_out.view(len(sentence), self.hidden_dim) lstm_feats = self.hidden2tag(lstm_out) return lstm_feats def _score_sentence(self, feats, tags): # Gives the score of a provided tag sequence score = torch.zeros(1) tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags]) for i, feat in enumerate(feats): score = score + \\ self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]] score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]] return score def _viterbi_decode(self, feats): backpointers = [] # Initialize the viterbi variables in log space init_vvars = torch.full((1, self.tagset_size), -10000.) init_vvars[0][self.tag_to_ix[START_TAG]] = 0 # forward_var at step i holds the viterbi variables for step i-1 forward_var = init_vvars for feat in feats: bptrs_t = [] # holds the backpointers for this step viterbivars_t = [] # holds the viterbi variables for this step for next_tag in range(self.tagset_size): # next_tag_var[i] holds the viterbi variable for tag i at the # previous step, plus the score of transitioning # from tag i to next_tag. # We don't include the emission scores here because the max # does not depend on them (we add them in below) next_tag_var = forward_var + self.transitions[next_tag] best_tag_id = argmax(next_tag_var) bptrs_t.append(best_tag_id) viterbivars_t.append(next_tag_var[0][best_tag_id].view(1)) # Now add in the emission scores, and assign forward_var to the set # of viterbi variables we just computed forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1) backpointers.append(bptrs_t) # Transition to STOP_TAG terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]] best_tag_id = argmax(terminal_var) path_score = terminal_var[0][best_tag_id] # Follow the back pointers to decode the best path. best_path = [best_tag_id] for bptrs_t in reversed(backpointers): best_tag_id = bptrs_t[best_tag_id] best_path.append(best_tag_id) # Pop off the start tag (we dont want to return that to the caller) start = best_path.pop() assert start == self.tag_to_ix[START_TAG] # Sanity check best_path.reverse() return path_score, best_path def neg_log_likelihood(self, sentence, tags): feats = self._get_lstm_features(sentence) forward_score = self._forward_alg(feats) gold_score = self._score_sentence(feats, tags) return forward_score - gold_score def forward(self, sentence): # dont confuse this with _forward_alg above. # Get the emission scores from the BiLSTM lstm_feats = self._get_lstm_features(sentence) # Find the best path, given the features. score, tag_seq = self._viterbi_decode(lstm_feats) return score, tag_seq 进行训练 START_TAG = \"\" STOP_TAG = \"\" EMBEDDING_DIM = 5 HIDDEN_DIM = 4 # Make up some training data training_data = [( \"the wall street journal reported today that apple corporation made money\".split(), \"B I I I O O O B I O O\".split() ), ( \"georgia tech is a university in georgia\".split(), \"B I O O O O B\".split() )] word_to_ix = {} for sentence, tags in training_data: for word in sentence: if word not in word_to_ix: word_to_ix[word] = len(word_to_ix) tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4} model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4) # Check predictions before training with torch.no_grad(): precheck_sent = prepare_sequence(training_data[0][0], word_to_ix) precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long) print(model(precheck_sent)) # Make sure prepare_sequence from earlier in the LSTM section is loaded for epoch in range( 300): # again, normally you would NOT do 300 epochs, it is toy data for sentence, tags in training_data: # Step 1\\. Remember that Pytorch accumulates gradients. # We need to clear them out before each instance model.zero_grad() # Step 2\\. Get our inputs ready for the network, that is, # turn them into Tensors of word indices. sentence_in = prepare_sequence(sentence, word_to_ix) targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long) # Step 3\\. Run our forward pass. loss = model.neg_log_likelihood(sentence_in, targets) # Step 4\\. Compute the loss, gradients, and update the parameters by # calling optimizer.step() loss.backward() optimizer.step() # Check predictions after training with torch.no_grad(): precheck_sent = prepare_sequence(training_data[0][0], word_to_ix) print(model(precheck_sent)) # We got it! 日期： (tensor(2.6907), [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1]) (tensor(20.4906), [0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2]) 练习：区分标记的新损失函数 我们没有必要在进行解码时创建计算图，因为我们不会从维特比路径得分反向传播。因为无论如何我们都有它，尝试训练标记器，其中损失函数是维特比路径得分和金标准路径得分之间的差异。应该清楚的是，当预测的标签序列是正确的标签序列时，该功能是非负的和0。这基本上是 结构感知器。 由于已经实现了Viterbi和scoresentence，因此这种修改应该很短。这是取决于训练实例的计算图形的形状的示例。虽然我没有尝试在静态工具包中实现它，但我想它可能但不那么直截了当。 拿起一些真实数据并进行比较！ 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"seq2seq_translation_tutorial.html":{"url":"seq2seq_translation_tutorial.html","title":"基于注意力机制的 seq2seq 神经网络翻译","keywords":"","body":"基于注意力机制的 seq2seq 神经网络翻译 译者：mengfu188 校对者：FontTian 作者: Sean Robertson 在这个项目中，我们将编写一个把法语翻译成英语的神经网络。 [KEY: > input, = target, il est en train de peindre un tableau . = he is painting a picture . pourquoi ne pas essayer ce vin delicieux ? = why not try that delicious wine ? elle n est pas poete mais romanciere . = she is not a poet but a novelist . vous etes trop maigre . = you re too skinny . … 取得了不同程度的成功 这是通过seq2seq网络来进行实现的，在这个网络中使用两个递归的神经网络（编码器网络和解码器网络）一起工作使得一段序列变成另一段序列。 编码器网络将输入序列变成一个向量，解码器网络将该向量展开为新的序列。 我们将使用注意力机制改进这个模型，它可以让解码器学会集中在输入序列的特定范围中。 推荐阅读： 我假设你至少已经了解Python，安装了PyTorch，并且了解什么是张量： https://pytorch.org/ PyTorch安装说明 PyTorch 深度学习: 60 分钟极速入门教程 开始使用PyTorch 跟着例子学习PyTorch 更加广泛而深入的了解PyTorch PyTorch for Former Torch Users 如果你是Lua Torch用户 这些内容有利于了解seq2seq网络及其工作机制： 用RNN编码器 - 解码器来学习用于统计机器翻译的短语表示 用神经网络进行seq2seq学习 通过共同学习对齐和翻译的神经机器翻译 神经会话模型 你还可以找到以前类似于编码器和解码器的教程，如用字符集RNN分类名称 和 用字符集RNN生成名称]，学习这些概念比较有帮助。 更多内容请阅读以下论文： 用RNN编码器 - 解码器来学习用于统计机器翻译的短语表示 用神经网络进行seq2seq学习 通过共同学习对齐和翻译的神经机器翻译 神经会话模型 需要如下： from __future__ import unicode_literals, print_function, division from io import open import unicodedata import string import re import random import torch import torch.nn as nn from torch import optim import torch.nn.functional as F device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") 加载数据文件 这个项目的数据是一组数以千计的英语到法语的翻译用例 这个问题在 Open Data Stack Exchange 上 点我打开翻译网址 https://tatoeba.org/ 这个网站的下载地址 https://tatoeba.org/eng/downloads - 更棒的是，有人将这些语言切分成单个文件: https://www.manythings.org/anki/ 由于翻译文件太大而不能放到repo中，请继续往下前下载数据到 data/eng-fra.txt 。该文件是一个使用制表符分割的翻译列表: I am cold. J'ai froid. 注意 从 这里 下载数据和解压到相关的路径. 与character-level RNN教程中使用的字符编码类似,我们将用语言中的每个单词 作为独热向量,或者除了单个单词之外(在单词的索引处)的大的零向量. 相较于可能 存在于一种语言中仅有十个字符相比,多数都是有大量的字,因此编码向量很大. 然而,我们会欺骗性的做一些数据修剪,保证每种语言只使用几千字. 我们之后需要将每个单词对应唯一的索引作为神经网络的输入和目标.为了追踪这些索引我们使用一个帮助类 Lang 类中有 词 → 索引 (word2index) 和 索引 → 词(index2word) 的字典, 以及每个词word2count 用来替换稀疏词汇。 SOS_token = 0 EOS_token = 1 class Lang: def __init__(self, name): self.name = name self.word2index = {} self.word2count = {} self.index2word = {0: \"SOS\", 1: \"EOS\"} self.n_words = 2 # Count SOS and EOS def addSentence(self, sentence): for word in sentence.split(' '): self.addWord(word) def addWord(self, word): if word not in self.word2index: self.word2index[word] = self.n_words self.word2count[word] = 1 self.index2word[self.n_words] = word self.n_words += 1 else: self.word2count[word] += 1 这些文件全部采用Unicode编码，为了简化起见，我们将Unicode字符转换成ASCII编码、所有内容小写、并修剪大部分标点符号。 # 感谢您将Unicode字符转换成ASCII # https://stackoverflow.com/a/518232/2809427 def unicodeToAscii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' ) # 小写，修剪和删除非字符字符 def normalizeString(s): s = unicodeToAscii(s.lower().strip()) s = re.sub(r\"([.!?])\", r\" \\1\", s) s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) return s 我们将按行分开并将每一行分成两列来读取文件。这些文件都是英语 -> 其他语言，所以如果我们想从其他语言翻译 -> 英语，添加reverse标志来翻转词语对。 def readLangs(lang1, lang2, reverse=False): print(\"Reading lines...\") # Read the file and split into lines lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\ read().strip().split('\\n') # Split every line into pairs and normalize pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines] # Reverse pairs, make Lang instances if reverse: pairs = [list(reversed(p)) for p in pairs] input_lang = Lang(lang2) output_lang = Lang(lang1) else: input_lang = Lang(lang1) output_lang = Lang(lang2) return input_lang, output_lang, pairs 简短的句子。这些句子的最大长度是10个单词（包括标点符号），同时我们将那些翻译为“I am”或“he is”等形式的句子进行了修改（考虑到之前清除的标点符号——‘）。 MAX_LENGTH = 10 eng_prefixes = ( \"i am \", \"i m \", \"he is\", \"he s \", \"she is\", \"she s \", \"you are\", \"you re \", \"we are\", \"we re \", \"they are\", \"they re \" ) def filterPair(p): return len(p[0].split(' ')) 完整的数据准备过程： 按行读取文本文件，将行拆分成对 规范文本，按长度和内容过滤 从句子中成对列出单词列表 def prepareData(lang1, lang2, reverse=False): input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse) print(\"Read %s sentence pairs\" % len(pairs)) pairs = filterPairs(pairs) print(\"Trimmed to %s sentence pairs\" % len(pairs)) print(\"Counting words...\") for pair in pairs: input_lang.addSentence(pair[0]) output_lang.addSentence(pair[1]) print(\"Counted words:\") print(input_lang.name, input_lang.n_words) print(output_lang.name, output_lang.n_words) return input_lang, output_lang, pairs input_lang, output_lang, pairs = prepareData('eng', 'fra', True) print(random.choice(pairs)) 输出: Reading lines... Read 135842 sentence pairs Trimmed to 10599 sentence pairs Counting words... Counted words: fra 4345 eng 2803 ['ils ne sont pas encore chez eux .', 'they re not home yet .'] Seq2Seq模型 递归神经网络（RNN）是一种对序列进行操作并利用自己的输出作为后序输入的网络 序列到序列网络（Sequence to Sequence network）, 也叫做 seq2seq 网络, 又或者是 编码器解码器网络（Encoder Decoder network）, 是一个由两个称为编码器解码器的RNN组成的模型。编码器读取输入序列并输出一个矢量，解码器读取该矢量并产生输出序列。 与每个输入对应一个输出的单个RNN的序列预测不同，seq2seq模型将我们从序列长度和顺序中解放出来，这使得它更适合两种语言的转换。 考虑这句话“Je ne suis pas le chat noir” → “I am not the black cat”.虽然大部分情况下输入输出序列可以对单词进行比较直接的翻译，但是很多时候单词的顺序却略有不同，例如: “chat noir” 和 “black cat”。由于 “ne/pas”结构, 输入的句子中还有另外一个单词.。因此直接从输入词的序列中直接生成正确的翻译是很困难的。 使用seq2seq模型时，编码器会创建一个向量，在理想的情况下，将输入序列的实际语义编码为单个向量 - 序列的一些N维空间中的单个点。 编码器 seq2seq网络的编码器是RNN，它为输入序列中的每个单词输出一些值。 对于每个输入单词，编码器输出一个向量和一个隐状态，并将该隐状态用于下一个输入的单词。 class EncoderRNN(nn.Module): def __init__(self, input_size, hidden_size): super(EncoderRNN, self).__init__() self.hidden_size = hidden_size self.embedding = nn.Embedding(input_size, hidden_size) self.gru = nn.GRU(hidden_size, hidden_size) def forward(self, input, hidden): embedded = self.embedding(input).view(1, 1, -1) output = embedded output, hidden = self.gru(output, hidden) return output, hidden def initHidden(self): return torch.zeros(1, 1, self.hidden_size, device=device) 解码器 解码器是一个接受编码器输出向量并输出一系列单词以创建翻译的RNN。 简单的编码器 在最简单的seq2seq解码器中，我们只使用编码器的最后输出。这最后一个输出有时称为上下文向量因为它从整个序列中编码上下文。该上下文向量用作解码器的初始隐藏状态。 在解码的每一步,解码器都被赋予一个输入指令和隐藏状态. 初始输入指令字符串开始的指令,第一个隐藏状态是上下文向量(编码器的最后隐藏状态). class DecoderRNN(nn.Module): def __init__(self, hidden_size, output_size): super(DecoderRNN, self).__init__() self.hidden_size = hidden_size self.embedding = nn.Embedding(output_size, hidden_size) self.gru = nn.GRU(hidden_size, hidden_size) self.out = nn.Linear(hidden_size, output_size) self.softmax = nn.LogSoftmax(dim=1) def forward(self, input, hidden): output = self.embedding(input).view(1, 1, -1) output = F.relu(output) output, hidden = self.gru(output, hidden) output = self.softmax(self.out(output[0])) return output, hidden def initHidden(self): return torch.zeros(1, 1, self.hidden_size, device=device) 我们鼓励你训练和观察这个模型的结果,但为了节省空间,我们将直入主题开始讲解注意力机制. 带有注意力机制的解码器 如果仅在编码器和解码器之间传递上下文向量,则该单个向量承担编码整个句子的负担. 注意力机制允许解码器网络针对解码器自身输出的每一步”聚焦”编码器输出的不同部分. 首先我们计算一组注意力权重. 这些将被乘以编码器输出矢量获得加权的组合. 结果(在代码中为attn_applied) 应该包含关于输入序列的特定部分的信息, 从而帮助解码器选择正确的输出单词. 注意权值的计算是用另一个前馈层attn进行的, 将解码器的输入和隐藏层状态作为输入. 由于训练数据中的输入序列（语句）长短不一,为了实际创建和训练此层, 我们必须选择最大长度的句子(输入长度,用于编码器输出),以适用于此层. 最大长度的句子将使用所有注意力权重,而较短的句子只使用前几个. class AttnDecoderRNN(nn.Module): def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH): super(AttnDecoderRNN, self).__init__() self.hidden_size = hidden_size self.output_size = output_size self.dropout_p = dropout_p self.max_length = max_length self.embedding = nn.Embedding(self.output_size, self.hidden_size) self.attn = nn.Linear(self.hidden_size * 2, self.max_length) self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size) self.dropout = nn.Dropout(self.dropout_p) self.gru = nn.GRU(self.hidden_size, self.hidden_size) self.out = nn.Linear(self.hidden_size, self.output_size) def forward(self, input, hidden, encoder_outputs): embedded = self.embedding(input).view(1, 1, -1) embedded = self.dropout(embedded) attn_weights = F.softmax( self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1) attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0)) output = torch.cat((embedded[0], attn_applied[0]), 1) output = self.attn_combine(output).unsqueeze(0) output = F.relu(output) output, hidden = self.gru(output, hidden) output = F.log_softmax(self.out(output[0]), dim=1) return output, hidden, attn_weights def initHidden(self): return torch.zeros(1, 1, self.hidden_size, device=device) 注意 还有其他形式的注意力通过使用相对位置方法来解决长度限制. 阅读关于 “local attention” 在 基于注意力机制的神经机器翻译的有效途径. 训练 准备训练数据 为了训练,对于每一对我们都需要输入张量(输入句子中的词的索引)和 目标张量(目标语句中的词的索引). 在创建这些向量时,我们会将EOS标记添加到两个序列中。 def indexesFromSentence(lang, sentence): return [lang.word2index[word] for word in sentence.split(' ')] def tensorFromSentence(lang, sentence): indexes = indexesFromSentence(lang, sentence) indexes.append(EOS_token) return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1) def tensorsFromPair(pair): input_tensor = tensorFromSentence(input_lang, pair[0]) target_tensor = tensorFromSentence(output_lang, pair[1]) return (input_tensor, target_tensor) 训练模型 为了训练我们通过编码器运行输入序列,并跟踪每个输出和最新的隐藏状态. 然后解码器被赋予 标志作为其第一个输入, 并将编码器的最后一个隐藏状态作为其第一个隐藏状态. “Teacher forcing” 是将实际目标输出用作每个下一个输入的概念,而不是将解码器的 猜测用作下一个输入.使用“Teacher forcing” 会使其更快地收敛,但是 当训练好的网络被利用时,它可能表现出不稳定性.. 您可以观察“Teacher forcing”网络的输出，这些网络使用连贯的语法阅读，但远离正确的翻译 - 直觉上它已经学会表示输出语法，并且一旦老师告诉它前几个单词就可以“提取”意义，但是 它没有正确地学习如何从翻译中创建句子。 由于PyTorch的autograd给我们的自由,我们可以随意选择使用“Teacher forcing”或不使用简单的if语句. 调高teacher_forcing_ratio来更好地使用它. teacher_forcing_ratio = 0.5 def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH): encoder_hidden = encoder.initHidden() encoder_optimizer.zero_grad() decoder_optimizer.zero_grad() input_length = input_tensor.size(0) target_length = target_tensor.size(0) encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device) loss = 0 for ei in range(input_length): encoder_output, encoder_hidden = encoder( input_tensor[ei], encoder_hidden) encoder_outputs[ei] = encoder_output[0, 0] decoder_input = torch.tensor([[SOS_token]], device=device) decoder_hidden = encoder_hidden use_teacher_forcing = True if random.random() 这是一个帮助函数，用于在给定当前时间和进度%的情况下打印经过的时间和估计的剩余时间。 import time import math def asMinutes(s): m = math.floor(s / 60) s -= m * 60 return '%dm %ds' % (m, s) def timeSince(since, percent): now = time.time() s = now - since es = s / (percent) rs = es - s return '%s (- %s)' % (asMinutes(s), asMinutes(rs)) 整个训练过程如下所示: 启动计时器 初始化优化器和准则 创建一组训练队 为进行绘图启动空损失数组 之后我们多次调用train函数，偶尔打印进度 (样本的百分比，到目前为止的时间，狙击的时间) 和平均损失 def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01): start = time.time() plot_losses = [] print_loss_total = 0 # Reset every print_every plot_loss_total = 0 # Reset every plot_every encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate) decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate) training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)] criterion = nn.NLLLoss() for iter in range(1, n_iters + 1): training_pair = training_pairs[iter - 1] input_tensor = training_pair[0] target_tensor = training_pair[1] loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) print_loss_total += loss plot_loss_total += loss if iter % print_every == 0: print_loss_avg = print_loss_total / print_every print_loss_total = 0 print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg)) if iter % plot_every == 0: plot_loss_avg = plot_loss_total / plot_every plot_losses.append(plot_loss_avg) plot_loss_total = 0 showPlot(plot_losses) 绘制结果 使用matplotlib完成绘图，使用plot_losses保存训练时的数组。 import matplotlib.pyplot as plt plt.switch_backend('agg') import matplotlib.ticker as ticker import numpy as np def showPlot(points): plt.figure() fig, ax = plt.subplots() # 该定时器用于定时记录时间 loc = ticker.MultipleLocator(base=0.2) ax.yaxis.set_major_locator(loc) plt.plot(points) 评估 评估与训练大部分相同,但没有目标,因此我们只是将解码器的每一步预测反馈给它自身. 每当它预测到一个单词时,我们就会将它添加到输出字符串中,并且如果它预测到我们在那里停止的EOS指令. 我们还存储解码器的注意力输出以供稍后显示. def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH): with torch.no_grad(): input_tensor = tensorFromSentence(input_lang, sentence) input_length = input_tensor.size()[0] encoder_hidden = encoder.initHidden() encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device) for ei in range(input_length): encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden) encoder_outputs[ei] += encoder_output[0, 0] decoder_input = torch.tensor([[SOS_token]], device=device) # SOS decoder_hidden = encoder_hidden decoded_words = [] decoder_attentions = torch.zeros(max_length, max_length) for di in range(max_length): decoder_output, decoder_hidden, decoder_attention = decoder( decoder_input, decoder_hidden, encoder_outputs) decoder_attentions[di] = decoder_attention.data topv, topi = decoder_output.data.topk(1) if topi.item() == EOS_token: decoded_words.append('') break else: decoded_words.append(output_lang.index2word[topi.item()]) decoder_input = topi.squeeze().detach() return decoded_words, decoder_attentions[:di + 1] 我们可以从训练集中对随机句子进行评估，并打印出输入、目标和输出，从而做出一些主观的质量判断： def evaluateRandomly(encoder, decoder, n=10): for i in range(n): pair = random.choice(pairs) print('>', pair[0]) print('=', pair[1]) output_words, attentions = evaluate(encoder, decoder, pair[0]) output_sentence = ' '.join(output_words) print(' 训练和评估 有了所有这些帮助函数(它看起来像是额外的工作，但它使运行多个实验更容易)，我们实际上可以初始化一个网络并开始训练。 请记住输入句子被严重过滤, 对于这个小数据集,我们可以使用包含256个隐藏节点 和单个GRU层的相对较小的网络.在MacBook CPU上约40分钟后,我们会得到一些合理的结果. 注 如果你运行这个笔记本，你可以训练，中断内核，评估，并在以后继续训练。 注释编码器和解码器初始化的行并再次运行 trainIters . hidden_size = 256 encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device) attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device) trainIters(encoder1, attn_decoder1, 75000, print_every=5000) 输出: 1m 47s (- 25m 8s) (5000 6%) 2.8641 3m 30s (- 22m 45s) (10000 13%) 2.2666 5m 15s (- 21m 1s) (15000 20%) 1.9537 7m 0s (- 19m 17s) (20000 26%) 1.7170 8m 46s (- 17m 32s) (25000 33%) 1.5182 10m 31s (- 15m 46s) (30000 40%) 1.3280 12m 15s (- 14m 0s) (35000 46%) 1.2137 14m 1s (- 12m 16s) (40000 53%) 1.0843 15m 48s (- 10m 32s) (45000 60%) 0.9847 17m 34s (- 8m 47s) (50000 66%) 0.8515 19m 20s (- 7m 2s) (55000 73%) 0.7940 21m 6s (- 5m 16s) (60000 80%) 0.7189 22m 53s (- 3m 31s) (65000 86%) 0.6490 24m 41s (- 1m 45s) (70000 93%) 0.5954 26m 26s (- 0m 0s) (75000 100%) 0.5257 evaluateRandomly(encoder1, attn_decoder1) 输出: > nous sommes contents que tu sois la . = we re glad you re here . > il est dependant a l heroine . = he is a heroin addict . > nous sommes les meilleurs . = we are the best . > tu es puissant . = you re powerful . > j ai peur des chauves souris . = i m afraid of bats . > tu es enseignant n est ce pas ? = you re a teacher right ? > je suis pret a tout faire pour toi . = i am ready to do anything for you . > c est desormais un homme . = he s a man now . > elle est une mere tres avisee . = she s a very wise mother . > je suis completement vanne . = i m completely exhausted . 可视化注意力 注意力机制的一个有用的特性是其高度可解释的输出。由于它用于加权输入序列的特定编码器输出，因此我们可以想象，在每个时间步骤中，查看网络最集中的位置。 你可以简单地运行plt.matshow(attentions)来查看显示为矩阵的注意力输出，列为输入步骤，行位输出步骤。 output_words, attentions = evaluate( encoder1, attn_decoder1, \"je suis trop froid .\") plt.matshow(attentions.numpy()) 为了获得更好的观看体验,我们将额外添加轴和标签: def showAttention(input_sentence, output_words, attentions): # Set up figure with colorbar fig = plt.figure() ax = fig.add_subplot(111) cax = ax.matshow(attentions.numpy(), cmap='bone') fig.colorbar(cax) # Set up axes ax.set_xticklabels([''] + input_sentence.split(' ') + [''], rotation=90) ax.set_yticklabels([''] + output_words) # Show label at every tick ax.xaxis.set_major_locator(ticker.MultipleLocator(1)) ax.yaxis.set_major_locator(ticker.MultipleLocator(1)) plt.show() def evaluateAndShowAttention(input_sentence): output_words, attentions = evaluate( encoder1, attn_decoder1, input_sentence) print('input =', input_sentence) print('output =', ' '.join(output_words)) showAttention(input_sentence, output_words, attentions) evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\") evaluateAndShowAttention(\"elle est trop petit .\") evaluateAndShowAttention(\"je ne crains pas de mourir .\") evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\") 输出: input = elle a cinq ans de moins que moi . output = she s five years younger than me . input = elle est trop petit . output = she s too slow . input = je ne crains pas de mourir . output = i m not scared to die . input = c est un jeune directeur plein de talent . output = he s a talented young player . 练习题 尝试使用不同的数据集 另一种语言对 人 → 机器 (例如 IOT 命令) 聊天 → 响应 问题 → 回答 将嵌入替换为预先训练过的单词嵌入，例如word2vec或者GloVe 尝试用更多的层次，更多的隐藏单位，更多的句子。比较训练时间和结果。 如果使用一个翻译文件，其中成对有两个相同的短语(I am test \\t I am test)，您可以将其用作自动编码器。试试这个： 训练为自动编码器 只保存编码器网络 训练一种新的翻译解码器 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"tut_generative.html":{"url":"tut_generative.html","title":"生成","keywords":"","body":"生成 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"dcgan_faces_tutorial.html":{"url":"dcgan_faces_tutorial.html","title":"DCGAN Tutorial","keywords":"","body":"DCGAN 教程 作者: Nathan Inkawhich 译者：wangshuai9517 校验: 片刻 介绍 本教程将通过一个例子来介绍DCGAN。我们将使用很多真正的名人照片训练一个生成对抗网络（GAN）后，生成新的假名人照片。这里的大多数代码来自于pytorch/examples中对DCGAN的实现，并且本文档将对DCGAN的实现进行全面解释，并阐明该模型是怎样工作的以及为什么能工作。但是不要担心，我们并不需要你事先了解GAN，但是可能需要先花一些时间来弄明白实际发生了什么。 此外，拥有一两个GPU将对节省运行时间很有帮助。 让我们从头开始吧。 对抗生成网络 什么是对抗生成网络(GAN)? 对抗生成网络（GAN）是一个教深度模型获取训练数据分布的一种框架，因此我们能够使用类似的分布来生成新的数据。对抗生成网络是Ian Goodfellow在2014年发明的，并首次发表在文章 Generative Adversarial Nets中。它们由两种不同的模块组成，一个生成器 generator 以及一个判别器 discriminator 。生成器的工作是产生看起来像训练图像的“假”图像。 判别器的工作是查看图像并输出它是否是来自真实训练图像或生成器的伪图像。在训练期间，生成器不断尝试通过产生越来越好的假图片来超越判别器，与此同时判别器逐渐更好的检测并正确分类真假图片。 这个过程最后逐渐的变得平衡，生成器生成完美的假图片，这些假图片看起来好像它们直接来自训练数据，并且判别器总是猜测生成器输出的图片真假都是50%。 现在，我们先定义一些整个教程中要使用的符号，首先从判别器开始。 x 表示图像数据。D(x) 表示判别网络，它的输出表示数据 x 来自与训练数据而不是生成数据的概率。这里 D(x) 的输入图像是大小为3x64x64。 直观地说，当 x 来自训练数据时，D(x)的值应当是大的；而当 x 来自发生器时，D(x) 的值应为小的。 D(x) 也可以被认为是传统的二元分类器。 对于生成器 z 表示从标准正态分布中采样的空间矢量（本征向量）。 G(z) 表示将本征向量 z 映射到数据空间的生成器函数。 G 的目标是估计训练数据来自的分布 p_{data} ，这样就可以从估计的分布 p_g 中生成假样本。 因此，D(G(z)) 表示生成器输出G是真实图片的概率。就像在 Goodfellow’s paper中描述的那样，D 和 G 在玩一个极大极小游戏。在这个游戏中 D 试图最大化正确分类真假图片的概率 logD(x) ，G 试图最小化 D 预测其输出为假图片的概率 log(1-D(G(x))) 。文章中GAN的损失函数是 \\underset{G}{\\text{min}} \\underset{D}{\\text{max}}V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}\\big[logD(x)\\big] + \\mathbb{E}_{z\\sim p_{z}(z)}\\big[log(1-D(G(x)))\\big] 理论上，这个极小极大游戏的目标是 p_g=p_{data}，如果输入是真实的或假的，则判别器会随机猜测。 然而，GAN的收敛理论仍在积极研究中，实际上模型并不总是训练到这一点。 什么是DCGAN? DCGAN是对上面描述的GAN的直接扩展，除了它分别在判别器和生成器中明确地使用卷积和卷积转置层。 DCGAN是在Radford等的文章Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks中首次被提出的。判别器由卷积层、批标准化 层以及LeakyReLU 激活层组成。输入是3x64x64的图像，输出是输入图像来自实际数据的概率。生成器由转置卷积层，批标准化层以及ReLU 激活层组成。 输入是一个本征向量（latent vector） z，它是从标准正态分布中采样得到的，输出是一个3x64x64 的RGB图像。 转置卷积层能够把本征向量转换成和图像具有相同大小。 在本文中，作者还提供了一些有关如何设置优化器，如何计算损失函数以及如何初始化模型权重的建议，所有这些都将在后面的章节中进行说明。 from __future__ import print_function #%matplotlib inline import argparse import os import random import torch import torch.nn as nn import torch.nn.parallel import torch.backends.cudnn as cudnn import torch.optim as optim import torch.utils.data import torchvision.datasets as dset import torchvision.transforms as transforms import torchvision.utils as vutils import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation from IPython.display import HTML # 为了可重复性设置随机种子 manualSeed = 999 #manualSeed = random.randint(1, 10000) # 如果你想有一个不同的结果使用这行代码 print(\"Random Seed: \", manualSeed) random.seed(manualSeed) torch.manual_seed(manualSeed) 输出: Random Seed: 999 变量 为了能够运行，定义一些变量： dataroot - 数据集文件夹的路径。我们将在后面的章节中讨论更多关于数据集的内容 workers - 数据加载器DataLoader加载数据时能够使用的进程数 batch_size - 训练时的批大小。在DCGAN文献中使用的批大小是128 image_size - 训练时使用的图片大小。 这里设置默认值为 64x64\\ 。如果想使用别的大小，生成器G和判别器D的结构也要改变。 想看更多详细内容请点击这里 nc - 输入图片的颜色通道个数。彩色图片是3 nz - 本征向量的长度 ngf - 生成器使用的特征图深度 ndf - 设置判别器使用的特征图的深度 num_epochs - 一共训练多少次。训练次数多很可能产生更好的结果但是需要训练更长的时间 lr - 训练时的学习率，DCGAN文章中使用的是0.0002 beta1 - Adam优化算法的beta1超参数。文章用使用的是0.5 ngpu - 可利用的GPU数量，如果设置为0则运行在CPU模式。如果设置的大于0则再行在那些数量的GPU # 数据集根目录 dataroot = \"data/celeba\" # 数据加载器能够使用的进程数量 workers = 2 # 训练时的批大小 batch_size = 128 # 训练图片的大小，所有的图片给都将改变到该大小 # 转换器使用的大小. image_size = 64 # 训练图片的通道数，彩色图片是3 nc = 3 # 本征向量z的大小(生成器的输入大小) nz = 100 # 生成器中特征图大小 ngf = 64 # 判别器中特征图大小 ndf = 64 # 训练次数 num_epochs = 5 # 优化器学习率 lr = 0.0002 # Adam优化器的Beta1超参 beta1 = 0.5 # 可利用的GPU数量，使用0将运行在CPU模式。 ngpu = 1 数据 本教程中我将使用 Celeb-A Faces 数据集 可以在链接中下载，或者在 谷歌网盘中下载。下载该数据集将产生一个名为 img_align_celeba.zip 的文件。 下载完成后，创建一个名为 celeba 的文件夹解压下载的数据集到该目录下。然后，在本笔记中设置 dataroot 到你刚才创建的文件夹 celeba 。最后得到的文件夹结构如下： /path/to/celeba -> img_align_celeba -> 188242.jpg -> 173822.jpg -> 284702.jpg -> 537394.jpg ... 这是一个很重要的步骤，因为我们将使用ImageFolder数据集类需要使用在数据集根目录下的子文件夹。现在，我们能够创建这个数据集，创建数据加载器以及设置在哪运行，最后可视化一些训练数据。 # 我们能够使用我们创建的数据集图片文件夹了 # 创建数据集 dataset = dset.ImageFolder(root=dataroot, transform=transforms.Compose([ transforms.Resize(image_size), transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ])) # 创建数据加载器 dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=workers) # 决定我们在哪个设备上运行 device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\") # 展示一些训练图片 real_batch = next(iter(dataloader)) plt.figure(figsize=(8,8)) plt.axis(\"off\") plt.title(\"Training Images\") plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0))) 实现方法（implementation） 随着我们变量的设置以及数据集的准备，我们将开始详细的介绍权重初始化策略、生成器、判别器、损失函数以及训练过程。 权重初始化 在DCGAN论文中，作者指出所有模型权重应从均值为0方差为0.2的正态分布随机初始化。 weights_init函数将未初始化模型作为输入，并初始化所有卷积，卷积转置和批标准化层以满足此标准。 初始化后立即将此功能应用于模型。 # 在netG和netD上调用的自定义权重初始化函数 def weights_init(m): classname = m.__class__.__name__ if classname.find('Conv') != -1: nn.init.normal_(m.weight.data, 0.0, 0.02) elif classname.find('BatchNorm') != -1: nn.init.normal_(m.weight.data, 1.0, 0.02) nn.init.constant_(m.bias.data, 0) 生成器 生成器 G 用于将本征向量 z 映射到数据空间。 由于我们的数据是图像，因此将 z 转换为数据空间意味着最终创建一个与训练图像大小相同的RGB图像（即3x64x64）。 实际上，这是通过一系列跨步的二维卷积转置层实现的，每个转换层与二维批标准化层和relu激活层配对。 生成器的输出通过tanh层，使其输出数据范围和输入图片一样，在 [-1, 1] 之间。 值得注意的是在转换层之后存在批标准化函数，因为这是DCGAN论文的关键贡献。 这些层有助于训练期间的梯度传播。 DCGAN论文中的生成器图片如下所示。 请注意，我们在变量定义部分 (nz, ngf 和 nc) 中设置的输入如何影响代码中的生成器体系结构。 nz 是z输入向量的长度，ngf 生成器要生成的特征图个数大小，nc 是输出图像中的通道数（对于RGB图像，设置为3）。 下面是生成器的代码。 # 生成器代码 class Generator(nn.Module): def __init__(self, ngpu): super(Generator, self).__init__() self.ngpu = ngpu self.main = nn.Sequential( # 输入是 Z, 对Z进行卷积 nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False), nn.BatchNorm2d(ngf * 8), nn.ReLU(True), # 输入特征图大小. (ngf*8) x 4 x 4 nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 4), nn.ReLU(True), # 输入特征图大小. (ngf*4) x 8 x 8 nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 2), nn.ReLU(True), # 输入特征图大小. (ngf*2) x 16 x 16 nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf), nn.ReLU(True), # 输入特征图大小. (ngf) x 32 x 32 nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False), nn.Tanh() # 输入特征图大小. (nc) x 64 x 64 ) def forward(self, input): return self.main(input) 现在，我们可以实例化生成器并对其使用 weights_init 函数。打印出生成器模型，用以查看生成器的结构。 # 创建生成器 netG = Generator(ngpu).to(device) # 如果期望使用多个GPU，设置一下。 if (device.type == 'cuda') and (ngpu > 1): netG = nn.DataParallel(netG, list(range(ngpu))) # 使用权重初始化函数 weights_init 去随机初始化所有权重 # mean=0, stdev=0.2. netG.apply(weights_init) # 输出该模型 print(netG) Out: Generator( (main): Sequential( (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace) (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace) (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace) (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (11): ReLU(inplace) (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (13): Tanh() ) ) 判别器 如上所述，判别器 D 是一个二分类网络，它将图像作为输入并输出输入图像是真实的概率（而不是假的）。 这里，D 采用3x64x64输入图像，通过一系列Conv2d，BatchNorm2d和LeakyReLU层处理它，并通过Sigmoid激活函数输出最终概率。 如果问题需要，可以使用更多层扩展此体系结构，但使用跨步卷积，BatchNorm和LeakyReLU具有重要意义。 DCGAN论文提到使用跨步卷积而不是使用pooling下采样是一种很好的做法，因为它可以让网络学习自己的pooling功能。批标准化和LeakyReLU函数也促进了健康的梯度流动，这对于 G 和 D 的学习过程至关重要。 判别器代码 class Discriminator(nn.Module): def __init__(self, ngpu): super(Discriminator, self).__init__() self.ngpu = ngpu self.main = nn.Sequential( # 输入大小 (nc) x 64 x 64 nn.Conv2d(nc, ndf, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, inplace=True), # state size. (ndf) x 32 x 32 nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 2), nn.LeakyReLU(0.2, inplace=True), # 输入大小. (ndf*2) x 16 x 16 nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 4), nn.LeakyReLU(0.2, inplace=True), # 输入大小. (ndf*4) x 8 x 8 nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 8), nn.LeakyReLU(0.2, inplace=True), # 输入大小. (ndf*8) x 4 x 4 nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False), nn.Sigmoid() ) def forward(self, input): return self.main(input) 现在，我们可以实例化判别器并对其应用weights_init函数。查看打印的模型以查看判别器对象的结构。 # 创建判别器 netD = Discriminator(ngpu).to(device) # 如果期望使用多GPU，设置一下 if (device.type == 'cuda') and (ngpu > 1): netD = nn.DataParallel(netD, list(range(ngpu))) # 使用权重初始化函数 weights_init 去随机初始化所有权重 # mean=0, stdev=0.2. netD.apply(weights_init) # 输出该模型 print(netD) Out: Discriminator( (main): Sequential( (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): LeakyReLU(negative_slope=0.2, inplace) (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (4): LeakyReLU(negative_slope=0.2, inplace) (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): LeakyReLU(negative_slope=0.2, inplace) (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (10): LeakyReLU(negative_slope=0.2, inplace) (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False) (12): Sigmoid() ) ) 损失函数和优化器 随着对判别器 D 和生成器 G 完成了设置， 我们能够详细的叙述它们怎么通过损失函数和优化器来进行学习的。我们将使用Binary Cross Entropy loss (BCELoss) 函数，其在pyTorch中的定义如下： \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right] 需要注意的是目标函数中两个log部分是怎么提供计算的(i.e. log(D(x)) and log(1-D(G(z))) 。 即将介绍的训练循环中我们将详细的介绍BCE公式的怎么使用输入 y 的。但重要的是要了解我们如何通过改变 y（即GT标签）来选择我们想要计算的部分损失。 下一步，我们定义真实图片标记为1，假图片标记为0。这个标记将在计算 D 和 G 的损失函数的时候使用，这是在原始的GAN文献中使用的惯例。最后我们设置两个单独的优化器，一个给判别器 D 使用，一个给生成器 G 使用。 就像DCGAN文章中说的那样，两个Adam优化算法都是用学习率为0.0002以及Beta1参数为0.5。为了保存追踪生成器学习的过程，我们将生成一个批固定不变的来自于高斯分布的本征向量(例如 fixed_noise)。在训练的循环中，我们将周期性的输入这个fixed_noise到生成器 G 中， 在训练都完成后我们将看一下由fixed_noise生成的图片。 # 初始化 BCE损失函数 criterion = nn.BCELoss() # 创建一个批次的本征向量用于可视化生成器训练的过程。 fixed_noise = torch.randn(64, nz, 1, 1, device=device) # 建立一个在训练中使用的真实和假的标记 real_label = 1 fake_label = 0 # 为G和D都设置Adam优化器 optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999)) optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999)) 训练 最后，既然已经定义了GAN框架的所有部分，我们就可以对其进行训练。 请注意，训练GAN在某种程度上是一种艺术形式，因为不正确的超参数设置会导致mode collapse，而对错误的解释很少。 在这里，我们将密切关注Goodfellow的论文中的算法1，同时遵守ganhacks中显示的一些最佳实践。 也就是说，我们将“为真实和假冒”图像构建不同的小批量，并调整G的目标函数以最大化logD(G(z))。 训练分为两个主要部分。 第1部分更新判别器Discriminator，第2部分更新生成器Generator。 Part 1 - 训练判别器 回想一下，训练判别器的目的是最大化将给定输入正确分类为真实或假的概率。 就Goodfellow而言，我们希望“通过提升其随机梯度来更新判别器”。 实际上，我们想要最大化损失log(D(x))+ log(1-D(G(z)))。 由于ganhacks的单独小批量建议，我们将分两步计算。 首先，我们将从训练集中构造一批实际样本，向前通过 D，计算损失（log(D(x))），然后计算梯度 向后传递。 其次，我们将用当前的生成器构造一批假样本，通过D 转发该批次，计算损失（log(1-D(G(z)))）和 accumulate 带有向后传递。 现在，随着从全真实和全假批量累积的梯度，我们称之为Discriminator优化器的一步。 Part 2 - 训练生成器 正如原始论文所述，我们希望通过最小化 log(1-D(G(z))) 来训练生成器Generator，以便产生更好的假样本。 如上所述，Goodfellow表明这不会提供足够的梯度，尤其是在学习过程的早期阶段。 作为修改，我们希望最大化 log(D(G(z)))。 在代码中，我们通过以下方式实现此目的：使用 Discriminator 对第1部分的 Generator 输出进行分类，使用真实标签作为 GT计算G的损失，在反向传递中计算G的梯度，最后使用优化器步骤更新G的参数。使用真实标签作为损失函数的GT标签似乎是违反直觉的，但这允许我们使用BCELoss的 log(x) 部分（而不是 log(1-x) 这部分）这正是我们想要的。 最后，我们将进行一些统计报告，在每个循环结束时，我们将通过生成器推送我们的fixed_noise批次，以直观地跟踪G训练的进度。 报告的训练统计数据是： Loss_D - 判别器损失是所有真实样本批次和所有假样本批次的损失之和 log(D(x)) + log(D(G(z))) . Loss_G - 生成器损失 log(D(G(z))) D(x) - 所有真实批次的判别器的平均输出（整批）。 这应该从接近1开始，然后当G变好时理论上收敛到0.5。 想想为什么会这样。 D(G(z)) - 所有假批次的平均判别器输出。 第一个数字是在D更新之前，第二个数字是在D更新之后。 当G变好时，这些数字应该从0开始并收敛到0.5。 想想为什么会这样。 Note: 此步骤可能需要一段时间，具体取决于您运行的循环数以及是否从数据集中删除了一些数据。 # 训练循环 # 保存跟踪进度的列表 img_list = [] G_losses = [] D_losses = [] iters = 0 print(\"Starting Training Loop...\") # 每个epoh for epoch in range(num_epochs): # 数据加载器中的每个批次 for i, data in enumerate(dataloader, 0): ############################ # (1) 更新 D 网络: 最大化 log(D(x)) + log(1 - D(G(z))) ########################### ## 使用所有真实样本批次训练 netD.zero_grad() # 格式化批 real_cpu = data[0].to(device) b_size = real_cpu.size(0) label = torch.full((b_size,), real_label, device=device) # 通过D向前传递真实批次 output = netD(real_cpu).view(-1) # 对所有真实样本批次计算损失 errD_real = criterion(output, label) # 计算后向传递中D的梯度 errD_real.backward() D_x = output.mean().item() ## 使用所有假样本批次训练 # 生成本征向量批次 noise = torch.randn(b_size, nz, 1, 1, device=device) # 使用生成器G生成假图片 fake = netG(noise) label.fill_(fake_label) # 使用判别器分类所有的假批次样本 output = netD(fake.detach()).view(-1) # 计算判别器D的损失对所有的假样本批次 errD_fake = criterion(output, label) # 对这个批次计算梯度 errD_fake.backward() D_G_z1 = output.mean().item() # 把所有真样本和假样本批次的梯度加起来 errD = errD_real + errD_fake # 更新判别器D optimizerD.step() ############################ # (2) 更新 G 网络: 最大化 log(D(G(z))) ########################### netG.zero_grad() label.fill_(real_label) # 假样本的标签对于生成器成本是真的 # 因为我们之更新了D，通过D执行所有假样本批次的正向传递 output = netD(fake).view(-1) # 基于这个输出计算G的损失 errG = criterion(output, label) # 为生成器计算梯度 errG.backward() D_G_z2 = output.mean().item() # 更新生成器G optimizerG.step() # 输出训练状态 if i % 50 == 0: print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f' % (epoch, num_epochs, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2)) # 为以后画损失图，保存损失 G_losses.append(errG.item()) D_losses.append(errD.item()) # 检查生成器generator做了什么，通过保存的fixed_noise通过G的输出 if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)): with torch.no_grad(): fake = netG(fixed_noise).detach().cpu() img_list.append(vutils.make_grid(fake, padding=2, normalize=True)) iters += 1 Out: Starting Training Loop... [0/5][0/1583] Loss_D: 1.7410 Loss_G: 4.7761 D(x): 0.5343 D(G(z)): 0.5771 / 0.0136 [0/5][50/1583] Loss_D: 1.7332 Loss_G: 25.4829 D(x): 0.9774 D(G(z)): 0.7441 / 0.0000 [0/5][100/1583] Loss_D: 1.6841 Loss_G: 11.6585 D(x): 0.4728 D(G(z)): 0.0000 / 0.0000 [0/5][150/1583] Loss_D: 1.2547 Loss_G: 8.7245 D(x): 0.9286 D(G(z)): 0.5209 / 0.0044 [0/5][200/1583] Loss_D: 0.7563 Loss_G: 8.9600 D(x): 0.9525 D(G(z)): 0.4514 / 0.0003 [0/5][250/1583] Loss_D: 1.0221 Loss_G: 2.5713 D(x): 0.5274 D(G(z)): 0.0474 / 0.1177 [0/5][300/1583] Loss_D: 0.3387 Loss_G: 3.8185 D(x): 0.8431 D(G(z)): 0.1066 / 0.0461 [0/5][350/1583] Loss_D: 0.5054 Loss_G: 3.6141 D(x): 0.7289 D(G(z)): 0.0758 / 0.0535 [0/5][400/1583] Loss_D: 0.8758 Loss_G: 6.5680 D(x): 0.8097 D(G(z)): 0.4017 / 0.0031 [0/5][450/1583] Loss_D: 0.2486 Loss_G: 3.5121 D(x): 0.9035 D(G(z)): 0.1054 / 0.0717 [0/5][500/1583] Loss_D: 1.5792 Loss_G: 4.3590 D(x): 0.3457 D(G(z)): 0.0053 / 0.0379 [0/5][550/1583] Loss_D: 0.8897 Loss_G: 3.9447 D(x): 0.5350 D(G(z)): 0.0349 / 0.0386 [0/5][600/1583] Loss_D: 0.5292 Loss_G: 4.4346 D(x): 0.8914 D(G(z)): 0.2768 / 0.0233 [0/5][650/1583] Loss_D: 0.3779 Loss_G: 4.7253 D(x): 0.7868 D(G(z)): 0.0627 / 0.0174 [0/5][700/1583] Loss_D: 0.7512 Loss_G: 2.6246 D(x): 0.6112 D(G(z)): 0.0244 / 0.1493 [0/5][750/1583] Loss_D: 0.4378 Loss_G: 5.0045 D(x): 0.8614 D(G(z)): 0.2028 / 0.0108 [0/5][800/1583] Loss_D: 0.5795 Loss_G: 6.0537 D(x): 0.8693 D(G(z)): 0.2732 / 0.0066 [0/5][850/1583] Loss_D: 0.8980 Loss_G: 6.5355 D(x): 0.8465 D(G(z)): 0.4226 / 0.0048 [0/5][900/1583] Loss_D: 0.5776 Loss_G: 7.7162 D(x): 0.9756 D(G(z)): 0.3707 / 0.0009 [0/5][950/1583] Loss_D: 0.5593 Loss_G: 5.6692 D(x): 0.9560 D(G(z)): 0.3494 / 0.0080 [0/5][1000/1583] Loss_D: 0.5036 Loss_G: 5.1312 D(x): 0.7775 D(G(z)): 0.0959 / 0.0178 [0/5][1050/1583] Loss_D: 0.5192 Loss_G: 4.5706 D(x): 0.8578 D(G(z)): 0.2605 / 0.0222 [0/5][1100/1583] Loss_D: 0.5645 Loss_G: 3.1618 D(x): 0.7133 D(G(z)): 0.1138 / 0.0768 [0/5][1150/1583] Loss_D: 0.2790 Loss_G: 4.5294 D(x): 0.8541 D(G(z)): 0.0909 / 0.0207 [0/5][1200/1583] Loss_D: 0.5334 Loss_G: 4.3445 D(x): 0.8567 D(G(z)): 0.2457 / 0.0245 [0/5][1250/1583] Loss_D: 0.7318 Loss_G: 2.2779 D(x): 0.6846 D(G(z)): 0.1485 / 0.1497 [0/5][1300/1583] Loss_D: 0.6939 Loss_G: 6.1172 D(x): 0.9123 D(G(z)): 0.3853 / 0.0041 [0/5][1350/1583] Loss_D: 0.4653 Loss_G: 3.7054 D(x): 0.8208 D(G(z)): 0.1774 / 0.0404 [0/5][1400/1583] Loss_D: 1.9711 Loss_G: 3.1569 D(x): 0.2704 D(G(z)): 0.0108 / 0.1390 [0/5][1450/1583] Loss_D: 0.4427 Loss_G: 5.8683 D(x): 0.9230 D(G(z)): 0.2600 / 0.0056 [0/5][1500/1583] Loss_D: 0.4432 Loss_G: 3.3681 D(x): 0.8001 D(G(z)): 0.1510 / 0.0633 [0/5][1550/1583] Loss_D: 0.4852 Loss_G: 3.2790 D(x): 0.7532 D(G(z)): 0.1100 / 0.0661 [1/5][0/1583] Loss_D: 0.3536 Loss_G: 4.5358 D(x): 0.8829 D(G(z)): 0.1714 / 0.0173 [1/5][50/1583] Loss_D: 0.4717 Loss_G: 4.7728 D(x): 0.8973 D(G(z)): 0.2750 / 0.0142 [1/5][100/1583] Loss_D: 0.4702 Loss_G: 2.3528 D(x): 0.7847 D(G(z)): 0.1468 / 0.1385 [1/5][150/1583] Loss_D: 0.4833 Loss_G: 2.9645 D(x): 0.7893 D(G(z)): 0.1607 / 0.0867 [1/5][200/1583] Loss_D: 0.6035 Loss_G: 2.0728 D(x): 0.6646 D(G(z)): 0.0852 / 0.1806 [1/5][250/1583] Loss_D: 0.3822 Loss_G: 3.1946 D(x): 0.7969 D(G(z)): 0.1024 / 0.0656 [1/5][300/1583] Loss_D: 0.3892 Loss_G: 3.3337 D(x): 0.7848 D(G(z)): 0.0969 / 0.0525 [1/5][350/1583] Loss_D: 1.7989 Loss_G: 7.5798 D(x): 0.9449 D(G(z)): 0.7273 / 0.0011 [1/5][400/1583] Loss_D: 0.4765 Loss_G: 3.0655 D(x): 0.7479 D(G(z)): 0.1116 / 0.0687 [1/5][450/1583] Loss_D: 0.3649 Loss_G: 3.1674 D(x): 0.8603 D(G(z)): 0.1619 / 0.0627 [1/5][500/1583] Loss_D: 0.6922 Loss_G: 4.5841 D(x): 0.9235 D(G(z)): 0.4003 / 0.0175 [1/5][550/1583] Loss_D: 0.6126 Loss_G: 4.6642 D(x): 0.8761 D(G(z)): 0.3199 / 0.0180 [1/5][600/1583] Loss_D: 0.7032 Loss_G: 4.6221 D(x): 0.9463 D(G(z)): 0.4365 / 0.0154 [1/5][650/1583] Loss_D: 0.4707 Loss_G: 3.3616 D(x): 0.7664 D(G(z)): 0.1280 / 0.0617 [1/5][700/1583] Loss_D: 0.3393 Loss_G: 2.4236 D(x): 0.9120 D(G(z)): 0.1771 / 0.1280 [1/5][750/1583] Loss_D: 0.6828 Loss_G: 4.4585 D(x): 0.8647 D(G(z)): 0.3546 / 0.0191 [1/5][800/1583] Loss_D: 0.7958 Loss_G: 3.6708 D(x): 0.8386 D(G(z)): 0.3987 / 0.0403 [1/5][850/1583] Loss_D: 0.4651 Loss_G: 2.7477 D(x): 0.7602 D(G(z)): 0.1334 / 0.0900 [1/5][900/1583] Loss_D: 0.8799 Loss_G: 4.7930 D(x): 0.9050 D(G(z)): 0.4710 / 0.0201 [1/5][950/1583] Loss_D: 0.3909 Loss_G: 2.7973 D(x): 0.7730 D(G(z)): 0.0902 / 0.0838 [1/5][1000/1583] Loss_D: 0.3822 Loss_G: 3.0223 D(x): 0.8699 D(G(z)): 0.1837 / 0.0709 [1/5][1050/1583] Loss_D: 0.4689 Loss_G: 2.2831 D(x): 0.7096 D(G(z)): 0.0536 / 0.1448 [1/5][1100/1583] Loss_D: 0.6676 Loss_G: 2.2773 D(x): 0.6669 D(G(z)): 0.1386 / 0.1443 [1/5][1150/1583] Loss_D: 0.5970 Loss_G: 4.1558 D(x): 0.9166 D(G(z)): 0.3554 / 0.0240 [1/5][1200/1583] Loss_D: 0.3622 Loss_G: 3.5782 D(x): 0.8590 D(G(z)): 0.1547 / 0.0481 [1/5][1250/1583] Loss_D: 0.5234 Loss_G: 2.5915 D(x): 0.7811 D(G(z)): 0.1990 / 0.1037 [1/5][1300/1583] Loss_D: 1.3243 Loss_G: 5.5428 D(x): 0.9882 D(G(z)): 0.6572 / 0.0088 [1/5][1350/1583] Loss_D: 0.4891 Loss_G: 1.9552 D(x): 0.7686 D(G(z)): 0.1540 / 0.1910 [1/5][1400/1583] Loss_D: 0.5639 Loss_G: 3.7796 D(x): 0.9137 D(G(z)): 0.3390 / 0.0343 [1/5][1450/1583] Loss_D: 1.7329 Loss_G: 5.0373 D(x): 0.9760 D(G(z)): 0.7332 / 0.0161 [1/5][1500/1583] Loss_D: 0.7999 Loss_G: 3.7268 D(x): 0.9029 D(G(z)): 0.4550 / 0.0384 [1/5][1550/1583] Loss_D: 0.4740 Loss_G: 2.3220 D(x): 0.7824 D(G(z)): 0.1625 / 0.1327 [2/5][0/1583] Loss_D: 0.8693 Loss_G: 3.8890 D(x): 0.9376 D(G(z)): 0.4822 / 0.0339 [2/5][50/1583] Loss_D: 0.3742 Loss_G: 2.5041 D(x): 0.8148 D(G(z)): 0.1310 / 0.1151 [2/5][100/1583] Loss_D: 1.1134 Loss_G: 1.5167 D(x): 0.4248 D(G(z)): 0.0335 / 0.3023 [2/5][150/1583] Loss_D: 0.5987 Loss_G: 3.2047 D(x): 0.8536 D(G(z)): 0.3121 / 0.0555 [2/5][200/1583] Loss_D: 2.0846 Loss_G: 1.5473 D(x): 0.1919 D(G(z)): 0.0054 / 0.2899 [2/5][250/1583] Loss_D: 0.5017 Loss_G: 3.0225 D(x): 0.8965 D(G(z)): 0.2986 / 0.0626 [2/5][300/1583] Loss_D: 1.3296 Loss_G: 4.1927 D(x): 0.9444 D(G(z)): 0.6574 / 0.0270 [2/5][350/1583] Loss_D: 0.4905 Loss_G: 2.7693 D(x): 0.8049 D(G(z)): 0.2090 / 0.0863 [2/5][400/1583] Loss_D: 0.4668 Loss_G: 2.1790 D(x): 0.7160 D(G(z)): 0.0815 / 0.1529 [2/5][450/1583] Loss_D: 0.4877 Loss_G: 2.4190 D(x): 0.6943 D(G(z)): 0.0693 / 0.1254 [2/5][500/1583] Loss_D: 0.7856 Loss_G: 2.2362 D(x): 0.6148 D(G(z)): 0.1698 / 0.1489 [2/5][550/1583] Loss_D: 0.6371 Loss_G: 1.3879 D(x): 0.6164 D(G(z)): 0.0852 / 0.3041 [2/5][600/1583] Loss_D: 0.6409 Loss_G: 2.8623 D(x): 0.7658 D(G(z)): 0.2684 / 0.0790 [2/5][650/1583] Loss_D: 0.6454 Loss_G: 1.5708 D(x): 0.6293 D(G(z)): 0.0944 / 0.2706 [2/5][700/1583] Loss_D: 0.8472 Loss_G: 2.0847 D(x): 0.5071 D(G(z)): 0.0181 / 0.1937 [2/5][750/1583] Loss_D: 1.2356 Loss_G: 0.3673 D(x): 0.3606 D(G(z)): 0.0328 / 0.7270 [2/5][800/1583] Loss_D: 0.4852 Loss_G: 2.7325 D(x): 0.8670 D(G(z)): 0.2630 / 0.0877 [2/5][850/1583] Loss_D: 0.6494 Loss_G: 4.5357 D(x): 0.8899 D(G(z)): 0.3756 / 0.0158 [2/5][900/1583] Loss_D: 0.5184 Loss_G: 2.7194 D(x): 0.8377 D(G(z)): 0.2540 / 0.0871 [2/5][950/1583] Loss_D: 0.9771 Loss_G: 4.6200 D(x): 0.9596 D(G(z)): 0.5432 / 0.0176 [2/5][1000/1583] Loss_D: 0.7509 Loss_G: 2.2864 D(x): 0.5861 D(G(z)): 0.1021 / 0.1539 [2/5][1050/1583] Loss_D: 0.4512 Loss_G: 3.2484 D(x): 0.8649 D(G(z)): 0.2313 / 0.0542 [2/5][1100/1583] Loss_D: 0.6856 Loss_G: 2.2425 D(x): 0.6405 D(G(z)): 0.1333 / 0.1508 [2/5][1150/1583] Loss_D: 0.5271 Loss_G: 3.0327 D(x): 0.8385 D(G(z)): 0.2552 / 0.0639 [2/5][1200/1583] Loss_D: 0.4058 Loss_G: 2.9557 D(x): 0.8769 D(G(z)): 0.2169 / 0.0694 [2/5][1250/1583] Loss_D: 0.5564 Loss_G: 2.9065 D(x): 0.8409 D(G(z)): 0.2835 / 0.0695 [2/5][1300/1583] Loss_D: 0.4703 Loss_G: 2.7865 D(x): 0.7825 D(G(z)): 0.1680 / 0.0850 [2/5][1350/1583] Loss_D: 0.5352 Loss_G: 3.1362 D(x): 0.8260 D(G(z)): 0.2582 / 0.0606 [2/5][1400/1583] Loss_D: 0.5281 Loss_G: 2.7742 D(x): 0.7970 D(G(z)): 0.2275 / 0.0835 [2/5][1450/1583] Loss_D: 0.6558 Loss_G: 1.8152 D(x): 0.6103 D(G(z)): 0.0795 / 0.2030 [2/5][1500/1583] Loss_D: 0.9446 Loss_G: 1.1492 D(x): 0.4593 D(G(z)): 0.0356 / 0.3947 [2/5][1550/1583] Loss_D: 0.9269 Loss_G: 0.7383 D(x): 0.5226 D(G(z)): 0.1333 / 0.5205 [3/5][0/1583] Loss_D: 0.4855 Loss_G: 2.1548 D(x): 0.7157 D(G(z)): 0.1059 / 0.1568 [3/5][50/1583] Loss_D: 0.7259 Loss_G: 1.1093 D(x): 0.5804 D(G(z)): 0.0797 / 0.3894 [3/5][100/1583] Loss_D: 0.7367 Loss_G: 1.0389 D(x): 0.5515 D(G(z)): 0.0405 / 0.4190 [3/5][150/1583] Loss_D: 0.5942 Loss_G: 3.4803 D(x): 0.9290 D(G(z)): 0.3709 / 0.0432 [3/5][200/1583] Loss_D: 1.3464 Loss_G: 0.6549 D(x): 0.3261 D(G(z)): 0.0242 / 0.5949 [3/5][250/1583] Loss_D: 0.5110 Loss_G: 2.2086 D(x): 0.7263 D(G(z)): 0.1327 / 0.1457 [3/5][300/1583] Loss_D: 1.4272 Loss_G: 3.3018 D(x): 0.9230 D(G(z)): 0.6654 / 0.0635 [3/5][350/1583] Loss_D: 0.6491 Loss_G: 3.0766 D(x): 0.8124 D(G(z)): 0.3127 / 0.0607 [3/5][400/1583] Loss_D: 0.5583 Loss_G: 2.9363 D(x): 0.8233 D(G(z)): 0.2759 / 0.0666 [3/5][450/1583] Loss_D: 0.9496 Loss_G: 0.6436 D(x): 0.4958 D(G(z)): 0.1367 / 0.5538 [3/5][500/1583] Loss_D: 0.4463 Loss_G: 2.2234 D(x): 0.7776 D(G(z)): 0.1545 / 0.1371 [3/5][550/1583] Loss_D: 0.5874 Loss_G: 3.6688 D(x): 0.8478 D(G(z)): 0.2930 / 0.0348 [3/5][600/1583] Loss_D: 0.3724 Loss_G: 2.6326 D(x): 0.8673 D(G(z)): 0.1854 / 0.0891 [3/5][650/1583] Loss_D: 0.7292 Loss_G: 4.4254 D(x): 0.9081 D(G(z)): 0.4234 / 0.0200 [3/5][700/1583] Loss_D: 0.4728 Loss_G: 2.8665 D(x): 0.8189 D(G(z)): 0.2115 / 0.0774 [3/5][750/1583] Loss_D: 0.5845 Loss_G: 3.3046 D(x): 0.8977 D(G(z)): 0.3490 / 0.0463 [3/5][800/1583] Loss_D: 0.5597 Loss_G: 2.2564 D(x): 0.7088 D(G(z)): 0.1497 / 0.1300 [3/5][850/1583] Loss_D: 0.6518 Loss_G: 2.5048 D(x): 0.7195 D(G(z)): 0.2183 / 0.1053 [3/5][900/1583] Loss_D: 0.7340 Loss_G: 1.4263 D(x): 0.6285 D(G(z)): 0.1806 / 0.2818 [3/5][950/1583] Loss_D: 1.4633 Loss_G: 4.9204 D(x): 0.9792 D(G(z)): 0.7093 / 0.0143 [3/5][1000/1583] Loss_D: 0.6643 Loss_G: 2.8332 D(x): 0.8548 D(G(z)): 0.3597 / 0.0751 [3/5][1050/1583] Loss_D: 0.7741 Loss_G: 2.9355 D(x): 0.7281 D(G(z)): 0.3064 / 0.0712 [3/5][1100/1583] Loss_D: 0.7279 Loss_G: 3.2299 D(x): 0.8867 D(G(z)): 0.4193 / 0.0544 [3/5][1150/1583] Loss_D: 0.6049 Loss_G: 1.9150 D(x): 0.6917 D(G(z)): 0.1645 / 0.1912 [3/5][1200/1583] Loss_D: 0.7431 Loss_G: 3.8188 D(x): 0.9334 D(G(z)): 0.4500 / 0.0306 [3/5][1250/1583] Loss_D: 0.5061 Loss_G: 1.9905 D(x): 0.7393 D(G(z)): 0.1531 / 0.1653 [3/5][1300/1583] Loss_D: 0.6979 Loss_G: 3.0183 D(x): 0.8182 D(G(z)): 0.3421 / 0.0616 [3/5][1350/1583] Loss_D: 0.9133 Loss_G: 4.0629 D(x): 0.9198 D(G(z)): 0.5131 / 0.0261 [3/5][1400/1583] Loss_D: 0.7075 Loss_G: 4.0061 D(x): 0.9188 D(G(z)): 0.4216 / 0.0266 [3/5][1450/1583] Loss_D: 0.7704 Loss_G: 2.3802 D(x): 0.7555 D(G(z)): 0.3348 / 0.1114 [3/5][1500/1583] Loss_D: 0.6055 Loss_G: 1.8402 D(x): 0.7011 D(G(z)): 0.1643 / 0.1995 [3/5][1550/1583] Loss_D: 0.7240 Loss_G: 3.2589 D(x): 0.8747 D(G(z)): 0.4069 / 0.0528 [4/5][0/1583] Loss_D: 0.8162 Loss_G: 2.8040 D(x): 0.8827 D(G(z)): 0.4435 / 0.0870 [4/5][50/1583] Loss_D: 0.5859 Loss_G: 2.2796 D(x): 0.6782 D(G(z)): 0.1312 / 0.1309 [4/5][100/1583] Loss_D: 0.6655 Loss_G: 3.5365 D(x): 0.8178 D(G(z)): 0.3262 / 0.0394 [4/5][150/1583] Loss_D: 1.8662 Loss_G: 5.4950 D(x): 0.9469 D(G(z)): 0.7590 / 0.0113 [4/5][200/1583] Loss_D: 0.7060 Loss_G: 3.6253 D(x): 0.9215 D(G(z)): 0.4316 / 0.0364 [4/5][250/1583] Loss_D: 0.5589 Loss_G: 2.1394 D(x): 0.7108 D(G(z)): 0.1513 / 0.1548 [4/5][300/1583] Loss_D: 0.7278 Loss_G: 1.2391 D(x): 0.5757 D(G(z)): 0.0987 / 0.3454 [4/5][350/1583] Loss_D: 0.7597 Loss_G: 2.8481 D(x): 0.7502 D(G(z)): 0.3094 / 0.0843 [4/5][400/1583] Loss_D: 0.6167 Loss_G: 2.2143 D(x): 0.6641 D(G(z)): 0.1315 / 0.1405 [4/5][450/1583] Loss_D: 0.6234 Loss_G: 1.7961 D(x): 0.7303 D(G(z)): 0.2208 / 0.2007 [4/5][500/1583] Loss_D: 0.6098 Loss_G: 4.9416 D(x): 0.9442 D(G(z)): 0.3978 / 0.0104 [4/5][550/1583] Loss_D: 0.6570 Loss_G: 3.6935 D(x): 0.9180 D(G(z)): 0.4015 / 0.0312 [4/5][600/1583] Loss_D: 0.4195 Loss_G: 2.3446 D(x): 0.7798 D(G(z)): 0.1319 / 0.1211 [4/5][650/1583] Loss_D: 0.5291 Loss_G: 2.5303 D(x): 0.7528 D(G(z)): 0.1875 / 0.1075 [4/5][700/1583] Loss_D: 0.5187 Loss_G: 2.0350 D(x): 0.7174 D(G(z)): 0.1431 / 0.1547 [4/5][750/1583] Loss_D: 0.8208 Loss_G: 1.0780 D(x): 0.5665 D(G(z)): 0.1128 / 0.3844 [4/5][800/1583] Loss_D: 0.5223 Loss_G: 3.0140 D(x): 0.8708 D(G(z)): 0.2871 / 0.0612 [4/5][850/1583] Loss_D: 2.9431 Loss_G: 1.0175 D(x): 0.0914 D(G(z)): 0.0162 / 0.4320 [4/5][900/1583] Loss_D: 0.5456 Loss_G: 1.7923 D(x): 0.7489 D(G(z)): 0.1972 / 0.2038 [4/5][950/1583] Loss_D: 0.4718 Loss_G: 2.3825 D(x): 0.7840 D(G(z)): 0.1772 / 0.1172 [4/5][1000/1583] Loss_D: 0.5174 Loss_G: 2.5070 D(x): 0.8367 D(G(z)): 0.2556 / 0.1074 [4/5][1050/1583] Loss_D: 0.8214 Loss_G: 0.8055 D(x): 0.5181 D(G(z)): 0.0694 / 0.4963 [4/5][1100/1583] Loss_D: 1.3243 Loss_G: 0.7562 D(x): 0.3284 D(G(z)): 0.0218 / 0.5165 [4/5][1150/1583] Loss_D: 0.9334 Loss_G: 5.1260 D(x): 0.8775 D(G(z)): 0.4817 / 0.0088 [4/5][1200/1583] Loss_D: 0.5141 Loss_G: 2.7230 D(x): 0.8067 D(G(z)): 0.2188 / 0.0872 [4/5][1250/1583] Loss_D: 0.6007 Loss_G: 1.9893 D(x): 0.6968 D(G(z)): 0.1667 / 0.1748 [4/5][1300/1583] Loss_D: 0.4025 Loss_G: 2.3066 D(x): 0.8101 D(G(z)): 0.1471 / 0.1412 [4/5][1350/1583] Loss_D: 0.5979 Loss_G: 3.2825 D(x): 0.8248 D(G(z)): 0.3003 / 0.0509 [4/5][1400/1583] Loss_D: 0.7430 Loss_G: 3.6521 D(x): 0.8888 D(G(z)): 0.4243 / 0.0339 [4/5][1450/1583] Loss_D: 1.0814 Loss_G: 5.4255 D(x): 0.9647 D(G(z)): 0.5842 / 0.0070 [4/5][1500/1583] Loss_D: 1.7211 Loss_G: 0.7875 D(x): 0.2588 D(G(z)): 0.0389 / 0.5159 [4/5][1550/1583] Loss_D: 0.5871 Loss_G: 2.1340 D(x): 0.7332 D(G(z)): 0.1982 / 0.1518 结果 最后，让我们看看我们做的怎么样。 在这里，我们将看看三个不同的结果。 首先，我们将看到判别器D和生成器G的损失在训练期间是如何变化的。 其次，我们将在每个批次可视化生成器G的输出。 第三，我们将查看一批实际数据以及来自生成器G一批假数据。 损失与训练迭代次数关系图 下面将绘制生成器和判别器的损失和训练迭代次数关系图。 plt.figure(figsize=(10,5)) plt.title(\"Generator and Discriminator Loss During Training\") plt.plot(G_losses,label=\"G\") plt.plot(D_losses,label=\"D\") plt.xlabel(\"iterations\") plt.ylabel(\"Loss\") plt.legend() plt.show() 生成器G的训练进度 我们在每一个批次训练完成之后都保存了生成器的输出。 现在我们可以通过动画可视化生成器G的训练进度。点击播放按钮开始动画. #%%capture fig = plt.figure(figsize=(8,8)) plt.axis(\"off\") ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list] ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True) HTML(ani.to_jshtml()) 真实图像 vs. 假图像 最后，让我们一起看看一些真实的图像和假图像。 # 从数据加载器中获取一批真实图像 real_batch = next(iter(dataloader)) # 画出真实图像 plt.figure(figsize=(15,15)) plt.subplot(1,2,1) plt.axis(\"off\") plt.title(\"Real Images\") plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0))) # 画出来自最后一次训练的假图像 plt.subplot(1,2,2) plt.axis(\"off\") plt.title(\"Fake Images\") plt.imshow(np.transpose(img_list[-1],(1,2,0))) plt.show() 下一步计划 我们已经到了教程的最后，但是你可以根据此教程研究以下内容： 训练更长的时间看看能够达到多好的结果 调整此模型以适合不同的数据集，如果可能你可以更改输入图片大小以及模型的架构 看看这里其他一些很酷的GAN项目 创建一个能够产生音乐的GAN模型 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"tut_reinforcement_learning.html":{"url":"tut_reinforcement_learning.html","title":"强化学习","keywords":"","body":"强化学习 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"reinforcement_q_learning.html":{"url":"reinforcement_q_learning.html","title":"Reinforcement Learning (DQN) Tutorial","keywords":"","body":"强化学习 (DQN) 教程 译者：平淡的天 作者: Adam Paszke 本教程将展示如何使用 PyTorch 在OpenAI Gym的任务集上训练一个深度Q学习 (DQN) 智能点。 任务 智能点需要决定两种动作：向左或向右来使其上的杆保持直立。你可以在 Gym website 找到一个有各种算法和可视化的官方排行榜。 当智能点观察环境的当前状态并选择动作时，环境将转换为新状态，并返回指示动作结果的奖励。在这项任务中，每增加一个时间步，奖励+1，如果杆子掉得太远或大车移动距离中心超过2.4个单位，环境就会终止。这意味着更好的执行场景将持续更长的时间，积累更大的回报。 Cartpole任务的设计为智能点输入代表环境状态（位置、速度等）的4个实际值。然而，神经网络完全可以通过观察场景来解决这个任务，所以我们将使用以车为中心的一块屏幕作为输入。因此，我们的结果无法直接与官方排行榜上的结果相比——我们的任务更艰巨。不幸的是，这会减慢训练速度，因为我们必须渲染所有帧。 严格地说，我们将以当前帧和前一个帧之间的差异来呈现状态。这将允许代理从一张图像中考虑杆子的速度。 包 首先你需要导入必须的包。我们需要 gym 作为环境 (使用 pip install gym 安装). 我们也需要 PyTorch 的如下功能: 神经网络 (torch.nn) 优化 (torch.optim) 自动微分 (torch.autograd) 视觉任务 (torchvision - a separate package). import gym import math import random import numpy as np import matplotlib import matplotlib.pyplot as plt from collections import namedtuple from itertools import count from PIL import Image import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F import torchvision.transforms as T env = gym.make('CartPole-v0').unwrapped # 建立 matplotlib is_ipython = 'inline' in matplotlib.get_backend() if is_ipython: from IPython import display plt.ion() # 如果使用gpu device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") 回放内存 我们将使用经验回放内存来训练DQN。它存储智能点观察到的转换，允许我们稍后重用此数据。通过从中随机抽样，组成批对象的转换将被取消相关性。结果表明，这大大稳定和改进了DQN训练过程。 因此，我们需要两个类别： Transition - 一个命名的元组，表示我们环境中的单个转换。它基本上将（状态、动作）对映射到它们的（下一个状态、奖励）结果，状态是屏幕差分图像，如后面所述。 ReplayMemory - 一个有界大小的循环缓冲区，用于保存最近观察到的转换。它还实现了一个.sample（）方法，用于选择一批随机转换进行训练。 Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward')) class ReplayMemory(object): def __init__(self, capacity): self.capacity = capacity self.memory = [] self.position = 0 def push(self, *args): \"\"\"保存变换\"\"\" if len(self.memory) 现在我们来定义自己的模型。但首先来快速了解一下DQN。 DQN 算法 我们的环境是确定的，所以这里提出的所有方程也都是确定性的，为了简单起见。在强化学习文献中，它们还包含对环境中随机转换的期望。 我们的目标是制定一项策略，试图最大化折扣、累积奖励 R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t，其中 R_{t_0} 也被认为是返回值。\\gamma 应该是介于 0 和 1 之间的常量，以确保和收敛。它使来自不确定的遥远未来的回报对我们的代理来说比它在不久的将来相当有信心的回报更不重要。 Q-Learning背后的主要思想是，如果我们有一个函数 Q^*: State \\times Action \\rightarrow \\mathbb{R}, 则如果我们在特定的状态下采取行动，那么我们可以很容易地构建一个最大化回报的策略： \\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a) 然而，我们并不了解世界的一切，因此我们无法访问 Q^*。但是，由于神经网络是通用的函数逼近器，我们可以简单地创建一个并训练它类似于 Q^*。 对于我们的训练更新规则，我们将假设某些策略的每个 Q 函数都遵循Bellman方程： Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s')) 等式两边的差异被称为时间差误差，即 \\delta: \\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a)) 为了尽量减少这个错误，我们将使用 Huber loss。Huber损失在误差很小的情况下表现为均方误差，但在误差较大的情况下表现为平均绝对误差——这使得当对 Q 的估计噪音很大时，对异常值的鲁棒性更强。我们通过从重放内存中取样的一批转换来计算 B： \\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta) \\begin{split}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases} \\frac{1}{2}{\\delta^2} & \\text{for } |\\delta| \\le 1, \\\\ |\\delta| - \\frac{1}{2} & \\text{otherwise.} \\end{cases}\\end{split} Q-网络 我们的模型是一个卷积神经网络，它可以处理当前和以前的帧之间的差异。它有两个输出，分别表示Q(s, \\mathrm{left}) 和 Q(s, \\mathrm{right})（其中 s是网络的输入）。实际上，网络正试图预测在给定电流输入的情况下采取每项行动的预期回报。 class DQN(nn.Module): def __init__(self, h, w): super(DQN, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2) self.bn2 = nn.BatchNorm2d(32) self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2) self.bn3 = nn.BatchNorm2d(32) # 线性输入连接的数量取决于conv2d层的输出，因此需要计算输入图像的大小。 def conv2d_size_out(size, kernel_size = 5, stride = 2): return (size - (kernel_size - 1) - 1) // stride + 1 convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w))) convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h))) linear_input_size = convw * convh * 32 self.head = nn.Linear(linear_input_size, 2) # 448 或者 512 # 使用一个元素调用以确定下一个操作，或在优化期间调用批处理。返回张量 def forward(self, x): x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) x = F.relu(self.bn3(self.conv3(x))) return self.head(x.view(x.size(0), -1)) 获取输入 下面的代码是用于从环境中提取和处理渲染图像的实用程序。它使用了torchvision 包，这样就可以很容易地组合图像转换。运行单元后，它将显示它提取的示例帧。 resize = T.Compose([T.ToPILImage(), T.Resize(40, interpolation=Image.CUBIC), T.ToTensor()]) def get_cart_location(screen_width): world_width = env.x_threshold * 2 scale = screen_width / world_width return int(env.state[0] * scale + screen_width / 2.0) # 车子的中心 def get_screen(): # 返回 gym 需要的400x600x3 图片, 但有时会更大，如800x1200x3. 将其转换为torch (CHW). screen = env.render(mode='rgb_array').transpose((2, 0, 1)) # 车子在下半部分，因此请剥去屏幕的顶部和底部。 _, screen_height, screen_width = screen.shape screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)] view_width = int(screen_width * 0.6) cart_location = get_cart_location(screen_width) if cart_location (screen_width - view_width // 2): slice_range = slice(-view_width, None) else: slice_range = slice(cart_location - view_width // 2, cart_location + view_width // 2) # 去掉边缘，这样我们就可以得到一个以车为中心的正方形图像。 screen = screen[:, :, slice_range] # 转化为 float, 重新裁剪, 转化为 torch 张量(这并不需要拷贝) screen = np.ascontiguousarray(screen, dtype=np.float32) / 255 screen = torch.from_numpy(screen) # 重新裁剪,加入批维度 (BCHW) return resize(screen).unsqueeze(0).to(device) env.reset() plt.figure() plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(), interpolation='none') plt.title('Example extracted screen') plt.show() 训练 超参数和配置 此单元实例化模型及其优化器，并定义一些实用程序： select_action - 将根据迭代次数贪婪策略选择一个行动。简单地说，我们有时会使用我们的模型来选择动作，有时我们只会对其中一个进行统一的采样。选择随机动作的概率将从 EPS_START 开始并以指数形式向 EPS_END衰减。 EPS_DECAY 控制衰减速率。 plot_durations - 一个帮助绘制迭代次数持续时间，以及过去100迭代次数的平均值（官方评估中使用的度量）。迭代次数将在包含主训练循环的单元下方，并在每迭代之后更新。 BATCH_SIZE = 128 GAMMA = 0.999 EPS_START = 0.9 EPS_END = 0.05 EPS_DECAY = 200 TARGET_UPDATE = 10 #获取屏幕大小，以便我们可以根据从ai-gym返回的形状正确初始化层。这一点上的典型尺寸接近3x40x90，这是在get_screen（）中抑制和缩小的渲染缓冲区的结果。 init_screen = get_screen() _, _, screen_height, screen_width = init_screen.shape policy_net = DQN(screen_height, screen_width).to(device) target_net = DQN(screen_height, screen_width).to(device) target_net.load_state_dict(policy_net.state_dict()) target_net.eval() optimizer = optim.RMSprop(policy_net.parameters()) memory = ReplayMemory(10000) steps_done = 0 def select_action(state): global steps_done sample = random.random() eps_threshold = EPS_END + (EPS_START - EPS_END) * \\ math.exp(-1. * steps_done / EPS_DECAY) steps_done += 1 if sample > eps_threshold: with torch.no_grad(): # t.max（1）将为每行的列返回最大值。max result的第二列是找到max元素的索引，因此我们选择预期回报较大的操作。 return policy_net(state).max(1)[1].view(1, 1) else: return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long) episode_durations = [] def plot_durations(): plt.figure(2) plt.clf() durations_t = torch.tensor(episode_durations, dtype=torch.float) plt.title('Training...') plt.xlabel('Episode') plt.ylabel('Duration') plt.plot(durations_t.numpy()) # 平均 100 次迭代画一次 if len(durations_t) >= 100: means = durations_t.unfold(0, 100, 1).mean(1).view(-1) means = torch.cat((torch.zeros(99), means)) plt.plot(means.numpy()) plt.pause(0.001) # 暂定一会等待屏幕更新 if is_ipython: display.clear_output(wait=True) display.display(plt.gcf()) 训练循环 最后，训练我们模型的代码。 在这里，您可以找到一个optimize_model函数，它执行优化的一个步骤。它首先对一批数据进行采样，将所有张量连接成一个张量，计算出Q(s_t, a_t) 和 V(s_{t+1}) = \\max_a Q(s_{t+1}, a)，并将它们组合成我们的损失。根据定义，如果 s是结束状态，我们设置 V(s) = 0。我们还使用目标网络来计算V(s_{t+1})`以增加稳定性。目标网络的权重大部分时间保持不变，但每隔一段时间就会更新一次策略网络的权重。这通常是一组步骤，但为了简单起见，我们将使用迭代次数。 def optimize_model(): if len(memory) 接下来，你可以找到主训练循环。开始时，我们重置环境并初始化state张量。然后，我们对一个操作进行采样，执行它，观察下一个屏幕和奖励（总是1），并对我们的模型进行一次优化。当插曲结束（我们的模型失败）时，我们重新启动循环。 num_episodes设置得很小。你可以下载并运行更多的epsiodes，比如300+来进行有意义的持续时间改进。 num_episodes = 50 for i_episode in range(num_episodes): # 初始化环境和状态 env.reset() last_screen = get_screen() current_screen = get_screen() state = current_screen - last_screen for t in count(): # 选择并执行动作 action = select_action(state) _, reward, done, _ = env.step(action.item()) reward = torch.tensor([reward], device=device) # 观察新状态 last_screen = current_screen current_screen = get_screen() if not done: next_state = current_screen - last_screen else: next_state = None # 在内存中储存当前参数 memory.push(state, action, next_state, reward) # 进入下一状态 state = next_state # 记性一步优化 (在目标网络) optimize_model() if done: episode_durations.append(t + 1) plot_durations() break #更新目标网络, 复制在 DQN 中的所有权重偏差 if i_episode % TARGET_UPDATE == 0: target_net.load_state_dict(policy_net.state_dict()) print('Complete') env.render() env.close() plt.ioff() plt.show() 下面是一个图表，它说明了整个结果数据流。 动作可以是随机选择的，也可以是基于一个策略，从gym环境中获取下一步的样本。我们将结果记录在回放内存中，并在每次迭代中运行优化步骤。优化从重放内存中随机抽取一批来训练新策略。“旧的”目标网也用于优化计算预期的Q值；它偶尔会更新以保持其最新。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"tut_extending_pytorch.html":{"url":"tut_extending_pytorch.html","title":"扩展 PyTorch","keywords":"","body":"扩展 PyTorch 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"numpy_extensions_tutorial.html":{"url":"numpy_extensions_tutorial.html","title":"用 numpy 和 scipy 创建扩展","keywords":"","body":"用 numpy 和 scipy 创建扩展 译者：cangyunye 校对者：FontTian 作者: Adam Paszke 修订者: Adam Dziedzic 在这个教程里，我们要完成两个任务: 创建一个无参神经网络层。 这里需要调用numpy作为实现的一部分。 创建一个权重自主优化的神经网络层。 这里需要调用Scipy作为实现的一部分。 import torch from torch.autograd import Function 无参数神经网络层示例 这一层并没有特意做什么任何有用的事或者去进行数学上的修正。 它只是被恰当的命名为BadFFTFunction 本层的实现方式 from numpy.fft import rfft2, irfft2 class BadFFTFunction(Function): def forward(self, input): numpy_input = input.detach().numpy() result = abs(rfft2(numpy_input)) return input.new(result) def backward(self, grad_output): numpy_go = grad_output.numpy() result = irfft2(numpy_go) return grad_output.new(result) # 由于本层没有任何参数，我们可以简单的声明为一个函数，而不是当做 nn.Module 类 def incorrect_fft(input): return BadFFTFunction()(input) 创建无参数神经网络层的示例方法: input = torch.randn(8, 8, requires_grad=True) result = incorrect_fft(input) print(result) result.backward(torch.randn(result.size())) print(input) 输出: tensor([[2.2488e-03, 5.1309e+00, 6.4310e+00, 6.0649e+00, 8.1197e+00], [3.4379e+00, 1.5772e+00, 1.0834e+01, 5.2234e+00, 1.0509e+01], [2.6480e+00, 1.2934e+01, 9.1619e+00, 1.6011e+01, 9.7914e+00], [4.0796e+00, 8.6867e+00, 8.8971e+00, 1.0232e+01, 5.7227e+00], [1.8085e+01, 5.4060e+00, 5.2141e+00, 3.5451e+00, 5.1584e+00], [4.0796e+00, 8.2662e+00, 1.1570e+01, 8.7164e+00, 5.7227e+00], [2.6480e+00, 4.5982e+00, 1.1056e+00, 8.8158e+00, 9.7914e+00], [3.4379e+00, 6.2059e+00, 5.9354e+00, 3.1194e+00, 1.0509e+01]], grad_fn=) tensor([[-0.6461, 0.3270, -1.2190, -0.5480, -1.7273, -0.7326, 0.6294, -0.2311], [ 0.4305, 1.7503, -0.2914, -0.4237, 0.5441, 1.6597, -0.5645, -0.7901], [ 0.4248, -2.5986, -0.9257, -0.8651, -0.1673, 1.5749, -1.1857, 1.2867], [-0.5180, 2.3175, -1.9279, 1.2128, 0.7789, 0.0385, -1.1871, 0.3431], [ 0.6934, 1.0216, -0.7450, 0.0463, -1.5447, -1.5220, 0.9389, -0.5811], [ 1.9286, -1.0957, 0.6878, -0.5469, -0.5505, 0.5088, 0.8965, 0.4874], [-0.2699, 0.3370, 0.3749, -0.3639, -0.0599, 0.8904, 0.1679, -1.8218], [-0.2963, 0.2246, 0.6617, 1.2258, 0.1530, 0.3114, 0.4568, 0.6181]], requires_grad=True) 参数化示例 在深度学习的文献中，这一层被意外的称作卷积convolution，尽管实际操作是交叉-关联性cross-correlation (唯一的区别是过滤器filter是为了卷积而翻转，而不是为了交叉关联)。 本层的可自优化权重的实现，依赖于交叉-关联cross-correlation 一个表示权重的过滤器filter (kernel)。 向后传播的函数backward计算的是输入数据的梯度以及过滤器的梯度。 from numpy import flip import numpy as np from scipy.signal import convolve2d, correlate2d from torch.nn.modules.module import Module from torch.nn.parameter import Parameter class ScipyConv2dFunction(Function): @staticmethod def forward(ctx, input, filter, bias): # detach so we can cast to NumPy input, filter, bias = input.detach(), filter.detach(), bias.detach() result = correlate2d(input.numpy(), filter.numpy(), mode='valid') result += bias.numpy() ctx.save_for_backward(input, filter, bias) return torch.as_tensor(result, dtype=input.dtype) @staticmethod def backward(ctx, grad_output): grad_output = grad_output.detach() input, filter, bias = ctx.saved_tensors grad_output = grad_output.numpy() grad_bias = np.sum(grad_output, keepdims=True) grad_input = convolve2d(grad_output, filter.numpy(), mode='full') # 上一行可以等效表示为: # grad_input = correlate2d(grad_output, flip(flip(filter.numpy(), axis=0), axis=1), mode='full') grad_filter = correlate2d(input.numpy(), grad_output, mode='valid') return torch.from_numpy(grad_input), torch.from_numpy(grad_filter).to(torch.float), torch.from_numpy(grad_bias).to(torch.float) class ScipyConv2d(Module): def __init__(self, filter_width, filter_height): super(ScipyConv2d, self).__init__() self.filter = Parameter(torch.randn(filter_width, filter_height)) self.bias = Parameter(torch.randn(1, 1)) def forward(self, input): return ScipyConv2dFunction.apply(input, self.filter, self.bias) 示例: module = ScipyConv2d(3, 3) print(\"Filter and bias: \", list(module.parameters())) input = torch.randn(10, 10, requires_grad=True) output = module(input) print(\"Output from the convolution: \", output) output.backward(torch.randn(8, 8)) print(\"Gradient for the input map: \", input.grad) 输出： Filter and bias: [Parameter containing: tensor([[-0.8330, 0.3568, 1.3209], [-0.5273, -0.9138, -1.0039], [-1.1179, 1.3722, 1.5137]], requires_grad=True), Parameter containing: tensor([[0.1973]], requires_grad=True)] Output from the convolution: tensor([[-0.7304, -3.5437, 2.4701, 1.0625, -1.8347, 3.3246, 2.5547, -1.1341], [-5.0441, -7.1261, 2.8344, 2.5797, -2.4117, -1.4123, -0.2520, -3.1231], [ 1.2296, -0.7957, 1.9413, 1.5257, 0.2727, 6.2466, 2.3363, 2.1833], [-2.6944, -3.3933, 2.3844, 0.2523, -2.0322, -3.1275, -0.2472, 1.5382], [ 3.6807, -1.1985, -3.9278, 0.8025, 3.3435, 6.6806, 1.1656, 1.3711], [-1.7426, 1.3875, 8.2674, -0.8234, -4.7534, 3.0932, 1.3048, 2.1184], [ 0.2095, 1.3225, 0.9022, 3.3324, 0.8768, -5.3459, -1.0970, -4.5304], [ 2.1688, -1.7967, -0.5568, -9.3585, 0.3259, 5.4264, 2.8449, 6.8120]], grad_fn=) Gradient for the input map: tensor([[ 7.7001e-01, -2.6786e-02, -1.0917e+00, -4.1148e-01, 2.2833e-01, -1.7494e+00, -1.4960e+00, 2.3307e-01, 2.2004e+00, 3.1210e+00], [ 7.0960e-02, 1.8954e+00, 2.0912e+00, -1.3058e+00, -6.1822e-02, 3.8630e+00, -5.1720e-01, -6.9586e+00, -2.5478e+00, -1.4459e+00], [ 9.3677e-01, -7.5248e-01, 3.0795e-03, -2.1788e+00, -2.6326e+00, -3.4089e+00, 2.2524e-01, 4.7127e+00, 3.7717e+00, 2.0393e+00], [-2.0010e+00, 2.7616e+00, 4.0060e+00, -2.0298e+00, 1.6074e+00, 2.3062e+00, -5.4927e+00, -5.3029e+00, 3.5081e+00, 4.5952e+00], [ 3.4492e-01, -2.3043e+00, -1.5235e+00, -3.3520e+00, -1.3291e-01, 1.4629e+00, 1.9298e+00, 4.5369e-01, -1.5986e+00, -2.3851e+00], [-2.3929e+00, 5.3965e+00, 5.1353e+00, -1.0269e+00, 2.1031e+00, -6.2344e+00, -3.6539e+00, -1.7951e+00, -5.6712e-01, 8.6987e-01], [ 1.1006e-01, -1.5961e+00, 1.2179e+00, 3.4799e-01, -7.1710e-01, 2.5705e+00, 4.5020e-01, 3.8066e+00, 4.8558e+00, 2.1423e+00], [-9.9457e-01, 1.5614e+00, 1.3985e+00, 3.6700e+00, -1.9708e+00, -2.4845e+00, 2.5387e+00, -1.2250e+00, -4.6877e+00, -3.3492e+00], [-4.5289e-01, 2.4210e+00, 3.3681e+00, -2.7785e+00, 1.5472e+00, -5.0358e-01, -9.7416e-01, 1.1032e+00, 2.0812e-01, 8.2830e-01], [ 1.1052e+00, -2.5233e+00, 2.0461e+00, 1.1886e-01, -4.8352e+00, 2.4197e-01, -1.5177e-01, -6.9245e-01, -1.8357e+00, -1.5302e+00]]) 梯度检查: from torch.autograd.gradcheck import gradcheck moduleConv = ScipyConv2d(3, 3) input = [torch.randn(20, 20, dtype=torch.double, requires_grad=True)] test = gradcheck(moduleConv, input, eps=1e-6, atol=1e-4) print(\"Are the gradients correct: \", test) 输出： Are the gradients correct: True 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"cpp_extension.html":{"url":"cpp_extension.html","title":"Custom C++   and CUDA Extensions","keywords":"","body":"自定义 C++ 与 CUDA 拓展 译者：P3n9W31 Author: Peter Goldsborough PyTorch 提供了大量与神经网络，任意张量代数（arbitrary tensor algebra），数据处理（data wrangling）和其他目的相关的操作。然而，你可能发现你还是会需要一些更加自定义的操作。例如，你有时可能希望使用一个你在某篇论文中找到的一个新型的激活函数，或者是实现一个为了你的研究所开发的新操作。 在 PyTorch 中集成这种自定义操作的最简单方法是通过 Python 语言对Function和Module进行扩写，正如在 这里所描述的那样。这种方式能让你充分的发挥自动微分（automatic differentiation）（让你不用去编写一些衍生的函数）与 Python 语言的常规情况下的表现力（usual expressiveness）的能力。但是有时候，可能在 C++ 语言中能够更好地实现你的一些操作。例如，你的代码可能因为被非常频繁的使用而需要 十分 快速，或者是即使调用的次数很少也会带来不小的性能负担。另一个原因是你的代码可能是建立在 C 或 C++ 语言之上的，或者你的代码需要与 C 或 C++ 语言进行交互与对接。为了解决上述的这些情况，PyTorch 提供了一种简单的用于编写自定义 C++ 扩展 的方法。 C++ 拓展是我们开发的一种能够让用户（你）自行创建一些 所含资源之外 的操作的机制，例如，与 PyTorch 的后端分离开来。这种方法与 PyTorch 原生操作的实现方式是 不同的 。C++ 扩展旨在为你提供与 PyTorch 后端集成操作相关的大部分样板（boilerplate），同时为基于 PyTorch 的项目提供高度灵活性。然而，一旦你将你的操作定义为了 C++ 拓展，将其转换为原生 PyTorch 函数就主要是代码组织的问题了，如果你决定在上游提供操作，则可以解决这个问题。 动机与例子 本篇文章的剩余部分将介绍一个编写和使用 C++（以及 CUDA）拓展的实例。如果你现在正在被一直催着或是在今天之前没有把该操作完成你就会被解雇的话，你可以跳过这一章节，直接去下一节的实施细节部分查看。 假设你已经找到了一种新型的循环（recurrent）的单元，它与现有技术相比具有优越的性能。该循环单元类似于 LSTM，但不同之处在于它缺少了 遗忘门 并使用 指数线性单元 （ELU）作为其内部激活功能。因为这个单元永远都不会忘记，所以我们叫它 LLTM，或是 长长期记忆 （Long-Long-Term-Memory）单元。 在 LLTMs 中的这两个与普通的 LSTMs 的不同点是十分重要的，以至于我们不能通过配置 PyTorch 中的 LSTMCell 来达到我们的目标。所以我们将只能创建一个自定义模块。第一个也是最简单的方法 - 可能在所有情况下都是良好的第一步——是使用 Python 在纯 PyTorch 中实现我们所需的功能。为此，我们需要继承 torch.nn.Module 并实现 LLTM 的正向传递。 这看起来就像这样： class LLTM(torch.nn.Module): def __init__(self, input_features, state_size): super(LLTM, self).__init__() self.input_features = input_features self.state_size = state_size # 3 * state_size for input gate, output gate and candidate cell gate. # input_features + state_size because we will multiply with [input, h]. self.weights = torch.nn.Parameter( torch.empty(3 * state_size, input_features + state_size)) self.bias = torch.nn.Parameter(torch.empty(3 * state_size)) self.reset_parameters() def reset_parameters(self): stdv = 1.0 / math.sqrt(self.state_size) for weight in self.parameters(): weight.data.uniform_(-stdv, +stdv) def forward(self, input, state): old_h, old_cell = state X = torch.cat([old_h, input], dim=1) # Compute the input, output and candidate cell gates with one MM. gate_weights = F.linear(X, self.weights, self.bias) # Split the combined gate weight matrix into its components. gates = gate_weights.chunk(3, dim=1) input_gate = F.sigmoid(gates[0]) output_gate = F.sigmoid(gates[1]) # Here we use an ELU instead of the usual tanh. candidate_cell = F.elu(gates[2]) # Compute the new cell state. new_cell = old_cell + candidate_cell * input_gate # Compute the new hidden state and output. new_h = F.tanh(new_cell) * output_gate return new_h, new_cell 我们可以按预期使用它： import torch X = torch.randn(batch_size, input_features) h = torch.randn(batch_size, state_size) C = torch.randn(batch_size, state_size) rnn = LLTM(input_features, state_size) new_h, new_C = rnn(X, (h, C)) 当然，如果可能的话，你应该使用这种方法来扩展 PyTorch。由于 PyTorch 对 CPU 与 GPU 的操作实施了高度优化，由 NVIDIA cuDNN，Intel MKL 或是 NNPACK 等库提供了支持，像上面那样的 PyTorch 代码一般情况下都是足够快速的。但是，我们也可以看到为什么在某些情况下还有进一步改进性能的空间。最明显的原因是PyTorch不了解你正在实施的 算法 。它只知道你用于编写算法的各个独立操作。因此，PyTorch 必须逐个执行你的操作。由于对操作的实现（或 核 ）的每次单独的调用都可能（可能涉及启动 CUDA 内核）具有一定量的开销，因此这种开销可能在许多函数的调用中变得显着。此外，运行我们的代码的 Python 解释器本身就可以减慢我们的程序。 因此，一个明显可以加快速度的方法是用 C++（或 CUDA）完成部分代码的重写部分并融合特定的操作组。融合意味着将许多函数的实现组合到单个函数中，这些函数会从更少的内核启动中受益，此外，这些函数还会从我们通过提高全局数据流的可见性来执行的其他优化中获益。 让我们来看看我们可以怎样使用 C++ 拓展来实现一个融合版本的 LLTM。我们首先使用纯 C++ 完成代码编写，使用驱动了大部分 PyTorch 后端的 ATen 库，并看看它能让我们多简单就完成 Python 代码的转换。然后我们将通过将一部分的模型移动到 CUDA 内核以从 GPU 提供的大规模并行性中受益，来进一步加快速度。 编写一个 C++ 拓展 C++ 扩展有两种形式：它们可以使用setuptools来进行“提前”构建，或者通过torch.utils.cpp_extension.load()来实现“实时”构建。我们将从第一种方法开始，稍后再讨论后者。 使用setuptools进行构建 对于\"提前\"这种形式，我们通过编写一个setup.py脚本来构建我们的 C++ 扩展，该脚本使用 setuptools 来编译我们的 C++ 代码。 对于 LLTM 而言，它看起来就像下面这样简单： from setuptools import setup from torch.utils.cpp_extension import CppExtension, BuildExtension setup(name='lltm', ext_modules=[CppExtension('lltm', ['lltm.cpp'])], cmdclass={'build_ext': BuildExtension}) 在这段代码中，CppExtension是setuptools.Extension的一个便利的包装器（wrapper），它传递正确的包含路径并将扩展语言设置为 C++。 等效的普通setuptools代码像下面这样简单： setuptools.Extension( name='lltm', sources=['lltm.cpp'], include_dirs=torch.utils.cpp_extension.include_paths(), language='c++') BuildExtension执行许多必需的配置步骤和检查，并在混合 C++/CUDA 扩展的情况下管理混合编译。 这就是我们现在真正需要了解的关于构建 C++ 扩展的所有内容！现在让我们来看看我们的 C++ 扩展的实现，它扩展到了lltm.cpp中。 编写 C++ 操作 让我们开始用 C++ 实现 LLTM！我们向后传递所需的一个函数是 sigmoid 的导数。这是一段足够小的代码，用于讨论编写 C++ 扩展时可用的整体环境： #include #include at::Tensor d_sigmoid(at::Tensor z) { auto s = at::sigmoid(z); return (1 - s) * s; } torch / torch.h是一站式（one-stop）头文件，包含编写 C++ 扩展所需的所有 PyTorch 位。 这包括： ATen 库，我们主要的张量计算接口 pybind11，我们为 C++ 代码创建 Python 绑定的方法 管理 ATen 和 pybind11 之间交互细节的头文件。 d_sigmoid（）的实现显示了如何使用 ATen API。PyTorch 的张量和变量接口是从 ATen 库自动生成的，因此我们可以或多或少地将我们的 Python 语言实现1:1转换为 C++ 语言实现。 我们所有计算的主要数据类型都是at::Tensor。可以在此处查看其完整的 API。另请注意，我们可以引用或任何其他 C 或 C++ 头文件——我们可以使用 C++ 11 的全部功能。 前向传播 接下来，我们可以将整个前向传播部分移植为 C++ 代码： #include std::vector lltm_forward( at::Tensor input, at::Tensor weights, at::Tensor bias, at::Tensor old_h, at::Tensor old_cell) { auto X = at::cat({old_h, input}, /*dim=*/1); auto gate_weights = at::addmm(bias, X, weights.transpose(0, 1)); auto gates = gate_weights.chunk(3, /*dim=*/1); auto input_gate = at::sigmoid(gates[0]); auto output_gate = at::sigmoid(gates[1]); auto candidate_cell = at::elu(gates[2], /*alpha=*/1.0); auto new_cell = old_cell + candidate_cell * input_gate; auto new_h = at::tanh(new_cell) * output_gate; return {new_h, new_cell, input_gate, output_gate, candidate_cell, X, gate_weights}; } 反向传播 C++ 的扩展 API 目前不为我们提供自动生成反向函数的方法。因此，我们还必须实施 LLTM 的反向传播部分，LLTM 计算相对于正向传播的每个输入的损失的导数。最终，我们将向前和向后函数放入torch.autograd.Function以创建一个漂亮的 Python 绑定。 向后功能稍微复杂一些，所以我们不会深入研究代码（如果你感兴趣，Alex Graves的论文是一个能让你了解跟多信息的好文章）： // tanh'(z) = 1 - tanh^2(z) at::Tensor d_tanh(at::Tensor z) { return 1 - z.tanh().pow(2); } // elu'(z) = relu'(z) + { alpha * exp(z) if (alpha * (exp(z) - 1)) 0).type_as(z) + mask.type_as(z) * (alpha * e); } std::vector lltm_backward( at::Tensor grad_h, at::Tensor grad_cell, at::Tensor new_cell, at::Tensor input_gate, at::Tensor output_gate, at::Tensor candidate_cell, at::Tensor X, at::Tensor gate_weights, at::Tensor weights) { auto d_output_gate = at::tanh(new_cell) * grad_h; auto d_tanh_new_cell = output_gate * grad_h; auto d_new_cell = d_tanh(new_cell) * d_tanh_new_cell + grad_cell; auto d_old_cell = d_new_cell; auto d_candidate_cell = input_gate * d_new_cell; auto d_input_gate = candidate_cell * d_new_cell; auto gates = gate_weights.chunk(3, /*dim=*/1); d_input_gate *= d_sigmoid(gates[0]); d_output_gate *= d_sigmoid(gates[1]); d_candidate_cell *= d_elu(gates[2]); auto d_gates = at::cat({d_input_gate, d_output_gate, d_candidate_cell}, /*dim=*/1); auto d_weights = d_gates.t().mm(X); auto d_bias = d_gates.sum(/*dim=*/0, /*keepdim=*/true); auto d_X = d_gates.mm(weights); const auto state_size = grad_h.size(1); auto d_old_h = d_X.slice(/*dim=*/1, 0, state_size); auto d_input = d_X.slice(/*dim=*/1, state_size); return {d_old_h, d_input, d_weights, d_bias, d_old_cell}; } 与Python绑定 一旦你使用 C++ 和 ATen 编写了操作，就可以使用 pybind11 以非常简单的方式将 C++ 函数或类绑定到 Python 上。关于 PyTorch 的 C++ 扩展的这一部分的问题或疑问将主要通过pybind11文档来解决。 对于我们的扩展，必要的绑定代码只是仅仅四行： PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) { m.def(\"forward\", &lltm_forward, \"LLTM forward\"); m.def(\"backward\", &lltm_backward, \"LLTM backward\"); } 有一点需要注意的是宏TORCH_EXTENSION_NAME。torch 的扩展部分将会把它定义为你在setup.py脚本中为扩展名命名的名称。在这种情况下，TORCH_EXTENSION_NAME的值将为“lltm”。这是为了避免必须在两个地方（构建脚本和 C++ 代码中）维护扩展名，因为两者之间的不匹配可能会导致令人讨厌且难以跟踪的问题。 使用你的拓展 我们现在设置为 PyTorch 导入我们的扩展。 此时，你的目录结构可能如下所示： pytorch/ lltm-extension/ lltm.cpp setup.py 现在，运行python setup.py install来构建和安装你的扩展。 运行结果应该是这样的： running install running bdist_egg running egg_info writing lltm.egg-info/PKG-INFO writing dependency_links to lltm.egg-info/dependency_links.txt writing top-level names to lltm.egg-info/top_level.txt reading manifest file 'lltm.egg-info/SOURCES.txt' writing manifest file 'lltm.egg-info/SOURCES.txt' installing library code to build/bdist.linux-x86_64/egg running install_lib running build_ext building 'lltm' extension gcc -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I~/local/miniconda/lib/python3.6/site-packages/torch/lib/include -I~/local/miniconda/lib/python3.6/site-packages/torch/lib/include/TH -I~/local/miniconda/lib/python3.6/site-packages/torch/lib/include/THC -I~/local/miniconda/include/python3.6m -c lltm.cpp -o build/temp.linux-x86_64-3.6/lltm.o -DTORCH_EXTENSION_NAME=lltm -std=c++11 cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++ g++ -pthread -shared -B ~/local/miniconda/compiler_compat -L~/local/miniconda/lib -Wl,-rpath=~/local/miniconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/lltm.o -o build/lib.linux-x86_64-3.6/lltm.cpython-36m-x86_64-linux-gnu.so creating build/bdist.linux-x86_64/egg copying build/lib.linux-x86_64-3.6/lltm_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg copying build/lib.linux-x86_64-3.6/lltm.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg creating stub loader for lltm.cpython-36m-x86_64-linux-gnu.so byte-compiling build/bdist.linux-x86_64/egg/lltm.py to lltm.cpython-36.pyc creating build/bdist.linux-x86_64/egg/EGG-INFO copying lltm.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO copying lltm.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO copying lltm.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO copying lltm.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt zip_safe flag not set; analyzing archive contents... __pycache__.lltm.cpython-36: module references __file__ creating 'dist/lltm-0.0.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it removing 'build/bdist.linux-x86_64/egg' (and everything under it) Processing lltm-0.0.0-py3.6-linux-x86_64.egg removing '~/local/miniconda/lib/python3.6/site-packages/lltm-0.0.0-py3.6-linux-x86_64.egg' (and everything under it) creating ~/local/miniconda/lib/python3.6/site-packages/lltm-0.0.0-py3.6-linux-x86_64.egg Extracting lltm-0.0.0-py3.6-linux-x86_64.egg to ~/local/miniconda/lib/python3.6/site-packages lltm 0.0.0 is already the active version in easy-install.pth Installed ~/local/miniconda/lib/python3.6/site-packages/lltm-0.0.0-py3.6-linux-x86_64.egg Processing dependencies for lltm==0.0.0 Finished processing dependencies for lltm==0.0.0 关于编译器的一个小注意事项：由于 ABI 版本问题，用于构建 C++ 扩展的编译器必须与 ABI 兼容，并且这里的编译器是必须是与构建 PyTorch 时采用的编译器一样的。实际上，这意味着你必须在 Linux 上使用 GCC 4.9 及更高版本。对于 Ubuntu 16.04 和其他更新的 Linux 发行版来说，这应该是默认的编译器。在MacOS上，你必须使用clang（没有任何与ABI版本相关的问题）。在最坏的情况下，你可以使用编译器从源代码构建 PyTorch，然后使用相同的编译器构建扩展。 构建扩展后，你只需使用在setup.py脚本中指定的名称在Python中导入它。请务必首先运行 import torch ，因为这将解析动态链接器必须看到的一些符号： In [1]: import torch In [2]: import lltm In [3]: lltm.forward Out[3]: 如果我们在函数或模块上调用help()，我们可以看到它的签名（signature）与我们的 C++ 代码匹配： In[4] help(lltm.forward) forward(...) method of builtins.PyCapsule instance forward(arg0: at::Tensor, arg1: at::Tensor, arg2: at::Tensor, arg3: at::Tensor, arg4: at::Tensor) -> List[at::Tensor] LLTM forward 既然我们现在能够从 Python 中调用我们的 C++ 函数，我们可以使用torch.autograd.Function和torch.nn.Module来包装（warp）它们，使它们成为 PyTorch 中的一等公民（first class citizens，关键的一部分）： import math import torch # Our module! import lltm class LLTMFunction(torch.autograd.Function): @staticmethod def forward(ctx, input, weights, bias, old_h, old_cell): outputs = lltm.forward(input, weights, bias, old_h, old_cell) new_h, new_cell = outputs[:2] variables = outputs[1:] + [weights] ctx.save_for_backward(*variables) return new_h, new_cell @staticmethod def backward(ctx, grad_h, grad_cell): outputs = lltm.backward( grad_h.contiguous(), grad_cell.contiguous(), *ctx.saved_variables) d_old_h, d_input, d_weights, d_bias, d_old_cell = outputs return d_input, d_weights, d_bias, d_old_h, d_old_cell class LLTM(torch.nn.Module): def __init__(self, input_features, state_size): super(LLTM, self).__init__() self.input_features = input_features self.state_size = state_size self.weights = torch.nn.Parameter( torch.empty(3 * state_size, input_features + state_size)) self.bias = torch.nn.Parameter(torch.empty(3 * state_size)) self.reset_parameters() def reset_parameters(self): stdv = 1.0 / math.sqrt(self.state_size) for weight in self.parameters(): weight.data.uniform_(-stdv, +stdv) def forward(self, input, state): return LLTMFunction.apply(input, self.weights, self.bias, *state) 性能比较 现在我们可以使用并调用来自 PyTorch 的 C++ 代码，我们可以运行一个小的基准测试来看看我们在 C++ 中重写的操作的性能。我们将运行 LLTM 中的前向转播与反向传播几次并测量运行的时间： import torch batch_size = 16 input_features = 32 state_size = 128 X = torch.randn(batch_size, input_features) h = torch.randn(batch_size, state_size) C = torch.randn(batch_size, state_size) rnn = LLTM(input_features, state_size) forward = 0 backward = 0 for _ in range(100000): start = time.time() new_h, new_C = rnn(X, (h, C)) forward += time.time() - start start = time.time() (new_h.sum() + new_C.sum()).backward() backward += time.time() - start print('Forward: {:.3f} us | Backward {:.3f} us'.format(forward * 1e6/1e5, backward * 1e6/1e5)) 如果运行我们在本文开头用纯 Python 编写原始 LLTM 的代码，我们将得到以下数字（在我的机器上）： Forward: 506.480 us | Backward 444.694 us 然后是运行全新的 C++ 版本的代码： Forward: 349.335 us | Backward 443.523 us 我们已经可以看到前向传播函数的显着加速（超过30％）。对于反向传播函数而言，我们也是可以看到加速效果的，尽管加速的效果不是很明显。我在上面写的反向传播并没有经过特别优化，它绝对还可以进行改进。此外，PyTorch 的自动差分引擎可以自动并行化计算图，可以使用更高效的整体操作流，并且这也是用 C++ 实现，因此预计运行速度会很快。尽管如此，这是一个良好的开端。 在GPU设备上的性能 关于 PyTorch 的 ATen 后端的一个很好的事实是它抽象了你正在运行代码的计算设备。这意味着我们为CPU编写的代码也可以在GPU上运行，并且各个操作将相应地分派到以 GPU 优化过后的实现中去。对于某些操作，如矩阵乘法（如mm或admm），这是一个很大的胜利。让我们看一下使用 CUDA 张量运行 C++ 代码可以获得多少的性能提升。我们不需要对代码作出任何改变，我们只需要将我们的张量放在 Python 中的 GPU 内存中，在创建时添加device = cuda_device参数或在创建后使用.to(cuda_device)即可： import torch assert torch.cuda.is_available() cuda_device = torch.device(\"cuda\") # device object representing GPU batch_size = 16 input_features = 32 state_size = 128 # Note the device=cuda_device arguments here X = torch.randn(batch_size, input_features, device=cuda_device) h = torch.randn(batch_size, state_size, device=cuda_device) C = torch.randn(batch_size, state_size, device=cuda_device) rnn = LLTM(input_features, state_size).to(cuda_device) forward = 0 backward = 0 for _ in range(100000): start = time.time() new_h, new_C = rnn(X, (h, C)) torch.cuda.synchronize() forward += time.time() - start start = time.time() (new_h.sum() + new_C.sum()).backward() torch.cuda.synchronize() backward += time.time() - start print('Forward: {:.3f} us | Backward {:.3f} us'.format(forward * 1e6/1e5, backward * 1e6/1e5)) 再一次将我们的普通 PyTorch 代码与我们的 C++ 版本进行比较，现在两者都运行在 CUDA 设备上，我们科技再次看到性能得到了提升。 对于 Python/PyTorch 来说： Forward: 187.719 us | Backward 410.815 us 然后是 C++ / ATen： Forward: 149.802 us | Backward 393.458 us 与非 CUDA 代码相比，这是一个很好的整体加速。但是，通过编写自定义 CUDA 内核，我们可以从 C++ 代码中得到更多的性能提升，我们将很快在下面介绍这些内核。在此之前，让我们讨论构建 C++ 扩展的另一种方法。 JIT 编译扩展 在之前，我提到有两种构建 C++ 扩展的方法：使用setuptools或者是实时（JIT）。 在对前者进行了说明之后，让我们再详细说明一下后者。JIT 编译机制通过在 PyTorch 的 API 中调用一个名为torch.utils.cpp_extension.load()的简单函数，为你提供了一种编译和加载扩展的方法。对于 LLTM，这看起来就像下面这样简单： from torch.utils.cpp_extension import load lltm = load(name=\"lltm\", sources=[\"lltm.cpp\"]) 在这里，我们为函数提供与setuptools相同的信息。 在后台，将执行以下操作： 创建临时目录 /tmp/torch_extensions/lltm 将一个 Ninja 构建文件发送到该临时目录， 将源文件编译为共享库 将此共享库导入为 Python 模块 实际上，如果你将verbose = True参数传递给cpp_extension.load（），该过程在进行的过程中将会告知你： Using /tmp/torch_extensions as PyTorch extensions root... Creating extension directory /tmp/torch_extensions/lltm... Emitting ninja build file /tmp/torch_extensions/lltm/build.ninja... Building extension module lltm... Loading extension module lltm... 生成的 Python 模块与 setuptools 生成的完全相同，但不需要维护单独的setup.py构建文件。如果你的设置更复杂并且你确实需要setuptools的全部功能，那么你可以编写自己的setup.py——但在很多情况下，这种JIT的方式就已经完全够用了。第一次运行此行代码时，将耗费一些时间，因为扩展正在后台进行编译。由于我们使用 Ninja 构建系统来构建源代码，因此重新编译的工作量是不断增加的，而当你第二次运行 Python 模块进行重新加载扩展时速度就会快得多了，而且如果你没有对扩展的源文件进行更改，需要的开销也将会很低。 编写一个 C++/CUDA 混合的拓展 为了真正将我们的实现的性能提升到一个新的水平，我们可以自定义 CUDA 内核并全手工的完成前向和反向传播中部分代码的编写。对于 LLTM 来说，这具有特别有效的前景，因为序列中存在大量逐点运算，所有这些运算都可以在单个 CUDA 内核中融合和并行化。让我们看看如何使用这种扩展机制编写这样的 CUDA 内核并将其与 PyTorch 整合到一起。 编写 CUDA 扩展的一般策略是首先编写一个 C++ 文件，该文件定义了将从 Python 中调用的函数，并使用 pybind11 将这些函数绑定到 Python 上。此外，该文件还将 声明 在 CUDA（.cu）文件中将定义的函数。然后，C++ 函数将进行一些检查，并最终将其调用转发给 CUDA 函数。在 CUDA 文件中，我们编写了实际的 CUDA 内核。接着，cpp_extension包将负责使用 C++ 编译器（如gcc）和使用 NVIDIA 的nvcc编译器的CUDA源编译 C++ 源代码。以此来确保每个编译器处理它最好编译的文件。最终，它们将链接到一个可从 Python 代码中进行访问的共享库。 我们将从 C++ 文件开始，我们将其称为lltm_cuda.cpp，例如： #include #include // CUDA forward declarations std::vector lltm_cuda_forward( at::Tensor input, at::Tensor weights, at::Tensor bias, at::Tensor old_h, at::Tensor old_cell); std::vector lltm_cuda_backward( at::Tensor grad_h, at::Tensor grad_cell, at::Tensor new_cell, at::Tensor input_gate, at::Tensor output_gate, at::Tensor candidate_cell, at::Tensor X, at::Tensor gate_weights, at::Tensor weights); // C++ interface #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\") #define CHECK_CONTIGUOUS(x) AT_ASSERTM(x.is_contiguous(), #x \" must be contiguous\") #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x) std::vector lltm_forward( at::Tensor input, at::Tensor weights, at::Tensor bias, at::Tensor old_h, at::Tensor old_cell) { CHECK_INPUT(input); CHECK_INPUT(weights); CHECK_INPUT(bias); CHECK_INPUT(old_h); CHECK_INPUT(old_cell); return lltm_cuda_forward(input, weights, bias, old_h, old_cell); } std::vector lltm_backward( at::Tensor grad_h, at::Tensor grad_cell, at::Tensor new_cell, at::Tensor input_gate, at::Tensor output_gate, at::Tensor candidate_cell, at::Tensor X, at::Tensor gate_weights, at::Tensor weights) { CHECK_INPUT(grad_h); CHECK_INPUT(grad_cell); CHECK_INPUT(input_gate); CHECK_INPUT(output_gate); CHECK_INPUT(candidate_cell); CHECK_INPUT(X); CHECK_INPUT(gate_weights); CHECK_INPUT(weights); return lltm_cuda_backward( grad_h, grad_cell, new_cell, input_gate, output_gate, candidate_cell, X, gate_weights, weights); } PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) { m.def(\"forward\", &lltm_forward, \"LLTM forward (CUDA)\"); m.def(\"backward\", &lltm_backward, \"LLTM backward (CUDA)\"); } 正如你所看到的，它主要是一个样板（boilerplate），检查和转发到我们将在 CUDA 文件中定义的函数。我们将这个文件命名为lltm_cuda_kernel.cu（注意.cu扩展名！）。NVCC 可以合理地编译 C++ 11，因此我们仍然可以使用 ATen 和 C++ 标准库（但torch.h不行）。 请注意，setuptools无法处理具有相同名称但扩展名不同的文件，因此如果使用setup.py方法而不是 JIT 方法，则必须为 CUDA 文件指定与 C++ 文件不同的名称（对于JIT） 方法，lltm.cpp和lltm.cu会正常工作）。 我们来看看这个文件的样子： #include #include #include #include template __device__ __forceinline__ scalar_t sigmoid(scalar_t z) { return 1.0 / (1.0 + exp(-z)); } 在这里，我们可以看到我刚刚描述的头文件，以及我们使用 CUDA 特定的声明，如__device__和__forceinline__以及像exp这样的函数。让我们继续使用我们将需要用到的一些辅助函数： template __device__ __forceinline__ scalar_t d_sigmoid(scalar_t z) { const auto s = sigmoid(z); return (1.0 - s) * s; } template __device__ __forceinline__ scalar_t d_tanh(scalar_t z) { const auto t = tanh(z); return 1 - (t * t); } template __device__ __forceinline__ scalar_t elu(scalar_t z, scalar_t alpha = 1.0) { return fmax(0.0, z) + fmin(0.0, alpha * (exp(z) - 1.0)); } template __device__ __forceinline__ scalar_t d_elu(scalar_t z, scalar_t alpha = 1.0) { const auto e = exp(z); const auto d_relu = z 现在实际上实现了一个函数，我们还需要两件事：一个函数执行我们不希望手动显式写入的操作并调用 CUDA 内核，然后实际的 CUDA 内核用于我们想要加速的部分。对于前向转播来说，第一个函数应如下所示： std::vector lltm_cuda_forward( at::Tensor input, at::Tensor weights, at::Tensor bias, at::Tensor old_h, at::Tensor old_cell) { auto X = at::cat({old_h, input}, /*dim=*/1); auto gates = at::addmm(bias, X, weights.transpose(0, 1)); const auto batch_size = old_cell.size(0); const auto state_size = old_cell.size(1); auto new_h = at::zeros_like(old_cell); auto new_cell = at::zeros_like(old_cell); auto input_gate = at::zeros_like(old_cell); auto output_gate = at::zeros_like(old_cell); auto candidate_cell = at::zeros_like(old_cell); const int threads = 1024; const dim3 blocks((state_size + threads - 1) / threads, batch_size); AT_DISPATCH_FLOATING_TYPES(gates.type(), \"lltm_forward_cuda\", ([&] { lltm_cuda_forward_kernel>>( gates.data(), old_cell.data(), new_h.data(), new_cell.data(), input_gate.data(), output_gate.data(), candidate_cell.data(), state_size); })); return {new_h, new_cell, input_gate, output_gate, candidate_cell, X, gates}; } 这里主要关注的是AT_DISPATCH_FLOATING_TYPES宏和内核启动（由>>进行表示）。虽然 ATen 会对我们所处理的张量的设备和数据类型进行抽象化，但是在运行时，张量仍将由具体设备上的具体类型的内存支持。因此，我们需要一种在运行时确定张量是什么类型的方法，然后选择性地调用相应的具有正确类型签名（signature）函数。手动完成这些部分，这将（概念上）看起来像这样： switch (tensor.type().scalarType()) { case at::ScalarType::Double: return function(tensor.data()); case at::ScalarType::Float: return function(tensor.data()); ... } AT_DISPATCH_FLOATING_TYPES 的目的是为我们处理此次调度。它需要一个类型（在我们的例子中是gates.type()），一个名称（用于错误消息）和一个 lambda 函数。在这个 lambda 函数中，类型别名scalar_t是可用的，并且被定义为张量在该上下文中实际处于运行时的类型。因此，如果我们有一个模板函数（我们的 CUDA 内核将作为模板函数），我们可以用这个scalar_t别名实例化它，然后正确的函数就会被调用。 在这种情况下，我们还希望检索张量的数据指针作为scalar_t类型的指针。 如果你想调度所有类型而不仅仅是浮点类型（Float和Double），你可以使用AT_DISPATCH_ALL_TYPES。 请注意，我们使用普通 ATen 执行的一些操作。这些操作仍将在 GPU 上运行，但使用的是 ATen 的默认实现。这是有道理的，因为 ATen 将使用高度优化的例程来处理诸如矩阵乘法（例如addmm）或者是一些我们自己十分难以实现以及完成性能提升的操作，如卷积操作。 至于内核启动本身，我们在这里指定每个 CUDA 块将具有1024个线程，并且整个 GPU 网格被分成尽可能多的 1 x 1024 线程块，并以一组一个线程的方式填充我们的矩阵。例如，如果我们的状态（state）大小为2048且批量大小为4，那么我们将以每个块1024个线程完成启动，总共 4 x 2 = 8 个块。如果你之前从未听说过 CUDA “块”或“网格”，那么关于 CUDA 的介绍性阅读可能会有所帮助。 实际的 CUDA 内核非常简单（如果你以前进行过 GPU 编程）： template __global__ void lltm_cuda_forward_kernel( const scalar_t* __restrict__ gates, const scalar_t* __restrict__ old_cell, scalar_t* __restrict__ new_h, scalar_t* __restrict__ new_cell, scalar_t* __restrict__ input_gate, scalar_t* __restrict__ output_gate, scalar_t* __restrict__ candidate_cell, size_t state_size) { const int column = blockIdx.x * blockDim.x + threadIdx.x; const int index = blockIdx.y * state_size + column; const int gates_row = blockIdx.y * (state_size * 3); if (column 这里最感兴趣的是，我们能够完全并行地为门矩阵中的每个单独组件计算所有的这些逐点运算。如果你能想象必须用一个巨大的for循环来连续超过一百万个元素的情况，你也可以理解为什么改进之后速度会更快了。 反向传播遵循相同的模式，在这里将不再详细说明： template __global__ void lltm_cuda_backward_kernel( scalar_t* __restrict__ d_old_cell, scalar_t* __restrict__ d_gates, const scalar_t* __restrict__ grad_h, const scalar_t* __restrict__ grad_cell, const scalar_t* __restrict__ new_cell, const scalar_t* __restrict__ input_gate, const scalar_t* __restrict__ output_gate, const scalar_t* __restrict__ candidate_cell, const scalar_t* __restrict__ gate_weights, size_t state_size) { const int column = blockIdx.x * blockDim.x + threadIdx.x; const int index = blockIdx.y * state_size + column; const int gates_row = blockIdx.y * (state_size * 3); if (column lltm_cuda_backward( at::Tensor grad_h, at::Tensor grad_cell, at::Tensor new_cell, at::Tensor input_gate, at::Tensor output_gate, at::Tensor candidate_cell, at::Tensor X, at::Tensor gate_weights, at::Tensor weights) { auto d_old_cell = at::zeros_like(new_cell); auto d_gates = at::zeros_like(gate_weights); const auto batch_size = new_cell.size(0); const auto state_size = new_cell.size(1); const int threads = 1024; const dim3 blocks((state_size + threads - 1) / threads, batch_size); AT_DISPATCH_FLOATING_TYPES(X.type(), \"lltm_backward_cuda\", ([&] { lltm_cuda_backward_kernel>>( d_old_cell.data(), d_gates.data(), grad_h.contiguous().data(), grad_cell.contiguous().data(), new_cell.contiguous().data(), input_gate.contiguous().data(), output_gate.contiguous().data(), candidate_cell.contiguous().data(), gate_weights.contiguous().data(), state_size); })); auto d_weights = d_gates.t().mm(X); auto d_bias = d_gates.sum(/*dim=*/0, /*keepdim=*/true); auto d_X = d_gates.mm(weights); auto d_old_h = d_X.slice(/*dim=*/1, 0, state_size); auto d_input = d_X.slice(/*dim=*/1, state_size); return {d_old_h, d_input, d_weights, d_bias, d_old_cell, d_gates}; } 将 C++/CUDA 操作与 PyTorch 集成 我们支持 CUDA 的操作与 PyTorch 的集成同样十分简单。如果你想写一个setup.py脚本，它可能看起来像这样： from setuptools import setup from torch.utils.cpp_extension import BuildExtension, CUDAExtension setup( name='lltm', ext_modules=[ CUDAExtension('lltm_cuda', [ 'lltm_cuda.cpp', 'lltm_cuda_kernel.cu', ]) ], cmdclass={ 'build_ext': BuildExtension }) 我们现在使用CUDAExtension()而不是CppExtension()。我们可以只指定.cu文件和.cpp文件——库可以解决所有麻烦。JIT 机制则更简单： from torch.utils.cpp_extension import load lltm = load(name='lltm', sources=['lltm_cuda.cpp', 'lltm_cuda_kernel.cu']) 性能比较 我们希望并行化与融合我们代码与 CUDA 的逐点操作将改善我们的 LLTM 的性能。让我们看看是否成立。我们可以运行在前面列出的代码来进行基准测试。我们之前的最快版本是基于 CUDA 的 C++ 代码： Forward: 149.802 us | Backward 393.458 us 现在使用我们的自定义 CUDA 内核： Forward: 129.431 us | Backward 304.641 us 性能得到了更多的提升！ 结论 你现在应该对 PyTorch 的 C++ 扩展机制以及使用它们的动机有一个很好的大致上的了解了。你可以在此处中找到本文中显示的代码示例。如果你有任何疑问，请使用论坛。如果你遇到任何问题，请务必查看我们的FAQ。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"torch_script_custom_ops.html":{"url":"torch_script_custom_ops.html","title":"Extending TorchScript with Custom C++   Operators","keywords":"","body":"＃使用自定义C++ 运算符扩展 TorchScript 作者：PyTorch 译者：ApacheCN PyTorch1.0版本向 PyTorch 引入了一个名为 [TorchScript]（https://pytorch.org/docs/master/jit.html）的新编程模型。TorchScript是Python编程语言的一个子集，可以通过TorchScript编译器进行解析，编译和优化。此外，已编译的TorchScript模型可以选择序列化为磁盘文件格式，您可以随后从纯C++（以及Python）程序进行加载和运行以进行推理。 TorchScript支持由torch包提供的大量操作，允许您将多种复杂模型纯粹表示为PyTorch的“标准库”中的一系列张量运算。然而，有时您可能会发现需要使用自定义C ++或CUDA函数扩展TorchScript。虽然我们建议您使用此这个选项时只在您的想法无法（足够有效）表达为一个简单的Python函数时，我们确实提供了一个非常友好和简单的界面，使用ATen 定义自定义C++ 和 CUDA 内核，PyTorch 的高性能C ++张量库。一旦绑定到TorchScript，您可以将这些自定义内核(或“ops”)嵌入到您的TorchScript模型中，并以Python和c++的序列化形式直接执行它们。 以下段落给出了一个编写TorchScript自定义操作的示例，以调用[OpenCV]（https://www.opencv.org），这是一个用C ++编写的计算机视觉库。我们将讨论如何在C ++中使用张量，如何有效地将它们转换为第三方张量格式（在这种情况下，OpenCV []（＃id1）Mats），如何在TorchScript运行时注册运算符，最后如何编译运算符并在Python和C ++中使用它。 本教程假设您通过pip或conda安装了PyTorch 1.0的preview release。有关获取最新版PyTorch 1.0 \\的说明，请参阅 [https://pytorch.org/get-started/locally](https://pytorch.org/get-started/locally）。或者，您可以从源代码编译PyTorch。[此文件]（https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md）中的文档将为您提供帮助。 在C ++中实现自定义运算符 对于本教程，我们将公开[warpPerspective]（https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html#warpperspective）函数，该函数将透视变换应用于图像， OpenCV to TorchScript作为自定义运算符。第一步是用C ++编写自定义运算符的实现。让我们调用这个实现op.cpp的文件，并使它看起来像这样： #include #include torch::Tensor warp_perspective(torch::Tensor image, torch::Tensor warp) { cv::Mat image_mat(/*rows=*/image.size(0), /*cols=*/image.size(1), /*type=*/CV_32FC1, /*data=*/image.data()); cv::Mat warp_mat(/*rows=*/warp.size(0), /*cols=*/warp.size(1), /*type=*/CV_32FC1, /*data=*/warp.data()); cv::Mat output_mat; cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{8, 8}); torch::Tensor output = torch::from_blob(output_mat.ptr(), /*sizes=*/{8, 8}); return output.clone(); } 该运算符的代码很短。在文件的顶部，我们包含OpenCV头文件 opencv2/opencv.hpp，以及 torch/script.h 头文件，它展示了我们需要编写自定义TorchScript运算符的 PyTorch C++ API所需的所有好东西.我们的函数 warp_perspective 有两个参数：输入image和我们希望应用于图像的warp变换矩阵。这些输入的类型是torch::Tensor，PyTorch在C++中的张量类型（它也是Python中所有张量的基础类型）。我们的warp_perspective函数的返回类型也将是torch::Tensor。 提示 有关ATen的更多信息，请参阅[本说明]（https://pytorch.org/cppdocs/notes/tensor_basics.html），ATen是为PyTorch提供`Tensor`类的库。此外，[本教程]（https://pytorch.org/cppdocs/notes/tensor_creation.html）描述了如何在C ++中分配和初始化新的张量对象（此运算符不需要）。 注意 TorchScript编译器了解固定数量的类型。只有这些类型可以用作自定义运算符的参数。目前这些类型是：torch::Tensor，torch::Scalar，double，int64_t和std::vector这些类型。注意onlydouble和notfloat和onlyint64_t和not支持其他整数类型，如int，short或long。 在我们的函数内部，我们需要做的第一件事是将PyTorch张量转换为OpenCV矩阵，因为OpenCV的 warpPerspective 期望cv::Mat对象作为输入。幸运的是，有一种方法可以做到这一点而无需复制任何数据。在前几行中， cv::Mat image_mat(/*rows=*/image.size(0), /*cols=*/image.size(1), /*type=*/CV_32FC1, /*data=*/image.data()); 我们正在调用OpenCV Mat 类的 这个构造函数 将我们的张量转换为Mat对象。我们传递原始image tensor的行数和列数，数据类型（我们将在本例中将其定义为float32），最后是一个指向底层数据的原始指针 - 一个float *。Mat类的这个构造函数的特殊之处在于它不复制输入数据。相反，它将简单地为在“Mat”上执行的所有操作引用该内存。如果对image_mat执行in-place操作，则这将反映在原始image张量中（反之亦然）。这允许我们使用库的本机矩阵类型调用后续的OpenCV例程，即使我们实际上将数据存储在PyTorch张量中。我们重复此过程将warp PyTorch张量转换为warp_mat OpenCV矩阵： cv::Mat warp_mat(/*rows=*/warp.size(0), /*cols=*/warp.size(1), /*type=*/CV_32FC1, /*data=*/warp.data()); 接下来，我们准备调用我们非常渴望在TorchScript中使用的OpenCV函数：warpPerspective。为此，我们传递OpenCV函数image_mat和warp_mat矩阵，以及一个名为output_mat的空输出矩阵。我们还指定了我们想要输出矩阵（图像）的大小dsize。对于此示例，它被硬编码为8 x 8： cv::Mat output_mat; cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{8, 8}); 我们的自定义运算符实现的最后一步是将output_mat转换回PyTorch张量，以便我们可以在PyTorch中进一步使用它。这与我们之前在另一个方向转换时所做的惊人相似。PyTorch提供了一个torch::from_blob方法。在本例中，blob意为指向内存的一些不透明的平面指针，我们想将其解释为PyTorch张量。对torch::from_blob的调用如下所示： torch::from_blob(output_mat.ptr(), /*sizes=*/{8, 8}) 我们在OpenCV Mat类上使用.ptr&lt;float&gt;()方法来获取指向底层数据的原始指针（就像之前的PyTorch张量的.data&lt;float&gt;()一样）。我们还指定了张量的输出形状，我们将其硬编码为8 x 8。然后torch::from_blob的输出为torch::Tensor，指向OpenCV矩阵拥有的内存。 在从运算符实现返回此张量之前，我们必须在张量上调用.clone()来执行基础数据的内存复制。原因是torch::from_blob返回不拥有其数据的张量。此时，数据仍归OpenCV矩阵所有。但是，此OpenCV矩阵将超出范围并在函数末尾取消分配。如果我们按原样返回output张量，那么当我们在函数外部使用它时，它将指向无效的内存。调用.clone()返回一个新的张量，其中包含新张量所拥有的原始数据的副本。因此返回外部世界是安全的。 使用TorchScript注册自定义运算符 现在已经在C ++中实现了我们的自定义运算符，我们需要使用TorchScript运行时和编译器注册它。这将允许TorchScript编译器在TorchScript代码中解析对自定义运算符的引用。注册非常简单。对于我们的情况，我们需要写： static auto registry = torch::jit::RegisterOperators(\"my_ops::warp_perspective\", &warp_perspective); 在我们的op.cpp文件的全局范围内的某个地方。这将创建一个全局变量registry，它将在其构造函数中使用TorchScript注册我们的运算符（即每个程序只注册一次）。我们指定运算符的名称，以及指向其实现的指针（我们之前编写的函数）。该名称由两部分组成：namespace（my_ops）和我们正在注册的特定运算符的名称（warp_perspective）。命名空间和运算符名称由两个冒号（::）分隔。 注意 如果要注册多个运算符，可以在构造函数之后将调用链接到.op()： static auto registry = torch::jit::RegisterOperators(\"my_ops::warp_perspective\", &warp_perspective) .op(\"my_ops::another_op\", &another_op) .op(\"my_ops::and_another_op\", &and_another_op); 在后台，RegisterOperators将执行一些相当复杂的C++模板元编程魔术技巧来推断我们传递它的函数指针的参数和返回值类型（&warp_perspective）。该信息用于为我们的运营商形成 function schema。函数模式是运算符的结构化表示 - 一种“签名”或“原型” - 由 TorchScript 编译器用于验证 TorchScript程序中的正确性。 构建自定义运算符 现在我们已经用C++ 实现了我们的自定义运算符并编写了它的注册代码，现在是时候将运算符构建到一个（共享）库中，我们可以将它加载到Python中进行研究和实验，或者加载到C ++中以便在非Python中进行推理环境。使用纯CMake或像setuptools这样的Python替代方法，存在多种构建运算符的方法。为简洁起见，以下段落仅讨论CMake方法。本教程的附录深入研究了基于Python的替代方案。 用CMake建设 要使用 CMake 构建系统将自定义运算符构建到共享库中，我们需要编写一个简短的CMakeLists.txt文件并将其与我们之前的op.cpp文件放在一起。为此，让我们商定一个如下所示的目录结构： warp-perspective/ op.cpp CMakeLists.txt 此外，请确保从 pytorch.org 获取最新版本的LibTorch发行版，该发行版包含PyTorch的C++ 库和 CMake 构建文件。将解压缩的分发放在文件系统中可访问的位置。以下段落将该位置称为/path/to/libtorch。我们的CMakeLists.txt文件的内容应该如下： cmake_minimum_required(VERSION 3.1 FATAL_ERROR) project(warp_perspective) find_package(Torch REQUIRED) find_package(OpenCV REQUIRED) # Define our library target add_library(warp_perspective SHARED op.cpp) # Enable C++11 target_compile_features(warp_perspective PRIVATE cxx_range_for) # Link against LibTorch target_link_libraries(warp_perspective \"${TORCH_LIBRARIES}\") # Link against OpenCV target_link_libraries(warp_perspective opencv_core opencv_imgproc) 警告 此设置对构建环境做出一些假设，特别是与OpenCV的安装有关的内容。上面的CMakeLists.txt文件在运行Ubuntu Xenial的Docker容器内进行了测试，并通过apt安装了libopencv-dev。如果它不适合您并且您感到卡住，请使用随附教程库中的Dockerfile构建一个隔离的，可重现的环境，在其中使用本教程中的代码。如果您遇到进一步的麻烦，请在教程库中提出问题或在我们的论坛中发帖提问。 要现在构建我们的运算符，我们可以从warp_perspective文件夹运行以下命令： $ mkdir build $ cd build $ cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch .. -- The C compiler identification is GNU 5.4.0 -- The CXX compiler identification is GNU 5.4.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Found torch: /libtorch/lib/libtorch.so -- Configuring done -- Generating done -- Build files have been written to: /warp_perspective/build $ make -j Scanning dependencies of target warp_perspective [ 50%] Building CXX object CMakeFiles/warp_perspective.dir/op.cpp.o [100%] Linking CXX shared library libwarp_perspective.so [100%] Built target warp_perspective 这将在build文件夹中放置一个libwarp_perspective.so共享库文件。在上面的cmake命令中，您应该将/path/to/libtorch替换为解压缩的LibTorch分发的路径。 我们将在下面详细介绍如何使用和调用我们的运算符，但为了尽早获得成功感，我们可以尝试在Python中运行以下代码： >>> import torch >>> torch.ops.load_library(\"/path/to/libwarp_perspective.so\") >>> print(torch.ops.my_ops.warp_perspective) 这里，/path/to/libwarp_perspective.so应该是我们刚刚构建的libwarp_perspective.so共享库的相对路径或绝对路径。如果一切顺利，这应该打印出类似的东西 这是我们稍后用来调用我们的自定义运算符的Python函数。 在Python中使用TorchScript自定义运算符 一旦我们的自定义运算符内置到共享库中，我们就可以在Python的TorchScript模型中使用此运算符。这有两个部分：首先将运算符加载到Python中，然后在TorchScript代码中使用运算符。 您已经了解了如何将运算符导入Python：torch.ops.load_library()。此函数获取包含自定义运算符的共享库的路径，并将其加载到当前进程中。加载共享库还将执行我们放入自定义运算符实现文件的全局RegisterOperators对象的构造函数。这将使用TorchScript编译器注册我们的自定义运算符，并允许我们在TorchScript代码中使用该运算符。 您可以将加载的运算符称为torch.ops.&lt;namespace&gt;.&lt;function&gt;，其中&lt;namespace&gt;是运算符名称的名称空间部分，&lt;function&gt;是运算符的函数名称。对于我们上面写的运算符，命名空间是my_ops和函数名warp_perspective，这意味着我们的运算符可用作torch.ops.my_ops.warp_perspective。虽然此函数可用于脚本或跟踪的TorchScript模块，但我们也可以在vanilla eager PyTorch中使用它并将其传递给常规的PyTorch张量： >>> import torch >>> torch.ops.load_library(\"libwarp_perspective.so\") >>> torch.ops.my_ops.warp_perspective(torch.randn(32, 32), torch.rand(3, 3)) tensor([[0.0000, 0.3218, 0.4611, ..., 0.4636, 0.4636, 0.4636], [0.3746, 0.0978, 0.5005, ..., 0.4636, 0.4636, 0.4636], [0.3245, 0.0169, 0.0000, ..., 0.4458, 0.4458, 0.4458], ..., [0.1862, 0.1862, 0.1692, ..., 0.0000, 0.0000, 0.0000], [0.1862, 0.1862, 0.1692, ..., 0.0000, 0.0000, 0.0000], [0.1862, 0.1862, 0.1692, ..., 0.0000, 0.0000, 0.0000]]) 注意 幕后发生的事情是，第一次在Python中访问torch.ops.namespace.function时，TorchScript编译器（在C ++版本中）将查看是否已注册函数namespace::function，如果已注册，则返回此函数的Python句柄，我们随后可以使用从Python调用我们的C ++运算符实现。这是TorchScript自定义运算符和C ++扩展之间的一个值得注意的区别：C ++扩展使用pybind11手动绑定，而TorchScript自定义ops由PyTorch本身绑定。 Pybind11为您提供了更多关于可以绑定到Python的类型和类的灵活性，因此建议用于纯粹的热切代码，但TorchScript操作不支持它。 从这里开始，您可以在脚本或跟踪代码中使用自定义运算符，就像使用torch包中的其他函数一样。实际上，像torch.matmul这样的“标准库”函数与自定义运算符的注册路径大致相同，这使得自定义运算符在TorchScript中的使用方式和位置方面确实是一流公民。 使用带有跟踪的自定义运算符 让我们首先将运算符嵌入到跟踪函数中。回想一下，对于跟踪，我们从一些香草Pytorch代码开始： def compute(x, y, z): return x.matmul(y) + torch.relu(z) 然后在其上调用torch.jit.trace。我们进一步传递torch.jit.trace一些示例输入，它将转发到我们的实现，以记录输入流经它时发生的操作序列。这样做的结果实际上是热切的PyTorch程序的“冻结”版本，TorchScript编译器可以进一步分析，优化和序列化： >>> inputs = [torch.randn(4, 8), torch.randn(8, 5), torch.randn(4, 5)] >>> trace = torch.jit.trace(compute, inputs) >>> print(trace.graph) graph(%x : Float(4, 8) %y : Float(8, 5) %z : Float(4, 5)) { %3 : Float(4, 5) = aten::matmul(%x, %y) %4 : Float(4, 5) = aten::relu(%z) %5 : int = prim::Constant[value=1]() %6 : Float(4, 5) = aten::add(%3, %4, %5) return (%6); } 现在，令人兴奋的启示是我们可以简单地将我们的自定义运算符放入我们的PyTorch跟踪中，就像它是torch.relu或任何其他torch函数一样： torch.ops.load_library(\"libwarp_perspective.so\") def compute(x, y, z): x = torch.ops.my_ops.warp_perspective(x, torch.eye(3)) return x.matmul(y) + torch.relu(z) 然后跟踪它： >>> inputs = [torch.randn(4, 8), torch.randn(8, 5), torch.randn(8, 5)] >>> trace = torch.jit.trace(compute, inputs) >>> print(trace.graph) graph(%x.1 : Float(4, 8) %y : Float(8, 5) %z : Float(8, 5)) { %3 : int = prim::Constant[value=3]() %4 : int = prim::Constant[value=6]() %5 : int = prim::Constant[value=0]() %6 : int[] = prim::Constant[value=[0, -1]]() %7 : Float(3, 3) = aten::eye(%3, %4, %5, %6) %x : Float(8, 8) = my_ops::warp_perspective(%x.1, %7) %11 : Float(8, 5) = aten::matmul(%x, %y) %12 : Float(8, 5) = aten::relu(%z) %13 : int = prim::Constant[value=1]() %14 : Float(8, 5) = aten::add(%11, %12, %13) return (%14); } 将TorchScript自定义操作集成到跟踪的PyTorch代码就像这样简单！ 使用自定义操作符和脚本 除了跟踪之外，另一种获得PyTorch程序的TorchScript表示的方法是直接在 TorchScript中编写代码_。 TorchScript在很大程度上是Python语言的一个子集，但有一些限制使得TorchScript编译器更容易推理程序。通过使用@torch.jit.script为自由函数和@torch.jit.script_method为类中的方法（必须也从torch.jit.ScriptModule派生）注释，将常规PyTorch代码转换为TorchScript。有关TorchScript注释的更多详细信息，请参见此处 使用TorchScript而不是跟踪的一个特殊原因是跟踪无法捕获PyTorch代码中的控制流。因此，让我们考虑一下这个使用控制流程的功能： def compute(x, y): if bool(x[0][0] == 42): z = 5 else: z = 10 return x.matmul(y) + z 要将此函数从vanilla PyTorch转换为TorchScript，我们使用@torch.jit.script对其进行注释： @torch.jit.script def compute(x, y): if bool(x[0][0] == 42): z = 5 else: z = 10 return x.matmul(y) + z 这将及时将compute函数编译成图表示，我们可以在compute.graph属性中检查它： >>> compute.graph graph(%x : Dynamic %y : Dynamic) { %14 : int = prim::Constant[value=1]() %2 : int = prim::Constant[value=0]() %7 : int = prim::Constant[value=42]() %z.1 : int = prim::Constant[value=5]() %z.2 : int = prim::Constant[value=10]() %4 : Dynamic = aten::select(%x, %2, %2) %6 : Dynamic = aten::select(%4, %2, %2) %8 : Dynamic = aten::eq(%6, %7) %9 : bool = prim::TensorToBool(%8) %z : int = prim::If(%9) block0() { -> (%z.1) } block1() { -> (%z.2) } %13 : Dynamic = aten::matmul(%x, %y) %15 : Dynamic = aten::add(%13, %z, %14) return (%15); } 现在，就像以前一样，我们可以使用我们的自定义运算符，就像我们脚本代码中的任何其他函数一样： torch.ops.load_library(\"libwarp_perspective.so\") @torch.jit.script def compute(x, y): if bool(x[0] == 42): z = 5 else: z = 10 x = torch.ops.my_ops.warp_perspective(x, torch.eye(3)) return x.matmul(y) + z 当TorchScript编译器看到对 torch.ops.my_ops.warp_perspective 的引用时，它将找到我们通过C++ 中的 RegisterOperators 对象注册的实现，并将其编译为其图表示： >>> compute.graph graph(%x.1 : Dynamic %y : Dynamic) { %20 : int = prim::Constant[value=1]() %16 : int[] = prim::Constant[value=[0, -1]]() %14 : int = prim::Constant[value=6]() %2 : int = prim::Constant[value=0]() %7 : int = prim::Constant[value=42]() %z.1 : int = prim::Constant[value=5]() %z.2 : int = prim::Constant[value=10]() %13 : int = prim::Constant[value=3]() %4 : Dynamic = aten::select(%x.1, %2, %2) %6 : Dynamic = aten::select(%4, %2, %2) %8 : Dynamic = aten::eq(%6, %7) %9 : bool = prim::TensorToBool(%8) %z : int = prim::If(%9) block0() { -> (%z.1) } block1() { -> (%z.2) } %17 : Dynamic = aten::eye(%13, %14, %2, %16) %x : Dynamic = my_ops::warp_perspective(%x.1, %17) %19 : Dynamic = aten::matmul(%x, %y) %21 : Dynamic = aten::add(%19, %z, %20) return (%21); } 请特别注意图表末尾对my_ops::warp_perspective的引用。 注意 TorchScript图表表示仍有可能发生变化。不要依赖它看起来像这样。 当在Python中使用我们的自定义运算符时，这就是它。简而言之，您使用torch.ops.load_library导入包含操作符的库，并像跟踪或脚本化的TorchScript代码一样调用您的自定义操作，就像任何其他torch操作符一样。 在C ++中使用TorchScript自定义运算符 TorchScript的一个有用功能是能够将模型序列化为磁盘文件。该文件可以通过线路发送，存储在文件系统中，更重要的是，可以动态反序列化和执行，而无需保留原始源代码。这可以在Python中实现，也可以在C ++中实现。为此，PyTorch为提供了一个纯C ++ API ，用于反序列化以及执行TorchScript模型。如果您还没有，请阅读关于在C ++ 中加载和运行序列化TorchScript模型的教程，接下来的几段将构建。 简而言之，即使从文件反序列化并在C ++中运行，自定义运算符也可以像常规torch运算符一样执行。对此的唯一要求是将我们之前构建的自定义操作符共享库与我们执行模型的C ++应用程序链接起来。在Python中，这只是调用torch.ops.load_library。在C ++中，您需要在您使用的任何构建系统中将共享库与主应用程序链接。以下示例将使用CMake展示此内容。 注意 从技术上讲，您也可以在运行时将共享库动态加载到C ++应用程序中，就像在Python中一样。在Linux上，你可以用dlopen 来做到这一点。在其他平台上存在等价物。 在上面链接的C ++执行教程的基础上，让我们从一个文件中的最小C ++应用程序开始，main.cpp在我们的自定义操作符的不同文件夹中，加载并执行序列化的TorchScript模型： #include // One-stop header. #include #include int main(int argc, const char* argv[]) { if (argc != 2) { std::cerr \\n\"; return -1; } //使用torch::jit::load()从文件反序列化ScriptModule。 std::shared_ptr module = torch::jit::load(argv[1]); std::vector inputs; inputs.push_back(torch::randn({4, 8})); inputs.push_back(torch::randn({8, 5})); torch::Tensor output = module->forward(std::move(inputs)).toTensor(); std::cout 连同一个小的CMakeLists.txt文件： cmake_minimum_required(VERSION 3.1 FATAL_ERROR) project(example_app) find_package(Torch REQUIRED) add_executable(example_app main.cpp) target_link_libraries(example_app \"${TORCH_LIBRARIES}\") target_compile_features(example_app PRIVATE cxx_range_for) 此时，我们应该能够构建应用程序： $ mkdir build $ cd build $ cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch .. -- The C compiler identification is GNU 5.4.0 -- The CXX compiler identification is GNU 5.4.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Found torch: /libtorch/lib/libtorch.so -- Configuring done -- Generating done -- Build files have been written to: /example_app/build $ make -j Scanning dependencies of target example_app [ 50%] Building CXX object CMakeFiles/example_app.dir/main.cpp.o [100%] Linking CXX executable example_app [100%] Built target example_app 并且在没有通过模型的情况下运行它： $ ./example_app usage: example_app 接下来，让我们序列化我们之前编写的使用自定义运算符的脚本函数： torch.ops.load_library(\"libwarp_perspective.so\") @torch.jit.script def compute(x, y): if bool(x[0][0] == 42): z = 5 else: z = 10 x = torch.ops.my_ops.warp_perspective(x, torch.eye(3)) return x.matmul(y) + z compute.save(\"example.pt\") 最后一行将脚本函数序列化为名为“example.pt”的文件。如果我们将这个序列化模型传递给我们的C ++应用程序，我们可以立即运行它： $ ./example_app example.pt terminate called after throwing an instance of 'torch::jit::script::ErrorReport' what(): Schema not found for node. File a bug report. Node: %16 : Dynamic = my_ops::warp_perspective(%0, %19) 或者可能不是。也许还没有。当然！我们尚未将自定义运算符库与我们的应用程序相关联。我们现在就这样做，为了正确地执行此操作，让我们稍微更新一下我们的文件组织，如下所示： example_app/ CMakeLists.txt main.cpp warp_perspective/ CMakeLists.txt op.cpp 这将允许我们将warp_perspective库CMake目标添加为我们的应用程序目标的子目录。 example_app文件夹中的最上层CMakeLists.txt应如下所示： cmake_minimum_required(VERSION 3.1 FATAL_ERROR) project(example_app) find_package(Torch REQUIRED) add_subdirectory(warp_perspective) add_executable(example_app main.cpp) target_link_libraries(example_app \"${TORCH_LIBRARIES}\") target_link_libraries(example_app -Wl,--no-as-needed warp_perspective) target_compile_features(example_app PRIVATE cxx_range_for) 这个基本的CMake配置看起来很像以前，除了我们将warp_perspective CMake构建添加为子目录。一旦其CMake代码运行，我们将example_app应用程序与warp_perspective共享库链接。 注意 上面的例子中嵌入了一个关键细节：warp_perspective链接线的-Wl,--no-as-needed前缀。这是必需的，因为我们实际上不会在应用程序代码中从warp_perspective共享库中调用任何函数。我们只需要运行全局RegisterOperators对象的构造函数。不方便的是，这会使链接器混乱并使其认为它可以完全跳过与库的链接。在Linux上，-Wl,--no-as-needed标志强制链接发生（注意：此标志特定于Linux！）。还有其他解决方法。最简单的是在运算符库中定义某些函数，您需要从主应用程序调用它。这可以像在某个头中声明的函数void init();一样简单，然后在运算符库中将其定义为void init() { }。在主应用程序中调用此init()函数将使链接器感觉这是一个值得链接的库。不幸的是，这超出了我们的控制范围，我们宁愿让您知道原因和简单的解决方法，而不是将一些不透明的宏交给您的代码中的plop。 现在，由于我们现在在最上层找到Torch包，warp_perspective子目录中的CMakeLists.txt文件可以缩短一点。它应该如下所示： find_package(OpenCV REQUIRED) add_library(warp_perspective SHARED op.cpp) target_compile_features(warp_perspective PRIVATE cxx_range_for) target_link_libraries(warp_perspective PRIVATE \"${TORCH_LIBRARIES}\") target_link_libraries(warp_perspective PRIVATE opencv_core opencv_photo) 让我们重新构建我们的示例应用程序，它还将与自定义运算符库链接。在最上层 example_app 目录中： $ mkdir build $ cd build $ cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch .. -- The C compiler identification is GNU 5.4.0 -- The CXX compiler identification is GNU 5.4.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Found torch: /libtorch/lib/libtorch.so -- Configuring done -- Generating done -- Build files have been written to: /warp_perspective/example_app/build $ make -j Scanning dependencies of target warp_perspective [ 25%] Building CXX object warp_perspective/CMakeFiles/warp_perspective.dir/op.cpp.o [ 50%] Linking CXX shared library libwarp_perspective.so [ 50%] Built target warp_perspective Scanning dependencies of target example_app [ 75%] Building CXX object CMakeFiles/example_app.dir/main.cpp.o [100%] Linking CXX executable example_app [100%] Built target example_app 如果我们现在运行example_app二进制文件并将其交给我们的序列化模型，我们应该得到一个圆满的结局： $ ./example_app example.pt 11.4125 5.8262 9.5345 8.6111 12.3997 7.4683 13.5969 9.0850 11.0698 9.4008 7.4597 15.0926 12.5727 8.9319 9.0666 9.4834 11.1747 9.0162 10.9521 8.6269 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 10.0000 [ Variable[CPUFloatType]{8,5} ] 成功！你现在准备推断了。 结论 本教程向您介绍了如何在C ++中实现自定义TorchScript运算符，如何将其构建到共享库中，如何在Python中使用它来定义TorchScript模型，以及最后如何将其加载到C ++应用程序中以进行推理工作负载。现在，您已准备好使用与第三方C ++库连接的C ++运算符扩展TorchScript模型，编写自定义高性能CUDA内核，或实现需要Python，TorchScript和C ++之间的界线平滑混合的任何其他用例。 与往常一样，如果您遇到任何问题或有疑问，您可以使用我们的论坛或 GitHub问题取得联系。此外，我们的常见问题解答（FAQ）页面可能会提供有用的信息。 附录A：构建自定义运算符的更多方法 “构建自定义运算符”部分介绍了如何使用CMake将自定义运算符构建到共享库中。本附录概述了另外两种编译方法。它们都使用Python作为编译过程的“驱动程序”或“接口”。此外，两者都重复使用，这是相当于TorchScript自定义运算符的香草（渴望）PyTorch，它依赖于 pybind11 来实现从C ++到Python的“显式”绑定。 第一种方法使用C ++扩展'方便的即时（JIT）编译接口，在第一次运行时在PyTorch脚本的后台编译代码。第二种方法依赖于古老的setuptools包，并涉及编写单独的setup.py文件。这允许更高级的配置以及与其他基于setuptools的项目的集成。我们将在下面详细探讨这两种方法。 使用JIT编译构建 PyTorch C ++扩展工具包提供的JIT编译功能允许将自定义运算符的编译直接嵌入到Python代码中，例如：在训练脚本的顶部。 注意 这里的“JIT编译”与TorchScript编译器中的JIT编译无关，以优化您的程序。它只是意味着您的自定义操作符C ++代码将在您第一次导入时在系统的/tmp目录下的文件夹中编译，就像您事先已经自己编译它一样。 这个JIT编译功能有两种形式。首先，您仍然将运算符实现保存在单独的文件（op.cpp）中，然后使用torch.utils.cpp_extension.load()编译扩展。通常，此函数将返回公开C ++扩展的Python模块。但是，由于我们没有将自定义运算符编译到自己的Python模块中，因此我们只想编译普通的共享库。幸运的是，torch.utils.cpp_extension.load()有一个参数is_python_module我们可以设置为False来表示我们只对构建共享库而不是Python模块感兴趣。然后torch.utils.cpp_extension.load()将编译并将共享库加载到当前进程中，就像之前torch.ops.load_library所做的那样： import torch.utils.cpp_extension torch.utils.cpp_extension.load( name=\"warp_perspective\", sources=[\"op.cpp\"], extra_ldflags=[\"-lopencv_core\", \"-lopencv_imgproc\"], is_python_module=False, verbose=True ) print(torch.ops.my_ops.warp_perspective) 这应该大致打印： JIT编译的第二种风格允许您将自定义TorchScript运算符的源代码作为字符串传递。为此，请使用torch.utils.cpp_extension.load_inline： import torch import torch.utils.cpp_extension op_source = \"\"\" #include #include torch::Tensor warp_perspective(torch::Tensor image, torch::Tensor warp) { cv::Mat image_mat(/*rows=*/image.size(0), /*cols=*/image.size(1), /*type=*/CV_32FC1, /*data=*/image.data()); cv::Mat warp_mat(/*rows=*/warp.size(0), /*cols=*/warp.size(1), /*type=*/CV_32FC1, /*data=*/warp.data()); cv::Mat output_mat; cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{64, 64}); torch::Tensor output = torch::from_blob(output_mat.ptr(), /*sizes=*/{64, 64}); return output.clone(); } static auto registry = torch::jit::RegisterOperators(\"my_ops::warp_perspective\", &warp_perspective); \"\"\" torch.utils.cpp_extension.load_inline( name=\"warp_perspective\", cpp_sources=op_source, extra_ldflags=[\"-lopencv_core\", \"-lopencv_imgproc\"], is_python_module=False, verbose=True, ) print(torch.ops.my_ops.warp_perspective) 当然，如果源代码相当短，最好只使用torch.utils.cpp_extension.load_inline。 使用Setuptools构建 从Python独家构建自定义运算符的第二种方法是使用setuptools。这样做的好处是setuptools具有非常强大和广泛的接口，用于构建用C ++编写的Python模块。但是，由于setuptools实际上是用于构建Python模块而不是普通的共享库（它没有Python期望从模块中获得的必要入口点），因此这条路线可能有点古怪。也就是说，你只需要一个setup.py文件代替CMakeLists.txt，如下所示： 请注意，我们在底部的BuildExtension中启用了no_python_abi_suffix选项。这指示setuptools在生成的共享库的名称中省略任何Python-3特定的ABI后缀。否则，在Python 3.7上，例如，库可能被称为warp_perspective.cpython-37m-x86_64-linux-gnu.so，其中cpython-37m-x86_64-linux-gnu是ABI标签，但我们真的只是想要它被称为warp_perspective.so 如果我们现在在setup.py所在的文件夹中的终端中运行python setup.py build develop，我们应该看到类似的内容： $ python setup.py build develop running build running build_ext building 'warp_perspective' extension creating build creating build/temp.linux-x86_64-3.7 gcc -pthread -B /root/local/miniconda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/TH -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/THC -I/root/local/miniconda/include/python3.7m -c op.cpp -o build/temp.linux-x86_64-3.7/op.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=warp_perspective -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11 cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++ creating build/lib.linux-x86_64-3.7 g++ -pthread -shared -B /root/local/miniconda/compiler_compat -L/root/local/miniconda/lib -Wl,-rpath=/root/local/miniconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/op.o -lopencv_core -lopencv_imgproc -o build/lib.linux-x86_64-3.7/warp_perspective.so running develop running egg_info creating warp_perspective.egg-info writing warp_perspective.egg-info/PKG-INFO writing dependency_links to warp_perspective.egg-info/dependency_links.txt writing top-level names to warp_perspective.egg-info/top_level.txt writing manifest file 'warp_perspective.egg-info/SOURCES.txt' reading manifest file 'warp_perspective.egg-info/SOURCES.txt' writing manifest file 'warp_perspective.egg-info/SOURCES.txt' running build_ext copying build/lib.linux-x86_64-3.7/warp_perspective.so -> Creating /root/local/miniconda/lib/python3.7/site-packages/warp-perspective.egg-link (link to .) Adding warp-perspective 0.0.0 to easy-install.pth file Installed /warp_perspective Processing dependencies for warp-perspective==0.0.0 Finished processing dependencies for warp-perspective==0.0.0 这将生成一个名为warp_perspective.so的共享库，我们可以像之前那样将其传递给torch.ops.load_library，以使我们的操作符对TorchScript可见： 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"tut_production_usage.html":{"url":"tut_production_usage.html","title":"生产性使用","keywords":"","body":"生产性使用 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"dist_tuto.html":{"url":"dist_tuto.html","title":"Writing Distributed Applications with PyTorch","keywords":"","body":"使用PyTorch编写分布式应用程序 译者：firdameng 作者：Soumith Chintala 在这个简短的教程中，我们将讨论PyTorch的分布式软件包。 我们将看到如何设置分布式设置，使用不同的通信策略，并查看包的内部部分。 开始 PyTorch中包含的分布式软件包（即torch.distributed）使研究人员和从业人员能够轻松地跨进程和计算机集群并行化他们的计算。 为此，它利用消息传递语义，允许每个进程将数据传递给任何其他进程。 与多处理（torch.multiprocessing）包相反，进程可以使用不同的通信后端，并且不限于在同一台机器上执行。 开始我们需要能够同时运行多个进程。 如果您有权访问计算群集，则应使用本地sysadmin进行检查，或使用您喜欢的协调工具。 （例如，pdsh，clustershell或其他）为了本教程的目的，我们将使用单个机器并使用以下模板建立多个进程。 \"\"\"run.py:\"\"\" #!/usr/bin/env python import os import torch import torch.distributed as dist from torch.multiprocessing import Process def run(rank, size): \"\"\" Distributed function to be implemented later. \"\"\" pass def init_processes(rank, size, fn, backend='tcp'): \"\"\" Initialize the distributed environment. \"\"\" os.environ['MASTER_ADDR'] = '127.0.0.1' os.environ['MASTER_PORT'] = '29500' dist.init_process_group(backend, rank=rank, world_size=size) fn(rank, size) if __name__ == \"__main__\": size = 2 processes = [] for rank in range(size): p = Process(target=init_processes, args=(rank, size, run)) p.start() processes.append(p) for p in processes: p.join() 上面的脚本产生了两个进程，每个进程将设置分布式环境，初始化进程组（dist.init_process_group），最后执行给定的运行函数。 我们来看看init_processes函数。 它确保每个进程都能够使用相同的IP地址和端口通过主站进行协调。 请注意，我们使用了TCP后端，但我们可以使用MPI或Gloo。 （参见5.1节）我们将在本教程结束时讨论dist.init_process_group中产生的特效，但它实质上允许进程通过共享其位置来相互通信。 点对点通信 发送与接收 将数据从一个进程传输到另一个进程称为点对点通信。 这些是通过send和recv函数或它们的直接对应部分isend和irecv实现的。 \"\"\"Blocking point-to-point communication.\"\"\" def run(rank, size): tensor = torch.zeros(1) if rank == 0: tensor += 1 # Send the tensor to process 1 dist.send(tensor=tensor, dst=1) else: # Receive tensor from process 0 dist.recv(tensor=tensor, src=0) print('Rank ', rank, ' has data ', tensor[0]) 在上面的例子中，两个进程都以零张量开始，然后进程0递增张量并将其发送到进程1，以便它们都以1.0结束。 请注意，进程1需要分配内存以存储它将接收的数据。 另请注意，send / recv正在阻塞：两个进程都会停止，直到通信完成。 另一方面，immediates是非阻塞的; 脚本继续执行，方法返回一个DistributedRequest对象，我们可以选择wait（）。 \"\"\"Non-blocking point-to-point communication.\"\"\" def run(rank, size): tensor = torch.zeros(1) req = None if rank == 0: tensor += 1 # Send the tensor to process 1 req = dist.isend(tensor=tensor, dst=1) print('Rank 0 started sending') else: # Receive tensor from process 0 req = dist.irecv(tensor=tensor, src=0) print('Rank 1 started receiving') req.wait() print('Rank ', rank, ' has data ', tensor[0]) 当使用immediates时，我们必须小心使用发送和接收的张量。 由于我们不知道何时将数据传递给另一个进程，因此我们不应该在req.wait（）完成之前修改发送的张量或访问接收的张量。 换一种说法， 在dist.isend（）之后写入张量将导致未定义的行为。 在dist.irecv（）之后读取张量将导致未定义的行为。 但是，在执行req.wait（）之后，我们保证发生通信，并且存储在tensor [0]中的值为1.0。 当我们想要对流程的通信进行细粒度控制时，点对点通信非常有用。 它们可用于实现奇妙的算法，例如百度DeepSpeech或Facebook的大规模实验中使用的算法。（参见4.1节） 集体通信 Scatter Gather Reduce All-Reduce Broadcast All_gather 与点对点通信相反，在集体中允许通信模式跨越组中所有进程。 组是我们所有进程的子集。 要创建组，我们可以将队列列表传递给dist.new_group（组）。 默认情况下，集合体在所有进程（也称为world）上执行。 例如，为了获得所有过程中所有张量的总和，我们可以使用dist.all_reduce（tensor，op，group）集合。 \"\"\" All-Reduce example.\"\"\" def run(rank, size): \"\"\" Simple point-to-point communication. \"\"\" group = dist.new_group([0, 1]) tensor = torch.ones(1) dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group) print('Rank ', rank, ' has data ', tensor[0]) 由于我们想要组中所有张量的总和，我们使用dist.reduce_op.SUM作为reduce运算符。 一般而言，任何可交换的数学运算都可以用作运算符。 开箱即用，PyTorch带有4个这样的运算符，所有运算符都在元素级别上运行： dist.reduce_op.SUM, dist.reduce_op.PRODUCT, dist.reduce_op.MAX, dist.reduce_op.MIN. 除了dist.all_reduce（tensor，op，group）之外，PyTorch目前共有6个集体。 dist.broadcast(tensor, src, group): Copies tensor from src to all other processes. dist.reduce(tensor, dst, op, group): Applies op to all tensor and stores the result in dst. dist.all_reduce(tensor, op, group): Same as reduce, but the result is stored in all processes. dist.scatter(tensor, src, scatter_list, group): Copies the i^{\\text{th}} tensor scatter_list[i] to the i^{\\text{th}} process. dist.gather(tensor, dst, gather_list, group): Copies tensor from all processes in dst. dist.all_gather(tensor_list, tensor, group): Copies tensor from all processes to tensor_list, on all processes. dist.barrier(group): block all processes in group until each one has entered this function. 分布式训练 注意：您可以在此GitHub存储库中找到此部分的 示例脚本 现在我们已经了解了分布式模块的工作原理，让我们编写一些有用的东西。 我们的目标是复制DistributedDataParallel的功能。 当然，这将是一个教学示例，在现实世界中，您应该使用上面链接的官方，经过良好测试和优化的版本。 很简单，我们想要实现随机梯度下降的分布式版本。 我们的脚本将允许所有进程在其批量数据上计算其模型的梯度，然后平均其渐变。 为了在更改进程数时确保类似的收敛结果，我们首先必须对数据集进行分区。 （您也可以使用tnt.dataset.SplitDataset，而不是下面的代码段。） \"\"\" Dataset partitioning helper \"\"\" class Partition(object): def __init__(self, data, index): self.data = data self.index = index def __len__(self): return len(self.index) def __getitem__(self, index): data_idx = self.index[index] return self.data[data_idx] class DataPartitioner(object): def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234): self.data = data self.partitions = [] rng = Random() rng.seed(seed) data_len = len(data) indexes = [x for x in range(0, data_len)] rng.shuffle(indexes) for frac in sizes: part_len = int(frac * data_len) self.partitions.append(indexes[0:part_len]) indexes = indexes[part_len:] def use(self, partition): return Partition(self.data, self.partitions[partition]) 通过上面的代码片段，我们现在可以使用以下几行简单地对任何数据集进行分区： \"\"\" Partitioning MNIST \"\"\" def partition_dataset(): dataset = datasets.MNIST('./data', train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])) size = dist.get_world_size() bsz = 128 / float(size) partition_sizes = [1.0 / size for _ in range(size)] partition = DataPartitioner(dataset, partition_sizes) partition = partition.use(dist.get_rank()) train_set = torch.utils.data.DataLoader(partition, batch_size=bsz, shuffle=True) return train_set, bsz 假设我们有2个副本，那么每个进程将具有60000/2 = 30000个样本的train_set。 我们还将批量大小除以副本数量，以保持总批量大小为128。 我们现在可以编写我们通常的前向后向优化训练代码，并添加一个函数调用来平均我们模型的渐变。 （以下内容主要来自官方的PyTorch MNIST示例。） \"\"\" Distributed Synchronous SGD Example \"\"\" def run(rank, size): torch.manual_seed(1234) train_set, bsz = partition_dataset() model = Net() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) num_batches = ceil(len(train_set.dataset) / float(bsz)) for epoch in range(10): epoch_loss = 0.0 for data, target in train_set: optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) epoch_loss += loss.item() loss.backward() average_gradients(model) optimizer.step() print('Rank ', dist.get_rank(), ', epoch ', epoch, ': ', epoch_loss / num_batches) 它仍然是实现average_gradients（模型）函数，它只是简单地接受一个模型并在整个空间中平均其渐变。 \"\"\" Gradient averaging. \"\"\" def average_gradients(model): size = float(dist.get_world_size()) for param in model.parameters(): dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM) param.grad.data /= size 我们成功实现了分布式同步SGD，可以在大型计算机集群上训练任何模型。 注意：虽然最后一句在技术上是正确的，但实现同步SGD的生产级实现需要更多技巧。 再次，使用已经过测试和优化的内容。 自定义Ring-Allreduce 作为一个额外的挑战，想象一下我们想要实现DeepSpeech的高效环allreduce。 使用点对点集合相当容易实现。 \"\"\" Implementation of a ring-reduce with addition. \"\"\" def allreduce(send, recv): rank = dist.get_rank() size = dist.get_world_size() send_buff = th.zeros(send.size()) recv_buff = th.zeros(send.size()) accum = th.zeros(send.size()) accum[:] = send[:] left = ((rank - 1) + size) % size right = (rank + 1) % size for i in range(size - 1): if i % 2 == 0: # Send send_buff send_req = dist.isend(send_buff, right) dist.recv(recv_buff, left) accum[:] += recv[:] else: # Send recv_buff send_req = dist.isend(recv_buff, right) dist.recv(send_buff, left) accum[:] += send[:] send_req.wait() recv[:] = accum[:] 在上面的脚本中，allreduce（send，recv）函数的签名与PyTorch中的签名略有不同。 它需要一个recv张量，并将所有发送张量的总和存储在其中。 作为练习留给读者，我们的版本和DeepSpeech中的版本之间仍然存在一个区别：它们的实现将梯度张量划分为块，以便最佳地利用通信带宽。 （提示：torch.chunk） 高级主题 我们现在准备发现torch.distributed的一些更高级的功能。 由于有很多内容需要介绍，本节分为两个小节： 通信后端：我们学习如何使用MPI和Gloo进行GPU-GPU通信。 初始化方法：我们了解如何在dist.init_process_group（）中最好地设置初始协调阶段。 通信后端 torch.distributed最优雅的方面之一是它能够在不同的后端之上进行抽象和构建。 如前所述，目前在PyTorch中实现了三个后端：TCP，MPI和Gloo。 根据所需的用例，它们各自具有不同的规格和权衡。 可以在此处找到支持功能的比较表。 请注意，自本教程创建以来，已添加第四个后端NCCL。 有关其使用和值的更多信息，请参阅torch.distributed docs的此部分。 TCP后端 到目前为止，我们已广泛使用TCP后端。 它作为一个开发平台非常方便，因为它可以保证在大多数机器和操作系统上运行。 它还支持CPU上的所有点对点和集合功能。 但是，不支持GPU，并且其通信例程不像MPI那样优化。 Gloo后端 Gloo后端为CPU和GPU提供了集体通信程序的优化实现。 它特别适用于GPU，因为它可以执行通信而无需使用GPUDirect将数据传输到CPU的内存。 它还能够使用NCCL执行快速的节点内通信，并实现其自己的节点间例程算法。 从版本0.2.0开始，Gloo后端自动包含在PyTorch的预编译二进制文件中。 正如您已经注意到的那样，如果您将模型放在GPU上，我们的分布式SGD示例将不起作用。 让我们通过首先替换init_processes中的backend ='gloo'来修复它（rank，size，fn，backend ='tcp'）。 此时，脚本仍将在CPU上运行，但在幕后使用Gloo后端。 为了使用多个GPU，我们还要进行以下修改： init_processes(rank, size, fn, backend='tcp') \\rightarrow init_processes(rank, size, fn, backend='gloo') Use device = torch.device(\"cuda:{}\".format(rank)) model = Net() \\rightarrow model = Net().to(device) Use data, target = data.to(device), target.to(device) 通过上述修改，我们的模型现在在两个GPU上进行培训，您可以通过运行nvidia-smi监控它们的使用情况。 MPI后端 消息传递接口（MPI）是高性能计算领域的标准化工具。 它允许进行点对点和集体通信，并且是torch.distributed的API的主要灵感。 存在MPI的若干实现（例如，Open-MPI，MVAPICH2，Intel MPI），每个实现针对不同目的而优化。 使用MPI后端的优势在于MPI在大型计算机集群上的广泛可用性和高级优化。 最近的一些实现也能够利用CUDA IPC和GPU Direct技术，以避免通过CPU进行内存复制。 不幸的是，PyTorch的二进制文件不能包含MPI实现，我们必须手动重新编译它。 幸运的是，这个过程非常简单，因为在编译时，PyTorch会自行查看可用的MPI实现。 以下步骤通过从源安装PyTorch来安装MPI后端。 创建并激活您的Anaconda环境，按照指南安装所有先决条件，但不要运行python setup.py install。 选择并安装您最喜欢的MPI实现。 请注意，启用支持CUDA的MPI可能需要一些额外的步骤。 在我们的例子中，我们将坚持不支持GPU的Open-MPI：conda install -c conda-forge openmpi 现在，转到克隆的PyTorch repo并执行python setup.py install。 为了测试我们新安装的后端，需要进行一些修改。 使用initprocesses（0,0，run，backend ='mpi'）替换if name =='_ main'下的内容： 运行mpirun -n 4 python myscript.py。 这些更改的原因是MPI需要在生成流程之前创建自己的环境。 MPI还将生成自己的进程并执行初始化方法中描述的握手，使得init_process_group的rankand size参数变得多余。 这实际上非常强大，因为您可以将其他参数传递给mpirun，以便为每个进程定制计算资源。 （例如每个进程的内核数量，将机器分配给特定的等级，以及更多内容）这样做，您应该获得与其他通信后端相同的熟悉输出。 初始化方法 为了完成本教程，我们来谈谈我们调用的第一个函数：dist.init_process_group（backend，init_method）。 特别是，我们将讨论不同的初始化方法，这些方法负责每个进程之间的初始协调步骤。 这些方法允许您定义如何完成此协调。 根据您的硬件设置，其中一种方法应该比其他方法更合适。 除了以下部分，您还应该查看官方文档。 在深入研究初始化方法之前，让我们从C / C ++的角度快速了解init_process_group背后的情况。 首先，解析和验证参数。 后端通过name2channel.at（）函数解析。 返回Channel类，将用于执行数据传输。 GIL被删除，并调用THDProcessGroupInit（）。 这会实例化通道并添加主节点的地址。 等级0的过程将执行主过程，而所有其他等级将是工作进程。 主进程 （1）为所有工作进程创建套接字。 （2）等待所有工作进程连接。 （3）向他们发送有关其他进程位置的信息。 每个工作进程 （1）为主进程创建一个套接字。 （2）发送自己的位置信息。 （3）接收有关其他工作进程的信息。 （4）打开套接字并与所有其他工作进程握手。 初始化完成，每个进程都相互建立连接。 环境变量 在本教程中，我们一直在使用环境变量初始化方法。 通过在所有计算机上设置以下四个环境变量，所有进程都能够正确连接到主进程，获取有关其他进程的信息，最后与它们握手。 MASTER_PORT：计算机上的一个空闲端口，用于承载排名为0的进程。 MASTER_ADDR：将以0级托管进程的计算机的IP地址。 WORLD_SIZE：进程总数，以便master知道要等待多少worker。 RANK：每个流程的等级，因此他们将知道它是否是worker的master。 共享文件系统 共享文件系统要求所有进程都可以访问共享文件系统，并通过共享文件协调它们。 这意味着每个进程都将打开文件，写入其信息，并等到每个人都这样做。 在所有必需信息将随时可用于所有流程之后。 为了避免竞争条件，文件系统必须支持通过fcntl锁定。 请注意，您可以手动指定排名，也可以让流程自行计算。 要为每个作业定义一个唯一的组名，您可以为多个作业使用相同的文件路径并安全地避免冲突。 dist.init_process_group(init_method='file:///mnt/nfs/sharedfile', world_size=4, group_name='mygroup') TCP初始化和多播 通过TCP初始化可以通过两种不同的方式实现： 通过提供具有等级0和世界大小的进程的IP地址。 通过提供任何有效的IP多播地址和世界大小。 在第一种情况下，所有工作进程将能够连接到等级为0的进程并按照上述步骤进行操作。 dist.init_process_group(init_method='tcp://10.1.1.20:23456', rank=args.rank, world_size=4) 在第二种情况下，多播地址指定可能处于活动状态的节点组，并且可以通过允许每个进程在执行上述过程之前进行初始握手来处理协调。 此外，TCP多播初始化还支持group_name参数（与共享文件方法一样），允许在同一群集上调度多个作业。 dist.init_process_group(init_method='tcp://[ff15:1e18:5d4c:4cf0:d02d:b659:53ba:b0a7]:23456', world_size=4) 致谢 我要感谢PyTorch开发人员在他们的实现，文档和测试方面做得很好。 当代码不清楚时，我总是可以依靠文档或测试来找到答案。 特别是，我要感谢Soumith Chintala，Adam Paszke和Natalia Gimelshein提供有见地的评论并回答有关早期草稿的问题。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"aws_distributed_training_tutorial.html":{"url":"aws_distributed_training_tutorial.html","title":"使用 Amazon AWS 进行分布式训练","keywords":"","body":"PyTorch 1.0 使用 Amazon AWS 进行分布式训练 译者：yportne13 作者: Nathan Inkawhich 编辑: Teng Li 在这篇教程中我们会展示如何使用 Amazon AWS 的两个多路GPU节点来设置，编写和运行 PyTorch 1.0 分布式训练程序。首先我们会介绍 AWS 设置, 然后是 PyTorch 环境配置, 最后是分布式训练的代码。你会发现想改成分布式应用你只需要对你目前写的训练程序做很少的代码改动, 绝大多数工作都只是一次性的环境配置。 Amazon AWS 设置 在这篇教程中我们会在两个多路 GPU 节点上运行分布式训练。在这一节中我们首先会展示如何创建节点，然后是设置安全组(security group)来让节点之间能够通信。 创建节点 在 Amazon AWS 上创建一个实例需要七个步骤。首先，登录并选择 Launch Instance. 1: 选择 Amazon Machine Image (AMI) - 我们选择 Deep Learning AMI (Ubuntu) Version 14.0。 正如介绍中所说的，这个实例安装了许多流行的深度学习框架并已经配置好了 CUDA, cuDNN 和 NCCL。 这是一个很好的开始。 2: 选择一个实例类型 - 选择GPU计算单元 p2.8xlarge。 注意，每个实例的价格不同，这个实例为每个节点提供 8 个 NVIDIA Tesla K80 GPU，并且提供了适合多路 GPU 分布式训练的架构。 3: 设置实例的细节 - 唯一需要设置的就是把 Number of instances 加到 2。其他的都可以保留默认设置。 4: 增加存储空间 - 注意, 默认情况下这些节点并没有很大的存储空间 (只有 75 GB)。对于这个教程, 我们只使用 STL-10 数据集, 存储空间是完全够用的。但如果你想要训练一个大的数据集比如 ImageNet , 你需要根据数据集和训练模型去增加存储空间。 5: 加 Tags - 这一步没有什么需要设置的，直接进入下一步。 6: 设置安全组(Security Group) - 这一步很重要。默认情况下同一安全组的两个节点无法在分布式训练设置下通信。 这里我们想要创建一个新的安全组并将两个节点加入组内。 但是我们没法在这一步完成这一设置。记住你设置的新的安全组名(例如 launch-wizard-12)然后进入下一步步骤7。 7: 确认实例启动 - 接下来，确认例程并启动它。 默认情况下这会自动开始两个实例的初始化。你可以通过控制面板监视初始化的进程。 设置安全组 我们刚才在创建实例的时候没办法正确设置安全组。当你启动好实例后，在 EC2 的控制面板选择 Network & Security > Security Groups 选项。 这将显示你有权限访问的安全组列表。 选择你在第六步创建的新的安全组(也就是 launch-wizard-12), 会弹出选项 Description, Inbound, Outbound, and Tags。 首先，选择 Inbound 的 Edit 选项添加规则以允许来自 launch-wizard-12 安全组内源(“Sources”)的所有流量(“All Traffic”)。 然后选择 Outbound 选项并做同样的工作。 现在，我们有效地允许了 launch-wizard-12 安全组所有类型的入站和出站流量(Inbound and Outbound traffic)。 必要的信息 继续下一步之前，我们必须找到并记住节点的IP地址。 在 EC2 的控制面板找到你正在运行的实例。 记下实例的 IPv4 Public IP 和 Private IPs。 在之后的文档中我们会把这些称为 node0-publicIP, node0-privateIP, node1-publicIP 和 node1-privateIP。 其中 public IP 地址用来 SSH 登录, private IP 用来节点间通信。 环境配置 下一个重要步骤是设置各个节点。 不幸的是，我们不能同时设置两个节点, 所以这一步必须每个节点分别做一遍。 然而，这是一次性的设置，一旦你的节点设置正常你就不需要再为你未来的分布式训练项目重新设置了。 第一步，登录节点，创建一个带 python 3.6 和 numpy 的 conda 环境。 创建完成后激活环境。 $ conda create -n nightly_pt python=3.6 numpy $ source activate nightly_pt 下一步，我们使用 pip 在 conda 环境中每日编译 (nightly build) 支持 Cuda 9.0 的 PyTorch 。 $ pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu90/torch_nightly.html 我们还需要安装 torchvision 来使用 torchvision 的模型和数据集。这次我们需要从源代码构建 torchvision 因为使用 pip 安装会默认安装老版本的 PyTorch 。 $ cd $ git clone https://github.com/pytorch/vision.git $ cd vision $ python setup.py install 最后, 一步很重要的步骤是为 NCCL 设置网络接口名。这步通过设置环境变量 NCCL_SOCKET_IFNAME 来实现。 为了获得正确的名字，在节点上执行 ifconfig 命令并看和节点对应的 privateIP (例如 ens3)接口名字。 然后设置环境变量如下 $ export NCCL_SOCKET_IFNAME=ens3 记住，对所有节点都执行这个操作。 你也许还需要考虑对 .bashrc 添加 NCCL_SOCKET_IFNAME。 注意到我们没有在节点间设置共享文件系统。 因此，每个节点都需要复制一份代码和数据集。 想要了解更多有关设置节点间共享文件系统参考这里. 分布式训练代码 实例开始运行，环境配置好了以后我们可以开始准备训练代码了。绝大多数代码是从 PyTorch ImageNet Example 来的，这些代码同样支持分布式训练。以这个代码为基础你可以搭自己的训练代码因为它有标准的训练循环，验证循环和准确率追踪函数。然而，你会注意到为了简洁起见参数解析和其他非必须的函数被去掉了。 在这个例子中我们会使用 torchvision.models.resnet18 模型并将会在 torchvision.datasets.STL10 数据集上训练它。 为了解决 STL-10 和 Resnet18 维度不匹配的问题, 我们将会使用一个变换把图片的尺寸改为 224x224。 注意到，对于分布式训练代码，模型和数据集的选择是正交(orthogonal)的, 你可以选择任何你想用的数据集和模型，操作的步骤是一样的。 让我们首先操作 import 和一些辅助函数。然后我们会定义 train 和 test 函数，这些都可以从 ImageNet Example 例程中大量复制出来。 结尾部分，我们会搭建代码的 main 部分来设置分布式训练。 最后我们会讨论如何让代码运行起来。 Imports 在分布式训练中特别需要 import 的东西是 torch.nn.parallel, torch.distributed, torch.utils.data.distributed 和 torch.multiprocessing。同时需要把多进程的 start 方法(multiprocessing start method) 设置为 spawn 或 forkserver (仅支持 Python 3), 因为默认是 fork 会导致使用多进程加载数据时锁死。 import time import sys import torch if __name__ == '__main__': torch.multiprocessing.set_start_method('spawn') import torch.nn as nn import torch.nn.parallel import torch.distributed as dist import torch.optim import torch.utils.data import torch.utils.data.distributed import torchvision.transforms as transforms import torchvision.datasets as datasets import torchvision.models as models from torch.multiprocessing import Pool, Process 辅助函数 我们还需要定义一些辅助函数和类来使训练更简单。 AverageMeter 类追踪训练的状态比如准确率和循环次数。accuracy 函数计算并返还模型的 top-k 准确率这样我们就可以跟踪学习进程。 这两个都是为了训练方便而不是为分布式训练特别设定的。 class AverageMeter(object): \"\"\"Computes and stores the average and current value\"\"\" def __init__(self): self.reset() def reset(self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 def update(self, val, n=1): self.val = val self.sum += val * n self.count += n self.avg = self.sum / self.count def accuracy(output, target, topk=(1,)): \"\"\"Computes the precision@k for the specified values of k\"\"\" with torch.no_grad(): maxk = max(topk) batch_size = target.size(0) _, pred = output.topk(maxk, 1, True, True) pred = pred.t() correct = pred.eq(target.view(1, -1).expand_as(pred)) res = [] for k in topk: correct_k = correct[:k].view(-1).float().sum(0, keepdim=True) res.append(correct_k.mul_(100.0 / batch_size)) return res 训练函数 为了简化 main 循环，最好把一步 training epoch 放进 train 函数中。 这个函数用于训练一个 epoch 的 train_loader 的输入模型。 唯一需要为分布式训练特别调整的是在前向传播前将数据和标签张量的 non_blocking 属性设置为 True。 这允许异步 GPU 复制数据也就是说计算和数据传输可以同时进行。 这个函数同时也输出训练状态这样我们就可以在整个 epoch 中跟踪进展。 另一个需要定义的函数是 adjust_learning_rate, 这个函数以一个固定的方式调低学习率。这也是一个标准的训练函数，有助于训练准确的模型。 def train(train_loader, model, criterion, optimizer, epoch): batch_time = AverageMeter() data_time = AverageMeter() losses = AverageMeter() top1 = AverageMeter() top5 = AverageMeter() # switch to train mode 转到训练模式 model.train() end = time.time() for i, (input, target) in enumerate(train_loader): # measure data loading time 计算加载数据的时间 data_time.update(time.time() - end) # Create non_blocking tensors for distributed training 为分布式训练创建 non_blocking 张量 input = input.cuda(non_blocking=True) target = target.cuda(non_blocking=True) # compute output 计算输出 output = model(input) loss = criterion(output, target) # measure accuracy and record loss 计算准确率并记录 loss prec1, prec5 = accuracy(output, target, topk=(1, 5)) losses.update(loss.item(), input.size(0)) top1.update(prec1[0], input.size(0)) top5.update(prec5[0], input.size(0)) # compute gradients in a backward pass 在反向传播中计算梯度 optimizer.zero_grad() loss.backward() # Call step of optimizer to update model params 调用一个 optimizer 步骤来更新模型参数 optimizer.step() # measure elapsed time 计算花费的时间 batch_time.update(time.time() - end) end = time.time() if i % 10 == 0: print('Epoch: [{0}][{1}/{2}]\\t' 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' 'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t' 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t' 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t' 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format( epoch, i, len(train_loader), batch_time=batch_time, data_time=data_time, loss=losses, top1=top1, top5=top5)) def adjust_learning_rate(initial_lr, optimizer, epoch): \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\" lr = initial_lr * (0.1 ** (epoch // 30)) for param_group in optimizer.param_groups: param_group['lr'] = lr 验证函数 为了进一步简化 main 循环和追踪进程我们可以把验证过程放进命名为 validate 的函数中。 这个函数对输入的验证集数据在输入模型上执行一个完整的验证步骤并返还验证集对该模型的 top-1 准确率。 和刚才一样，你会注意到这里唯一需要为分布式训练特别设置的特性依然是在传递进模型前将训练数据和标签值设定 non_blocking=True。 def validate(val_loader, model, criterion): batch_time = AverageMeter() losses = AverageMeter() top1 = AverageMeter() top5 = AverageMeter() # switch to evaluate mode 转到验证模式 model.eval() with torch.no_grad(): end = time.time() for i, (input, target) in enumerate(val_loader): input = input.cuda(non_blocking=True) target = target.cuda(non_blocking=True) # compute output 计算输出 output = model(input) loss = criterion(output, target) # measure accuracy and record loss 计算准确率并记录 loss prec1, prec5 = accuracy(output, target, topk=(1, 5)) losses.update(loss.item(), input.size(0)) top1.update(prec1[0], input.size(0)) top5.update(prec5[0], input.size(0)) # measure elapsed time 计算花费时间 batch_time.update(time.time() - end) end = time.time() if i % 100 == 0: print('Test: [{0}/{1}]\\t' 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t' 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t' 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format( i, len(val_loader), batch_time=batch_time, loss=losses, top1=top1, top5=top5)) print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}' .format(top1=top1, top5=top5)) return top1.avg 输入 随着辅助函数的出现，我们进入了有趣的部分。这里我们将会定义程序的输入部分。一些输入的参数是标准的训练模型的输入比如 batch size 和训练的 epoch 数, 而有些则是我们的分布式训练任务特别需要的。需要的输入参数是： batch_size - 分布式训练组中单一进程的 batch size。 整个分布式模型总的 batch size 是 batch_size*world_size workers - 每个进程中数据加载使用的工作进程数 num_epochs - 总的训练的 epoch 数 starting_lr - 开始训练时的学习率 world_size - 分布式训练环境的进程数 dist_backend - 分布式训练通信使用的后端框架 (也就是 NCCL, Gloo, MPI 等)。 在这篇教程中因为我们使用了多个多路 GPU 节点因此推荐 NCCL。 dist_url - 确定进程组的初始化方法的 URL。 这可能包含 IP 地址和 rank0 进程的端口或者是一个在共享文件系统中的 non-existant 文件。 这里由于我们没有共享文件系统因此是包含 node0-privateIP 和要使用的 node0 的端口的 url。 print(\"Collect Inputs...\") # Batch Size for training and testing 训练和测试的 batch size batch_size = 32 # Number of additional worker processes for dataloading 数据加载的额外工作进程数 workers = 2 # Number of epochs to train for 训练的 epoch 数 num_epochs = 2 # Starting Learning Rate 初始学习率 starting_lr = 0.1 # Number of distributed processes 分布式进程数 world_size = 4 # Distributed backend type 分布式后端类型 dist_backend = 'nccl' # Url used to setup distributed training 设置分布式训练的 url dist_url = \"tcp://172.31.22.234:23456\" 初始化进程组 在使用 PyTorch 进行分布式训练中有一个很重要的部分是正确设置进程组, 也就是初始化 torch.distributed 包的第一步。为了完成这一步我们将会使用 torch.distributed.init_process_group 函数，这个函数需要几个输入参数。首先，需要输入 backend 参数，这个参数描述了需要什么后端(也就是 NCCL, Gloo, MPI 等)。 输入参数 init_method 同时也是包含 rank0 地址和端口的 url 或是共享文件系统上的 non-existant 文件路径。注意，为了使用文件的 initmethod, 所有机器必须有访问文件的权限，和使用 url 方法类似，所有机器必须要能够联网通信所以确保防火墙和网络设置正确。 _init_process_group 函数也接受 rank 和 world_size 参数，这些参数表明了进程运行时的编号并分别展示了集群内的进程数。init_method 也可以是 “env://”。 在这种情况下，rank0 机器的地址和端口将会分别从以下环境变量中读出来：MASTERADDR, MASTER_PORT。 如果 _rank 和 world_size 参数没有在 init_process_group 函数中表示出来，他们都可以从以下环境变量中分别读出来：RANK, WORLD_SIZE。 另一个重要步骤，尤其是当一个节点使用多路 gpu 的时候，就是设置进程的 local_rank。 例如，如果你有两个节点，每个节点有8个 GPU 并且你希望使用所有 GPU 来训练那么设置 world\\_size=16 这样每个节点都会有一个本地编号为 0-7 的进程。 这个本地编号(local_rank) 是用来为进程配置设备 (也就是所使用的 GPU ) 并且之后用来创建分布式数据并行模型时配置设备。 在这样的假定环境下同样推荐使用 NCCL 后端因为 NCCL 更适合多路 gpu 节点。 print(\"Initialize Process Group...\") # Initialize Process Group 初始化进程组 # v1 - init with url 使用 url 初始化 dist.init_process_group(backend=dist_backend, init_method=dist_url, rank=int(sys.argv[1]), world_size=world_size) # v2 - init with file 使用文件初始化 # dist.init_process_group(backend=\"nccl\", init_method=\"file:///home/ubuntu/pt-distributed-tutorial/trainfile\", rank=int(sys.argv[1]), world_size=world_size) # v3 - init with environment variables 使用环境变量初始化 # dist.init_process_group(backend=\"nccl\", init_method=\"env://\", rank=int(sys.argv[1]), world_size=world_size) # Establish Local Rank and set device on this node 设置节点的本地化编号和设备 local_rank = int(sys.argv[2]) dp_device_ids = [local_rank] torch.cuda.set_device(local_rank) 初始化模型 下一个主要步骤是初始化训练模型。这里我们将会使用 torchvision.models 中的 resnet18 模型但是你可以选用任何一种模型。首先，我们初始化模型并将它放进显存中。然后，我们创建模型 DistributedDataParallel, 它负责分配数据进出模型，这对分布式训练很重要。 DistributedDataParallel 模块同时也计算整体的平均梯度, 这样我们就不需要在训练步骤计算平均梯度。 还要注意到这是一个阻塞函数 (blocking function), 也就是程序执行时会在这个函数等待直到 world_size 进程加入进程组。 同时注意到，我们将我们的设备 ids 表以参数的形式传递，这个参数还包含了我们正在使用的本地编号 (也就是 GPU)。 最后，我们设定了训练模型使用的 loss function 和 optimizer。 print(\"Initialize Model...\") # Construct Model 构建模型 model = models.resnet18(pretrained=False).cuda() # Make model DistributedDataParallel model = torch.nn.parallel.DistributedDataParallel(model, device_ids=dp_device_ids, output_device=local_rank) # define loss function (criterion) and optimizer 定义 loss 函数和 optimizer criterion = nn.CrossEntropyLoss().cuda() optimizer = torch.optim.SGD(model.parameters(), starting_lr, momentum=0.9, weight_decay=1e-4) 初始化数据加载器 (dataloader) 准备训练的最后一步是确认使用什么数据集。 这里我们使用 torchvision.datasets.STL10 中的 STL-10 dataset。 STL10 数据集是一个 10 分类 96x96px 彩色图片集。为了在我们的模型中使用它，我们在一个变换中把图片的尺寸调整为 224x224px。 在这节中特别需要为分布式训练准备的东西是为训练集使用 DistributedSampler，这是设计来与 DistributedDataParallel 模型相结合的。 这个对象控制进入分布式环境的数据集以确保模型不是对同一个子数据集训练，以达到训练目标。最后，我们创建 DataLoader 负责向模型喂数据。 如果你的节点上没有 STL-10 数据集那么它会自动下载到节点上。如果你想要使用你自己的数据集那么下载你的数据集，搭建你自己的数据操作函数和加载器。 print(\"Initialize Dataloaders...\") # Define the transform for the data. Notice, we must resize to 224x224 with this dataset and model. 定义数据的变换。尺寸转为224x224 transform = transforms.Compose( [transforms.Resize(224), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # Initialize Datasets. STL10 will automatically download if not present 初始化数据集。如果没有STL10数据集则会自动下载 trainset = datasets.STL10(root='./data', split='train', download=True, transform=transform) valset = datasets.STL10(root='./data', split='test', download=True, transform=transform) # Create DistributedSampler to handle distributing the dataset across nodes when training 创建分布式采样器来控制训练中节点间的数据分发 # This can only be called after torch.distributed.init_process_group is called 这个只能在 torch.distributed.init_process_group 被调用后调用 train_sampler = torch.utils.data.distributed.DistributedSampler(trainset) # Create the Dataloaders to feed data to the training and validation steps 创建数据加载器，在训练和验证步骤中喂数据 train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=(train_sampler is None), num_workers=workers, pin_memory=False, sampler=train_sampler) val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=workers, pin_memory=False) 训练循环 最后一步是定义训练循环。我们已经完成了设置分布式训练的绝大多数工作了，这一步不是特别为分布式训练做的。 唯一的细节是在 DistributedSampler 中记录目前的 epoch 数， 因为采样器是根据 epoch 来决定如何打乱分配数据进各个进程。 更新采样器后，循环执行一个完整的 epoch， 一个完整的验证步骤然后打印目前模型的表现并和目前表现最好的模型对比。 在训练了 num_epochs 后, 循环退出，教程结束。注意，因为这只是个例程，我们没有保存模型，但如果想要训练结束后保存表现最好的模型请看这里。 best_prec1 = 0 for epoch in range(num_epochs): # Set epoch count for DistributedSampler 为分布式采样器设置 epoch 数 train_sampler.set_epoch(epoch) # Adjust learning rate according to schedule 调整学习率 adjust_learning_rate(starting_lr, optimizer, epoch) # train for one epoch 训练1个 epoch print(\"\\nBegin Training Epoch {}\".format(epoch+1)) train(train_loader, model, criterion, optimizer, epoch) # evaluate on validation set 在验证集上验证 print(\"Begin Validation @ Epoch {}\".format(epoch+1)) prec1 = validate(val_loader, model, criterion) # remember best prec@1 and save checkpoint if desired 保存最佳的prec@1，如果需要的话保存检查点 # is_best = prec1 > best_prec1 best_prec1 = max(prec1, best_prec1) print(\"Epoch Summary: \") print(\"\\tEpoch Accuracy: {}\".format(prec1)) print(\"\\tBest Accuracy: {}\".format(best_prec1)) 运行代码 和其他 PyTorch 教程不一样, 这个代码也许不能直接以 notebook 的形式执行。 为了运行它需要以 .py 形式下载这份文件(或者使用这个来转换它)然后复制到各个节点上。 聪明的读者也许注意到了我们写死了(硬编码，hardcode) node0-privateIP 和 world\\_size=4 但把 rank 和 local_rank 以 arg[1] 和 arg[2] 命令行参数的形式分别输入。 上传后对每个节点分别打开两个 ssh 终端。 对 node0 的第一个终端，运行 $ python main.py 0 0 对 node0 的第二个终端，运行 $ python main.py 1 1 对 node1 的第一个终端，运行 $ python main.py 2 0 对 node1 的第二个终端，运行 $ python main.py 3 1 程序会开始运行并等待直到四个进程都加入进程组后打印 “Initialize Model…” 。 注意到第一个参数不能重复因为这是进程的全局编号(唯一的)。 第二个参数可重复因为这是节点上进程的本地编号。 如果你对每个节点运行 nvidia-smi，你会看见每个节点上有两个进程，一个运行在 GPU0 上，另一个运行在 GPU1 上。 我们现在已经实现了一个分布式训练的范例！ 希望你可以通过这个教程学会如何在你自己的数据集上搭建你自己的模型，即使你不是使用同样的分布式环境。 如果你在使用 AWS，切记在你不使用时关掉你的节点不然月末你会发现你要交好多钱。 接下来看什么 看看 launcher utility 以了解另一种启动运行的方式 看看 torch.multiprocessing.spawn utility 以了解另一种简单的启动多路分布式进程的方式。 PyTorch ImageNet Example 已经实现并可以演示如何使用它。 如果可能，请设置一个NFS，这样你只需要一个数据集副本 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"ONNXLive.html":{"url":"ONNXLive.html","title":"ONNX 现场演示教程","keywords":"","body":"ONNX 现场演示教程 译者：冯宝宝 本教程将向您展示如何使用ONNX将已从PyTorch导出的神经模型传输模型转换为Apple CoreML格式。这将允许您在Apple设备上轻松运行深度学习模型，在这种情况下，可以从摄像机直播演示。 什么是ONNX ONNX（开放式神经网络交换）是一种表示深度学习模型的开放格式。借助ONNX，AI开发人员可以更轻松地在最先进的工具之间移动模型，并选择最适合它们的组合。ONNX由合作伙伴社区开发和支持。 您可以访问 onnx.ai，了解有关ONNX的更多信息以及支持的工具。 教程预览 本教程将带你走过如下主要4步： 下载（或训练）Pytorch风格装换模型 将PyTorch模型转换至ONNX模型 将ONNX模型转换至CoreML模型 在支持风格转换iOS App中运行CoreML模型 环境准备 我们将在虚拟环境工作，以避免与您的本地环境冲突。在本教程中使用Python 3.6，但其他版本也应该可以正常工作。 python3.6 -m venv venv source ./venv/bin/activate 我们需要安装Pytorch和 onnx->coreml 转换器： pip install torchvision onnx-coreml 如果要在iPhone上运行iOS样式传输应用程序，还需要安装XCode。您也可以在Linux中转换模型，但要运行iOS应用程序本身，您将需要一台Mac。 下载（或训练）Pytorch风格装换模型 在本教程中，我们将使用与pytorch一起发布的样式传输模型，地址为https://github.com/pytorch/examples/tree/master/fast_neural_style。如果您想使用其他PyTorch或ONNX模型，请随意跳过此步骤。 这些模型用于在静态图像上应用样式传输，并且实际上没有针对视频进行优化以获得足够快的速度。但是，如果我们将分辨率降低到足够低，它们也可以很好地处理视频。 我们先下载模型： git clone https://github.com/pytorch/examples cd examples/fast_neural_style 如果您想自己训练模型，您刚刚克隆下载的的pytorch/examples存储库有更多关于如何执行此操作的信息。目前，我们只需使用存储库提供的脚本下载预先训练的模型： ./download_saved_models.sh 此脚本下载预先训练的PyTorch模型并将它们放入saved_models文件夹中。 你的目录中现在应该有4个文件，candy.pth，mosaic.pth，rain_princess.pth和udnie.pth。 将PyTorch模型转换至ONNX模型 现在我们已将预先训练好的PyTorch模型作为saved_models文件夹中的.pth文件，我们需要将它们转换为ONNX格式。模型定义在我们之前克隆的pytorch/examples存储库中，通过几行python我们可以将它导出到ONNX。在这种情况下，我们将调用torch.onnx._export而不是实际运行神经网络，它将PyTorch作为api提供，以直接从PyTorch导出ONNX格式的模型。但是，在这种情况下，我们甚至不需要这样做，因为脚本已经存在Neural_style / neural_style.py，它将为我们执行此操作。如果要将其应用于其他模型，也可以查看该脚本。 从PyTorch导出ONNX格式本质上是追踪您的神经网络，因此这个api调用将在内部运行网络“虚拟数据”以生成图形。为此，它需要输入图像来应用样式转移，其可以简单地是空白图像。但是，此图像的像素大小很重要，因为这将是导出的样式传输模型的大小。为了获得良好的性能，我们将使用250x540的分辨率。如果您不太关心FPS，可以随意采取更大的分辨率，更多关于风格转移质量。 让我们使用ImageMagick创建我们想要的分辨率的空白图像： convert -size 250x540 xc:white png24:dummy.jpg 然后用它来导出PyTorch模型用它来导出PyTorch模型： python ./neural_style/neural_style.py eval --content-image dummy.jpg --output-image dummy-out.jpg --model ./saved_models/candy.pth --cuda 0 --export_onnx ./saved_models/candy.onnx python ./neural_style/neural_style.py eval --content-image dummy.jpg --output-image dummy-out.jpg --model ./saved_models/udnie.pth --cuda 0 --export_onnx ./saved_models/udnie.onnx python ./neural_style/neural_style.py eval --content-image dummy.jpg --output-image dummy-out.jpg --model ./saved_models/rain_princess.pth --cuda 0 --export_onnx ./saved_models/rain_princess.onnx python ./neural_style/neural_style.py eval --content-image dummy.jpg --output-image dummy-out.jpg --model ./saved_models/mosaic.pth --cuda 0 --export_onnx ./saved_models/mosaic.onnx 你应该得到4个文件，candy.onnx，mosaic.onnx，rain_princess.onnx和udnie.onnx，由相应的.pth文件创建。 将ONNX模型转换至CoreML模型 现在我们有了ONNX模型，我们可以将它们转换为CoreML模型，以便在Apple设备上运行它们。为此，我们使用之前安装的onnx-coreml转换器。转换器附带一个convert-onnx-to-coreml脚本，上面的安装步骤添加到我们的路径中。遗憾的是，这对我们不起作用，因为我们需要将网络的输入和输出标记为图像，并且虽然这是转换器支持的，但只有在从python调用转换器时才支持它。 通过查看样式传输模型（例如在像Netron这样的应用程序中打开.onnx文件），我们看到输入名为'0'，输出名为'186'。这些只是PyTorch分配的数字ID。我们需要将它们标记为图像。 所以让我们创建一个python小文件并将其命名为onnx_to_coreml.py。这可以通过使用touch命令创建，并使用您喜欢的编辑器进行编辑，以添加以下代码行。 import sys from onnx import onnx_pb from onnx_coreml import convert model_in = sys.argv[1] model_out = sys.argv[2] model_file = open(model_in, 'rb') model_proto = onnx_pb.ModelProto() model_proto.ParseFromString(model_file.read()) coreml_model = convert(model_proto, image_input_names=['0'], image_output_names=['186']) coreml_model.save(model_out) 现在来运行: python onnx_to_coreml.py ./saved_models/candy.onnx ./saved_models/candy.mlmodel python onnx_to_coreml.py ./saved_models/udnie.onnx ./saved_models/udnie.mlmodel python onnx_to_coreml.py ./saved_models/rain_princess.onnx ./saved_models/rain_princess.mlmodel python onnx_to_coreml.py ./saved_models/mosaic.onnx ./saved_models/mosaic.mlmodel 现在，您的saved_models目录中应该有4个CoreML模型：candy.mlmodel，mosaic.mlmodel，rain_princess.mlmodel和udnie.mlmodel。 在支持风格转换iOS App中运行CoreML模型 此存储库（即您当前正在阅读README.md的存储库）包含一个iOS应用程序，可以在手机摄像头的实时摄像头流上运行CoreML样式传输模型。 git clone https://github.com/onnx/tutorials 并在XCode中打开tutorials/examples/CoreML/NNXLive/ONNXLive.xcodeproj项目。我们建议使用XCode 9.3和iPhone X。在旧设备或XCode版本上可能会出现问题。 在Models/文件夹中，项目包含一些.mlmodel文件。我们将用我们刚刚创建的模型替换它们。 然后你在iPhone上运行应用程序就可以了。点击屏幕可切换模型。 结论 我们希望本教程能够概述ONNX的内容以及如何使用它来在框架之间转换神经网络，在这种情况下，神经风格的传输模型从PyTorch转移到CoreML。 您可以随意尝试这些步骤并在自己的模型上进行测试。如果您遇到任何问题或想要提供反馈，请告诉我们。我们倾听你的想法。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"cpp_export.html":{"url":"cpp_export.html","title":"在 C++ 中加载 PYTORCH 模型","keywords":"","body":"在C++中加载PYTORCH模型 译者：talengu PyTorch的主要接口为Python。虽然Python有动态编程和易于迭代的优势，但在很多情况下，正是Python的这些属性会带来不利。我们经常遇到的生产环境，要满足低延迟和严格部署要求。对于生产场景而言，C++通常是首选语言，也能很方便的将其绑定到另一种语言，如Java，Rust或Go。本教程将介绍从将PyTorch训练的模型序列化表示，到C++语言加载和执行的过程。 第一步：将PyTorch模型转换为Torch Script PyTorch模型从Python到C++的转换由Torch Script实现。Torch Script是PyTorch模型的一种表示，可由Torch Script编译器理解，编译和序列化。如果使用基础的“eager”API编写的PyTorch模型，则必须先将模型转换为Torch Script，当然这也是比较容易的。如果已有模型的Torch Script，则可以跳到本教程的下一部分。 将PyTorch模型转换为Torch Script有两种方法。 第一种方法是Tracing。该方法通过将样本输入到模型中一次来对该过程进行评估从而捕获模型结构.并记录该样本在模型中的flow。该方法适用于模型中很少使用控制flow的模型。 第二个方法就是向模型添加显式注释(Annotation)，通知Torch Script编译器它可以直接解析和编译模型代码，受Torch Script语言强加的约束。 小贴士 可以在官方的Torch Script 参考中找到这两种方法的完整文档，以及有关使用哪个方法的细节指导。 利用Tracing将模型转换为Torch Script 要通过tracing来将PyTorch模型转换为Torch脚本,必须将模型的实例以及样本输入传递给torch.jit.trace函数。这将生成一个 torch.jit.ScriptModule对象，并在模块的forward方法中嵌入模型评估的跟踪： import torch import torchvision # 获取模型实例 model = torchvision.models.resnet18() # 生成一个样本供网络前向传播 forward() example = torch.rand(1, 3, 224, 224) # 使用 torch.jit.trace 生成 torch.jit.ScriptModule 来跟踪 traced_script_module = torch.jit.trace(model, example) 现在，跟踪的ScriptModule可以与常规PyTorch模块进行相同的计算： In[1]: output = traced_script_module(torch.ones(1, 3, 224, 224)) In[2]: output[0, :5] Out[2]: tensor([-0.2698, -0.0381, 0.4023, -0.3010, -0.0448], grad_fn=) 通过Annotation将Model转换为Torch Script 在某些情况下，例如，如果模型使用特定形式的控制流，如果想要直接在Torch Script中编写模型并相应地标注(annotate)模型。例如，假设有以下普通的 Pytorch模型： import torch class MyModule(torch.nn.Module): def __init__(self, N, M): super(MyModule, self).__init__() self.weight = torch.nn.Parameter(torch.rand(N, M)) def forward(self, input): if input.sum() > 0: output = self.weight.mv(input) else: output = self.weight + input return output 由于此模块的forward方法使用依赖于输入的控制流，因此它不适合利用Tracing的方法生成Torch Script。为此,可以通过继承torch.jit.ScriptModule并将@ torch.jit.script_method标注添加到模型的forward中的方法，来将model转换为ScriptModule： import torch class MyModule(torch.jit.ScriptModule): def __init__(self, N, M): super(MyModule, self).__init__() self.weight = torch.nn.Parameter(torch.rand(N, M)) @torch.jit.script_method def forward(self, input): if input.sum() > 0: output = self.weight.mv(input) else: output = self.weight + input return output my_script_module = MyModule() 现在，创建一个新的MyModule对象会直接生成一个可序列化的ScriptModule实例了。 第二步：将Script Module序列化为一个文件 不论是从上面两种方法的哪一种方法获得了ScriptModule,都可以将得到的ScriptModule序列化为一个文件,然后C++就可以不依赖任何Python代码来执行该Script所对应的Pytorch模型。 假设我们想要序列化前面trace示例中显示的ResNet18模型。要执行此序列化，只需在模块上调用 save并给个文件名： traced_script_module.save(\"model.pt\") 这将在工作目录中生成一个model.pt文件。现在可以离开Python，并准备跨越到C ++语言调用。 第三步:在C++中加载你的Script Module 要在C ++中加载序列化的PyTorch模型，应用程序必须依赖于PyTorch C ++ API - 也称为LibTorch。LibTorch发行版包含一组共享库，头文件和CMake构建配置文件。虽然CMake不是依赖LibTorch的要求，但它是推荐的方法，并且将来会得到很好的支持。在本教程中，我们将使用CMake和LibTorch构建一个最小的C++应用程序，加载并执行序列化的PyTorch模型。 最小的C++应用程序 以下内容可以做到加载模块： #include // One-stop header. #include #include int main(int argc, const char* argv[]) { if (argc != 2) { std::cerr \\n\"; return -1; } // Deserialize the ScriptModule from a file using torch::jit::load(). std::shared_ptr module = torch::jit::load(argv[1]); assert(module != nullptr); std::cout 头文件包含运行该示例所需的LibTorch库中的所有相关include。main函数接受序列化ScriptModule的文件路径作为其唯一的命令行参数，然后使用torch::jit::load()函数反序列化模块，得到一个指向torch::jit::script::Module的共享指针，相当于C ++中的torch.jit.ScriptModule对象。最后，我们只验证此指针不为null。我们展示如何在接下来执行它。 依赖库LibTorch和构建应用程序 我们将上面的代码保存到名为example-app.cpp的文件中。对应的构建它的简单CMakeLists.txt为： cmake_minimum_required(VERSION 3.0 FATAL_ERROR) project(custom_ops) find_package(Torch REQUIRED) add_executable(example-app example-app.cpp) target_link_libraries(example-app \"${TORCH_LIBRARIES}\") set_property(TARGET example-app PROPERTY CXX_STANDARD 11) 我们构建示例应用程序的最后一件事是下载LibTorch发行版。从PyTorch网站的下载页面获取最新的稳定版本 download page。如果下载并解压缩最新存档，则有以下目录结构： libtorch/ bin/ include/ lib/ share/ lib/ 包含含链接的共享库, include/ 包含程序需要include的头文件, share/包含必要的CMake配置文件使得 find_package(Torch) 。 小贴士 在Windows平台上, debug and release builds are not ABI-compatible. 如果要使用debug, 要使用 源码编译 PyTorch方法。 最后一步是构建应用程序。为此，假设我们的示例目录布局如下： example-app/ CMakeLists.txt example-app.cpp 我们现在可以运行以下命令从example-app/文件夹中构建应用程序： mkdir build cd build cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch .. make 其中 /path/to/libtorch 应该是解压缩的LibTorch发行版的完整路径。如果一切顺利，它将看起来像这样： root@4b5a67132e81:/example-app# mkdir build root@4b5a67132e81:/example-app# cd build root@4b5a67132e81:/example-app/build# cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch .. -- The C compiler identification is GNU 5.4.0 -- The CXX compiler identification is GNU 5.4.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Configuring done -- Generating done -- Build files have been written to: /example-app/build root@4b5a67132e81:/example-app/build# make Scanning dependencies of target example-app [ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o [100%] Linking CXX executable example-app [100%] Built target example-app 如果我们提供前面的序列化ResNet18模型的路径给example-app，C++输出的结果应该是 OK: root@4b5a67132e81:/example-app/build# ./example-app model.pt ok 在C++代码中运行Script Module 在C ++中成功加载了我们的序列化ResNet18后，我们再加几行执行代码，添加到C++应用程序的main()函数中： // Create a vector of inputs. std::vector inputs; inputs.push_back(torch::ones({1, 3, 224, 224})); // Execute the model and turn its output into a tensor. at::Tensor output = module->forward(inputs).toTensor(); std::cout 前两行设置我们模型的输入。 创建了一个 torch::jit::IValue (script::Module 对象可接受和返回的一种数据类型) 的向量和添加一个输入。要创建输入张量，我们使用torch::ones()（C++ API）和python中的torch.ones 一样。 然后我们运行script::Module的forward方法，传入我们创建的输入向量，返回一个新的IValue，通过调用toTensor()可将其转换为张量。 小贴士 更多关于torch::ones 和 PyTorch的对应 C++ API的内容 https://pytorch.org/cppdocs。PyTorch C++ API 和Python API差不多，可以使你像python 中一样操作处理tensors。 在最后一行中，我们打印输出的前五个条目。由于我们在本教程前面的Python中为我们的模型提供了相同的输入，因此理想情况下我们应该看到相同的输出。让我们通过重新编译我们的应用程序并使用相同的序列化模型运行它来尝试： root@4b5a67132e81:/example-app/build# make Scanning dependencies of target example-app [ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o [100%] Linking CXX executable example-app [100%] Built target example-app root@4b5a67132e81:/example-app/build# ./example-app model.pt -0.2698 -0.0381 0.4023 -0.3010 -0.0448 [ Variable[CPUFloatType]{1,5} ] 作为参考，之前Python代码的输出是： tensor([-0.2698, -0.0381, 0.4023, -0.3010, -0.0448], grad_fn=) 由此可见,C++的输出与Python的输出是一样的,成功啦! 小贴士 将你的模型放到GPU上，可以写成model->to(at::kCUDA);。确保你的输入也在CUDA的存储空间里面，可以使用tensor.to(at::kCUDA)检查，这个函数返回一个新的在CUDA里面的tensor。 第五步:进阶教程和详细API 本教程希望能使你理解PyTorch模型从python到c++的调用过程。通过上述教程，你能够通过“eager” PyTorch做一个简单模型，转成ScriptModule，并序列化保存。然后在C++里面通过 script::Module加载运行模型。 当然，还有好多内容我们没有涉及。举个例子，你希望在C++或者CUDA中实现ScriptModule中的自定义操作，然后就可以在C++调用运行ScriptModule模型。这种是可以做到的，可以参考this。下面还有一些文档可以参考，比较有帮助： Torch Script 参考: https://pytorch.org/docs/master/jit.html PyTorch C++ API 文档: https://pytorch.org/cppdocs/ PyTorch Python API 文档: https://pytorch.org/docs/ 如果有任何bug或者问题，可以向社区 Pytorch forum 或者 Pytorch GitHub issues 寻求帮助。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"tut_other_language.html":{"url":"tut_other_language.html","title":"其它语言中的 PyTorch","keywords":"","body":"其它语言中的 PyTorch 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"cpp_frontend.html":{"url":"cpp_frontend.html","title":"使用 PyTorch C++ 前端","keywords":"","body":"使用 PyTorch C++ 前端 译者：solerji PyTorch C++ 前端 是PyTorch机器学习框架的一个纯C++接口。PyTorch的主接口是Python，Python API位于一个基础的C++代码库之上，提供了基本的数据结构和功能，例如张量和自动求导。C++前端暴露了一个纯的C++11的API，在C++底层代码库之上扩展了机器学习训练和推理所需的工具扩展。这包括用于神经网络建模的内置组件集合；扩展此集合的自定义模块API；流行的优化算法库（如随机梯度下降）；使用API定义和加载数据集的并行数据加载程序；序列化例行程序等等。 本教程将为您介绍一个用C++ 前端对模型进行训练的端到端示例。具体地说，我们将训练一个 DCGAN——一种生成模型——来生成 MNIST数字的图像。虽然看起来这是一个简单的例子，但它足以让你对 PyTorch C++ frontend有一个深刻的认识，并勾起你对训练更复杂模型的兴趣。我们将从设计它的原因开始，告诉你为什么你应该使用C++前端，然后直接深入解释和训练我们的模型。 小贴士 可以在 this lightning talk from CppCon 2018 网站观看有关C++前端的快速介绍。 小贴士 这份笔记提供了C++前端组件和设计理念的全面概述。 小贴士 在 https://pytorch.org/cppdocs你可以找到工作人员的API说明文档，这些PyTorch C++ 生态系统的文档是很有用的。 动机 在我们开始令人兴奋的GANs和MNIST数字的旅程之前，让我们往回看，讨论一下为什么我们一开始要使用C++前端而不是Python。我们（the PyTorch team）创建了C++前端，以便在不能使用Python的环境中或者是没有适合该作业的工具的情况下进行研究。此类环境的示例包括： 低延迟系统：您可能希望在具有高帧/秒和低延迟的要求的纯C++游戏引擎中进行强化学习研究。由于Python解释器的速度慢，Python可能根本无法被跟踪，使用纯C++库这样的环境比Python库更合适。 高度多线程环境：由于全局解释器锁（GIL），一次不能运行多个系统线程。多道处理是另一种选择，但它不具有可扩展性，并且有显著的缺点。C++没有这样的约束，线程易于使用和创建。需要大量并行化的模型，像那些用于深度神经进化 Deep Neuroevolution的模型，可以从中受益。 现有的C++代码库：您可能是一个现有的C++应用程序的所有者，在后台服务器上为Web页面提供服务，以在照片编辑软件中绘制3D图形，并希望将机器学习方法集成到您的系统中。C++前端允许您保留在C++中，免除了在Python和C++之间来回绑定的麻烦，同时保留了传统 PyTorch（Python）体验的大部分灵活性和直观性。 C++前端不打算与Python前端竞争，它是为了补充Python前端。我们知道由于它简单、灵活和直观的API研究人员和工程师都喜欢PyTorch。我们的目标是确保您可以在每个可能的环境中利用这些核心设计原则，包括上面描述的那些。如果这些场景中的一个描述了你的用例，或者如果你只是感兴趣的话，跟着我们在下面的文章中详细探究C++前端。 小贴士 C++前端试图提供尽可能接近Python前端的API。如果你对Python前端有经验，并且想知道：“我如何用C++前端做这个东西？”你可以以Python的方式编写代码，在Python中，通常可以使用与C++相同的函数和方法（只要记住用双冒号替换点）。 编写基本应用程序 让我们开始编写一个小的C++应用程序，以验证我们在安装和构建环境上是一致的。首先，您需要获取 LibTorch分发的副本——我们已经准备好了ZIP存档，它封装了使用C++前端所需的所有相关的头文件、库和 CMake 构建文件。Libtorch发行版可在Linux, MacOS 和 Windows的PyTorch website上下载。本教程的其余部分将假设一个基本的Ubuntu Linux环境，您也可以在MacOS或Windows上继续自由地学习。 小贴士 关于安装PyTrac C++ 在 Installing C++ Distributions of PyTorch 的文档更详细地描述了以下步骤。 第一步是通过从PyTorch网站检索到的链接在本地下载 LibTorch发行版。对于普通的Ubuntu Linux环境，这意味着运行： wget https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip unzip libtorch-shared-with-deps-latest.zip 接下来，让我们编写一个名为 dcgan.cpp 的小型C++文件，它包括 torch/torch.h ，现在只需打印出三×三的身份矩阵： #include #include int main() { torch::Tensor tensor = torch::eye(3); std::cout 我们将使用CMakeLists.txt文件构建这个小应用程序以及我们稍后的完整训练脚本： cmake_minimum_required(VERSION 3.0 FATAL_ERROR) project(dcgan) find_package(Torch REQUIRED) add_executable(dcgan dcgan.cpp) target_link_libraries(dcgan \"${TORCH_LIBRARIES}\") set_property(TARGET dcgan PROPERTY CXX_STANDARD 11) 笔记 虽然CMake是LibTorch推荐的构建系统，但这并不是一个硬性要求。您还可以使用Visual Studio项目文件、Qmake、plain Makefiles或任何其他您觉得合适的构建环境。但是，我们不提供开箱即用的支持。 记下上述CMake文件中的第4行： find_package(Torch REQUIRED).。这将指示CMake查找LibTorch库的构建配置。为了让CMake知道在哪里找到这些文件，我们必须在调用 cmake时设置 CMAKE_PREFIX_PATH 。在进行此操作之前，让我们就 dcgan应用程序的以下目录结构达成一致： dcgan/ CMakeLists.txt dcgan.cpp 此外，我将特别指出解压LibTorch分发的路径 /path/to/libtorch。请注意，这必须是绝对路径。我们用编写 $PWD/../../libtorch 的做法获取相应的绝对路径；如果将 CMAKE_PREFIX_PATH 设置为../../libtorch它将以意想不到的方式中断。现在，我们已经准备好构建我们的应用程序： root@fa350df05ecf:/home# mkdir build root@fa350df05ecf:/home# cd build root@fa350df05ecf:/home/build# cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch .. -- The C compiler identification is GNU 5.4.0 -- The CXX compiler identification is GNU 5.4.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Found torch: /path/to/libtorch/lib/libtorch.so -- Configuring done -- Generating done -- Build files have been written to: /home/build root@fa350df05ecf:/home/build# make -j Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcgan 在上文，我们首先在 dcgan 目录中创建了一个 build 文件夹，然后进入这个文件夹，运行 cmake 命令生成必要的build（Make）文件，最后通过运行 make -j.成功编译了项目。现在，我们将项目设置为执行最小的二进制文件，基本项目配置这一部分就完成了： root@fa350df05ecf:/home/build# ./dcgan 1 0 0 0 1 0 0 0 1 [ Variable[CPUFloatType]{3,3} ] 在我看来它就像一个身份矩阵！ 定义神经网络模型 既然我们已经配置了基本环境，那么我们可以深入了解本教程中更有趣的部分。首先，我们将讨论如何在C++前端中定义和交互模块。我们将从基本的、小规模的示例模块开始，然后使用C++前端提供的内置模块的广泛库来实现一个成熟的GAN。 模块API基础知识 依据Python接口，基于C++前端的神经网络由可重用的模块组成，称为模块。它有一个基本模块类，从中派生所有其他模块。在Python中，这个类是 torch.nn.Module ，在C++中是 torch::nn::Module模块。除了实现模块封装的算法的 forward() 方法外，模块通常还包含三种子对象：参数、缓冲区和子模块。 参数和缓冲区以张量的形式存储状态。参数记录，而缓冲区不记录。参数通常是神经网络的可训练权重。缓冲区的示例包括用于批处理规范化的平均值和方差。为了重用特定的逻辑块和状态块，PyTorch API允许嵌套模块。嵌套模块称为子模块。 必须显式注册参数、缓冲区和子模块。注册后，可以使用parameters() or buffers()等方法来检索整个（嵌套）模块层次结构中所有参数的容器。类似地，类似于 to(...)的方法（例如 to(torch::kCUDA) 将所有参数和缓冲区从CPU移动到CUDA内存）在整个模块层次结构上工作。 定义模块并注册参数 为了将这些随机数放入代码中，让我们考虑一下在Python接口中编写这个简单模块： import torch class Net(torch.nn.Module): def __init__(self, N, M): super(Net, self).__init__() self.W = torch.nn.Parameter(torch.randn(N, M)) self.b = torch.nn.Parameter(torch.randn(M)) def forward(self, input): return torch.addmm(self.b, input, self.W) 在C++中它长这样： #include struct Net : torch::nn::Module { Net(int64_t N, int64_t M) { W = register_parameter(\"W\", torch::randn({N, M})); b = register_parameter(\"b\", torch::randn(M)); } torch::Tensor forward(torch::Tensor input) { return torch::admm(b, input, W); } torch::Tensor W, b; }; 就像在Python中一样，我们定义了一个类 Net （为了简单起见，这里是 struct 而不是一个 class）并从模块基类派生它。在构造函数内部，我们使用 torch::randn 创建张量，就像在Python中使用torch.randn一样。一个有趣的区别是我们如何注册参数。在Python中，我们用torch.nn.Parameter类来包装张量，而在C++中，我们必须通过 register_parameter 参数方法来传递张量。原因是Python API可以检测到属性的类型为 torch.nn.Parameter ，并自动注册这些张量。在C++中，反射是非常有限的，因此提供了一种更传统的（和不太神奇的）方法。 注册子模块并遍历模块层次结构 同样，我们可以注册参数，也可以注册子模块。在Python中，当子模块被指定为模块的属性时，将自动检测和注册子模块： class Net(torch.nn.Module): def __init__(self, N, M): super(Net, self).__init__() # Registered as a submodule behind the scenes self.linear = torch.nn.Linear(N, M) self.another_bias = torch.nn.Parameter(torch.rand(M)) def forward(self, input): return self.linear(input) + self.another_bias 例如，这允许使用 parameters() 方法递归访问模块层次结构中的所有参数： >>> net = Net(4, 5) >>> print(list(net.parameters())) [Parameter containing: tensor([0.0808, 0.8613, 0.2017, 0.5206, 0.5353], requires_grad=True), Parameter containing: tensor([[-0.3740, -0.0976, -0.4786, -0.4928], [-0.1434, 0.4713, 0.1735, -0.3293], [-0.3467, -0.3858, 0.1980, 0.1986], [-0.1975, 0.4278, -0.1831, -0.2709], [ 0.3730, 0.4307, 0.3236, -0.0629]], requires_grad=True), Parameter containing: tensor([ 0.2038, 0.4638, -0.2023, 0.1230, -0.0516], requires_grad=True)] 为了在C++中注册子模块，使用恰当命名的 register_module() 方法注册一个就像 torch::nn::Linear:的模块。 struct Net : torch::nn::Module { Net(int64_t N, int64_t M) : linear(register_module(\"linear\", torch::nn::Linear(N, M))) { another_bias = register_parameter(\"b\", torch::randn(M)); } torch::Tensor forward(torch::Tensor input) { return linear(input) + another_bias; } torch::nn::Linear linear; torch::Tensor another_bias; }; 小贴士 您可以在这里的 torch::nn 命名空间文档中找到可用内置模块的完整列表，如 torch::nn::Linear, torch::nn::Dropout 和 torch::nn::Conv2d 。 上面代码的一个微妙之处就是，为什么我们要在构造函数的初始值设定项列表中创建子模块，而在构造函数主体中创建参数。这是一个很好的理由，我们将在下面进一步讨论C++前端的 ownership model 。最终，我们可以像在Python中那样递归地访问树的模块的参数。调用参数 parameters() 返回一个我们可以迭代的 std::vector&lt;torch::Tensor&gt;： int main() { Net net(4, 5); for (const auto& p : net.parameters()) { std::cout 输出的结果是： root@fa350df05ecf:/home/build# ./dcgan 0.0345 1.4456 -0.6313 -0.3585 -0.4008 [ Variable[CPUFloatType]{5} ] -0.1647 0.2891 0.0527 -0.0354 0.3084 0.2025 0.0343 0.1824 -0.4630 -0.2862 0.2500 -0.0420 0.3679 -0.1482 -0.0460 0.1967 0.2132 -0.1992 0.4257 0.0739 [ Variable[CPUFloatType]{5,4} ] 0.01 * 3.6861 -10.1166 -45.0333 7.9983 -20.0705 [ Variable[CPUFloatType]{5} ] 就像在Python中一样这里有三个参数。为了看到这些参数的名称，C++ API提供了一个 named_parameters()参数方法，它像Python一样返回 named_parameters()： Net net(4, 5); for (const auto& pair : net.named_parameters()) { std::cout 我们可以再次执行来查看输出： root@fa350df05ecf:/home/build# make && ./dcgan 11:13:48 Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcgan b: -0.1863 -0.8611 -0.1228 1.3269 0.9858 [ Variable[CPUFloatType]{5} ] linear.weight: 0.0339 0.2484 0.2035 -0.2103 -0.0715 -0.2975 -0.4350 -0.1878 -0.3616 0.1050 -0.4982 0.0335 -0.1605 0.4963 0.4099 -0.2883 0.1818 -0.3447 -0.1501 -0.0215 [ Variable[CPUFloatType]{5,4} ] linear.bias: -0.0250 0.0408 0.3756 -0.2149 -0.3636 [ Variable[CPUFloatType]{5} ] 笔记 torch::nn::Module 的文档 包含在模块层次结构上操作的方法的完整清单。 在正向模式中运行网络 为了在C++中运行网络，我们只需调用我们定义的 forward() 方法： int main() { Net net(4, 5); std::cout 输出内容如下： root@fa350df05ecf:/home/build# ./dcgan 0.8559 1.1572 2.1069 -0.1247 0.8060 0.8559 1.1572 2.1069 -0.1247 0.8060 [ Variable[CPUFloatType]{2,5} ] 模块所有权 现在，我们知道如何定义C++中的模块、寄存器参数、寄存器子模块、通过参数 parameters() 等方法遍历模块层次结构，和最后运行模块的 forward() 方法。在C++ API中有更多的方法、类和主题要我们思考，但接下来我会向你介绍完整清单 文档 。我们在一秒钟内实现 DCGAN模型和端到端训练管道的同时，还将涉及更多的概念。在我们这样做之前，让我简单地介绍一下C++前端的所有权模型，它提供了 torch::nn::Module.模块的子类。 对于这个论述，所有权模型指的是模块的存储和传递方式，它决定了谁或什么拥有一个特定的模块实例。在Python中，对象总是动态分配（在堆上）并具有引用语义。这很容易操作，也很容易理解。事实上，在Python中，您大可以忘记对象的位置以及它们是如何被引用的，而更专注于完成工作。 C++是一种这个领域提供了更多的选择的低级语言。它更加了复杂，并严重影响了C++前端的设计和人机工程学。特别地，对于C++前端中的模块，我们可以选择使用值语义或引用语义。第一种情况是最简单的，并在迄今为止的示例中显示：当传递给函数时，在堆栈上分配的模块对象，可以复制、移动（使用 std::move)）或通过引用和指针获取： struct Net : torch::nn::Module { }; void a(Net net) { } void b(Net& net) { } void c(Net* net) { } int main() { Net net; a(net); a(std::move(net)); b(net); c(&net); } 对于第二种情况——引用语义——我们可以使用 std::shared_ptr.。引用语义的优点在于，与Python一样，它减少了认知模块如何传递给函数以及如何声明参数（假设在任何地方都使用shared_ptr ）。 struct Net : torch::nn::Module {}; void a(std::shared_ptr net) { } int main() { auto net = std::make_shared(); a(net); } 据以往经验，来自动态语言的研究人员更倾向于引用语义而不是值语义，即使后者对于而言C++更为“本土”。还需要注意的是，为了接近PythonAPI的人机工程学，torch::nn::Module的设计依赖于所有权的共享。例如，以我们之前（此处简称）对Net的定义为例： struct Net : torch::nn::Module { Net(int64_t N, int64_t M) : linear(register_module(\"linear\", torch::nn::Linear(N, M))) { } torch::nn::Linear linear; }; 为了使用 linear 子模块，我们希望将其直接存储在我们的类中。但是，我们也希望模块基类了解并能够访问这个子模块。为此，它必须存储对此子模块的引用。在这一点上，我们已经达到了所有权共享的需求。 torch::nn::Module 类和 具体类 Net 都需要引用子模块。因此，基类将模块存储为shared_ptr，具体的类也必须存储。 等等！在上面的代码中我没有看到它提及共享资源！为什么会这样？因为std::shared_ptr&lt;MyModule&gt;是一个很难输入的类型。为了保持研究人员的工作效率，我们提出了一个精心设计的方案来隐藏应该提及的共享资源——这是保留值语义的好处，它同时保留了引用语义。要了解这是如何工作的，我们可以查看核心库中torch::nn::Linear模块的简化定义（完整定义如下）： struct LinearImpl : torch::nn::Module { LinearImpl(int64_t in, int64_t out); Tensor forward(const Tensor& input); Tensor weight, bias; }; TORCH_MODULE(Linear); 简而言之：模块不是 Linear,而是 LinearImpl.。它是一个宏定义，即 TORCH_MODULE 定义的真正的 Linear 。这个“生成的”类实际上是std::shared_ptr&lt;LinearImpl&gt;的封装。它是一个封装，而不是一个简单的类型定义，因此，构造函数仍然可以按预期工作，即您仍然可以编写 torch::nn::Linear(3, 4)而不需要写 std::make_shared&lt;LinearImpl&gt;(3, 4)。我们将宏创建的类称为模块容器。与（共享）指针类似，您可以使用箭头操作符（如 model-&gt;forward(...))访问基础对象。最终的结果是一个与PythonAPI非常相似的所有权模型。引用语义成为默认语义，但不需要额外输入std::shared_ptr 或者 std::make_shared。对于我们的网络，使用模块保持器API如下所示： struct NetImpl : torch::nn::Module {}; TORCH_MODULE(Net); void a(Net net) { } int main() { Net net; a(net); } 这里有一个微妙的问题值得一提。默认构造的 std::shared_ptr 为“空”，即包含空指针。什么是默认构造的 Linear 或者Net？嗯，这是一个棘手的选择。我们可以说它应该是一个空的（空） std::shared_ptr&lt;LinearImpl&gt。但是，请记住， Linear(3, 4) 与 std::make_shared&lt;LinearImpl&gt;(3, 4)相同。这意味着，如果我们已经决定 Linear linear；应该是一个空指针，那么就没有办法构造一个不接受任何构造函数参数的模块，或者默认所有这些参数。因此，在当前API中，默认构造的模块持有者（如 Linear()) ）调用底层模块的默认构造函数（LinearImpl()）。如果底层模块没有默认的构造函数，则会得到一个编译器错误。要构造空容器，可以将nullptr传递给容器的构造函数。 实际上，这意味着您可以使用前面所示的子模块，其中模块在初始值 initializer list中注册和构造： struct Net : torch::nn::Module { Net(int64_t N, int64_t M) : linear(register_module(\"linear\", torch::nn::Linear(N, M))) { } torch::nn::Linear linear; }; 或者，您可以先用一个空指针构造所有者，然后在构造函数中分配给它（对Pythonistas更熟悉）： struct Net : torch::nn::Module { Net(int64_t N, int64_t M) { linear = register_module(\"linear\", torch::nn::Linear(N, M)); } torch::nn::Linear linear{nullptr}; // construct an empty holder }; 总之：您应该使用哪种所有权模型——哪种语义？C++前端的API最优化支持模块持有者提供的所有权模型。这种机制的唯一缺点是在模块声明下面多了一行样板文件。也就是说，最简单的模型仍然是在C++模块的介绍中所显示的值语义模型。对于小的、简单的脚本，您也可以摆脱它。但你迟早会发现，出于技术原因，并不总是支持它。例如，序列化API(torch::save 和 torch::load)只支持模块持有者（或纯 shared_ptr）。因此，模块持有者API是用C++前端定义模块的推荐方式，今后我们将在本教程中使用该API。 定义DCGAN模块 现在，我们有了必要的背景和介绍，来为我们在本篇文章中要解决的机器学习任务定义模块。回顾一下：我们的任务是从MNIST 数据集中生成数字图像。我们想用生成对抗网络 (GAN) 来解决这个问题。特别是，我们将使用一个 DCGAN 体系结构——它是第一个也是最简单的体系结构之一，但对于这个任务来说已经完全足够了。 小贴士 您可以在此 存储库中找到本教程中介绍的完整源代码。 什么是 GAN aGAN？ GAN由两个不同的神经网络模型组成：发生器和鉴别器。生成器接收来自噪声分布的样本，其目的是将每个噪声样本转换为类似于目标分布的图像——在我们的例子中是MNIST数据集。鉴别器反过来接收来自MNIST数据集的真实图像或来自生成器的假图像。它被要求发出一个概率来判断一个特定图像是真实的（接近 1)）还是虚假的（接近 0)）。从鉴别器上得到的生成器生成图片的真实度的反馈被用来训练生成器；鉴别器的辨识度的反馈已经被用来优化鉴别器。理论上，发生器和鉴别器之间的微妙平衡使它们串联改进，导致发生器生成的图像与目标分布不可区分，从而愚弄鉴别器的辨识，使真实和虚假图像的概率均为 0.5 。对于我们来说，最终的结果是一台机器，它接收噪声作为输入，并生成数字的真实图像作为输出。 生成器模块 我们首先定义生成器模块，它由一系列转置的二维卷积、批处理规范化和ReLU激活单元组成。与Python一样，这里的PyTorch为模型定义提供了两个API：一个功能性的API，输入通过连续的函数传递，另一个面向对象的API，我们在其中构建一个包含整个模型作为子模块的 Sequential 模块。让我们看看我们的生成器如何使用这两种API，您可以自己决定您喜欢哪一种。首先，使用 Sequential:： using namespace torch; nn::Sequential generator( // Layer 1 nn::Conv2d(nn::Conv2dOptions(kNoiseSize, 256, 4) .with_bias(false) .transposed(true)), nn::BatchNorm(256), nn::Functional(torch::relu), // Layer 2 nn::Conv2d(nn::Conv2dOptions(256, 128, 3) .stride(2) .padding(1) .with_bias(false) .transposed(true)), nn::BatchNorm(128), nn::Functional(torch::relu), // Layer 3 nn::Conv2d(nn::Conv2dOptions(128, 64, 4) .stride(2) .padding(1) .with_bias(false) .transposed(true)), nn::BatchNorm(64), nn::Functional(torch::relu), // Layer 4 nn::Conv2d(nn::Conv2dOptions(64, 1, 4) .stride(2) .padding(1) .with_bias(false) .transposed(true)), nn::Functional(torch::tanh)); 小贴士 Sequential 模块只执行函数组合。第一个子模块的输出成为第二个子模块的输入，第三个子模块的输出成为第四个子模块的输入，以此类推。 所选的特定模块（如 nn::Conv2d 和nn::BatchNorm）遵循前面概述的结构。 kNoiseSize常量确定输入噪声矢量的大小，并设置为 100.。请注意，我们在激活函数中使用了torch::nn::Functional模块，将内部层的torch::relu传递给它，最后激活的是 torch::tanh 。当然，超参数是通过梯度的下降发现的。 笔记 Python前端为每个激活功能都有一个模块，比如 torch.nn.ReLU 或torch.nn.Tanh。在C++中，我们只提供 Functional 模块，您可以通过 Functional的转发forward()中调用的任何C++函数。 注意 对于第二种方法，我们在定义自己的模块的forward()方法中显式地在模块之间传递输入（以函数方式）： struct GeneratorImpl : nn::Module { GeneratorImpl() : conv1(nn::Conv2dOptions(kNoiseSize, 512, 4) .with_bias(false) .transposed(true)), batch_norm1(512), conv2(nn::Conv2dOptions(512, 256, 4) .stride(2) .padding(1) .with_bias(false) .transposed(true)), batch_norm2(256), conv3(nn::Conv2dOptions(256, 128, 4) .stride(2) .padding(1) .with_bias(false) .transposed(true)), batch_norm3(128), conv4(nn::Conv2dOptions(128, 64, 4) .stride(2) .padding(1) .with_bias(false) .transposed(true)), batch_norm4(64), conv5(nn::Conv2dOptions(64, 1, 4) .stride(2) .padding(1) .with_bias(false) .transposed(true)) {} torch::Tensor forward(torch::Tensor x) { x = torch::relu(batch_norm1(conv1(x))); x = torch::relu(batch_norm2(conv2(x))); x = torch::relu(batch_norm3(conv3(x))); x = torch::relu(batch_norm4(conv4(x))); x = torch::tanh(conv5(x)); return x; } nn::Conv2d conv1, conv2, conv3, conv4, conv5; nn::BatchNorm batch_norm1, batch_norm2, batch_norm3, batch_norm4; }; TORCH_MODULE(Generator); Generator generator; 无论使用哪种方法，我们现在都可以在生成器上调用 forward() 来将Generator噪声样本映射到图像。 笔 一个简短的关于路径选择的选项被传递到C++模块中的像 Conv2d 这样的内置模块：每个模块都有一些必需的选项，比如 BatchNorm.的特征数。如果只需要配置所需的选项，则可以将它们直接传递给模块的构造函数，如BatchNorm(128) 或 Dropout(0.5) 或 Conv2d(8, 4, 2) （用于输入通道计数、输出通道计数和内核大小）。但是，如果您需要修改其他选项（通常是默认的），例如使用 Conv2d的 with_bias ，则需要构造并传递一个选项对象。C++前端中的每个模块都有一个相关的选项结构，称为模块选项，其中 Module 是ModuleOptions ，比如 Linear 的LinearOptions 。这是我们为上面的 Conv2d 模块所做的。 鉴别器模块 鉴别器类似于一系列卷积、批量规范化和激活。然而，现在卷积是常规的而不是转置的，我们使用一个alpha值为0.2的leaky ReLU而不是vanilla ReLU。而且，最终的激活变成了一个Sigmoid，它将值压缩到0到1之间的范围。然后我们可以将这些压缩值解释为鉴别器分配给图像真实的概率： nn::Sequential discriminator( // Layer 1 nn::Conv2d( nn::Conv2dOptions(1, 64, 4).stride(2).padding(1).with_bias(false)), nn::Functional(torch::leaky_relu, 0.2), // Layer 2 nn::Conv2d( nn::Conv2dOptions(64, 128, 4).stride(2).padding(1).with_bias(false)), nn::BatchNorm(128), nn::Functional(torch::leaky_relu, 0.2), // Layer 3 nn::Conv2d( nn::Conv2dOptions(128, 256, 4).stride(2).padding(1).with_bias(false)), nn::BatchNorm(256), nn::Functional(torch::leaky_relu, 0.2), // Layer 4 nn::Conv2d( nn::Conv2dOptions(256, 1, 3).stride(1).padding(0).with_bias(false)), nn::Functional(torch::sigmoid)); 笔记 当我们传递给 Functional 函数接受的参数多于一个tensor时，我们可以将它们传递给 Functional 构造函数，后者将把它们转发给每个函数调用。对于上面的leaky ReLU，这意味着调用了torch::leaky_relu(previous_output_tensor, 0.2) 。 加载数据 既然我们已经定义了生成器和鉴别器模型，我们需要一些可以用来训练这些模型的数据。C++前端与Python一样，具有强大的并行数据加载程序。这个数据加载器可以从数据集（您可以自己定义）中读取批量数据，并提供许多配置。 笔记 Python数据装载器使用多处理。C++数据装载器是多线程的，并且不启动任何新进程。 数据加载器是C++前端 data API的一部分，包含在 torch::data:: 命名空间中。此API由几个不同的组件组成： 数据加载器类， 用于定义数据集的API， 用于定义转换的API，可应用于数据集， 用于定义采样器的API，该采样器生成用于索引数据集的索引， 现有数据集、转换和采样器的库。 对于本教程，我们可以使用带有C++前端的 MNIST 数据集。让我们为此实例化一个torch::data::datasets::MNIST，并应用两种转换：首先，我们对图像进行规格化，使其在 -1 到 +1 的范围内（从原始范围 0 到1）。其次，我们应用了堆栈排序规则，它将 a batch of tensors沿着第一个维度堆叠成一个tensor： auto dataset = torch::data::datasets::MNIST(\"./mnist\") .map(torch::data::transforms::Normalize(0.5, 0.5)) .map(torch::data::transforms::Stack<>()); 请注意， MNIST数据集应该位于./mnist目录中，相对于执行训练二进制文件的位置。您可以使用此脚本 下载MNIST数据集。 接下来，我们创建一个数据加载器并将这个数据集传递给它。要创建新的数据加载器，我们使用torch::data::make_data_loader，它返回正确类型的std::unique_ptr（这取决于数据集的类型、采样器的类型和一些其他实现细节）： auto dataloader = torch::data::make_data_loader(std::move(dataset)); 数据加载器确实有很多选项。你可以在 这里检查整套设备。例如，为了加速数据加载，我们可以增加工人的数量。默认值为零，这意味着将使用主线程。如果将 workers 设置为 2,，则会同时生成两个线程来加载数据。我们还应该将批大小从默认值 1 增加到更合理的值，比如 64 （ kBatchSize)的值）。因此，让我们创建一个 DataLoaderOptions 对象并设置适当的属性: auto dataloader = torch::data::make_data_loader( std::move(dataset), torch::data::DataLoaderOptions().batch_size(kBatchSize).workers(2)); 现在，我们可以编写一个循环来加载批数据，现在只输出到控制台： for (torch::data::Example<>& batch : *data_loader) { std::cout () 在这种情况下，数据加载器返回的类型是 torch::data::Example.。此类型是一个简单结构，具有 data 字段和标签 target 字段。因为我们之前应用了 Stack 排序规则，所以数据加载器只返回一个这样的示例。如果我们没有应用排序规则，数据加载器将生成 std::vector&lt;torch::data::Example&lt;&gt;&gt; ，在批处理中每个示例有一个元素。 如果重新生成并运行此代码，则应该看到如下内容： root@fa350df05ecf:/home/build# make Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcgan root@fa350df05ecf:/home/build# make [100%] Built target dcgan root@fa350df05ecf:/home/build# ./dcgan Batch size: 64 | Labels: 5 2 6 7 2 1 6 7 0 1 6 2 3 6 9 1 8 4 0 6 5 3 3 0 4 6 6 6 4 0 8 6 0 6 9 2 4 0 2 8 6 3 3 2 9 2 0 1 4 2 3 4 8 2 9 9 3 5 8 0 0 7 9 9 Batch size: 64 | Labels: 2 2 4 7 1 2 8 8 6 9 0 2 2 9 3 6 1 3 8 0 4 4 8 8 8 9 2 6 4 7 1 5 0 9 7 5 4 3 5 4 1 2 8 0 7 1 9 6 1 6 5 3 4 4 1 2 3 2 3 5 0 1 6 2 Batch size: 64 | Labels: 4 5 4 2 1 4 8 3 8 3 6 1 5 4 3 6 2 2 5 1 3 1 5 0 8 2 1 5 3 2 4 4 5 9 7 2 8 9 2 0 6 7 4 3 8 3 5 8 8 3 0 5 8 0 8 7 8 5 5 6 1 7 8 0 Batch size: 64 | Labels: 3 3 7 1 4 1 6 1 0 3 6 4 0 2 5 4 0 4 2 8 1 9 6 5 1 6 3 2 8 9 2 3 8 7 4 5 9 6 0 8 3 0 0 6 4 8 2 5 4 1 8 3 7 8 0 0 8 9 6 7 2 1 4 7 Batch size: 64 | Labels: 3 0 5 5 9 8 3 9 8 9 5 9 5 0 4 1 2 7 7 2 0 0 5 4 8 7 7 6 1 0 7 9 3 0 6 3 2 6 2 7 6 3 3 4 0 5 8 8 9 1 9 2 1 9 4 4 9 2 4 6 2 9 4 0 Batch size: 64 | Labels: 9 6 7 5 3 5 9 0 8 6 6 7 8 2 1 9 8 8 1 1 8 2 0 7 1 4 1 6 7 5 1 7 7 4 0 3 2 9 0 6 6 3 4 4 8 1 2 8 6 9 2 0 3 1 2 8 5 6 4 8 5 8 6 2 Batch size: 64 | Labels: 9 3 0 3 6 5 1 8 6 0 1 9 9 1 6 1 7 7 4 4 4 7 8 8 6 7 8 2 6 0 4 6 8 2 5 3 9 8 4 0 9 9 3 7 0 5 8 2 4 5 6 2 8 2 5 3 7 1 9 1 8 2 2 7 Batch size: 64 | Labels: 9 1 9 2 7 2 6 0 8 6 8 7 7 4 8 6 1 1 6 8 5 7 9 1 3 2 0 5 1 7 3 1 6 1 0 8 6 0 8 1 0 5 4 9 3 8 5 8 4 8 0 1 2 6 2 4 2 7 7 3 7 4 5 3 Batch size: 64 | Labels: 8 8 3 1 8 6 4 2 9 5 8 0 2 8 6 6 7 0 9 8 3 8 7 1 6 6 2 7 7 4 5 5 2 1 7 9 5 4 9 1 0 3 1 9 3 9 8 8 5 3 7 5 3 6 8 9 4 2 0 1 2 5 4 7 Batch size: 64 | Labels: 9 2 7 0 8 4 4 2 7 5 0 0 6 2 0 5 9 5 9 8 8 9 3 5 7 5 4 7 3 0 5 7 6 5 7 1 6 2 8 7 6 3 2 6 5 6 1 2 7 7 0 0 5 9 0 0 9 1 7 8 3 2 9 4 Batch size: 64 | Labels: 7 6 5 7 7 5 2 2 4 9 9 4 8 7 4 8 9 4 5 7 1 2 6 9 8 5 1 2 3 6 7 8 1 1 3 9 8 7 9 5 0 8 5 1 8 7 2 6 5 1 2 0 9 7 4 0 9 0 4 6 0 0 8 6 ... 这意味着我们能够成功地从 MNIST 数据集中加载数据。 编写迭代训练 现在让我们完成示例中的算法部分，并实现生成器和鉴别器之间的微妙跳跃。首先，我们将创建两个优化器，一个用于生成器，一个用于鉴别器。我们使用的优化器实现了 Adam 算法： torch::optim::Adam generator_optimizer( generator->parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5)); torch::optim::Adam discriminator_optimizer( discriminator->parameters(), torch::optim::AdamOptions(5e-4).beta1(0.5)); 笔记 在本文中，C++前端提供了实现 Adagrad, Adam, LBFGS, RMSprop 和 SGD的优化器。这个文档是最新的清单。 接下来，我们需要更新我们的迭代训练。在每个周期，我们将添加一个外循环来使用数据加载器，然后编写GAN的训练代码： for (int64_t epoch = 1; epoch & batch : *data_loader) { // Train discriminator with real images. discriminator->zero_grad(); torch::Tensor real_images = batch.data; torch::Tensor real_labels = torch::empty(batch.data.size(0)).uniform_(0.8, 1.0); torch::Tensor real_output = discriminator->forward(real_images); torch::Tensor d_loss_real = torch::binary_cross_entropy(real_output, real_labels); d_loss_real.backward(); // Train discriminator with fake images. torch::Tensor noise = torch::randn({batch.data.size(0), kNoiseSize, 1, 1}); torch::Tensor fake_images = generator->forward(noise); torch::Tensor fake_labels = torch::zeros(batch.data.size(0)); torch::Tensor fake_output = discriminator->forward(fake_images.detach()); torch::Tensor d_loss_fake = torch::binary_cross_entropy(fake_output, fake_labels); d_loss_fake.backward(); torch::Tensor d_loss = d_loss_real + d_loss_fake; discriminator_optimizer.step(); // Train generator. generator->zero_grad(); fake_labels.fill_(1); fake_output = discriminator->forward(fake_images); torch::Tensor g_loss = torch::binary_cross_entropy(fake_output, fake_labels); g_loss.backward(); generator_optimizer.step(); std::printf( \"\\r[%2ld/%2ld][%3ld/%3ld] D_loss: %.4f | G_loss: %.4f\", epoch, kNumberOfEpochs, ++batch_index, batches_per_epoch, d_loss.item(), g_loss.item()); } } 以上，我们首先在真实图像上对鉴别器进行评估，Adam应该为其分配高衰减率。为此，我们使用 torch::empty(batch.data.size(0)).uniform_(0.8, 1.0) 作为目标概率。 笔记 我们选取0.8到1.0之间均匀分布的随机值，而不是1.0，以使鉴别器训练更加健壮。这个技巧叫做label smoothing。 在评估鉴别器之前，我们将其参数的梯度归零。计算完损失后，我们通过调用 d_loss.backward() 来计算新的梯度，从而在网络中进行反向传播。我们在假图像上不断重复。我们不使用来自数据集的图像，而是让生成器通过向其提供一批随机噪声来为此创建假图像。然后我们把这些假图像转发给鉴别器。这一次，我们希望鉴别器发出低概率，理想情况下全部为0。一旦我们计算了一批真实图像和一批假图像的鉴别器损失，我们就可以一步一步地对鉴别器的优化器进行升级，以更新其参数。 为了训练生成器，我们再次将其梯度调零，然后在伪图像上重新评估鉴别器。然而，这次我们希望鉴别器分配的概率非常接近1.0，这将表明生成器可以生成图像，欺骗鉴别器认为它们是真实的（从数据集）。为此，我们将fake_labels tensor填充为所有 tensor。最后，我们对生成器的优化器执行步骤，以更新其参数。 我们现在应该准备好在CPU上训练我们的模型了。我们还没有任何代码来捕获状态或示例输出，但我们将在稍后添加此代码。现在，让我们观察一下我们的模型在做什么——稍后我们将根据生成的图像来验证这件事是否有意义。重建和运行应输出如下内容： root@3c0711f20896:/home/build# make && ./dcgan Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcga [ 1/10][100/938] D_loss: 0.6876 | G_loss: 4.1304 [ 1/10][200/938] D_loss: 0.3776 | G_loss: 4.3101 [ 1/10][300/938] D_loss: 0.3652 | G_loss: 4.6626 [ 1/10][400/938] D_loss: 0.8057 | G_loss: 2.2795 [ 1/10][500/938] D_loss: 0.3531 | G_loss: 4.4452 [ 1/10][600/938] D_loss: 0.3501 | G_loss: 5.0811 [ 1/10][700/938] D_loss: 0.3581 | G_loss: 4.5623 [ 1/10][800/938] D_loss: 0.6423 | G_loss: 1.7385 [ 1/10][900/938] D_loss: 0.3592 | G_loss: 4.7333 [ 2/10][100/938] D_loss: 0.4660 | G_loss: 2.5242 [ 2/10][200/938] D_loss: 0.6364 | G_loss: 2.0886 [ 2/10][300/938] D_loss: 0.3717 | G_loss: 3.8103 [ 2/10][400/938] D_loss: 1.0201 | G_loss: 1.3544 [ 2/10][500/938] D_loss: 0.4522 | G_loss: 2.6545 ... GPU移动到GPU 虽然我们当前的脚本可以在CPU上运行得很好，但我们都知道在GPU上卷积要快得多。让我们快速讨论一下如何将我们的训练转移到GPU上。我们需要为此做两件事：将GPU设备指定的传递给我们分配的 tensors ，并且通过 to() 方法将其他tensors显式复制到所有tensors和模块都具有的GPU上。实现这两者的最简单方法是在我们的训练脚本顶层创建一个 torch::Device 的实例，然后将该设备传递给tensors工厂方法，如 torch::zeros 和 to() 方法。我们可以从CPU设备开始： // Place this somewhere at the top of your training script. torch::Device device(torch::kCPU); 像这样分配新tensor torch::Tensor fake_labels = torch::zeros(batch.data.size(0)); 应更新以将device作为最后一个参数： torch::Tensor fake_labels = torch::zeros(batch.data.size(0), device); 对于创建不在我们手中的 tensors，比如来自 MNIST数据集的 tensors，我们必须插入显式to()调用。这意味着 torch::Tensor real_images = batch.data; 变成如下 torch::Tensor real_images = batch.data.to(device); 我们的模型参数应该被移动到合适的设备上 generator->to(device); discriminator->to(device); 笔记 如果一个 tensor已经存在于提供to()给的设备上，则调用是no-op。不进行额外的复制。 在这一点上，我们刚刚使以前的CPU驻留代码更加明确。但是，现在也很容易将设备更改为CUDA设备： torch::Device device(torch::kCUDA) 现在所有的 tensors都将活动在GPU上，为所有的操作调用快速的CUDA内核，而不需要我们更改任何下游代码。如果我们想要指定一个特定的设备索引，它可以作为第二个参数传递给Device构造函数。如果我们希望不同的tensors存在于不同的设备上，我们可以传递单独的设备实例（例如，一个在CUDA设备0上，另一个在CUDA设备1上）。我们甚至可以动态地进行此配置，这通常有助于使我们的训练脚本更易于移植： torch::Device device = torch::kCPU; if (torch::cuda::is_available()) { std::cout 甚至是这样 torch::Device device(torch::cuda::is_available() ? torch::kCUDA : torch::kCPU); 检查点和恢复训练状态 我们对训练脚本的最后一个增强点是定期保存模型参数的状态、优化器的状态以及一些生成的图像样本。如果我们的计算机在训练过程中崩溃，前两个将允许我们恢复训练状态。对于长期的训练，这是绝对必要的。幸运的是，C++前端提供了一个API来序列化和反序列化模型和优化器状态，以及独立的 tensors。 它的核心API是 torch::save(thing,filename) 和 torch::load(thing,filename)，其中thing可以是 torch::nn::Module 子类或优化程序实例，如我们训练脚本中的 Adam 对象。让我们更新我们的训练循环，以检查模型和优化器在特定时间间隔的状态： if (batch_index % kCheckpointEvery == 0) { // Checkpoint the model and optimizer state. torch::save(generator, \"generator-checkpoint.pt\"); torch::save(generator_optimizer, \"generator-optimizer-checkpoint.pt\"); torch::save(discriminator, \"discriminator-checkpoint.pt\"); torch::save(discriminator_optimizer, \"discriminator-optimizer-checkpoint.pt\"); // Sample the generator and save the images. torch::Tensor samples = generator->forward(torch::randn({8, kNoiseSize, 1, 1}, device)); torch::save((samples + 1.0) / 2.0, torch::str(\"dcgan-sample-\", checkpoint_counter, \".pt\")); std::cout checkpoint \" 其中 kCheckpointEvery 是一个整数，设置为 100 ，每 100 批检查一次， checkpoint_counter是一个计数器，每当我们建立一个检查点时都会冲撞到它。 要恢复训练状态，可以在创建所有模型和优化器之后。但在训练循环之前添加类似这样的行： torch::optim::Adam generator_optimizer( generator->parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5)); torch::optim::Adam discriminator_optimizer( discriminator->parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5)); if (kRestoreFromCheckpoint) { torch::load(generator, \"generator-checkpoint.pt\"); torch::load(generator_optimizer, \"generator-optimizer-checkpoint.pt\"); torch::load(discriminator, \"discriminator-checkpoint.pt\"); torch::load( discriminator_optimizer, \"discriminator-optimizer-checkpoint.pt\"); } int64_t checkpoint_counter = 0; for (int64_t epoch = 1; epoch & batch : *data_loader) { 检验生成的图像 我们的训练脚本现在完成了。我们在CPU或者GPU上准备好培训我们的GAN。为了检查我们训练过程的中间输出，我们添加了代码以定期将图像样本保存到 \"dcgan-sample-xxx.pt\"文件中。我们可以编写一个小的python脚本来加载tensors并用matplotlib显示它们： from __future__ import print_function from __future__ import unicode_literals import argparse import matplotlib.pyplot as plt import torch parser = argparse.ArgumentParser() parser.add_argument(\"-i\", \"--sample-file\", required=True) parser.add_argument(\"-o\", \"--out-file\", default=\"out.png\") parser.add_argument(\"-d\", \"--dimension\", type=int, default=3) options = parser.parse_args() module = torch.jit.load(options.sample_file) images = list(module.parameters())[0] for index in range(options.dimension * options.dimension): image = images[index].detach().cpu().reshape(28, 28).mul(255).to(torch.uint8) array = image.numpy() axis = plt.subplot(options.dimension, options.dimension, 1 + index) plt.imshow(array, cmap=\"gray\") axis.get_xaxis().set_visible(False) axis.get_yaxis().set_visible(False) plt.savefig(options.out_file) print(\"Saved \", options.out_file) 现在让我们把这个模型训练大概30次： root@3c0711f20896:/home/build# make && ./dcgan 10:17:57 Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcgan CUDA is available! Training on GPU. [ 1/30][200/938] D_loss: 0.4953 | G_loss: 4.0195 -> checkpoint 1 [ 1/30][400/938] D_loss: 0.3610 | G_loss: 4.8148 -> checkpoint 2 [ 1/30][600/938] D_loss: 0.4072 | G_loss: 4.36760 -> checkpoint 3 [ 1/30][800/938] D_loss: 0.4444 | G_loss: 4.0250 -> checkpoint 4 [ 2/30][200/938] D_loss: 0.3761 | G_loss: 3.8790 -> checkpoint 5 [ 2/30][400/938] D_loss: 0.3977 | G_loss: 3.3315 ... -> checkpoint 120 [30/30][938/938] D_loss: 0.3610 | G_loss: 3.8084 并在绘图中显示图像： root@3c0711f20896:/home/build# python display.py -i dcgan-sample-100.pt Saved out.png 它看起来应该是这样： 数字！万岁！现在轮到你了：你能改进模型使数字看起来更好吗？ 结论 本教程希望给您一个易了解的PyTrac C++前端的摘要。像PyTorch这样的机器学习库必然具有数量庞大的API。因此，这里有许多概念我们没有时间来讨论。但是，我鼓励您尝试使用API，并在遇到困难时参考 我们的文档 ，尤其是库API 部分。此外，请记住，我们会尽可能使C++前端可以遵循Python前端的语义和设计，这样您就可以利用这个特点来提高学习速度。 小贴士 您可以在此存储库中找到本教程中介绍的完整源代码。 和往常一样，如果你遇到任何问题或有任何疑问，你可以使用我们的 论坛 或GitHub issues问题联系。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"docs_notes.html":{"url":"docs_notes.html","title":"注解","keywords":"","body":"注解 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes_autograd.html":{"url":"notes_autograd.html","title":"自动求导机制","keywords":"","body":"自动求导机制 译者：冯宝宝校验：AlexJakin 本说明将概述autograd（自动求导）如何工作并记录每一步操作。了解这些并不是绝对必要的，但我们建议您熟悉它，因为它将帮助你编写更高效，更清晰的程序，并可以帮助您进行调试。 反向排除子图 每个张量都有一个标志：requires_grad，允许从梯度计算中细致地排除子图，并可以提高效率。 requires_grad 只要有单个输入进行梯度计算操作，则其输出也需要梯度计算。相反，只有当所有输入都不需要计算梯度时，输出才不需要梯度计算。如果其中所有的张量都不需要进行梯度计算，后向计算不会在子图中执行。 >>> x = torch.randn(5, 5) # requires_grad=False by default >>> y = torch.randn(5, 5) # requires_grad=False by default >>> z = torch.randn((5, 5), requires_grad=True) >>> a = x + y >>> a.requires_grad False >>> b = a + z >>> b.requires_grad True 当你想要冻结部分模型或者事先知道不会使用某些参数的梯度时，这个requires_grad标志非常有用。例如，如果要微调预训练的CNN，只需在冻结的基础中切换requires_grad标志就够了，并且直到计算到达最后一层，才会保存中间缓冲区，，其中仿射变换将使用所需要梯度的权重 ，网络的输出也需要它们。 model = torchvision.models.resnet18(pretrained=True) for param in model.parameters(): param.requires_grad = False # Replace the last fully-connected layer # Parameters of newly constructed modules have requires_grad=True by default model.fc = nn.Linear(512, 100) # Optimize only the classifier optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9) 自动求导是如何记录编码历史的 自动求导是反向自动分化系统。从概念上讲，自动求导会记录一个图形，记录在执行操作时创建数据的所有操作，为您提供有向无环图，其叶子是输入张量，根节点是输出张量。通过从根到叶跟踪此图，您可以使用链法则自动计算梯度。 在内部，autograd将此图表示为Function对象（实际表达式）的图形，可以用来计算评估图形的结果。 当计算前向传播时，自动求导同时执行所请求的计算并建立表示计算梯度的函数的图形（每个torch.Tensor的.grad_fn属性是该图的入口点）。当前向传播完成时，我们在后向传播中评估该图以计算梯度。 需要注意的一点是，在每次迭代时都会从头开始重新创建图形，这正是允许使用任意Python控制流语句的原因，它可以在每次迭代时更改图形的整体形状和大小。 在开始训练之前，不必编码所有可能的路径 - 您运行的是您所区分的部分。 使用autograd进行in-place操作 在autograd中支持in-place操作是一件很难的事情，大多数情况下，我们不鼓励使用它们。Autograd积极的缓冲区释放和重用使其非常高效，实际上在in-place操作会大幅降低内存使用量的情况也非常少。除非在巨大的内存压力下运行，否则你可能永远不需要使用它们。 限制in-place操作适用性的主要原因有两个： 这个操作可能会覆盖梯度计算所需的值。 实际上，每个in-place操作需要重写计算图。out-of-place版本只是分配新对象并保留对旧图的引用，而in-place操作则需要将所有输入的creator更改为表示此操作的Function。这就比较麻烦，特别是如果有许多变量引用同一存储（例如通过索引或转置创建的），并且如果被修改输入的存储被任何其他张量引用，这样的话，in-place函数会抛出错误。 In-place正确性检查 每一个张量都有一个版本计算器，每次在任何操作中标记都会递增。 当Function保存任何用于后向传播的张量时，也会保存包含张量的版本计数器。一旦访问self.saved_tensors后，它将被检查，如果它大于保存的值，则会引发错误。这可以确保如果您使用in-place函数而没有看到任何错误，则可以确保计算出的梯度是正确的。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes_broadcasting.html":{"url":"notes_broadcasting.html","title":"广播语义","keywords":"","body":"广播语义 译者：冯宝宝 校验：AlexJakin 许许多多的PyTorch操作都支持NumPy Broadcasting Semantics。 简而言之，如果PyTorch操作支持广播，那么它的Tensor参数可以自动扩展为相同的类型大小（不需要复制数据）。 一般语义 如果遵守以下规则，则两个张量是“可广播的”： 每个张量至少有一个维度； 遍历张量维度大小时，从末尾随开始遍历，两个张量的维度大小必须相等，它们其中一个为1，或者一个不存在。 例如： >>> x=torch.empty(5,7,3) >>> y=torch.empty(5,7,3) # 相同形状的张量可以被广播(上述规则总是成立的) >>> x=torch.empty((0,)) >>> y=torch.empty(2,2) # x和y不能被广播,因为x没有维度 # can line up trailing dimensions >>> x=torch.empty(5,3,4,1) >>> y=torch.empty( 3,1,1) # x和y能够广播. # 1st trailing dimension: both have size 1 # 2nd trailing dimension: y has size 1 # 3rd trailing dimension: x size == y size # 4th trailing dimension: y dimension doesn't exist # 但是: >>> x=torch.empty(5,2,4,1) >>> y=torch.empty( 3,1,1) # x和y不能被广播 （ ） 如果x,y两个张量是可以广播的，则通过计算得到的张量大小遵循以下原则： 如果x和y的维数不相等，则在维度较小的张量的前面增加1个维度，使它们的长度相等。 然后,生成新张量维度的大小是x和y在每个维度上的最大值。 例如： # can line up trailing dimensions to make reading easier >>> x=torch.empty(5,1,4,1) >>> y=torch.empty( 3,1,1) >>> (x+y).size() torch.Size([5, 3, 4, 1]) # but not necessary: >>> x=torch.empty(1) >>> y=torch.empty(3,1,7) >>> (x+y).size() torch.Size([3, 1, 7]) >>> x=torch.empty(5,2,4,1) >>> y=torch.empty(3,1,1) >>> (x+y).size() RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1 In - place 语义 一个复杂因素是in-place操作不允许in-place张量像广播那样改变形状。 例如： >>> x=torch.empty(5,3,4,1) >>> y=torch.empty(3,1,1) >>> (x.add_(y)).size() torch.Size([5, 3, 4, 1]) # but: >>> x=torch.empty(1,3,1) >>> y=torch.empty(3,1,7) >>> (x.add_(y)).size() RuntimeError: The expanded size of the tensor (1) must match the existing size (7) at non-singleton dimension 2. 向后兼容性 PyTorch的早期版本允许某些逐点函数在具有不同形状的张量上执行，只要每个张量中的元素数量相等即可。 然后通过将每个张量视为1维来执行逐点运算。PyTorch现在支持广播，并且“1维”逐点行为被认为已弃用，并且在张量不可广播但具有相同数量的元素的情况下将生成Python警告。 注意，在两个张量不具有相同形状但是可广播并且具有相同数量元素的情况下，广播的引入可能导致向后不兼容。例如： >>> torch.add(torch.ones(4,1), torch.randn(4)) 以前可能会产生一个torch.Size（[4,1]）的Tensor，但现在会产生一个torch.Size（[4,4]）这样的Tensor。 为了帮助识别代码中可能存在广播引起的向后不兼容性的情况，您可以设置torch.utils.backcompat.broadcast_warning.enabled 为 True，在这种情况下会产生python警告。 >>> torch.utils.backcompat.broadcast_warning.enabled=True >>> torch.add(torch.ones(4,1), torch.ones(4)) __main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements. Changing behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional. 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes_cuda.html":{"url":"notes_cuda.html","title":"CUDA 语义","keywords":"","body":"CUDA 语义 译者：片刻 校验：AlexJakin torch.cuda 用于设置和运行 CUDA 操作。它会跟踪当前选定的GPU，并且默认情况下会在该设备上创建您分配的所有 CUDA tensors。可以使用 torch.cuda.device 上下文管理器更改所选设备。 但是，一旦分配了 tensor，就可以对其进行操作而不管所选择的设备如何，结果将始终与 tensor 放在同一设备上。 默认情况下不允许跨 GPU 操作，除了 copy_() 具有类似复制功能的其他方法，例如 to() 和 cuda()。除非您启用点对点内存访问，否则任何尝试在不同设备上传播的 tensor 上启动操作都会引发错误。 下面我们用一个小例子来展示: cuda = torch.device('cuda') # Default CUDA device cuda0 = torch.device('cuda:0') cuda2 = torch.device('cuda:2') # GPU 2 (these are 0-indexed) x = torch.tensor([1., 2.], device=cuda0) # x.device is device(type='cuda', index=0) y = torch.tensor([1., 2.]).cuda() # y.device is device(type='cuda', index=0) with torch.cuda.device(1): # allocates a tensor on GPU 1 a = torch.tensor([1., 2.], device=cuda) # transfers a tensor from CPU to GPU 1 b = torch.tensor([1., 2.]).cuda() # a.device and b.device are device(type='cuda', index=1) # You can also use ``Tensor.to`` to transfer a tensor: b2 = torch.tensor([1., 2.]).to(device=cuda) # b.device and b2.device are device(type='cuda', index=1) c = a + b # c.device is device(type='cuda', index=1) z = x + y # z.device is device(type='cuda', index=0) # even within a context, you can specify the device # (or give a GPU index to the .cuda call) d = torch.randn(2, device=cuda2) e = torch.randn(2).to(cuda2) f = torch.randn(2).cuda(cuda2) # d.device, e.device, and f.device are all device(type='cuda', index=2) 异步执行 默认情况下，GPU 操作是异步的。当您调用使用 GPU 的函数时，操作将排入特定设备，但不一定要在以后执行。这允许我们并行执行更多计算，包括在 CPU 或其他 GPU 上的操作。 通常，异步计算的效果对于调用者是不可见的，因为 (1) 每个设备按照它们排队的顺序执行操作，以及 (2) PyTorch 在 CPU 和 GPU 之间或两个 GPU 之间复制数据时自动执行必要的同步。因此，计算将如同每个操作同步执行一样进行。 您可以通过设置环境变量强制进行同步计算 CUDA_LAUNCH_BLOCKING=1。这在 GPU 上发生错误时非常方便。（使用异步执行时，直到实际执行操作后才会报告此类错误，因此堆栈跟踪不会显示请求的位置。） 异步计算的结果是没有同步的时间测量是不精确的。要获得精确的测量结果，应该在测量之前调用torch.cuda.synchronize()，或者使用torch.cuda.Event记录时间如下： start_event = torch.cuda.Event(enable_timing=True) end_event = torch.cuda.Event(enable_timing=True) start_event.record() # 在这里执行一些操作 end_event.record() torch.cuda.synchronize() # Wait for the events to be recorded! elapsed_time_ms = start_event.elapsed_time(end_event) 作为一个例外，有几个函数，例如 to() 和 copy_() 允许一个显式 non_blocking 参数，它允许调用者在不需要时绕过同步。另一个例外是 CUDA streams，如下所述。 CUDA streams CUDA stream 是执行的线性序列属于特定的设备。您通常不需要显式创建一个：默认情况下，每个设备使用自己的 “default” stream。 每个流内的操作按创建顺序进行序列化，但不同流的操作可以按任何相对顺序同时执行，除非使用显式同步功能（如 synchronize() 或 wait_stream() ）。例如，以下代码不正确: cuda = torch.device('cuda') s = torch.cuda.Stream() # Create a new stream. A = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0) with torch.cuda.stream(s): # sum() may start execution before normal_() finishes! B = torch.sum(A) 当 “current stream” 是 default stream 时，PyTorch 在数据移动时自动执行必要的同步，如上所述。但是，使用 non-default streams 时，用户有责任确保正确同步。 内存管理 PyTorch 使用缓存内存分配器来加速内存分配。这允许在没有设备同步的情况下快速释放内存。但是，分配器管理的未使用内存仍将显示为使用 nvidia-smi。您可以使用 memory_allocated() 和 max_memory_allocated() 监视张量占用的内存，并使用 memory_cached() 和 max_memory_cached() 监视缓存分配器管理的内存。调用 empty_cache() 可以从 PyTorch 释放所有 unused 的缓存内存，以便其他 GPU 应用程序可以使用它们。但是，tensor 占用的 GPU 内存不会被释放，因此无法增加 PyTorch 可用的 GPU 内存量。 最佳做法 设备无关的代码 由于 PyTorch 的结构，您可能需要显式编写设备无关（CPU或GPU）代码; 一个例子可能是创建一个新的张量作为递归神经网络的初始隐藏状态。 第一步是确定是否应该使用GPU。常见的模式是使用Python的 argparse 模块读入用户参数，并有一个可用于禁用 CUDA 的标志，并结合使用 is_available()。在下文中，args.device 结果 torch.device 可以用于将 tensor 移动到 CPU 或 CUDA 的对象。 import argparse import torch parser = argparse.ArgumentParser(description='PyTorch Example') parser.add_argument('--disable-cuda', action='store_true', help='Disable CUDA') args = parser.parse_args() args.device = None if not args.disable_cuda and torch.cuda.is_available(): args.device = torch.device('cuda') else: args.device = torch.device('cpu') 现在 args.device 我们可以使用它在所需的设备上创建 Tensor。 x = torch.empty((8, 42), device=args.device) net = Network().to(device=args.device) 这可以在许多情况下用于产生与设备无关的代码。以下是使用 dataloader 时的示例: cuda0 = torch.device('cuda:0') # CUDA GPU 0 for i, x in enumerate(train_loader): x = x.to(cuda0) 在系统上使用多个 GPU 时，可以使用 CUDA_VISIBLE_DEVICES 环境标志来管理 PyTorch 可用的 GPU。如上所述，要手动控制创建张量的GPU，最佳做法是使用 torch.cuda.device 上下文管理器。 print(\"Outside device is 0\") # On device 0 (default in most scenarios) with torch.cuda.device(1): print(\"Inside device is 1\") # On device 1 print(\"Outside device is still 0\") # On device 0 如果你有一个 tensor 并且想在同一个设备上创建一个相同类型的新 tensor，那么你可以使用一个 torch.Tensor.new_* 方法（参见参考资料 torch.Tensor）。虽然前面提到的 torch.* factory 函数（ Creation Ops ）依赖于当前 GPU 上下文和您传入的属性参数，但 torch.Tensor.new_* 方法会保留设备和 tensor 的其他属性。 在创建在前向传递期间需要在内部创建新 tensor 的模块时，这是建议的做法。 cuda = torch.device('cuda') x_cpu = torch.empty(2) x_gpu = torch.empty(2, device=cuda) x_cpu_long = torch.empty(2, dtype=torch.int64) y_cpu = x_cpu.new_full([3, 2], fill_value=0.3) print(y_cpu) tensor([[ 0.3000, 0.3000], [ 0.3000, 0.3000], [ 0.3000, 0.3000]]) y_gpu = x_gpu.new_full([3, 2], fill_value=-5) print(y_gpu) tensor([[-5.0000, -5.0000], [-5.0000, -5.0000], [-5.0000, -5.0000]], device='cuda:0') y_cpu_long = x_cpu_long.new_tensor([[1, 2, 3]]) print(y_cpu_long) tensor([[ 1, 2, 3]]) 如果你想创建一个与另一个 tensor 相同类型和大小的 tensor，并用一个或零填充它，ones_like() 或 zeros_like() 作为方便的辅助函数（也保留 Tensor 的 torch.device 和 torch.dtype ）提供。 x_cpu = torch.empty(2, 3) x_gpu = torch.empty(2, 3) y_cpu = torch.ones_like(x_cpu) y_gpu = torch.zeros_like(x_gpu) 使用固定内存缓冲区 当源自固定（页面锁定）内存时，主机到 GPU 副本的速度要快得多。CPU tensor 和存储器公开一种 pin_memory() 方法，该方法返回对象的副本，数据放在固定区域中。 此外，一旦您固定张量或存储，您就可以使用异步GPU副本。只需将一个额外的 non_blocking=True 参数传递给一个 cuda() 调用。这可以用于通过计算重叠数据传输。 您可以 DataLoader 通过传递 pin_memory=True 给构造函数使返回批处理放置在固定内存中。 使用 nn.DataParallel 而不是多处理 涉及批量输入和多个 GPU 的大多数用例应默认 DataParallel 使用多个GPU。即使使用GIL，单个 Python 进程也可以使多个 GPU 饱和。 从版本 0.1.9 开始，可能无法充分利用大量 GPUs (8+)。但是，这是一个正在积极开发的已知问题。一如既往，测试您的用例。 使用带有 multiprocessing 的 CUDA 模型有一些重要的注意事项 ; 除非注意完全满足数据处理要求，否则您的程序可能会有不正确或未定义的行为。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes_extending.html":{"url":"notes_extending.html","title":"Extending PyTorch","keywords":"","body":"扩展PyTorch 译者：PEGASUS1993 本章中，将要介绍使用我们的C库如何扩展torch.nn，torch.autograd和编写自定义的C扩展工具。 扩展torch.autograd 添加操作autograd需要Function为每个操作实现一个新的子类。回想一下，Function使用autograd来计算结果和梯度，并对操作历史进行编码。每个新功能都需要您实现两种方法： forward() - 执行操作的代码。如果您指定了默认值，则可以根据需求使用任意参数，其中一些参数可选。这里支持各种Python对象。Variable参数在调用之前会被转换Tensor，并且它们的使用情况将在graph中注册。请注意，此逻辑不会遍历lists/dicts/和其他任何数据的结构，并且只考虑被直接调用的Variables参数。如果有多个输出你可以返回单个Tensor或Tensor格式的元组。另外，请参阅Function文档查找只能被forward()调用的有用方法的说明。 backward() - 计算梯度的公式. 它将被赋予与输出一样多的Variable参数, 其中的每一个表示对应梯度的输出. 它应该返回与输入一样多的Variable, 其中的每一个表示都包含其相应输入的梯度. 如果输入不需要计算梯度 (请参阅needs_input_grad属性),或者是非Variable对象,则可返回None类.此外,如果你在forward()方法中有可选的参数,则可以返回比输入更多的梯度,只要它们都是None类型即可. 你可以从下面的代码看到torch.nn模块的Linear函数, 以及注解 # Inherit from Function class Linear(Function): # bias is an optional argument def forward(self, input, weight, bias=None): self.save_for_backward(input, weight, bias) output = input.mm(weight.t()) if bias is not None: output += bias.unsqueeze(0).expand_as(output) return output # This function has only a single output, so it gets only one gradient def backward(self, grad_output): # This is a pattern that is very convenient - at the top of backward # unpack saved_tensors and initialize all gradients w.r.t. inputs to # None. Thanks to the fact that additional trailing Nones are # ignored, the return statement is simple even when the function has # optional inputs. input, weight, bias = self.saved_tensors grad_input = grad_weight = grad_bias = None # These needs_input_grad checks are optional and there only to # improve efficiency. If you want to make your code simpler, you can # skip them. Returning gradients for inputs that don't require it is # not an error. if self.needs_input_grad[0]: grad_input = grad_output.mm(weight) if self.needs_input_grad[1]: grad_weight = grad_output.t().mm(input) if bias is not None and self.needs_input_grad[2]: grad_bias = grad_output.sum(0).squeeze(0) return grad_input, grad_weight, grad_bias 现在，为了更方便使用这些自定义操作，推荐使用apply方法： linear = LinearFunction.apply 我们下面给出一个由非变量参数进行参数化的函数的例子: class MulConstant(Function): @staticmethod def forward(ctx, tensor, constant): # ctx is a context object that can be used to stash information # for backward computation ctx.constant = constant return tensor * constant @staticmethod def backward(ctx, grad_output): # We return as many input gradients as there were arguments. # Gradients of non-Tensor arguments to forward must be None. return grad_output * ctx.constant, None 注意 向后输入，即grad_output，也可以是跟踪历史的张量。因此，如果使用可微运算来实现向后运算（例如，调用另一个自定义函数），则更高阶导数将起作用。 你可能想检测你刚刚实现的backward方法是否正确的计算了梯度。你可以使用小的有限差分法(Finite Difference)进行数值估计。 from torch.autograd import gradcheck # gradcheck takes a tuple of tensors as input, check if your gradient # evaluated with these tensors are close enough to numerical # approximations and returns True if they all verify this condition. input = (Variable(torch.randn(20,20).double(), requires_grad=True), Variable(torch.randn(30,20).double(), requires_grad=True),) test = gradcheck(Linear.apply, input, eps=1e-6, atol=1e-4) print(test) 有关有限差分梯度比较的更多详细信息，请参见数值梯度检查。 扩展 torch.nn nn模块包含两种接口 - modules和他们的功能版本。你可以用两种方法扩展它,但是我们建议，在扩展layer的时候使用modules， 因为modules保存着参数和buffer。如果使用无参数操作的话，那么建议使用激活函数，池化等函数。 在上面的章节中,添加操作的功能版本已经介绍过了。 增加一个Module。 由于nn大量使用autograd。所以， 添加一个新的Module类需要实现一个Function类, 它会执行对应的操作并且计算梯度。我们只需要很少的代码就可以实现上面Linear模块的功能。现在，我们需要实现两个函数： __init__ (optional) - 接收kernel sizes内核大小，特征数量等参数，并初始化parameters参数和buffers缓冲区。 forward() - 实例化Function并使用它来执行操作。它与上面显示的functional wrapper非常相似。 下面是实现Linear模块的方式： class Linear(nn.Module): def __init__(self, input_features, output_features, bias=True): super(Linear, self).__init__() self.input_features = input_features self.output_features = output_features # nn.Parameter is a special kind of Variable, that will get # automatically registered as Module's parameter once it's assigned # as an attribute. Parameters and buffers need to be registered, or # they won't appear in .parameters() (doesn't apply to buffers), and # won't be converted when e.g. .cuda() is called. You can use # .register_buffer() to register buffers. # nn.Parameters require gradients by default. self.weight = nn.Parameter(torch.Tensor(output_features, input_features)) if bias: self.bias = nn.Parameter(torch.Tensor(output_features)) else: # You should always register all possible parameters, but the # optional ones can be None if you want. self.register_parameter('bias', None) # Not a very smart way to initialize weights self.weight.data.uniform_(-0.1, 0.1) if bias is not None: self.bias.data.uniform_(-0.1, 0.1) def forward(self, input): # See the autograd section for explanation of what happens here. return LinearFunction.apply(input, self.weight, self.bias) def extra_repr(self): # (Optional)Set the extra information about this module. You can test # it by printing an object of this class. return 'in_features={}, out_features={}, bias={}'.format( self.in_features, self.out_features, self.bias is not None ) 编写自定义的C++扩展 有关详细说明和示例，请参阅此PyTorch教程。 文档可在torch.utils.cpp_extension.获得。 编写自定义的C扩展 可用示例可以在这个Github仓库里面查看参考。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes_faq.html":{"url":"notes_faq.html","title":"Frequently Asked Questions","keywords":"","body":"常见问题解答 译者：冯宝宝 我的模型报告“cuda runtime error(2): out of memory” 正如错误消息所示，您的GPU显存已耗尽。由于经常在PyTorch中处理大量数据，因此小错误会迅速导致程序耗尽所有GPU资源; 幸运的是，这些情况下的修复通常很简单。这里有一些常见点需要检查： 不要在训练循环中积累历史记录。 默认情况下，涉及需要梯度计算的变量将保留历史记录。这意味着您应该避免在计算中使用这些变量，因为这些变量将超出您的训练循环，例如，在跟踪统计数据时。相反，您应该分离变量或访问其基础数据。 有时，当可微分变量发生时，它可能是不明显的。考虑以下训练循环（从源代码中删除）： total_loss = 0 for i in range(10000): optimizer.zero_grad() output = model(input) loss = criterion(output) loss.backward() optimizer.step() total_loss += loss 在这里，total_loss在您的训练循环中累积历史记录，因为丢失是具有自动记录历史的可微分变量。 您可以通过编写total_loss + = float（loss）来解决此问题。 此问题的其他实例：1。 不要抓住你不需要的张量或变量。 如果将张量或变量分配给本地，则在本地超出范围之前，Python不会解除分配。您可以使用del x释放此引用。 同样，如果将张量或向量分配给对象的成员变量，则在对象超出范围之前不会释放。如果您没有保留不需要的临时工具，您将获得最佳的内存使用量。 本地规模大小可能比您预期的要大。 例如： for i in range(5): intermediate = f(input[i]) result += g(intermediate) output = h(result) return output 在这里，即使在执行h时，中间变量仍然存在，因为它的范围超出了循环的末尾。要提前释放它，你应该在完成它时使用del。 不要在太大的序列上运行RNN。 通过RNN反向传播所需的存储量与RNN的长度成线性关系; 因此，如果您尝试向RNN提供过长的序列，则会耗尽内存。 这种现象的技术术语是随着时间的推移而反向传播，并且有很多关于如何实现截断BPTT的参考，包括在单词语言模型示例中; 截断由重新打包功能处理，如本论坛帖子中所述。 不要使用太大的线性图层。 线性层nn.Linear（m，n）使用O(nm)存储器：也就是说，权重的存储器需求与特征的数量成比例。 以这种方式很容易占用你的存储（并且记住，你将至少需要两倍存储权值的内存量，因为你还需要存储梯度。） My GPU memory isn’t freed properly PyTorch使用缓存内存分配器来加速内存分配。 因此，nvidia-smi中显示的值通常不会反映真实的内存使用情况。 有关GPU内存管理的更多详细信息，请参阅内存管理 。 如果在Python退出后你的GPU内存仍旧没有被释放，那么很可能是一些Python子进程仍处于活动状态。你可以通过ps -elf |grep python找到它们并用kill -9 [pid]手动结束这些进程。 My data loader workers return identical random numbers 您可能正在数据集中使用其他库来生成随机数。 例如，当通过fork启动工作程序子进程时，NumPy的RNG会重复。有关如何使用worker_init_fn选项在工作程序中正确设置随机种子的文档，请参阅torch.utils.data.DataLoader文档。 My recurrent network doesn’t work with data parallelism 在具有DataParallel或data_parallel()的模块中使用pack sequence -> recurrent network -> unpack sequence模式时有一个非常微妙的地方。每个设备上的forward()的输入只会是整个输入的一部分。由于默认情况下，解包操作torch.nn.utils.rnn.pad_packed_sequence()仅填充到其所见的最长输入，即该特定设备上的最长输入，所以在将结果收集在一起时会发生尺寸的不匹配。因此，您可以利用pad_packed_sequence()的 total_length参数来确保forward()调用返回相同长度的序列。例如，你可以写： from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence class MyModule(nn.Module): # ... __init__, 以及其他访求 # padding_input 的形状是[B x T x *]（batch_first 模式），包含按长度排序的序列 # B 是批量大小 # T 是最大序列长度 def forward(self, padded_input, input_lengths): total_length = padded_input.size(1) # get the max sequence length packed_input = pack_padded_sequence(padded_input, input_lengths, batch_first=True) packed_output, _ = self.my_lstm(packed_input) output, _ = pad_packed_sequence(packed_output, batch_first=True, total_length=total_length) return output m = MyModule().cuda() dp_m = nn.DataParallel(m) 另外，在批量的维度为dim 1（即 batch_first = False ）时需要注意数据的并行性。在这种情况下，pack_padded_sequence 函数的的第一个参数 padding_input 维度将是 [T x B x *] ，并且应该沿dim 1 （第1轴）分散，但第二个参数 input_lengths 的维度为 [B]，应该沿dim 0 （第0轴）分散。需要额外的代码来操纵张量的维度。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes_multiprocessing.html":{"url":"notes_multiprocessing.html","title":"Multiprocessing best practices","keywords":"","body":"多进程最佳实践 译者：cvley torch.multiprocessing 是 Python 的 multiprocessing 的直接替代模块。它支持完全相同的操作，但进行了扩展，这样所有的张量就可以通过一个 multiprocessing.Queue 进行传递，将数据移动到共享内存并只将句柄传递到另一个进程。 注意 当一个 Tensor 传递到另一个进程时，Tensor 的数据是共享的。如果 torch.Tensor.grad 不是 None, 也会被共享。在一个没有 torch.Tensor.grad 域的 Tensor 被送到其他进程时，一个标准的进程专用的 .grad Tensor 会被创建，而它在所有的进程中不会自动被共享，与 Tensor 数据的共享方式不同。 这就允许实现各种训练方法，比如 Hogwild、A3C，或者其他那些需要异步操作的方法。 共享 CUDA 张量 进程间共享 CUDA 张量仅支持 Python 3，使用的是 spawn 或者 forkserver 启动方法。Python 2 中的 multiprocessing 仅使用 fork 来创建子进程，而 CUDA 运行时不支持该方法。 警告 CUDA API 需要分配给其他进程的显存在它们还在使用的情况下一直有效。你需要仔细确保共享的 CUDA 张量若非必须，不会超出使用范围。这对于共享模型参数不会是一个问题，但传递其他类型的数据时需要谨慎。注意该限制并不适用于共享 CPU 内存。 也可以参考：使用 nn.DataParallel 替代 multiprocessing 最佳实践和提示 避免和处理死锁 当创建一个新进程时，很多情况会发生，最常见的就是后台线程间的死锁。如果任何一个线程有锁的状态或者引入了一个模块，然后调用了fork，子进程很有可能处于中断状态，并以另外的方式死锁或者失败。注意即使你没这么做，Python 内建的库也有可能这么做——无需舍近求远，multiprocessing即是如此。multiprocessing.Queue 实际上是一个非常复杂的类，可以创建多个线程用于串行、发送和接收对象，它们也会出现前面提到的问题。如果你发现自己遇到了这种情况，尝试使用 multiprocessing.queues.SimpleQueue，它不会使用额外的线程。 我们在尽最大努力为你化繁为简，确保不会发生死锁的情况，但有时也会出现失控的情况。如果你遇到任何暂时无法解决的问题，可以在论坛上求助，我们将会研究是否可以修复。 通过 Queue 传递重用缓存 记住每次将一个 Tensor 放进一个 multiprocessing.Queue 时，它就会被移动到共享内存中。如果它已经被共享，那将不会有操作，否则将会触发一次额外的内存拷贝，而这将会拖慢整个进程。即使你有一个进程池把数据发送到一个进程，并把缓存送回来——这近乎于无操作，在发送下一个批次的数据时避免拷贝。 异步多进程训练（如Hogwild） 使用 torch.multiprocessing，可以异步训练一个模型，参数要么一直共享，要么周期性同步。在第一个情况下，我们建议传递整个模型的对象，而对于后一种情况，我们将以仅传递 state_dict()。 我们建议使用 multiprocessing.Queue在进程间传递 PyTorch 对象。当使用fork命令时，可以进行诸如继承共享内存中的张量和存储的操作，然而这个操作容易产生问题，应该小心使用，仅建议高级用户使用。Queue，尽管有时不是一个那么优雅的解决方案，但在所有的情况下都可以合理使用。 警告 你应该注意那些不在if __name__ == '__main__'中的全局声明。如果使用了一个不是fork的系统调用，它们将会在所有子进程中执行。 Hogwild 在示例仓库中可以找到一个具体的Hogwild实现，但除了完整的代码结构之外，下面也有一个简化的例子： import torch.multiprocessing as mp from model import MyModel def train(model): # Construct data_loader, optimizer, etc. for data, labels in data_loader: optimizer.zero_grad() loss_fn(model(data), labels).backward() optimizer.step() # This will update the shared parameters if __name__ == '__main__': num_processes = 4 model = MyModel() # NOTE: this is required for the ``fork`` method to work model.share_memory() processes = [] for rank in range(num_processes): p = mp.Process(target=train, args=(model,)) p.start() processes.append(p) for p in processes: p.join() 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes_randomness.html":{"url":"notes_randomness.html","title":"Reproducibility","keywords":"","body":"再生性 译者：ApacheCN PyTorch版本，单个提交或不同平台无法保证完全可重现的结果。此外，即使使用相同的种子，也不需要在CPU和GPU执行之间重现结果。 但是，为了在一个特定平台和PyTorch版本上对您的特定问题进行计算确定，需要采取几个步骤。 PyTorch中涉及两个伪随机数生成器，您需要手动播种以使运行可重现。此外，您应该确保您的代码依赖于使用随机数的所有其他库也使用固定种子。 PyTorch 您可以使用为所有设备（CPU和CUDA）播种RNG： import torch torch.manual_seed(0) 有一些PyTorch函数使用CUDA函数，这些函数可能是非确定性的来源。一类这样的CUDA函数是原子操作，特别是atomicAdd，其中对于相同值的并行加法的顺序是未确定的，并且对于浮点变量，是结果中的变化源。在前向中使用atomicAdd的PyTorch函数包括，。 许多操作具有向后使用atomicAdd，特别是许多形式的池，填充和采样。目前没有简单的方法来避免这些功能中的非确定性。 CuDNN 在CuDNN后端运行时，必须设置另外两个选项： torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False 警告 确定性模式可能会对性能产生影响，具体取决于您的型号。 NumPy的 如果您或您使用的任何库依赖于Numpy，您也应该为Numpy RNG播种。这可以通过以下方式完成： import numpy as np np.random.seed(0) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes_serialization.html":{"url":"notes_serialization.html","title":"Serialization semantics","keywords":"","body":"序列化的相关语义 译者：yuange250 最佳方案 保存模型的推荐方法 Pytorch主要有两种方法可用于序列化和保存一个模型。 第一种只存取模型的参数（更为推荐）： 保存参数： torch.save(the_model.state_dict(), PATH) 读取参数： the_model = TheModelClass(*args, **kwargs) the_model.load_state_dict(torch.load(PATH)) 第二种方法则将整个模型都保存下来： torch.save(the_model, PATH) 读取的时候也是读取整个模型： the_model = torch.load(PATH) 在第二种方法中, 由于特定的序列化的数据与其特定的类别(class)相绑定，并且在序列化的时候使用了固定的目录结构，所以在很多情况下，如在其他的一些项目中使用，或者代码进行了较大的重构的时候，很容易出现问题。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"notes_windows.html":{"url":"notes_windows.html","title":"Windows FAQ","keywords":"","body":"Windows FAQ 译者：冯宝宝 从源码中构建 包含可选组件 Windows PyTorch有两个受支持的组件：MKL和MAGMA。 以下是使用它们构建的步骤。 REM Make sure you have 7z and curl installed. REM Download MKL files curl https://s3.amazonaws.com/ossci-windows/mkl_2018.2.185.7z -k -O 7z x -aoa mkl_2018.2.185.7z -omkl REM Download MAGMA files REM cuda90/cuda92/cuda100 is also available in the following line. set CUDA_PREFIX=cuda80 curl -k https://s3.amazonaws.com/ossci-windows/magma_2.4.0_%CUDA_PREFIX%_release.7z -o magma.7z 7z x -aoa magma.7z -omagma REM Setting essential environment variables set \"CMAKE_INCLUDE_PATH=%cd%\\\\mkl\\\\include\" set \"LIB=%cd%\\\\mkl\\\\lib;%LIB%\" set \"MAGMA_HOME=%cd%\\\\magma\" 为Windows构建加速CUDA Visual Studio当前不支持并行自定义任务。 作为替代方案，我们可以使用Ninja来并行化CUDA构建任务。 只需键入几行代码即可使用它。 REM Let's install ninja first. pip install ninja REM Set it as the cmake generator set CMAKE_GENERATOR=Ninja 脚本一键安装 你可以参考这些脚本。它会给你指导方向。 扩展 CFEI扩展 对CFFI扩展的支持是非常试验性的。在Windows下启用它通常有两个步骤。 首先，在Extension对象中指定其他库以使其在Windows上构建。 ffi = create_extension( '_ext.my_lib', headers=headers, sources=sources, define_macros=defines, relative_to=__file__, with_cuda=with_cuda, extra_compile_args=[\"-std=c99\"], libraries=['ATen', '_C'] # Append cuda libaries when necessary, like cudart ) 其次，这是“由extern THCState *state状态引起的未解决的外部符号状态”的工作场所; 将源代码从C更改为C ++。 下面列出了一个例子。 #include #include THCState *state = at::globalContext().thc_state; extern \"C\" int my_lib_add_forward_cuda(THCudaTensor *input1, THCudaTensor *input2, THCudaTensor *output) { if (!THCudaTensor_isSameSizeAs(state, input1, input2)) return 0; THCudaTensor_resizeAs(state, output, input1); THCudaTensor_cadd(state, output, input1, 1.0, input2); return 1; } extern \"C\" int my_lib_add_backward_cuda(THCudaTensor *grad_output, THCudaTensor *grad_input) { THCudaTensor_resizeAs(state, grad_input, grad_output); THCudaTensor_fill(state, grad_input, 1); return 1; } C++扩展 与前一种类型相比，这种类型的扩展具有更好的支持。不过它仍然需要一些手动配置。首先，打开VS 2017的x86_x64交叉工具命令提示符。然后，在其中打开Git-Bash。它通常位于C：\\Program Files\\Git\\git-bash.exe中。最后，您可以开始编译过程。 安装 在Win32 找不到安装包 Solving environment: failed PackagesNotFoundError: The following packages are not available from current channels: - pytorch Current channels: - https://conda.anaconda.org/pytorch/win-32 - https://conda.anaconda.org/pytorch/noarch - https://repo.continuum.io/pkgs/main/win-32 - https://repo.continuum.io/pkgs/main/noarch - https://repo.continuum.io/pkgs/free/win-32 - https://repo.continuum.io/pkgs/free/noarch - https://repo.continuum.io/pkgs/r/win-32 - https://repo.continuum.io/pkgs/r/noarch - https://repo.continuum.io/pkgs/pro/win-32 - https://repo.continuum.io/pkgs/pro/noarch - https://repo.continuum.io/pkgs/msys2/win-32 - https://repo.continuum.io/pkgs/msys2/noarch Pytorch不能在32位系统中工作运行。请安装使用64位的Windows和Python。 导入错误 from torch._C import * ImportError: DLL load failed: The specified module could not be found. 问题是由基本文件丢失导致的。实际上，除了VC2017可再发行组件和一些mkl库之外，我们几乎包含了PyTorch对conda包所需的所有基本文件。您可以通过键入以下命令来解决此问题。 conda install -c peterjc123 vc vs2017_runtime conda install mkl_fft intel_openmp numpy mkl 至于wheel包(轮子)，由于我们没有包含一些库和VS2017可再发行文件，请手动安装它们。可以下载VS 2017可再发行安装程序)。你还应该注意你的Numpy的安装。 确保它使用MKL而不是OpenBLAS版本的。您可以输入以下命令。 pip install numpy mkl intel-openmp mkl_fft 另外一种可能是你安装了GPU版本的Pytorch但是电脑中并没有NVIDIA的显卡。碰到这种情况，就把GPU版本的Pytorch换成CPU版本的就好了。 from torch._C import * ImportError: DLL load failed: The operating system cannot run %1. 这实际上是Anaconda的上游问题。使用conda-forge通道初始化环境时,将出现此问题。您可以通过此命令修复intel-openmp库。 使用（多处理） 无if语句保护的多进程处理错误 RuntimeError: An attempt has been made to start a new process before the current process has finished its bootstrapping phase. This probably means that you are not using fork to start your child processes and you have forgotten to use the proper idiom in the main module: if __name__ == '__main__': freeze_support() ... The \"freeze_support()\" line can be omitted if the program is not going to be frozen to produce an executable. 在Windows上实现多进程处理是不同的，它使用的是spawn而不是fork。 因此，我们必须使用if子句包装代码，以防止代码执行多次。将您的代码重构为以下结构。 import torch def main() for i, data in enumerate(dataloader): # do something here if __name__ == '__main__': main() 多进程处理错误“坏道” ForkingPickler(file, protocol).dump(obj) BrokenPipeError: [Errno 32] Broken pipe 当在父进程完成发送数据之前子进程结束时，会发生此问题。您的代码可能有问题。您可以通过将DataLoader的num_worker减少为零来调试代码，并查看问题是否仍然存在。 多进程处理错误“驱动程序关闭” Couldn’t open shared file mapping: , error code: at torch\\lib\\TH\\THAllocator.c:154 [windows] driver shut down 请更新您的显卡驱动程序。如果这种情况持续存在，则可能是您的显卡太旧或所需要的计算能力对您的显卡负担太重。请根据这篇文章.)更新TDR设置。 CUDA IPC操作 THCudaCheck FAIL file=torch\\csrc\\generic\\StorageSharing.cpp line=252 error=63 : OS call failed or operation not supported on this OS Windows不支持它们。在CUDA张量上进行多处理这样的事情无法成功，有两种选择: 1.不要使用多处理。将Data Loader的num_worker设置为零。 2.采用共享CPU张量方法。确保您的自定义DataSet返回CPU张量。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"docs_package_ref.html":{"url":"docs_package_ref.html","title":"包参考","keywords":"","body":"包参考 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"torch.html":{"url":"torch.html","title":"torch","keywords":"","body":"torch 译者：cluster torch package 包含 多维张量和之前定义过数学运算的数据结构。此外，它提供了支持许多高效序列化的张量(或者任意类型) 的程序集，以及一些有用工具。 它支持CUDA环境，使你可以在NVIDIA GPU进行张量运算(要求compute capability 版本3.0及以上) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"torch_tensors.html":{"url":"torch_tensors.html","title":"Tensors","keywords":"","body":"Tensors 译者：dyywinner cluster torch.is_tensor(obj) 如果obj是一个PyTorch张量，则返回True. Parameters: obj (Object) – Object to test torch.is_storage(obj) 如果obj是一个PyTorch 存储对象，则返回True. Parameters: obj (Object) – Object to test torch.set_default_dtype(d) 将d设置为默认浮点类型(dtype). 该类型将在 torch.tensor() 中作为类型推断的默认浮点类型。初始默认浮点类型为torch.float32。 Parameters: d (torch.dtype) – 默认浮点类型 Example: >>> torch.tensor([1.2, 3]).dtype # 初始默认浮点类型为 torch.float32 floating point is torch.float32 torch.float32 >>> torch.set_default_dtype(torch.float64) >>> torch.tensor([1.2, 3]).dtype # 一个新的浮点类型的张量 torch.float64 torch.get_default_dtype() → torch.dtype 获取当前默认浮点类型 torch.dtype. Example: >>> torch.get_default_dtype() # 初始的默认浮点类型是 torch.float32 torch.float32 >>> torch.set_default_dtype(torch.float64) >>> torch.get_default_dtype() # 默认浮点类型为 torch.float64 torch.float64 >>> torch.set_default_tensor_type(torch.FloatTensor) # 设置默认张量类型也会影响默认浮点类型 >>> torch.get_default_dtype() # 变化到 torch.float32( 此类型(dtype)来自于torch.FloatTensor ) torch.float32 torch.set_default_tensor_type(t) 设置默认的 torch.Tensor 类型到浮点张量类型 t. 该类型将在 torch.tensor() 中作为类型推断的默认浮点类型。 初始默认浮点张量类型为 torch.FloatTensor. Parameters: t (type or string) – 浮点张量的类型或者它的名称 Example: >>> torch.tensor([1.2, 3]).dtype # 初始默认浮点类型为 torch.float32 torch.float32 >>> torch.set_default_tensor_type(torch.DoubleTensor) >>> torch.tensor([1.2, 3]).dtype # 一个新的浮点张量 torch.float64 torch.numel(input) → int 返回 input 张量中元素总数. Parameters: input (Tensor) – the input tensor Example: >>> a = torch.randn(1, 2, 3, 4, 5) >>> torch.numel(a) 120 >>> a = torch.zeros(4,4) >>> torch.numel(a) 16 torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None) 设置打印选项. 从 NumPy 剽窃过来的（滑稽） Parameters: precision – 浮点输出的有效位数 (默认为 4). threshold – 输出时的阈值，当数组元素总和超过阈值，会被截断输出 (默认为 1000). edgeitems – 每个维度所统计的数组条目数量(默认：3). linewidth – 每一行输出的字符长度 (默认为80). 阈值矩阵将忽略该参数. profile – 打印输出的美观程度 默认值为Sane. 可以用后面括号中的选项覆盖( default, short, full). torch.set_flush_denormal(mode) → bool CPU不支持非规格化浮点数 . 如果你的系统支持非规格化数字模式(flush denormal mode)并且可成功配置该模式则返回 True . set_flush_denormal() 可以使用在支持SSE3的x86架构. Parameters: mode (bool) – 是否开启 flush denormal mode Example: >>> torch.set_flush_denormal(True) True >>> torch.tensor([1e-323], dtype=torch.float64) tensor([ 0.], dtype=torch.float64) >>> torch.set_flush_denormal(False) True >>> torch.tensor([1e-323], dtype=torch.float64) tensor(9.88131e-324 * [ 1.0000], dtype=torch.float64) Creation Ops Note 随机采样创造随机数的方式在 Random sampling 列举。其中包括 torch.rand() torch.rand_like() torch.randn() torch.randn_like() torch.randint() torch.randint_like() torch.randperm() . 你可以使用 torch.empty() ，并使用 In-place random sampling 方法去从更宽泛的范围采样,生成 torch.Tensor . torch.tensor(data, dtype=None, device=None, requires_grad=False) → Tensor 用 data 构建张量. Warning torch.tensor() 会拷贝 data. 如果你有一个张量( data )，并且想要避免拷贝, 请使用 torch.Tensor.requires_grad_() 或者 torch.Tensor.detach(). 如果你有一个NumPy数组(ndarray) 并且想要避免拷贝, 请使用 torch.from_numpy(). Warning 当 data 为一个名为 x 的张量， torch.tensor() 读取 ‘the data’ (无论传输了什么), 都会构建一个 leaf variable(计算图模型中事先创建的、而非运算得到的变量). 因此 torch.tensor(x) 等价于 x.clone().detach() ，同时 torch.tensor(x, requires_grad=True) 等价于 x.clone().detach().requires_grad_(True). 我们推荐这种使用 clone() and detach() 的写法. Parameters: data (array_like) – 初始化张量的数据. 允许的类型有 list, tuple, NumPy ndarray, scalar(标量), 以及其他类型. dtype (torch.dtype, optional) – 返回的张量所要求的数据类型. 默认: 如果此参数为 None,从 data中推断数据类型. device (torch.device, optional) – 返回的张量所要求的硬件. 默认: 如果此参数为 None,对当前张量类型使用当前硬件(参考 torch.set_default_tensor_type()). device 可以为 提供CPU张量类型的CPU和 支持CUDA张量类型的CUDA设备. requires_grad (bool, optional) – 对返回的张量自动求导时是否需要记录操作. 默认: False. Example: >>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]]) tensor([[ 0.1000, 1.2000], [ 2.2000, 3.1000], [ 4.9000, 5.2000]]) >>> torch.tensor([0, 1]) # 输入数据推断 tensor([ 0, 1]) >>> torch.tensor([[0.11111, 0.222222, 0.3333333]], dtype=torch.float64, device=torch.device('cuda:0')) # 创建一个 torch.cuda.DoubleTensor tensor([[ 0.1111, 0.2222, 0.3333]], dtype=torch.float64, device='cuda:0') >>> torch.tensor(3.14159) # 创建一个标量 (零维张量) tensor(3.1416) >>> torch.tensor([]) # 创建一个空张量 (形状是 (0,)) tensor([]) torch.sparse_coo_tensor(indices, values, size=None, dtype=None, device=None, requires_grad=False) → Tensor 用非0元素值values和下标indices在COO(顺序标注)构建一个稀疏矩阵。一个稀疏向量可以是未合并的(uncoalesced), 在这种情况下,在索引中会存在有重复坐标 ,这个索引的值是所有重复值数量的和: torch.sparse. Parameters: indices (array_like) – 给张量初始化数据。可以是列表,元组,Numpy矩阵(ndarry类型),标量和其他类型。之后将在内部被映射成torch.LongTensor。 因此矩阵中的非零元素下标的坐标,需要是二维的，并且第一维是张量的维度，第二维是非零元素数量。 values (array_like) – 初始化张量的值。可以是列表,元组,Numpy矩阵(ndarry类型),标量和其他类型。 size (list, tuple, or torch.Size, optional) – 稀疏矩阵的形状。如果不提供size形状，将会被自动优化为可以装下所有非零元素的最小大小。 dtype (torch.dtype, optional) – 张量返回值的期望数据类型。如果没有，则默认为values。 device (torch.device, optional) – 返回的张量所要求的硬件. 默认: 如果此参数为 None,对当前张量类型使用当前硬件(参考 torch.set_default_tensor_type()). device 可以为 提供CPU张量类型的CPU和 支持CUDA张量类型的CUDA设备.torch.set_default_tensor_type \"torch.set_default_tensor_type\")). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) – 对返回的张量自动求导时是否需要记录操作. 默认: False. Example: >>> i = torch.tensor([[0, 1, 1], [2, 0, 2]]) >>> v = torch.tensor([3, 4, 5], dtype=torch.float32) >>> torch.sparse_coo_tensor(i, v, [2, 4]) tensor(indices=tensor([[0, 1, 1], [2, 0, 2]]), values=tensor([3., 4., 5.]), size=(2, 4), nnz=3, layout=torch.sparse_coo) >>> torch.sparse_coo_tensor(i, v) # Shape inference tensor(indices=tensor([[0, 1, 1], [2, 0, 2]]), values=tensor([3., 4., 5.]), size=(2, 3), nnz=3, layout=torch.sparse_coo) >>> torch.sparse_coo_tensor(i, v, [2, 4], dtype=torch.float64, device=torch.device('cuda:0')) tensor(indices=tensor([[0, 1, 1], [2, 0, 2]]), values=tensor([3., 4., 5.]), device='cuda:0', size=(2, 4), nnz=3, dtype=torch.float64, layout=torch.sparse_coo) # 使用下列常量创建一个空稀疏张量: # 1\\. sparse_dim + dense_dim = len(SparseTensor.shape) # 2\\. SparseTensor._indices().shape = (sparse_dim, nnz) # 3\\. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:]) # # 比如，使用nnz = 0, dense_dim = 0和 # sparse_dim = 1 (这里的下标是一个二维张量形状shape = (1, 0)) 来创建一个空稀疏矩阵 >>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1]) tensor(indices=tensor([], size=(1, 0)), values=tensor([], size=(0,)), size=(1,), nnz=0, layout=torch.sparse_coo) # 然后使用nnz = 0, dense_dim = 1 和 sparse_dim = 1 # 来创建一个空稀疏矩阵 >>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2]) tensor(indices=tensor([], size=(1, 0)), values=tensor([], size=(0, 2)), size=(1, 2), nnz=0, layout=torch.sparse_coo) torch.as_tensor(data, dtype=None, device=None) → Tensor 转换 data 到 torch.Tensor 类型. 如果 data 已经是相同 dtype 和 device 的张量， 那就不会执行拷贝，否则将会返回一个新的张量(如果 data Tensor 中 requires_grad=True,则返回的张量计算图会被保留)相似地，如果 data dtype 为 ndarry 并且 device 为CPU，则拷贝不会发生。 Parameters: data (array_like) – 提供张量初始化的数据结构。可能是ist, tuple, NumPy ndarray, scalar 或其他类型。 dtype (torch.dtype, optional) – 提供张量初始化的值。可能是ist, tuple, NumPy ndarray, scalar 或其他类型。 device (torch.device, optional) – 返回张量所需要的设备。默认：若为空，则当前的设备提供给默认张量类型(see torch.set_default_tensor_type()). device 将为支持CPU张量的CPU和支持CUDA张量类型的CUDA设备。 Example: >>> a = numpy.array([1, 2, 3]) >>> t = torch.as_tensor(a) >>> t tensor([ 1, 2, 3]) >>> t[0] = -1 >>> a array([-1, 2, 3]) >>> a = numpy.array([1, 2, 3]) >>> t = torch.as_tensor(a, device=torch.device('cuda')) >>> t tensor([ 1, 2, 3]) >>> t[0] = -1 >>> a array([1, 2, 3]) torch.from_numpy(ndarray) → Tensor 从一个 numpy.ndarray 创建一个 Tensor. 返回的张量和 ndarry 共享相同的内存。 对张量的修饰将会反映在 ndarry，反之亦然。返回的张量 大小不可变。 Example: >>> a = numpy.array([1, 2, 3]) >>> t = torch.from_numpy(a) >>> t tensor([ 1, 2, 3]) >>> t[0] = -1 >>> a array([-1, 2, 3]) torch.zeros(*sizes, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor 返回一个用标量 0 填充的张量，其中可变长参数 sizes 定义了该张量形状(shape). Parameters: sizes (int...) – 定义输出张量形状的整数序列. 可以是可变长的参数 或者是像 列表元组这样的集合。 out (Tensor, optional) – 输出张量 dtype (torch.dtype, optional) – 返回张量的数据类型. 默认: 如果为 None, 使用全局默认值 (参考 torch.set_default_tensor_type()). layout (torch.layout, optional) – 返回张量的层数. Default: torch.strided. TODO device (torch.device, optional) – 返回张量所需的设备. 默认: 如果为 None, 则当前的设备提供给默认张量类型(see torch.set_default_tensor_type()). device 将为支持CPU张量的CPU和支持CUDA张量类型的CUDA设备。 requires_grad (bool, optional) – 自动梯度计算是否需要记录在返回张量上的操作。默认: False. Example: >>> torch.zeros(2, 3) tensor([[ 0., 0., 0.], [ 0., 0., 0.]]) >>> torch.zeros(5) tensor([ 0., 0., 0., 0., 0.]) torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor 返回用标量0填充的张量，大小和input的size一样. torch.zeros_like(input) 等价于 torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device). Warning 截止到 0.4, 该函数不再支持out关键字. 同时，老版的 torch.zeros_like(input, out=output) 等价于 torch.zeros(input.size(), out=output). Parameters: input (Tensor) – input的size属性决定输出张量大小 dtype (torch.dtype, optional) – 返回张量的数据类型. 默认: 如果为 None, 使用input的dtype属性 . layout (torch.layout, optional) – 返回张量的层数. Default: 默认为input的layout属性. device (torch.device, optional) – 返回张量所需的设备. 默认: 如果为 None, 则为input的device属性. requires_grad (bool, optional) – 自动梯度计算是否需要记录在返回张量上的操作。默认: False. Example: >>> input = torch.empty(2, 3) >>> torch.zeros_like(input) tensor([[ 0., 0., 0.], [ 0., 0., 0.]]) torch.ones(*sizes, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor 返回一个用标量 1 填充的张量，其中可变长参数 sizes 定义了该张量形状(shape). Parameters: sizes (int...) – 定义输出张量形状的整数序列. 可以是可变长的参数 或者是像 列表元组这样的集合。 out (Tensor, optional) – 输出张量 dtype (torch.dtype, optional) – 返回张量的数据类型. 默认: 如果为 None, 使用全局默认值 (参考 torch.set_default_tensor_type()). layout (torch.layout, optional) – 返回张量的层数. Default: torch.strided. device (torch.device, optional) – 返回张量所需的设备. 默认: 如果为 None, 则当前的设备提供给默认张量类型(see torch.set_default_tensor_type()). device 将为支持CPU张量的CPU和支持CUDA张量类型的CUDA设备。 requires_grad (bool, optional) – 自动梯度计算是否需要记录在返回张量上的操作。默认: False. Example: >>> torch.ones(2, 3) tensor([[ 1., 1., 1.], [ 1., 1., 1.]]) >>> torch.ones(5) tensor([ 1., 1., 1., 1., 1.]) torch.ones_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor 返回用标量1填充的张量，大小和input的size一样. torch.ones_like(input) 等价于 torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device) Warning 截止到 0.4, 该函数不再支持out关键字. 同时，老版的 torch.ones_like(input, out=output) 等价于 torch.ones(input.size(), out=output). Parameters: input (Tensor) – input的size属性决定输出张量大小 dtype (torch.dtype, optional) – 返回张量的数据类型. 默认: 如果为 None, 使用input的dtype属性 . layout (torch.layout, optional) – 返回张量的层数. Default: 默认为input的layout属性. device (torch.device, optional) – 返回张量所需的设备. 默认: 如果为 None, 则为input的device属性. requires_grad (bool, optional) – 自动梯度计算是否需要记录在返回张量上的操作。默认: False. Example: >>> input = torch.empty(2, 3) >>> torch.ones_like(input) tensor([[ 1., 1., 1.], [ 1., 1., 1.]]) torch.arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor 返回一个一维张量，大小为，值为区间 [start,end)内，以step为步距,从start开始的数列. 注意: 非整型数 step 和 end 比较时存在浮点四舍五入误差;为避免不一致，建议在end后面加上一个小的epsilon. Parameters: start (Number) – 点集的起始值. 默认为0. end (Number) – 点集的终值. step (Number) – 每对相邻点之间的距离 . 默认为 1. out (Tensor, optional) – 输出的张量 dtype (torch.dtype, optional) – 返回张量的数据类型. 默认: 如果为 None, 使用全局默认值. (参考 torch.set_default_tensor_type()). 若 dtype 未提供, 则从其他输入参数推断数据类型. 如果 start, end, stop 中存在浮点数, 则 dtype 会使用默认数据类型, 请查看 get_default_dtype(). 否则, dtype 会使用 torch.int64. layout (torch.layout, optional) – 返回张量的层数. Default: torch.strided. device (torch.device, optional) – 返回张量所需的设备. 默认: 如果为 None, 则当前的设备提供给默认张量类型(see torch.set_default_tensor_type()). device 将为支持CPU张量的CPU和支持CUDA张量类型的CUDA设备。 requires_grad (bool, optional) – 自动梯度计算是否需要记录在返回张量上的操作。默认: False. Example: >>> torch.arange(5) tensor([ 0, 1, 2, 3, 4]) >>> torch.arange(1, 4) tensor([ 1, 2, 3]) >>> torch.arange(1, 2.5, 0.5) tensor([ 1.0000, 1.5000, 2.0000]) torch.range(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor 返回一个一维张量，大小为，值从start到end，以step为步距的数列. Warning 这个函数被弃用，改为 torch.arange(). Parameters: start (Number) – 点集的起始值. 默认为0. end (Number) – 点集的终值. step (Number) – 每对相邻点之间的距离 . 默认为 1. out (Tensor, optional) – 输出的张量 dtype (torch.dtype, optional) – 返回张量的数据类型. 默认: 如果为 None, 使用全局默认值. (参考 torch.set_default_tensor_type()). 若 dtype 未提供, 则从其他输入参数推断数据类型. 如果 start, end, stop 中存在浮点数, 则 dtype 会使用默认数据类型, 请查看 get_default_dtype(). 否则, dtype 会使用 torch.int64. layout (torch.layout, optional) – 返回张量的层数. Default: torch.strided. device (torch.device, optional) – 返回张量所需的设备. 默认: 如果为 None, 则当前的设备提供给默认张量类型(see torch.set_default_tensor_type()). device 将为支持CPU张量的CPU和支持CUDA张量类型的CUDA设备。 requires_grad (bool, optional) – 自动梯度计算是否需要记录在返回张量上的操作。默认: False. Example: >>> torch.range(1, 4) tensor([ 1., 2., 3., 4.]) >>> torch.range(1, 4, 0.5) tensor([ 1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000, 4.0000]) torch.linspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor 返回共steps数量在start 和 end之间的等距点，从而组成的一维张量. 输出张量大小为steps，维度为一维. Parameters: start (float) – 点集的起始值. end (float) –点集的终值. steps (int) – start 和 end之间的样本点数目. 默认: 100. out (Tensor, optional) – 输出张量 dtype (torch.dtype, optional) – 返回张量的数据类型. 默认: 如果为 None, 使用全局默认值. (参考 torch.set_default_tensor_type()). layout (torch.layout, optional) – 返回张量的层数. Default: torch.strided. device (torch.device, optional) – 返回张量所需的设备. 默认: 如果为 None, 则当前的设备提供给默认张量类型(see torch.set_default_tensor_type()). device 将为支持CPU张量的CPU和支持CUDA张量类型的CUDA设备。 requires_grad (bool, optional) – 自动梯度计算是否需要记录在返回张量上的操作。默认: False. Example: >>> torch.linspace(3, 10, steps=5) tensor([ 3.0000, 4.7500, 6.5000, 8.2500, 10.0000]) >>> torch.linspace(-10, 10, steps=5) tensor([-10., -5., 0., 5., 10.]) >>> torch.linspace(start=-10, end=10, steps=5) tensor([-10., -5., 0., 5., 10.]) torch.logspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor 返回共有steps数量的一维张量，点集由 和 之间对数分布的点组成. 输出张量大小为steps，维度为一维. Parameters: start (float) – 点集的起始值. end (float) –点集的终值. steps (int) – start 和 end之间的样本点数目. 默认: 100. out (Tensor, optional) – 输出张量 dtype (torch.dtype, optional) – 返回张量的数据类型. 默认: 如果为 None, 使用全局默认值. (参考 torch.set_default_tensor_type()). layout (torch.layout, optional) – 返回张量的层数. Default: torch.strided. device (torch.device, optional) – 返回张量所需的设备. 默认: 如果为 None, 则当前的设备提供给默认张量类型(see torch.set_default_tensor_type()). device 将为支持CPU张量的CPU和支持CUDA张量类型的CUDA设备。 requires_grad (bool, optional) – 自动梯度计算是否需要记录在返回张量上的操作。默认: False. Example: >>> torch.logspace(start=-10, end=10, steps=5) tensor([ 1.0000e-10, 1.0000e-05, 1.0000e+00, 1.0000e+05, 1.0000e+10]) >>> torch.logspace(start=0.1, end=1.0, steps=5) tensor([ 1.2589, 2.1135, 3.5481, 5.9566, 10.0000]) torch.eye(n, m=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor 返回二维张量，对角线上是1，其它地方是0. Parameters: n (int) – the number of rows m (int, optional) – the number of columns with default being n out (Tensor, optional) – the output tensor dtype (torch.dtype, optional) – the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) – the desired layout of returned Tensor. Default: torch.strided. device (torch.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False. Returns: A 2-D tensor with ones on the diagonal and zeros elsewhere Return type: Tensor --- --- Example: >>> torch.eye(3) tensor([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]) torch.empty(*sizes, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor Returns a tensor filled with uninitialized data. The shape of the tensor is defined by the variable argument sizes. Parameters: sizes (int...) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple. out (Tensor, optional) – the output tensor dtype (torch.dtype, optional) – the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) – the desired layout of returned Tensor. Default: torch.strided. device (torch.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False. Example: >>> torch.empty(2, 3) tensor(1.00000e-08 * [[ 6.3984, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000]]) torch.empty_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor Returns an uninitialized tensor with the same size as input. torch.empty_like(input) is equivalent to torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device). Parameters: input (Tensor) – the size of input will determine size of the output tensor dtype (torch.dtype, optional) – the desired data type of returned Tensor. Default: if None, defaults to the dtype of input. layout (torch.layout, optional) – the desired layout of returned tensor. Default: if None, defaults to the layout of input. device (torch.device, optional) – the desired device of returned tensor. Default: if None, defaults to the device of input. requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False. Example: >>> torch.empty((2,3), dtype=torch.int64) tensor([[ 9.4064e+13, 2.8000e+01, 9.3493e+13], [ 7.5751e+18, 7.1428e+18, 7.5955e+18]]) torch.full(size, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor Returns a tensor of size size filled with fill_value. Parameters: size (int...) – a list, tuple, or torch.Size of integers defining the shape of the output tensor. fill_value – the number to fill the output tensor with. out (Tensor, optional) – the output tensor dtype (torch.dtype, optional) – the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) – the desired layout of returned Tensor. Default: torch.strided. device (torch.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False. Example: >>> torch.full((2, 3), 3.141592) tensor([[ 3.1416, 3.1416, 3.1416], [ 3.1416, 3.1416, 3.1416]]) torch.full_like(input, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor Returns a tensor with the same size as input filled with fill_value. torch.full_like(input, fill_value) is equivalent to torch.full_like(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). Parameters: input (Tensor) – the size of input will determine size of the output tensor fill_value – the number to fill the output tensor with. dtype (torch.dtype, optional) – the desired data type of returned Tensor. Default: if None, defaults to the dtype of input. layout (torch.layout, optional) – the desired layout of returned tensor. Default: if None, defaults to the layout of input. device (torch.device, optional) – the desired device of returned tensor. Default: if None, defaults to the device of input. requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False. Indexing, Slicing, Joining, Mutating Ops torch.cat(tensors, dim=0, out=None) → Tensor Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty. torch.cat() can be seen as an inverse operation for torch.split() and torch.chunk(). torch.cat() can be best understood via examples. Parameters: tensors (sequence of Tensors) – any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension. dim (int, optional) – the dimension over which the tensors are concatenated out (Tensor, optional) – the output tensor Example: >>> x = torch.randn(2, 3) >>> x tensor([[ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497]]) >>> torch.cat((x, x, x), 0) tensor([[ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497], [ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497], [ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497]]) >>> torch.cat((x, x, x), 1) tensor([[ 0.6580, -1.0969, -0.4614, 0.6580, -1.0969, -0.4614, 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497, -0.1034, -0.5790, 0.1497, -0.1034, -0.5790, 0.1497]]) torch.chunk(tensor, chunks, dim=0) → List of Tensors Splits a tensor into a specific number of chunks. Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by chunks. Parameters: tensor (Tensor) – the tensor to split chunks (int) – number of chunks to return dim (int) – dimension along which to split the tensor torch.gather(input, dim, index, out=None) → Tensor Gathers values along an axis specified by dim. For a 3-D tensor the output is specified by: out[i][j][k] = input[index[i][j][k]][j][k] # if dim == 0 out[i][j][k] = input[i][index[i][j][k]][k] # if dim == 1 out[i][j][k] = input[i][j][index[i][j][k]] # if dim == 2 If input is an n-dimensional tensor with size and dim = i, then index must be an -dimensional tensor with size where and out will have the same size as index. Parameters: input (Tensor) – the source tensor dim (int) – the axis along which to index index (LongTensor) – the indices of elements to gather out (Tensor, optional) – the destination tensor Example: >>> t = torch.tensor([[1,2],[3,4]]) >>> torch.gather(t, 1, torch.tensor([[0,0],[1,0]])) tensor([[ 1, 1], [ 4, 3]]) torch.index_select(input, dim, index, out=None) → Tensor Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor. The returned tensor has the same number of dimensions as the original tensor (input). The dimth dimension has the same size as the length of index; other dimensions have the same size as in the original tensor. Note The returned tensor does not use the same storage as the original tensor. If out has a different shape than expected, we silently change it to the correct shape, reallocating the underlying storage if necessary. Parameters: input (Tensor) – the input tensor dim (int) – the dimension in which we index index (LongTensor) – the 1-D tensor containing the indices to index out (Tensor, optional) – the output tensor Example: >>> x = torch.randn(3, 4) >>> x tensor([[ 0.1427, 0.0231, -0.5414, -1.0009], [-0.4664, 0.2647, -0.1228, -1.1068], [-1.1734, -0.6571, 0.7230, -0.6004]]) >>> indices = torch.tensor([0, 2]) >>> torch.index_select(x, 0, indices) tensor([[ 0.1427, 0.0231, -0.5414, -1.0009], [-1.1734, -0.6571, 0.7230, -0.6004]]) >>> torch.index_select(x, 1, indices) tensor([[ 0.1427, -0.5414], [-0.4664, -0.1228], [-1.1734, 0.7230]]) torch.masked_select(input, mask, out=None) → Tensor Returns a new 1-D tensor which indexes the input tensor according to the binary mask mask which is a ByteTensor. The shapes of the mask tensor and the input tensor don’t need to match, but they must be broadcastable. Note The returned tensor does not use the same storage as the original tensor Parameters: input (Tensor) – the input data mask (ByteTensor) – the tensor containing the binary mask to index with out (Tensor, optional) – the output tensor Example: >>> x = torch.randn(3, 4) >>> x tensor([[ 0.3552, -2.3825, -0.8297, 0.3477], [-1.2035, 1.2252, 0.5002, 0.6248], [ 0.1307, -2.0608, 0.1244, 2.0139]]) >>> mask = x.ge(0.5) >>> mask tensor([[ 0, 0, 0, 0], [ 0, 1, 1, 1], [ 0, 0, 0, 1]], dtype=torch.uint8) >>> torch.masked_select(x, mask) tensor([ 1.2252, 0.5002, 0.6248, 2.0139]) torch.narrow(input, dimension, start, length) → Tensor Returns a new tensor that is a narrowed version of input tensor. The dimension dim is input from start to start + length. The returned tensor and input tensor share the same underlying storage. Parameters: input (Tensor) – the tensor to narrow dimension (int) – the dimension along which to narrow start (int) – the starting dimension length (int) – the distance to the ending dimension Example: >>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) >>> torch.narrow(x, 0, 0, 2) tensor([[ 1, 2, 3], [ 4, 5, 6]]) >>> torch.narrow(x, 1, 1, 2) tensor([[ 2, 3], [ 5, 6], [ 8, 9]]) torch.nonzero(input, out=None) → LongTensor Returns a tensor containing the indices of all non-zero elements of input. Each row in the result contains the indices of a non-zero element in input. If input has n dimensions, then the resulting indices tensor out is of size , where is the total number of non-zero elements in the input tensor. Parameters: input (Tensor) – the input tensor out (LongTensor__, optional) – the output tensor containing indices Example: >>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1])) tensor([[ 0], [ 1], [ 2], [ 4]]) >>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0], [0.0, 0.4, 0.0, 0.0], [0.0, 0.0, 1.2, 0.0], [0.0, 0.0, 0.0,-0.4]])) tensor([[ 0, 0], [ 1, 1], [ 2, 2], [ 3, 3]]) torch.reshape(input, shape) → Tensor Returns a tensor with the same data and number of elements as input, but with the specified shape. When possible, the returned tensor will be a view of input. Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior. See torch.Tensor.view() on when it is possible to return a view. A single dimension may be -1, in which case it’s inferred from the remaining dimensions and the number of elements in input. Parameters: input (Tensor) – the tensor to be reshaped shape (tuple of python:ints) – the new shape Example: >>> a = torch.arange(4.) >>> torch.reshape(a, (2, 2)) tensor([[ 0., 1.], [ 2., 3.]]) >>> b = torch.tensor([[0, 1], [2, 3]]) >>> torch.reshape(b, (-1,)) tensor([ 0, 1, 2, 3]) torch.split(tensor, split_size_or_sections, dim=0) Splits the tensor into chunks. If split_size_or_sections is an integer type, then tensor will be split into equally sized chunks (if possible). Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by split_size. If split_size_or_sections is a list, then tensor will be split into len(split_size_or_sections) chunks with sizes in dim according to split_size_or_sections. Parameters: tensor (Tensor) – tensor to split. split_size_or_sections (int) or (list(int)) – size of a single chunk or list of sizes for each chunk dim (int) – dimension along which to split the tensor. torch.squeeze(input, dim=None, out=None) → Tensor Returns a tensor with all the dimensions of input of size 1 removed. For example, if input is of shape: then the out tensor will be of shape: . When dim is given, a squeeze operation is done only in the given dimension. If input is of shape: , squeeze(input, 0) leaves the tensor unchanged, but squeeze(input, 1) will squeeze the tensor to the shape . Note The returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other. Parameters: input (Tensor) – the input tensor dim (int, optional) – if given, the input will be squeezed only in this dimension out (Tensor, optional) – the output tensor Example: >>> x = torch.zeros(2, 1, 2, 1, 2) >>> x.size() torch.Size([2, 1, 2, 1, 2]) >>> y = torch.squeeze(x) >>> y.size() torch.Size([2, 2, 2]) >>> y = torch.squeeze(x, 0) >>> y.size() torch.Size([2, 1, 2, 1, 2]) >>> y = torch.squeeze(x, 1) >>> y.size() torch.Size([2, 2, 1, 2]) torch.stack(seq, dim=0, out=None) → Tensor Concatenates sequence of tensors along a new dimension. All tensors need to be of the same size. Parameters: seq (sequence of Tensors) – sequence of tensors to concatenate dim (int) – dimension to insert. Has to be between 0 and the number of dimensions of concatenated tensors (inclusive) out (Tensor, optional) – the output tensor torch.t(input) → Tensor Expects input to be a matrix (2-D tensor) and transposes dimensions 0 and 1. Can be seen as a short-hand function for transpose(input, 0, 1). Parameters: input (Tensor) – the input tensor Example: >>> x = torch.randn(2, 3) >>> x tensor([[ 0.4875, 0.9158, -0.5872], [ 0.3938, -0.6929, 0.6932]]) >>> torch.t(x) tensor([[ 0.4875, 0.3938], [ 0.9158, -0.6929], [-0.5872, 0.6932]]) torch.take(input, indices) → Tensor Returns a new tensor with the elements of input at the given indices. The input tensor is treated as if it were viewed as a 1-D tensor. The result takes the same shape as the indices. Parameters: input (Tensor) – the input tensor indices (LongTensor) – the indices into tensor Example: >>> src = torch.tensor([[4, 3, 5], [6, 7, 8]]) >>> torch.take(src, torch.tensor([0, 2, 5])) tensor([ 4, 5, 8]) torch.transpose(input, dim0, dim1) → Tensor Returns a tensor that is a transposed version of input. The given dimensions dim0 and dim1 are swapped. The resulting out tensor shares it’s underlying storage with the input tensor, so changing the content of one would change the content of the other. Parameters: input (Tensor) – the input tensor dim0 (int) – the first dimension to be transposed dim1 (int) – the second dimension to be transposed Example: >>> x = torch.randn(2, 3) >>> x tensor([[ 1.0028, -0.9893, 0.5809], [-0.1669, 0.7299, 0.4942]]) >>> torch.transpose(x, 0, 1) tensor([[ 1.0028, -0.1669], [-0.9893, 0.7299], [ 0.5809, 0.4942]]) torch.unbind(tensor, dim=0) → seq Removes a tensor dimension. Returns a tuple of all slices along a given dimension, already without it. Parameters: tensor (Tensor) – the tensor to unbind dim (int) – dimension to remove Example: >>> torch.unbind(torch.tensor([[1, 2, 3], >>> [4, 5, 6], >>> [7, 8, 9]])) (tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9])) torch.unsqueeze(input, dim, out=None) → Tensor Returns a new tensor with a dimension of size one inserted at the specified position. The returned tensor shares the same underlying data with this tensor. A dim value within the range [-input.dim() - 1, input.dim() + 1) can be used. Negative dim will correspond to unsqueeze() applied at dim = dim + input.dim() + 1. Parameters: input (Tensor) – the input tensor dim (int) – the index at which to insert the singleton dimension out (Tensor, optional) – the output tensor Example: >>> x = torch.tensor([1, 2, 3, 4]) >>> torch.unsqueeze(x, 0) tensor([[ 1, 2, 3, 4]]) >>> torch.unsqueeze(x, 1) tensor([[ 1], [ 2], [ 3], [ 4]]) torch.where(condition, x, y) → Tensor Return a tensor of elements selected from either x or y, depending on condition. The operation is defined as: Note The tensors condition, x, y must be broadcastable. Parameters: condition (ByteTensor) – When True (nonzero), yield x, otherwise yield y x (Tensor) – values selected at indices where condition is True y (Tensor) – values selected at indices where condition is False Returns: A tensor of shape equal to the broadcasted shape of condition, x, y Return type: Tensor --- --- Example: >>> x = torch.randn(3, 2) >>> y = torch.ones(3, 2) >>> x tensor([[-0.4620, 0.3139], [ 0.3898, -0.7197], [ 0.0478, -0.1657]]) >>> torch.where(x > 0, x, y) tensor([[ 1.0000, 0.3139], [ 0.3898, 1.0000], [ 0.0478, 1.0000]]) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"torch_random_sampling.html":{"url":"torch_random_sampling.html","title":"Random sampling","keywords":"","body":"随机抽样 译者：ApacheCN torch.manual_seed(seed) 设置用于生成随机数的种子。返回torch._C.Generator对象。 参数： 种子（ int） - 所需种子。 torch.initial_seed() 返回用于生成随机数的初始种子，如Python long。 torch.get_rng_state() 将随机数生成器状态返回为torch.ByteTensor。 torch.set_rng_state(new_state) 设置随机数生成器状态。 Parameters: new_state （ torch.ByteTensor） - 理想状态 torch.default_generator = torch.bernoulli(input, *, generator=None, out=None) → Tensor 从伯努利分布中绘制二进制随机数（0或1）。 input张量应该是包含用于绘制二进制随机数的概率的张量。因此，input中的所有值必须在以下范围内： 。 输出张量的 元素将根据input中给出的 概率值绘制值 。 返回的out张量仅具有值0或1，并且与input具有相同的形状。 out可以有整数dtype，但是：attr input必须有浮点dtype。 参数： 输入（ Tensor） - 伯努利分布的概率值的输入张量 out （ Tensor， 任选） - 输出张量 例： >>> a = torch.empty(3, 3).uniform_(0, 1) # generate a uniform random matrix with range [0, 1] >>> a tensor([[ 0.1737, 0.0950, 0.3609], [ 0.7148, 0.0289, 0.2676], [ 0.9456, 0.8937, 0.7202]]) >>> torch.bernoulli(a) tensor([[ 1., 0., 0.], [ 0., 0., 0.], [ 1., 1., 1.]]) >>> a = torch.ones(3, 3) # probability of drawing \"1\" is 1 >>> torch.bernoulli(a) tensor([[ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.]]) >>> a = torch.zeros(3, 3) # probability of drawing \"1\" is 0 >>> torch.bernoulli(a) tensor([[ 0., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.]]) torch.multinomial(input, num_samples, replacement=False, out=None) → LongTensor 返回张量，其中每行包含从位于张量input的相应行中的多项概率分布中采样的num_samples索引。 注意 input的行不需要求和为一（在这种情况下我们使用值作为权重），但必须是非负的，有限的并且具有非零和。 根据每个样本的时间（第一个样本放在第一列中）从左到右排序指数。 如果input是矢量，out是大小为num_samples的矢量。 如果input是具有m行的矩阵，则out是形状矩阵 。 如果更换为True，则更换样品。 如果没有，则绘制它们而不替换它们，这意味着当为一行绘制样本索引时，不能再为该行绘制它。 这意味着num_samples必须低于input长度（或者input的列数，如果它是矩阵）的约束。 Parameters: 输入（ Tensor） - 包含概率的输入张量 num_samples （ int） - 要抽取的样本数量 替代（ bool， 可选） - 是否与替换有关 out （ Tensor， 任选） - 输出张量 Example: >>> weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights >>> torch.multinomial(weights, 4) tensor([ 1, 2, 0, 0]) >>> torch.multinomial(weights, 4, replacement=True) tensor([ 2, 1, 1, 1]) torch.normal() torch.normal(mean, std, out=None) → Tensor 返回从单独的正态分布中提取的随机数的张量，其中给出了均值和标准差。 mean 是一个张量，具有每个输出元素正态分布的均值 std 是一个张量，每个输出元素的正态分布的标准差 mean 和 std 的形状不需要匹配，但每个张量中的元素总数需要相同。 Note 当形状不匹配时， mean 的形状用作返回输出张量的形状 Parameters: 意味着（ 张量 ） - 每个元素的张量意味着 std （ Tensor） - 每元素标准差的张量 out （ Tensor， 任选） - 输出张量 Example: >>> torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1)) tensor([ 1.0425, 3.5672, 2.7969, 4.2925, 4.7229, 6.2134, 8.0505, 8.1408, 9.0563, 10.0566]) torch.normal(mean=0.0, std, out=None) → Tensor 与上面的函数类似，但是所有绘制元素之间共享均值。 Parameters: 意味着（ 漂浮 ， 任选） - 所有分布的均值 std （ Tensor） - 每元素标准差的张量 out （ Tensor， 任选） - 输出张量 Example: >>> torch.normal(mean=0.5, std=torch.arange(1., 6.)) tensor([-1.2793, -1.0732, -2.0687, 5.1177, -1.2303]) torch.normal(mean, std=1.0, out=None) → Tensor 与上述功能类似，但标准偏差在所有绘制元素之间共享。 Parameters: 意味着（ 张量 ） - 每个元素的张量意味着 std （ float， 可选） - 所有分布的标准差 out （ Tensor， 任选） - 输出张量 Example: >>> torch.normal(mean=torch.arange(1., 6.)) tensor([ 1.1552, 2.6148, 2.6535, 5.8318, 4.2361]) torch.rand(*sizes, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor 从区间 上的均匀分布返回填充随机数的张量 张量的形状由变量参数sizes定义。 Parameters: sizes （ int ... ） - 定义输出张量形状的整数序列。可以是可变数量的参数，也可以是列表或元组之类的集合。 out （ Tensor， 任选） - 输出张量 dtype （ torch.dtype ，可选） - 返回张量的所需数据类型。默认值：if None，使用全局默认值（参见 torch.set_default_tensor_type() ）。 布局（ torch.layout ，可选） - 返回Tensor的理想布局。默认值：torch.strided。 设备（ torch.device ，可选） - 返回张量的所需设备。默认值：如果None，则使用当前设备作为默认张量类型（参见 torch.set_default_tensor_type() ）。 device将是CPU张量类型的CPU和CUDA张量类型的当前CUDA设备。 requires_grad （ bool， 可选） - 如果autograd应该记录对返回张量的操作。默认值：False。 Example: >>> torch.rand(4) tensor([ 0.5204, 0.2503, 0.3525, 0.5673]) >>> torch.rand(2, 3) tensor([[ 0.8237, 0.5781, 0.6879], [ 0.3816, 0.7249, 0.0998]]) torch.rand_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor 返回与input大小相同的张量，该张量用间隔 上的均匀分布填充随机数。 torch.rand_like(input)相当于torch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)。 Parameters: 输入（ Tensor） - input的大小将决定输出张量的大小 dtype （ torch.dtype ，可选） - 返回的Tensor的理想数据类型。默认值：if None，默认为input的dtype。 布局（ torch.layout ，可选） - 返回张量的理想布局。默认值：if None，默认为input的布局。 设备（ torch.device ，可选） - 返回张量的所需设备。默认值：如果None，默认为input的设备。 requires_grad （ bool， 可选） - 如果autograd应该记录对返回张量的操作。默认值：False。 torch.randint(low=0, high, size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor 返回填充了在low（包括）和high（不包括）之间统一生成的随机整数的张量。 张量的形状由变量参数size定义。 Parameters: 低（ int， 任选） - 从分布中得出的最小整数。默认值：0。 高（ int） - 高于从分布中提取的最高整数。 大小（ 元组 ） - 定义输出张量形状的元组。 out （ Tensor， 任选） - 输出张量 dtype （ torch.dtype ，可选） - 返回张量的所需数据类型。默认值：if None，使用全局默认值（参见 torch.set_default_tensor_type() ）。 布局（ torch.layout ，可选） - 返回Tensor的理想布局。默认值：torch.strided。 设备（ torch.device ，可选） - 返回张量的所需设备。默认值：如果None，则使用当前设备作为默认张量类型（参见 torch.set_default_tensor_type() ）。 device将是CPU张量类型的CPU和CUDA张量类型的当前CUDA设备。 requires_grad （ bool， 可选） - 如果autograd应该记录对返回张量的操作。默认值：False。 Example: >>> torch.randint(3, 5, (3,)) tensor([4, 3, 4]) >>> torch.randint(10, (2, 2)) tensor([[0, 2], [5, 5]]) >>> torch.randint(3, 10, (2, 2)) tensor([[4, 5], [6, 7]]) torch.randint_like(input, low=0, high, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor 返回与Tensor input具有相同形状的张量，填充在low（包括）和high（不包括）之间均匀生成的随机整数。 Parameters: 输入（ Tensor） - input的大小将决定输出张量的大小 低（ int， 任选） - 从分布中得出的最小整数。默认值：0。 高（ int） - 高于从分布中提取的最高整数。 dtype （ torch.dtype ，可选） - 返回的Tensor的理想数据类型。默认值：if None，默认为input的dtype。 布局（ torch.layout ，可选） - 返回张量的理想布局。默认值：if None，默认为input的布局。 设备（ torch.device ，可选） - 返回张量的所需设备。默认值：如果None，默认为input的设备。 requires_grad （ bool， 可选） - 如果autograd应该记录对返回张量的操作。默认值：False。 torch.randn(*sizes, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor 返回一个填充了正态分布中随机数的张量，其均值为0和方差1（也称为标准正态分布）。 The shape of the tensor is defined by the variable argument sizes. Parameters: sizes （ int ... ） - 定义输出张量形状的整数序列。可以是可变数量的参数，也可以是列表或元组之类的集合。 out （ Tensor， 任选） - 输出张量 dtype （ torch.dtype ，可选） - 返回张量的所需数据类型。默认值：if None，使用全局默认值（参见 torch.set_default_tensor_type() ）。 布局（ torch.layout ，可选） - 返回Tensor的理想布局。默认值：torch.strided。 设备（ torch.device ，可选） - 返回张量的所需设备。默认值：如果None，则使用当前设备作为默认张量类型（参见 torch.set_default_tensor_type() ）。 device将是CPU张量类型的CPU和CUDA张量类型的当前CUDA设备。 requires_grad （ bool， 可选） - 如果autograd应该记录对返回张量的操作。默认值：False。 Example: >>> torch.randn(4) tensor([-2.1436, 0.9966, 2.3426, -0.6366]) >>> torch.randn(2, 3) tensor([[ 1.5954, 2.8929, -1.0923], [ 1.1719, -0.4709, -0.1996]]) torch.randn_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor 返回与input具有相同大小的张量，该张量用正态分布中的随机数填充，均值为0且方差为1. torch.randn_like(input)等效于torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)。 Parameters: 输入（ Tensor） - input的大小将决定输出张量的大小 dtype （ torch.dtype ，可选） - 返回的Tensor的理想数据类型。默认值：if None，默认为input的dtype。 布局（ torch.layout ，可选） - 返回张量的理想布局。默认值：if None，默认为input的布局。 设备（ torch.device ，可选） - 返回张量的所需设备。默认值：如果None，默认为input的设备。 requires_grad （ bool， 可选） - 如果autograd应该记录对返回张量的操作。默认值：False。 torch.randperm(n, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False) → LongTensor 返回从0到n - 1的整数的随机排列。 Parameters: n （ int） - 上限（不包括） out （ Tensor， 任选） - 输出张量 dtype （ torch.dtype ，可选） - 返回张量的所需数据类型。默认值：torch.int64。 布局（ torch.layout ，可选） - 返回Tensor的理想布局。默认值：torch.strided。 设备（ torch.device ，可选） - 返回张量的所需设备。默认值：如果None，则使用当前设备作为默认张量类型（参见 torch.set_default_tensor_type() ）。 device将是CPU张量类型的CPU和CUDA张量类型的当前CUDA设备。 requires_grad （ bool， 可选） - 如果autograd应该记录对返回张量的操作。默认值：False。 Example: >>> torch.randperm(4) tensor([2, 1, 0, 3]) 就地随机抽样 Tensors还定义了一些更多的就地随机抽样函数。点击查看他们的文档： torch.Tensor.bernoulli_() - torch.bernoulli() 的原位版本 torch.Tensor.cauchy_() - 从Cauchy分布中提取的数字 torch.Tensor.exponential_() - 从指数分布中提取的数字 torch.Tensor.geometric_() - 从几何分布中提取的元素 torch.Tensor.log_normal_() - 来自对数正态分布的样本 torch.Tensor.normal_() - torch.normal() 的原位版本 torch.Tensor.random_() - 从离散均匀分布中采样的数字 torch.Tensor.uniform_() - 从连续均匀分布中采样的数字 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"torch_serialization_parallelism_utilities.html":{"url":"torch_serialization_parallelism_utilities.html","title":"Serialization, Parallelism, Utilities","keywords":"","body":"序列化 译者：ApacheCN torch.save(obj, f, pickle_module=, pickle_protocol=2) 将对象保存到磁盘文件。 另请参阅：保存模型的推荐方法 参数： obj - 保存对象 f - 类似文件的对象（必须实现写入和刷新）或包含文件名的字符串 pickle_module - 用于腌制元数据和对象的模块 pickle_protocol - 可以指定覆盖默认协议 警告 如果您使用的是Python 2，则torch.save不支持将StringIO.StringIO作为有效的类文件对象。这是因为write方法应该返回写入的字节数; StringIO.write（）不会这样做。 请使用像io.BytesIO这样的东西。 例 >>> # Save to file >>> x = torch.tensor([0, 1, 2, 3, 4]) >>> torch.save(x, 'tensor.pt') >>> # Save to io.BytesIO buffer >>> buffer = io.BytesIO() >>> torch.save(x, buffer) torch.load(f, map_location=None, pickle_module=) 从文件加载用 torch.save() 保存的对象。 torch.load() 使用Python的unpickling设施，但特别是处理作为张量传感器的存储器。它们首先在CPU上反序列化，然后移动到它们保存的设备上。如果此操作失败（例如，因为运行时系统没有某些设备），则会引发异常。但是，可以使用map_location参数将存储重新映射到另一组设备。 如果map_location是可调用的，则每个序列化存储器将调用一次，其中包含两个参数：存储和位置。存储参数将是驻留在CPU上的存储的初始反序列化。每个序列化存储都有一个与之关联的位置标记，用于标识从中保存的设备，此标记是传递给map_location的第二个参数。内置位置标签是CPU张量的‘cpu’和CUDA张量的‘cuda:device_id’（例如‘cuda:2’）。 map_location应返回None或存储。如果map_location返回存储，它将用作最终反序列化对象，已移动到正确的设备。否则， 将回退到默认行为，就像未指定map_location一样。 如果map_location是一个字符串，它应该是一个设备标签，应该加载所有张量。 否则，如果map_location是一个dict，它将用于将文件（键）中出现的位置标记重新映射到指定存储位置（值）的位置标记。 用户扩展可以使用register_package注册自己的位置标记以及标记和反序列化方法。 Parameters: f - 类文件对象（必须实现read，readline，tell和seek），或包含文件名的字符串 map_location - 一个函数，torch.device，string或dict，指定如何重新映射存储位置 pickle_module - 用于取消元数据和对象取消的模块（必须与用于序列化文件的pickle_module相匹配） 注意 当您在包含GPU张量的文件上调用 torch.load() 时，默认情况下这些张量将被加载到GPU。在加载模型检查点时，可以调用torch.load(.., map_location=’cpu’)然后调用load_state_dict()以避免GPU RAM激增。 Example >>> torch.load('tensors.pt') # Load all tensors onto the CPU >>> torch.load('tensors.pt', map_location=torch.device('cpu')) # Load all tensors onto the CPU, using a function >>> torch.load('tensors.pt', map_location=lambda storage, loc: storage) # Load all tensors onto GPU 1 >>> torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1)) # Map tensors from GPU 1 to GPU 0 >>> torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'}) # Load tensor from io.BytesIO object >>> with open('tensor.pt') as f: buffer = io.BytesIO(f.read()) >>> torch.load(buffer) 排比 torch.get_num_threads() → int 获取用于并行化CPU操作的OpenMP线程数 torch.set_num_threads(int) 设置用于并行化CPU操作的OpenMP线程数 在本地禁用渐变计算 上下文管理器torch.no_grad()，torch.enable_grad()和torch.set_grad_enabled()有助于本地禁用和启用梯度计算。有关其用法的更多详细信息，请参见本地禁用梯度计算。 例子： >>> x = torch.zeros(1, requires_grad=True) >>> with torch.no_grad(): ... y = x * 2 >>> y.requires_grad False >>> is_train = False >>> with torch.set_grad_enabled(is_train): ... y = x * 2 >>> y.requires_grad False >>> torch.set_grad_enabled(True) # this can also be used as a function >>> y = x * 2 >>> y.requires_grad True >>> torch.set_grad_enabled(False) >>> y = x * 2 >>> y.requires_grad False 公用事业 torch.compiled_with_cxx11_abi() 返回是否使用_GLIBCXX_USE_CXX11_ABI = 1构建PyTorch 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"torch_math_operations.html":{"url":"torch_math_operations.html","title":"Math operations","keywords":"","body":"Math operations 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"torch_math_operations_pointwise_ops.html":{"url":"torch_math_operations_pointwise_ops.html","title":"Pointwise Ops","keywords":"","body":"逐点行动 译者：ApacheCN torch.abs(input, out=None) → Tensor 计算给定input张量的逐元素绝对值。 参数： 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 例： >>> torch.abs(torch.tensor([-1, -2, 3])) tensor([ 1, 2, 3]) torch.acos(input, out=None) → Tensor 返回带有input元素的反余弦的新张量。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([ 0.3348, -0.5889, 0.2005, -0.1584]) >>> torch.acos(a) tensor([ 1.2294, 2.2004, 1.3690, 1.7298]) torch.add() torch.add(input, value, out=None) 将标量value添加到输入input的每个元素并返回新的结果张量。 如果input的类型为FloatTensor或DoubleTensor，则value必须是实数，否则应为整数。 Parameters: 输入（ Tensor） - 输入张量 值（号码） - 要添加到input的每个元素的数字 关键字参数： ？ Example: >>> a = torch.randn(4) >>> a tensor([ 0.0202, 1.0985, 1.3506, -0.6056]) >>> torch.add(a, 20) tensor([ 20.0202, 21.0985, 21.3506, 19.3944]) torch.add(input, value=1, other, out=None) 张量other的每个元素乘以标量value并添加到张量input的每个元素。返回结果张量。 input和other的形状必须是可播放的。 如果other的类型为FloatTensor或DoubleTensor，则value必须是实数，否则应为整数。 Parameters: 输入（ Tensor） - 第一个输入张量 值（数字） - other的标量乘数 其他（ Tensor） - 第二个输入张量 Keyword Arguments: ? Example: >>> a = torch.randn(4) >>> a tensor([-0.9732, -0.3497, 0.6245, 0.4022]) >>> b = torch.randn(4, 1) >>> b tensor([[ 0.3743], [-1.7724], [-0.5811], [-0.8017]]) >>> torch.add(a, 10, b) tensor([[ 2.7695, 3.3930, 4.3672, 4.1450], [-18.6971, -18.0736, -17.0994, -17.3216], [ -6.7845, -6.1610, -5.1868, -5.4090], [ -8.9902, -8.3667, -7.3925, -7.6147]]) torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None) → Tensor 通过tensor2执行tensor1的逐元素划分，将结果乘以标量value并将其添加到 tensor 。 tensor ，tensor1和tensor2的形状必须是可播放的。 对于FloatTensor或DoubleTensor类型的输入，value必须是实数，否则是整数。 Parameters: 张量（ Tensor） - 要加的张量 值（数 ， 可选） - 的乘数 tensor1 （ Tensor） - 分子张量 张量2 （ 张量 ） - 分母张量 out （ Tensor， 任选） - 输出张量 Example: >>> t = torch.randn(1, 3) >>> t1 = torch.randn(3, 1) >>> t2 = torch.randn(1, 3) >>> torch.addcdiv(t, 0.1, t1, t2) tensor([[-0.2312, -3.6496, 0.1312], [-1.0428, 3.4292, -0.1030], [-0.5369, -0.9829, 0.0430]]) torch.addcmul(tensor, value=1, tensor1, tensor2, out=None) → Tensor 通过tensor2执行tensor1的逐元素乘法，将结果乘以标量value并将其添加到 tensor 。 The shapes of tensor, tensor1, and tensor2 must be broadcastable. For inputs of type FloatTensor or DoubleTensor, value must be a real number, otherwise an integer. Parameters: 张量（ Tensor） - 要加的张量 值（数 ， 可选） - 的乘数 tensor1 （ Tensor） - 要倍增的张量 tensor2 （ Tensor） - 要倍增的张量 out （ Tensor， 任选） - 输出张量 Example: >>> t = torch.randn(1, 3) >>> t1 = torch.randn(3, 1) >>> t2 = torch.randn(1, 3) >>> torch.addcmul(t, 0.1, t1, t2) tensor([[-0.8635, -0.6391, 1.6174], [-0.7617, -0.5879, 1.7388], [-0.8353, -0.6249, 1.6511]]) torch.asin(input, out=None) → Tensor 返回具有input元素的反正弦的新张量。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([-0.5962, 1.4985, -0.4396, 1.4525]) >>> torch.asin(a) tensor([-0.6387, nan, -0.4552, nan]) torch.atan(input, out=None) → Tensor 返回带有input元素反正切的新张量。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([ 0.2341, 0.2539, -0.6256, -0.6448]) >>> torch.atan(a) tensor([ 0.2299, 0.2487, -0.5591, -0.5727]) torch.atan2(input1, input2, out=None) → Tensor 返回带有input1和input2元素的反正切的新张量。 input1和input2的形状必须是可播放的。 Parameters: input1 （ Tensor） - 第一个输入张量 input2 （ Tensor） - 第二个输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([ 0.9041, 0.0196, -0.3108, -2.4423]) >>> torch.atan2(a, torch.randn(4)) tensor([ 0.9833, 0.0811, -1.9743, -1.4151]) torch.ceil(input, out=None) → Tensor 返回具有input元素的ceil的新张量，该元素是大于或等于每个元素的最小整数。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([-0.6341, -1.4208, -1.0900, 0.5826]) >>> torch.ceil(a) tensor([-0., -1., -1., 1.]) torch.clamp(input, min, max, out=None) → Tensor 将input中的所有元素钳位到[ min ， max ]范围内并返回结果张量： 如果input的类型为FloatTensor或DoubleTensor，则 min 和 max 必须为实数，否则它们应为整数。 Parameters: 输入（ Tensor） - 输入张量 min （ Number ） - 要被钳位的范围的下限 max （ Number ） - 要钳位的范围的上限 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([-1.7120, 0.1734, -0.0478, -0.0922]) >>> torch.clamp(a, min=-0.5, max=0.5) tensor([-0.5000, 0.1734, -0.0478, -0.0922]) torch.clamp(input, *, min, out=None) → Tensor 将input中的所有元素钳位为大于或等于 min 。 如果input的类型为FloatTensor或DoubleTensor，则value应为实数，否则应为整数。 Parameters: 输入（ Tensor） - 输入张量 值（数字） - 输出中每个元素的最小值 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([-0.0299, -2.3184, 2.1593, -0.8883]) >>> torch.clamp(a, min=0.5) tensor([ 0.5000, 0.5000, 2.1593, 0.5000]) torch.clamp(input, *, max, out=None) → Tensor 将input中的所有元素钳位为小于或等于 max 。 If input is of type FloatTensor or DoubleTensor, value should be a real number, otherwise it should be an integer. Parameters: 输入（ Tensor） - 输入张量 值（数字） - 输出中每个元素的最大值 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([ 0.7753, -0.4702, -0.4599, 1.1899]) >>> torch.clamp(a, max=0.5) tensor([ 0.5000, -0.4702, -0.4599, 0.5000]) torch.cos(input, out=None) → Tensor 返回具有input元素的余弦的新张量。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([ 1.4309, 1.2706, -0.8562, 0.9796]) >>> torch.cos(a) tensor([ 0.1395, 0.2957, 0.6553, 0.5574]) torch.cosh(input, out=None) → Tensor 返回具有input元素的双曲余弦值的新张量。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([ 0.1632, 1.1835, -0.6979, -0.7325]) >>> torch.cosh(a) tensor([ 1.0133, 1.7860, 1.2536, 1.2805]) torch.div() torch.div(input, value, out=None) → Tensor 将输入input的每个元素与标量value分开，并返回一个新的结果张量。 如果input的类型为FloatTensor或DoubleTensor，value应为实数，否则应为整数 Parameters: 输入（ Tensor） - 输入张量 值（号码） - 要分配给input的每个元素的数字 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(5) >>> a tensor([ 0.3810, 1.2774, -0.2972, -0.3719, 0.4637]) >>> torch.div(a, 0.5) tensor([ 0.7620, 2.5548, -0.5944, -0.7439, 0.9275]) torch.div(input, other, out=None) → Tensor 张量input的每个元素除以张量other的每个元素。返回结果张量。 input和other的形状必须是可播放的。 Parameters: 输入（ Tensor） - 分子张量 其他（ Tensor） - 分母张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4, 4) >>> a tensor([[-0.3711, -1.9353, -0.4605, -0.2917], [ 0.1815, -1.0111, 0.9805, -1.5923], [ 0.1062, 1.4581, 0.7759, -1.2344], [-0.1830, -0.0313, 1.1908, -1.4757]]) >>> b = torch.randn(4) >>> b tensor([ 0.8032, 0.2930, -0.8113, -0.2308]) >>> torch.div(a, b) tensor([[-0.4620, -6.6051, 0.5676, 1.2637], [ 0.2260, -3.4507, -1.2086, 6.8988], [ 0.1322, 4.9764, -0.9564, 5.3480], [-0.2278, -0.1068, -1.4678, 6.3936]]) torch.digamma(input, out=None) → Tensor 计算input上伽玛函数的对数导数。 参数： 输入（ Tensor） - 计算digamma函数的张量 Example: >>> a = torch.tensor([1, 0.5]) >>> torch.digamma(a) tensor([-0.5772, -1.9635]) torch.erf(tensor, out=None) → Tensor 计算每个元素的错误函数。错误函数定义如下： Parameters: 张量（ 张量 ） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> torch.erf(torch.tensor([0, -1., 10.])) tensor([ 0.0000, -0.8427, 1.0000]) torch.erfc(input, out=None) → Tensor 计算input的每个元素的互补误差函数。互补误差函数定义如下： Parameters: 张量（ 张量 ） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> torch.erfc(torch.tensor([0, -1., 10.])) tensor([ 1.0000, 1.8427, 0.0000]) torch.erfinv(input, out=None) → Tensor 计算input的每个元素的反向误差函数。反向误差函数在 范围内定义为： Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> torch.erfinv(torch.tensor([0, 0.5, -1.])) tensor([ 0.0000, 0.4769, -inf]) torch.exp(input, out=None) → Tensor 返回具有输入张量input元素的指数的新张量。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> torch.exp(torch.tensor([0, math.log(2.)])) tensor([ 1., 2.]) torch.expm1(input, out=None) → Tensor 返回一个新的张量，其元素的指数减去input的1。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> torch.expm1(torch.tensor([0, math.log(2.)])) tensor([ 0., 1.]) torch.floor(input, out=None) → Tensor 返回一个新的张量，其中包含input元素的最低值，这是每个元素小于或等于的最大整数。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([-0.8166, 1.5308, -0.2530, -0.2091]) >>> torch.floor(a) tensor([-1., 1., -1., -1.]) torch.fmod(input, divisor, out=None) → Tensor 计算除法的元素余数。 被除数和除数可以包含整数和浮点数。余数与被除数input具有相同的符号。 当divisor是张量时，input和divisor的形状必须是可广播。 Parameters: 输入（ Tensor） - 股息 除数（ 张量 或 漂浮 ） - 除数，可能是与被除数相同形状的数字或张量 out （ Tensor， 任选） - 输出张量 Example: >>> torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2) tensor([-1., -0., -1., 1., 0., 1.]) >>> torch.fmod(torch.tensor([1., 2, 3, 4, 5]), 1.5) tensor([ 1.0000, 0.5000, 0.0000, 1.0000, 0.5000]) torch.frac(input, out=None) → Tensor 计算input中每个元素的小数部分。 Example: >>> torch.frac(torch.tensor([1, 2.5, -3.2])) tensor([ 0.0000, 0.5000, -0.2000]) torch.lerp(start, end, weight, out=None) 是否基于标量weight对两个张量start和end进行线性插值，并返回得到的out张量。 start和end的形状必须是可播放的。 Parameters: 启动（ Tensor） - 张量与起点 结束（ Tensor） - 带有终点的张量 体重（ 漂浮 ） - 插值公式的权重 out （ Tensor， 任选） - 输出张量 Example: >>> start = torch.arange(1., 5.) >>> end = torch.empty(4).fill_(10) >>> start tensor([ 1., 2., 3., 4.]) >>> end tensor([ 10., 10., 10., 10.]) >>> torch.lerp(start, end, 0.5) tensor([ 5.5000, 6.0000, 6.5000, 7.0000]) torch.log(input, out=None) → Tensor 返回具有input元素的自然对数的新张量。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(5) >>> a tensor([-0.7168, -0.5471, -0.8933, -1.4428, -0.1190]) >>> torch.log(a) tensor([ nan, nan, nan, nan, nan]) torch.log10(input, out=None) → Tensor 返回一个新的张量，其对数为input元素的基数10。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.rand(5) >>> a tensor([ 0.5224, 0.9354, 0.7257, 0.1301, 0.2251]) >>> torch.log10(a) tensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476]) torch.log1p(input, out=None) → Tensor 返回一个自然对数为（1 + input）的新张量。 注意 对于input的小值，此函数比 torch.log() 更准确 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(5) >>> a tensor([-1.0090, -0.9923, 1.0249, -0.5372, 0.2492]) >>> torch.log1p(a) tensor([ nan, -4.8653, 0.7055, -0.7705, 0.2225]) torch.log2(input, out=None) → Tensor 返回一个新的张量，其对数为input元素的基数2。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.rand(5) >>> a tensor([ 0.8419, 0.8003, 0.9971, 0.5287, 0.0490]) >>> torch.log2(a) tensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504]) torch.mul() torch.mul(input, value, out=None) 将输入input的每个元素与标量value相乘，并返回一个新的结果张量。 If input is of type FloatTensor or DoubleTensor, value should be a real number, otherwise it should be an integer Parameters: 输入（ Tensor） - 输入张量 值（号码） - 要与input的每个元素相乘的数字 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(3) >>> a tensor([ 0.2015, -0.4255, 2.6087]) >>> torch.mul(a, 100) tensor([ 20.1494, -42.5491, 260.8663]) torch.mul(input, other, out=None) 张量input的每个元素乘以张量other的每个元素。返回结果张量。 The shapes of input and other must be broadcastable. Parameters: 输入（ Tensor） - 第一个被乘数张量 其他（ Tensor） - 第二个被乘数张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4, 1) >>> a tensor([[ 1.1207], [-0.3137], [ 0.0700], [ 0.8378]]) >>> b = torch.randn(1, 4) >>> b tensor([[ 0.5146, 0.1216, -0.5244, 2.2382]]) >>> torch.mul(a, b) tensor([[ 0.5767, 0.1363, -0.5877, 2.5083], [-0.1614, -0.0382, 0.1645, -0.7021], [ 0.0360, 0.0085, -0.0367, 0.1567], [ 0.4312, 0.1019, -0.4394, 1.8753]]) torch.mvlgamma(input, p) → Tensor 用维度 元素计算多变量log-gamma函数，由下式给出： 其中 和 是Gamma函数。 如果任何元素小于或等于 ，则抛出错误。 Parameters: 输入（ Tensor） - 计算多变量log-gamma函数的张量 p （ int） - 维数 Example: >>> a = torch.empty(2, 3).uniform_(1, 2) >>> a tensor([[1.6835, 1.8474, 1.1929], [1.0475, 1.7162, 1.4180]]) >>> torch.mvlgamma(a, 2) tensor([[0.3928, 0.4007, 0.7586], [1.0311, 0.3901, 0.5049]]) torch.neg(input, out=None) → Tensor 返回一个新的张量，其元素为input。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(5) >>> a tensor([ 0.0090, -0.2262, -0.0682, -0.2866, 0.3940]) >>> torch.neg(a) tensor([-0.0090, 0.2262, 0.0682, 0.2866, -0.3940]) torch.pow() torch.pow(input, exponent, out=None) → Tensor 使用exponent获取input中每个元素的功效，并返回带有结果的张量。 exponent可以是单个float编号，也可以是Tensor，其元素数与input相同。 当exponent是标量值时，应用的操作是： 当exponent是张量时，应用的操作是： 当exponent是张量时，input和exponent的形状必须是可广播。 Parameters: 输入（ Tensor） - 输入张量 指数（ float或 张量） - 指数值 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([ 0.4331, 1.2475, 0.6834, -0.2791]) >>> torch.pow(a, 2) tensor([ 0.1875, 1.5561, 0.4670, 0.0779]) >>> exp = torch.arange(1., 5.) >>> a = torch.arange(1., 5.) >>> a tensor([ 1., 2., 3., 4.]) >>> exp tensor([ 1., 2., 3., 4.]) >>> torch.pow(a, exp) tensor([ 1., 4., 27., 256.]) torch.pow(base, input, out=None) → Tensor base是标量float值，input是张量。返回的张量out与input具有相同的形状 适用的操作是： Parameters: base （ float） - 电源操作的标量基值 输入（ Tensor） - 指数张量 out （ Tensor， 任选） - 输出张量 Example: >>> exp = torch.arange(1., 5.) >>> base = 2 >>> torch.pow(base, exp) tensor([ 2., 4., 8., 16.]) torch.reciprocal(input, out=None) → Tensor 返回具有input元素倒数的新张量 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([-0.4595, -2.1219, -1.4314, 0.7298]) >>> torch.reciprocal(a) tensor([-2.1763, -0.4713, -0.6986, 1.3702]) torch.remainder(input, divisor, out=None) → Tensor Computes the element-wise remainder of division. 除数和被除数可以包含整数和浮点数。其余部分与除数具有相同的符号。 When divisor is a tensor, the shapes of input and divisor must be broadcastable. Parameters: 输入（ Tensor） - 股息 除数（ 张量 或 漂浮 ） - 可能是一个除数数字或与被除数相同形状的张量 out （ Tensor， 任选） - 输出张量 Example: >>> torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2) tensor([ 1., 0., 1., 1., 0., 1.]) >>> torch.remainder(torch.tensor([1., 2, 3, 4, 5]), 1.5) tensor([ 1.0000, 0.5000, 0.0000, 1.0000, 0.5000]) 也可以看看 torch.fmod() ，它计算与C库函数fmod()等效的除法的元素余数。 torch.round(input, out=None) → Tensor 返回一个新的张量，input的每个元素四舍五入到最接近的整数。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([ 0.9920, 0.6077, 0.9734, -1.0362]) >>> torch.round(a) tensor([ 1., 1., 1., -1.]) torch.rsqrt(input, out=None) → Tensor 返回一个新的张量，其具有input的每个元素的平方根的倒数。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([-0.0370, 0.2970, 1.5420, -0.9105]) >>> torch.rsqrt(a) tensor([ nan, 1.8351, 0.8053, nan]) torch.sigmoid(input, out=None) → Tensor 返回带有input元素的sigmoid的新张量。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([ 0.9213, 1.0887, -0.8858, -1.7683]) >>> torch.sigmoid(a) tensor([ 0.7153, 0.7481, 0.2920, 0.1458]) torch.sign(input, out=None) → Tensor 返回带有input元素符号的新张量。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.tensor([0.7, -1.2, 0., 2.3]) >>> a tensor([ 0.7000, -1.2000, 0.0000, 2.3000]) >>> torch.sign(a) tensor([ 1., -1., 0., 1.]) torch.sin(input, out=None) → Tensor 返回带有input元素正弦的新张量。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([-0.5461, 0.1347, -2.7266, -0.2746]) >>> torch.sin(a) tensor([-0.5194, 0.1343, -0.4032, -0.2711]) torch.sinh(input, out=None) → Tensor 返回具有input元素的双曲正弦的新张量。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([ 0.5380, -0.8632, -0.1265, 0.9399]) >>> torch.sinh(a) tensor([ 0.5644, -0.9744, -0.1268, 1.0845]) torch.sqrt(input, out=None) → Tensor 返回具有input元素的平方根的新张量。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([-2.0755, 1.0226, 0.0831, 0.4806]) >>> torch.sqrt(a) tensor([ nan, 1.0112, 0.2883, 0.6933]) torch.tan(input, out=None) → Tensor 返回具有input元素正切的新张量。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([-1.2027, -1.7687, 0.4412, -1.3856]) >>> torch.tan(a) tensor([-2.5930, 4.9859, 0.4722, -5.3366]) torch.tanh(input, out=None) → Tensor 返回具有input元素的双曲正切的新张量。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([ 0.8986, -0.7279, 1.1745, 0.2611]) >>> torch.tanh(a) tensor([ 0.7156, -0.6218, 0.8257, 0.2553]) torch.trunc(input, out=None) → Tensor 返回具有input元素的截断整数值的新张量。 Parameters: 输入（ Tensor） - 输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([ 3.4742, 0.5466, -0.8008, -0.9079]) >>> torch.trunc(a) tensor([ 3., 0., -0., -0.]) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"torch_math_operations_reduction_ops.html":{"url":"torch_math_operations_reduction_ops.html","title":"Reduction Ops","keywords":"","body":"减少行动 译者：ApacheCN torch.argmax(input, dim=None, keepdim=False) 返回维度上张量的最大值的索引。 这是 torch.max() 返回的第二个值。有关此方法的确切语义，请参阅其文档。 参数： 输入（ Tensor） - 输入张量 dim （ int） - 降低的维数。如果None，则返回展平输入的argmax。 keepdim （ bool） - 输出张量是否保留dim。如果dim=None，则忽略。 例： >>> a = torch.randn(4, 4) >>> a tensor([[ 1.3398, 0.2663, -0.2686, 0.2450], [-0.7401, -0.8805, -0.3402, -1.1936], [ 0.4907, -1.3948, -1.0691, -0.3132], [-1.6092, 0.5419, -0.2993, 0.3195]]) >>> torch.argmax(a, dim=1) tensor([ 0, 2, 0, 1]) torch.argmin(input, dim=None, keepdim=False) 返回维度上张量的最小值的索引。 这是 torch.min() 返回的第二个值。有关此方法的确切语义，请参阅其文档。 Parameters: 输入（ Tensor） - 输入张量 dim （ int） - 降低的维数。如果None，则返回展平输入的argmin。 keepdim （ bool） - 输出张量是否保留dim。如果dim=None，则忽略。 Example: >>> a = torch.randn(4, 4) >>> a tensor([[ 0.1139, 0.2254, -0.1381, 0.3687], [ 1.0100, -1.1975, -0.0102, -0.4732], [-0.9240, 0.1207, -0.7506, -1.0213], [ 1.7809, -1.2960, 0.9384, 0.1438]]) >>> torch.argmin(a, dim=1) tensor([ 2, 1, 3, 1]) torch.cumprod(input, dim, dtype=None) → Tensor 返回维度dim中input元素的累积乘积。 例如，如果input是大小为N的向量，则结果也将是具有元素的大小为N的向量。 Parameters: 输入（ Tensor） - 输入张量 昏暗（ int） - 执行操作的维度 dtype （ torch.dtype ，可选） - 返回张量的所需数据类型。如果指定，则在执行操作之前将输入张量转换为dtype。这对于防止数据类型溢出很有用。默认值：无。 Example: >>> a = torch.randn(10) >>> a tensor([ 0.6001, 0.2069, -0.1919, 0.9792, 0.6727, 1.0062, 0.4126, -0.2129, -0.4206, 0.1968]) >>> torch.cumprod(a, dim=0) tensor([ 0.6001, 0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065, 0.0014, -0.0006, -0.0001]) >>> a[5] = 0.0 >>> torch.cumprod(a, dim=0) tensor([ 0.6001, 0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000, 0.0000, -0.0000, -0.0000]) torch.cumsum(input, dim, out=None, dtype=None) → Tensor 返回维度dim中input的元素的累积和。 For example, if input is a vector of size N, the result will also be a vector of size N, with elements. Parameters: 输入（ Tensor） - 输入张量 昏暗（ int） - 执行操作的维度 dtype （ torch.dtype ，可选） - 返回张量的所需数据类型。如果指定，则在执行操作之前将输入张量转换为dtype。这对于防止数据类型溢出很有用。默认值：无。 Example: >>> a = torch.randn(10) >>> a tensor([-0.8286, -0.4890, 0.5155, 0.8443, 0.1865, -0.1752, -2.0595, 0.1850, -1.1571, -0.4243]) >>> torch.cumsum(a, dim=0) tensor([-0.8286, -1.3175, -0.8020, 0.0423, 0.2289, 0.0537, -2.0058, -1.8209, -2.9780, -3.4022]) torch.dist(input, other, p=2) → Tensor 返回（input - other）的p范数 input和other的形状必须是可播放的。 Parameters: 输入（ Tensor） - 输入张量 其他（ Tensor） - 右侧输入张量 p （ 漂浮 ， 任选） - 要计算的范数 Example: >>> x = torch.randn(4) >>> x tensor([-1.5393, -0.8675, 0.5916, 1.6321]) >>> y = torch.randn(4) >>> y tensor([ 0.0967, -1.0511, 0.6295, 0.8360]) >>> torch.dist(x, y, 3.5) tensor(1.6727) >>> torch.dist(x, y, 3) tensor(1.6973) >>> torch.dist(x, y, 0) tensor(inf) >>> torch.dist(x, y, 1) tensor(2.6537) torch.logsumexp(input, dim, keepdim=False, out=None) 返回给定维dim中input张量的每一行的求和指数的对数。计算在数值上是稳定的。 对于由dim和其他指数 给出的总和指数 ，结果是 如果keepdim为True，则输出张量与input的大小相同，但尺寸为dim的大小为1.否则，dim被挤压（参见 torch.squeeze() ），导致输出张量比input少1个维度。 Parameters: 输入（ Tensor） - 输入张量 昏暗（ int或 元组python：整数） - 要减少的维度或维度 keepdim （ bool） - 输出张量是否保留dim out （ Tensor， 任选） - 输出张量 Example:: >>> a = torch.randn(3, 3) >>> torch.logsumexp(a, 1) tensor([ 0.8442, 1.4322, 0.8711]) torch.mean() torch.mean(input) → Tensor 返回input张量中所有元素的平均值。 参数： 输入（ Tensor） - 输入张量 Example: >>> a = torch.randn(1, 3) >>> a tensor([[ 0.2294, -0.5481, 1.3288]]) >>> torch.mean(a) tensor(0.3367) torch.mean(input, dim, keepdim=False, out=None) → Tensor 返回给定维dim中input张量的每一行的平均值。如果dim是维度列表，请减少所有维度。 如果keepdim为True，则输出张量与input的大小相同，但尺寸为1的尺寸dim除外。dim被挤压（见） torch.squeeze() ），导致输出张量具有1（或len(dim)）更少的维度。 Parameters: 输入（ Tensor） - 输入张量 昏暗（ int） - 减少的维度 keepdim （ bool， 可选） - 输出张量是否保留dim out （ Tensor） - 输出张量 Example: >>> a = torch.randn(4, 4) >>> a tensor([[-0.3841, 0.6320, 0.4254, -0.7384], [-0.9644, 1.0131, -0.6549, -1.4279], [-0.2951, -1.3350, -0.7694, 0.5600], [ 1.0842, -0.9580, 0.3623, 0.2343]]) >>> torch.mean(a, 1) tensor([-0.0163, -0.5085, -0.4599, 0.1807]) >>> torch.mean(a, 1, True) tensor([[-0.0163], [-0.5085], [-0.4599], [ 0.1807]]) torch.median() torch.median(input) → Tensor 返回input张量中所有元素的中值。 Parameters: input (Tensor) – the input tensor Example: >>> a = torch.randn(1, 3) >>> a tensor([[ 1.5219, -1.5212, 0.2202]]) >>> torch.median(a) tensor(0.2202) torch.median(input, dim=-1, keepdim=False, values=None, indices=None) -> (Tensor, LongTensor) 返回给定维dim中input张量的每一行的中值。还将中值的索引位置返回为LongTensor。 默认情况下，dim是input张量的最后一个维度。 如果keepdim为True，则输出张量与input的尺寸相同，但尺寸为dim的尺寸为1.否则，dim被挤压（参见 torch.squeeze() ），导致输出张量比input少1个维度。 Parameters: 输入（ Tensor） - 输入张量 昏暗（ int） - 减少的维度 keepdim （ bool） - 输出张量是否保留dim 值（ 张量 ， 可选） - 输出张量 指数（ 张量 ， 任选） - 输出指数张量 Example: >>> a = torch.randn(4, 5) >>> a tensor([[ 0.2505, -0.3982, -0.9948, 0.3518, -1.3131], [ 0.3180, -0.6993, 1.0436, 0.0438, 0.2270], [-0.2751, 0.7303, 0.2192, 0.3321, 0.2488], [ 1.0778, -1.9510, 0.7048, 0.4742, -0.7125]]) >>> torch.median(a, 1) (tensor([-0.3982, 0.2270, 0.2488, 0.4742]), tensor([ 1, 4, 4, 3])) torch.mode(input, dim=-1, keepdim=False, values=None, indices=None) -> (Tensor, LongTensor) 返回给定维dim中input张量的每一行的模式值。还将模式值的索引位置作为LongTensor返回。 By default, dim is the last dimension of the input tensor. 如果keepdim为True，则输出张量与input的尺寸相同，但尺寸为dim的尺寸为1.否则，dim被挤压（参见 torch.squeeze() ），导致输出张量的尺寸比input少1。 注意 尚未为torch.cuda.Tensor定义此功能。 Parameters: 输入（ Tensor） - 输入张量 昏暗（ int） - 减少的维度 keepdim （ bool） - 输出张量是否保留dim 值（ 张量 ， 可选） - 输出张量 指数（ 张量 ， 任选） - 输出指数张量 Example: >>> a = torch.randn(4, 5) >>> a tensor([[-1.2808, -1.0966, -1.5946, -0.1148, 0.3631], [ 1.1395, 1.1452, -0.6383, 0.3667, 0.4545], [-0.4061, -0.3074, 0.4579, -1.3514, 1.2729], [-1.0130, 0.3546, -1.4689, -0.1254, 0.0473]]) >>> torch.mode(a, 1) (tensor([-1.5946, -0.6383, -1.3514, -1.4689]), tensor([ 2, 2, 3, 2])) torch.norm(input, p='fro', dim=None, keepdim=False, out=None) 返回给定张量的矩阵范数或向量范数。 Parameters: 输入（ Tensor） - 输入张量 p （ int， 漂浮 ， ] inf ， -inf ， '来'__， 'nuc'__， 任选） - 规范的顺序。默认值：'fro'可以计算以下规范： | ord |矩阵规范|矢量规范| | --- | --- | --- | |没有| Frobenius规范| 2范数| | '来'| Frobenius规范| - | | 'nuc'|核规范| - | |其他|当昏暗是无|时，作为vec规范sum（abs（x） ord）（1./ord）| 昏暗（ int， 2元组python：ints ， 2-list of python：ints ， 可选） - 如果是int，将计算向量范数，如果是2元组的int，将计算矩阵范数。如果值为None，则当输入张量仅具有两个维度时将计算矩阵范数，当输入张量仅具有一个维度时将计算向量范数。如果输入张量具有两个以上的维度，则向量范数将应用于最后一个维度。 keepdim （ bool， 任选） - 输出张量是否保留dim。如果dim = None和out = None，则忽略。默认值：False out （ Tensor， 可选） - 输出张量。如果dim = None和out = None，则忽略。 Example: >>> import torch >>> a = torch.arange(9, dtype= torch.float) - 4 >>> b = a.reshape((3, 3)) >>> torch.norm(a) tensor(7.7460) >>> torch.norm(b) tensor(7.7460) >>> torch.norm(a, float('inf')) tensor(4.) >>> torch.norm(b, float('inf')) tensor([4., 3., 4.]) >>> c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float) >>> torch.norm(c, dim=0) tensor([1.4142, 2.2361, 5.0000]) >>> torch.norm(c, dim=1) tensor([3.7417, 4.2426]) >>> torch.norm(c, p=1, dim=1) tensor([6., 6.]) >>> d = torch.arange(8, dtype= torch.float).reshape(2,2,2) >>> torch.norm(d, dim=(1,2)) tensor([ 3.7417, 11.2250]) >>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :]) (tensor(3.7417), tensor(11.2250)) torch.prod() torch.prod(input, dtype=None) → Tensor 返回input张量中所有元素的乘积。 Parameters: 输入（ Tensor） - 输入张量 dtype （ torch.dtype ，可选） - 返回张量的所需数据类型。如果指定，则在执行操作之前将输入张量转换为dtype。这对于防止数据类型溢出很有用。默认值：无。 Example: >>> a = torch.randn(1, 3) >>> a tensor([[-0.8020, 0.5428, -1.5854]]) >>> torch.prod(a) tensor(0.6902) torch.prod(input, dim, keepdim=False, dtype=None) → Tensor 返回给定维dim中input张量的每一行的乘积。 If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input. Parameters: 输入（ Tensor） - 输入张量 昏暗（ int） - 减少的维度 keepdim （ bool） - 输出张量是否保留dim dtype （ torch.dtype ，可选） - 返回张量的所需数据类型。如果指定，则在执行操作之前将输入张量转换为dtype。这对于防止数据类型溢出很有用。默认值：无。 Example: >>> a = torch.randn(4, 2) >>> a tensor([[ 0.5261, -0.3837], [ 1.1857, -0.2498], [-1.1646, 0.0705], [ 1.1131, -1.0629]]) >>> torch.prod(a, 1) tensor([-0.2018, -0.2962, -0.0821, -1.1831]) torch.std() torch.std(input, unbiased=True) → Tensor 返回input张量中所有元素的标准偏差。 如果unbiased为False，则将通过偏差估算器计算标准偏差。否则，将使用贝塞尔的修正。 Parameters: 输入（ Tensor） - 输入张量 无偏（ bool） - 是否使用无偏估计 Example: >>> a = torch.randn(1, 3) >>> a tensor([[-0.8166, -1.3802, -0.3560]]) >>> torch.std(a) tensor(0.5130) torch.std(input, dim, keepdim=False, unbiased=True, out=None) → Tensor 返回给定维dim中input张量的每一行的标准偏差。 If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input. If unbiased is False, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel’s correction will be used. Parameters: 输入（ Tensor） - 输入张量 昏暗（ int） - 减少的维度 keepdim （ bool） - 输出张量是否保留dim 无偏（ bool） - 是否使用无偏估计 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4, 4) >>> a tensor([[ 0.2035, 1.2959, 1.8101, -0.4644], [ 1.5027, -0.3270, 0.5905, 0.6538], [-1.5745, 1.3330, -0.5596, -0.6548], [ 0.1264, -0.5080, 1.6420, 0.1992]]) >>> torch.std(a, dim=1) tensor([ 1.0311, 0.7477, 1.2204, 0.9087]) torch.sum() torch.sum(input, dtype=None) → Tensor 返回input张量中所有元素的总和。 Parameters: 输入（ Tensor） - 输入张量 dtype （ torch.dtype ，可选） - 返回张量的所需数据类型。如果指定，则在执行操作之前将输入张量转换为dtype。这对于防止数据类型溢出很有用。默认值：无。 Example: >>> a = torch.randn(1, 3) >>> a tensor([[ 0.1133, -0.9567, 0.2958]]) >>> torch.sum(a) tensor(-0.5475) torch.sum(input, dim, keepdim=False, dtype=None) → Tensor 返回给定维dim中input张量的每一行的总和。如果dim是维度列表，请减少所有维度。 If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s). Parameters: 输入（ Tensor） - 输入张量 昏暗（ int或 元组python：整数） - 要减少的维度或维度 keepdim （ bool） - 输出张量是否保留dim dtype （ torch.dtype ，可选） - 返回张量的所需数据类型。如果指定，则在执行操作之前将输入张量转换为dtype。这对于防止数据类型溢出很有用。默认值：无。 Example: >>> a = torch.randn(4, 4) >>> a tensor([[ 0.0569, -0.2475, 0.0737, -0.3429], [-0.2993, 0.9138, 0.9337, -1.6864], [ 0.1132, 0.7892, -0.1003, 0.5688], [ 0.3637, -0.9906, -0.4752, -1.5197]]) >>> torch.sum(a, 1) tensor([-0.4598, -0.1381, 1.3708, -2.6217]) >>> b = torch.arange(4 * 5 * 6).view(4, 5, 6) >>> torch.sum(b, (2, 1)) tensor([ 435., 1335., 2235., 3135.]) torch.unique(input, sorted=False, return_inverse=False, dim=None) 返回输入张量的唯一标量元素作为1-D张量。 Parameters: 输入（ Tensor） - 输入张量 排序（ bool） - 是否在返回作为输出之前按升序对唯一元素进行排序。 return_inverse （ bool） - 是否还返回原始输入中元素在返回的唯一列表中结束的索引。 dim （ int） - 应用唯一的维度。如果是None，则返回展平输入的唯一值。默认值：None |返回：|包含张量的张量或元组 ＆GT; 输出（ Tensor ）：唯一标量元素的输出列表。 ＆GT; inverse_indices （ Tensor ）:(可选）如果return_inverse为True，将会有第二个返回的张量（与输入相同的形状），表示原始元素的索引输入映射到输出中;否则，此函数只返回单个张量。 返回类型： （ Tensor ， Tensor （可选）） Example: >>> output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long)) >>> output tensor([ 2, 3, 1]) >>> output, inverse_indices = torch.unique( torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True) >>> output tensor([ 1, 2, 3]) >>> inverse_indices tensor([ 0, 2, 1, 2]) >>> output, inverse_indices = torch.unique( torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True) >>> output tensor([ 1, 2, 3]) >>> inverse_indices tensor([[ 0, 2], [ 1, 2]]) torch.var() torch.var(input, unbiased=True) → Tensor 返回input张量中所有元素的方差。 如果unbiased是False，则通过偏差估计器计算方差。否则，将使用贝塞尔的修正。 Parameters: 输入（ Tensor） - 输入张量 无偏（ bool） - 是否使用无偏估计 Example: >>> a = torch.randn(1, 3) >>> a tensor([[-0.3425, -1.2636, -0.4864]]) >>> torch.var(a) tensor(0.2455) torch.var(input, dim, keepdim=False, unbiased=True, out=None) → Tensor 返回给定维dim中input张量的每一行的方差。 If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the outputs tensor having 1 fewer dimension than input. If unbiased is False, then the variance will be calculated via the biased estimator. Otherwise, Bessel’s correction will be used. Parameters: 输入（ Tensor） - 输入张量 昏暗（ int） - 减少的维度 keepdim （ bool） - 输出张量是否保留dim 无偏（ bool） - 是否使用无偏估计 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4, 4) >>> a tensor([[-0.3567, 1.7385, -1.3042, 0.7423], [ 1.3436, -0.1015, -0.9834, -0.8438], [ 0.6056, 0.1089, -0.3112, -1.4085], [-0.7700, 0.6074, -0.1469, 0.7777]]) >>> torch.var(a, 1) tensor([ 1.7444, 1.1363, 0.7356, 0.5112]) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"torch_math_operations_comparison_ops.html":{"url":"torch_math_operations_comparison_ops.html","title":"Comparison Ops","keywords":"","body":"比较行动 译者：ApacheCN torch.allclose(self, other, rtol=1e-05, atol=1e-08, equal_nan=False) → bool 此函数检查所有self和other是否满足条件： 元素，对于self和other的所有元素。此函数的行为类似于 numpy.allclose 参数： 自（ 张量 ） - 首先进行张量比较 其他（ Tensor） - 第二张量来比较 atol （ 漂浮 ， 任选） - 绝对耐受。默认值：1e-08 rtol （ 漂浮 ， 任选） - 相对耐受。默认值：1e-05 equal_nan （ 漂浮 ， 任选） - 如果True，那么两个NaN s将是比较平等。默认值：False 例： >>> torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08])) False >>> torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09])) True >>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')])) False >>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True) True torch.argsort(input, dim=None, descending=False) 返回按值按升序对给定维度的张量进行排序的索引。 这是 torch.sort() 返回的第二个值。有关此方法的确切语义，请参阅其文档。 Parameters: 输入（ Tensor） - 输入张量 昏暗（ int， 可选） - 排序的维度 降序（ bool， 任选） - 控制排序顺序（升序或降序） Example: >>> a = torch.randn(4, 4) >>> a tensor([[ 0.0785, 1.5267, -0.8521, 0.4065], [ 0.1598, 0.0788, -0.0745, -1.2700], [ 1.2208, 1.0722, -0.7064, 1.2564], [ 0.0669, -0.2318, -0.8229, -0.9280]]) >>> torch.argsort(a, dim=1) tensor([[2, 0, 3, 1], [3, 2, 1, 0], [2, 1, 0, 3], [3, 2, 1, 0]]) torch.eq(input, other, out=None) → Tensor 计算元素明确的平等 第二个参数可以是数字或张量，其形状为可广播的第一个参数。 Parameters: 输入（ Tensor） - 要比较的张量 其他（ 张量 或 漂浮 ） - 张量或值比较 out （ Tensor， 可选） - 输出张量。必须是ByteTensor 返回： 在比较为真的每个位置包含1的torch.ByteTensor 返回类型： Tensor Example: >>> torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) tensor([[ 1, 0], [ 0, 1]], dtype=torch.uint8) torch.equal(tensor1, tensor2) → bool True如果两个张量具有相同的尺寸和元素，则False。 Example: >>> torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2])) True torch.ge(input, other, out=None) → Tensor 按元素计算 。 The second argument can be a number or a tensor whose shape is broadcastable with the first argument. Parameters: 输入（ Tensor） - 要比较的张量 其他（ 张量 或 漂浮 ） - 张量或值比较 out （ Tensor， 任选） - 输出张量必须是ByteTensor Returns: A torch.ByteTensor containing a 1 at each location where comparison is true Return type: Tensor Example: >>> torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) tensor([[ 1, 1], [ 0, 1]], dtype=torch.uint8) torch.gt(input, other, out=None) → Tensor 按元素计算 。 The second argument can be a number or a tensor whose shape is broadcastable with the first argument. Parameters: 输入（ Tensor） - 要比较的张量 其他（ 张量 或 漂浮 ） - 张量或值比较 out （ Tensor， 任选） - 输出张量必须是ByteTensor Returns: A torch.ByteTensor containing a 1 at each location where comparison is true Return type: Tensor Example: >>> torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) tensor([[ 0, 1], [ 0, 0]], dtype=torch.uint8) torch.isfinite(tensor) 返回一个新的张量，其布尔元素表示每个元素是否为Finite。 参数： 张量（ 张量 ） - 张量来检查 返回： torch.ByteTensor在有限元的每个位置包含1，否则为0 Return type: Tensor Example: >>> torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')])) tensor([ 1, 0, 1, 0, 0], dtype=torch.uint8) torch.isinf(tensor) 返回一个新的张量，其布尔元素表示每个元素是否为+/-INF。 Parameters: tensor (Tensor) – A tensor to check Returns: torch.ByteTensor在+/-INF元素的每个位置包含1，否则为0 Return type: Tensor Example: >>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')])) tensor([ 0, 1, 0, 1, 0], dtype=torch.uint8) torch.isnan(tensor) 返回一个新的张量，其布尔元素表示每个元素是否为NaN。 Parameters: tensor (Tensor) – A tensor to check Returns: torch.ByteTensor在NaN元素的每个位置包含1。 Return type: Tensor Example: >>> torch.isnan(torch.tensor([1, float('nan'), 2])) tensor([ 0, 1, 0], dtype=torch.uint8) torch.kthvalue(input, k, dim=None, keepdim=False, out=None) -> (Tensor, LongTensor) 返回给定维度上给定input张量的k个最小元素。 如果未给出dim，则选择input的最后一个尺寸。 返回(values, indices)的元组，其中indices是维度dim中原始input张量中第k个最小元素的索引。 如果keepdim为True，values和indices张量都与input的尺寸相同，但尺寸为dim的尺寸除外。否则，dim被挤压（见 torch.squeeze() ），导致values和indices张量的尺寸比input张量小1。 Parameters: 输入（ Tensor） - 输入张量 k （ int） - k为第k个最小元素 昏暗（ int， 可选） - 找到kth值的维度 keepdim （ bool） - 输出张量是否保留dim out （ 元组 ， 任选） - （Tensor，LongTensor）的输出元组可以任意给出用作输出缓冲区 Example: >>> x = torch.arange(1., 6.) >>> x tensor([ 1., 2., 3., 4., 5.]) >>> torch.kthvalue(x, 4) (tensor(4.), tensor(3)) >>> x=torch.arange(1.,7.).resize_(2,3) >>> x tensor([[ 1., 2., 3.], [ 4., 5., 6.]]) >>> torch.kthvalue(x,2,0,True) (tensor([[ 4., 5., 6.]]), tensor([[ 1, 1, 1]])) torch.le(input, other, out=None) → Tensor 按元素计算 。 The second argument can be a number or a tensor whose shape is broadcastable with the first argument. Parameters: 输入（ Tensor） - 要比较的张量 其他（ 张量 或 漂浮 ） - 张量或值比较 out （ Tensor， 任选） - 输出张量必须是ByteTensor Returns: A torch.ByteTensor containing a 1 at each location where comparison is true Return type: Tensor Example: >>> torch.le(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) tensor([[ 1, 0], [ 1, 1]], dtype=torch.uint8) torch.lt(input, other, out=None) → Tensor 按元素计算 。 The second argument can be a number or a tensor whose shape is broadcastable with the first argument. Parameters: 输入（ Tensor） - 要比较的张量 其他（ 张量 或 漂浮 ） - 张量或值比较 out （ Tensor， 任选） - 输出张量必须是ByteTensor Returns: A torch.ByteTensor containing a 1 at each location where comparison is true Return type: Tensor Example: >>> torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) tensor([[ 0, 0], [ 1, 0]], dtype=torch.uint8) torch.max() torch.max(input) → Tensor 返回input张量中所有元素的最大值。 Parameters: 输入（ Tensor） - 输入张量 Example: >>> a = torch.randn(1, 3) >>> a tensor([[ 0.6763, 0.7445, -2.2369]]) >>> torch.max(a) tensor(0.7445) torch.max(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor) 返回给定维dim中input张量的每一行的最大值。第二个返回值是找到的每个最大值的索引位置（argmax）。 如果keepdim为True，则输出张量与input的尺寸相同，但尺寸为dim的尺寸为1.否则，dim被挤压（参见 torch.squeeze() ），导致输出张量的尺寸比input少1。 Parameters: 输入（ Tensor） - 输入张量 昏暗（ int） - 减少的维度 keepdim （ bool） - 输出张量是否保留dim out （ 元组 ， 可选） - 两个输出张量的结果元组（max，max_indices） Example: >>> a = torch.randn(4, 4) >>> a tensor([[-1.2360, -0.2942, -0.1222, 0.8475], [ 1.1949, -1.1127, -2.2379, -0.6702], [ 1.5717, -0.9207, 0.1297, -1.8768], [-0.6172, 1.0036, -0.6060, -0.2432]]) >>> torch.max(a, 1) (tensor([ 0.8475, 1.1949, 1.5717, 1.0036]), tensor([ 3, 0, 0, 1])) torch.max(input, other, out=None) → Tensor 张量input的每个元素与张量other的对应元素进行比较，并采用逐元素最大值。 input和other的形状不需要匹配，但它们必须是可广播的。 注意 当形状不匹配时，返回的输出张量的形状遵循广播规则。 Parameters: 输入（ Tensor） - 输入张量 其他（ Tensor） - 第二个输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([ 0.2942, -0.7416, 0.2653, -0.1584]) >>> b = torch.randn(4) >>> b tensor([ 0.8722, -1.7421, -0.4141, -0.5055]) >>> torch.max(a, b) tensor([ 0.8722, -0.7416, 0.2653, -0.1584]) torch.min() torch.min(input) → Tensor 返回input张量中所有元素的最小值。 Parameters: input (Tensor) – the input tensor Example: >>> a = torch.randn(1, 3) >>> a tensor([[ 0.6750, 1.0857, 1.7197]]) >>> torch.min(a) tensor(0.6750) torch.min(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor) 返回给定维dim中input张量的每一行的最小值。第二个返回值是找到的每个最小值的索引位置（argmin）。 If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensors having 1 fewer dimension than input. Parameters: 输入（ Tensor） - 输入张量 昏暗（ int） - 减少的维度 keepdim （ bool） - 输出张量是否保留dim out （ 元组 ， 任选） - 两个输出张量的元组（min，min_indices） Example: >>> a = torch.randn(4, 4) >>> a tensor([[-0.6248, 1.1334, -1.1899, -0.2803], [-1.4644, -0.2635, -0.3651, 0.6134], [ 0.2457, 0.0384, 1.0128, 0.7015], [-0.1153, 2.9849, 2.1458, 0.5788]]) >>> torch.min(a, 1) (tensor([-1.1899, -1.4644, 0.0384, -0.1153]), tensor([ 2, 0, 1, 0])) torch.min(input, other, out=None) → Tensor 将张量input的每个元素与张量other的对应元素进行比较，并采用逐元素最小值。返回结果张量。 The shapes of input and other don’t need to match, but they must be broadcastable. Note When the shapes do not match, the shape of the returned output tensor follows the broadcasting rules. Parameters: 输入（ Tensor） - 输入张量 其他（ Tensor） - 第二个输入张量 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4) >>> a tensor([ 0.8137, -1.1740, -0.6460, 0.6308]) >>> b = torch.randn(4) >>> b tensor([-0.1369, 0.1555, 0.4019, -0.1929]) >>> torch.min(a, b) tensor([-0.1369, -1.1740, -0.6460, -0.1929]) torch.ne(input, other, out=None) → Tensor 按元素计算 。 The second argument can be a number or a tensor whose shape is broadcastable with the first argument. Parameters: 输入（ Tensor） - 要比较的张量 其他（ 张量 或 漂浮 ） - 张量或值比较 out （ Tensor， 任选） - 输出张量必须是ByteTensor Returns: 在比较为真的每个位置包含1的torch.ByteTensor。 Return type: Tensor Example: >>> torch.ne(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) tensor([[ 0, 1], [ 1, 0]], dtype=torch.uint8) torch.sort(input, dim=None, descending=False, out=None) -> (Tensor, LongTensor) 按值按升序对给定维度的input张量元素进行排序。 If dim is not given, the last dimension of the input is chosen. 如果descending是True，则元素按值按降序排序。 返回元组（sorted_tensor，sorted_indices），其中sorted_indices是原始input张量中元素的索引。 Parameters: 输入（ Tensor） - 输入张量 昏暗（ int， 可选） - 排序的维度 降序（ bool， 任选） - 控制排序顺序（升序或降序） out （ 元组 ， 任选） - （Tensor，LongTensor）的输出元组可以选择将其用作输出缓冲区 Example: >>> x = torch.randn(3, 4) >>> sorted, indices = torch.sort(x) >>> sorted tensor([[-0.2162, 0.0608, 0.6719, 2.3332], [-0.5793, 0.0061, 0.6058, 0.9497], [-0.5071, 0.3343, 0.9553, 1.0960]]) >>> indices tensor([[ 1, 0, 2, 3], [ 3, 1, 0, 2], [ 0, 3, 1, 2]]) >>> sorted, indices = torch.sort(x, 0) >>> sorted tensor([[-0.5071, -0.2162, 0.6719, -0.5793], [ 0.0608, 0.0061, 0.9497, 0.3343], [ 0.6058, 0.9553, 1.0960, 2.3332]]) >>> indices tensor([[ 2, 0, 0, 1], [ 0, 1, 1, 2], [ 1, 2, 2, 0]]) torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -> (Tensor, LongTensor) 返回给定维度上给定input张量的k最大元素。 If dim is not given, the last dimension of the input is chosen. 如果largest为False，则返回k最小元素。 返回(values, indices)元组，其中indices是原始input张量中元素的索引。 布尔选项sorted如果True，将确保返回的k元素本身已排序 Parameters: 输入（ Tensor） - 输入张量 k （ int） - “top-k”中的k 昏暗（ int， 可选） - 排序的维度 最大（ bool， 可选） - 控制是否返回最大或最小元素 排序（ bool， 可选） - 控制是否按排序顺序返回元素 out （ 元组 ， 任选） - （Tensor，LongTensor）的输出元组，可以选择性给予用作输出缓冲区 Example: >>> x = torch.arange(1., 6.) >>> x tensor([ 1., 2., 3., 4., 5.]) >>> torch.topk(x, 3) (tensor([ 5., 4., 3.]), tensor([ 4, 3, 2])) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"torch_math_operations_spectral_ops.html":{"url":"torch_math_operations_spectral_ops.html","title":"Spectral Ops","keywords":"","body":"光谱行动 译者：ApacheCN torch.fft(input, signal_ndim, normalized=False) → Tensor 复杂到复杂的离散傅立叶变换 该方法计算复数到复数的离散傅立叶变换。忽略批量维度，它计算以下表达式： 其中 = signal_ndim是信号的维数， 是信号维数 的大小。 该方法支持1D，2D和3D复杂到复合变换，由signal_ndim表示。 input必须是最后一个尺寸为2的张量，表示复数的实部和虚部，并且至少应具有signal_ndim + 1尺寸和任意数量的前导批量尺寸。如果normalized设置为True，则通过将其除以 来将结果标准化，以便操作符是单一的。 将实部和虚部一起作为input的相同形状的一个张量返回。 该函数的反函数是 ifft() 。 注意 对于CUDA张量，LRU高速缓存用于cuFFT计划，以加速在具有相同配置的相同几何的张量上重复运行FFT方法。 更改torch.backends.cuda.cufft_plan_cache.max_size（CUDA 10及更高版本上的默认值为4096，旧版本的CUDA上为1023）控制此缓存的容量。一些cuFFT计划可能会分配GPU内存。您可以使用torch.backends.cuda.cufft_plan_cache.size查询当前缓存中的计划数量，使用torch.backends.cuda.cufft_plan_cache.clear()清除缓存。 警告 对于CPU张量，此方法目前仅适用于MKL。使用torch.backends.mkl.is_available()检查是否安装了MKL。 参数： 输入（ Tensor） - 至少signal_ndim + 1维度的输入张量 signal_ndim （ int） - 每个信号中的维数。 signal_ndim只能是1,2或3 归一化（ bool， 任选） - 控制是否返回归一化结果。默认值：False 返回： 包含复数到复数傅立叶变换结果的张量 返回类型： Tensor 例： >>> # unbatched 2D FFT >>> x = torch.randn(4, 3, 2) >>> torch.fft(x, 2) tensor([[[-0.0876, 1.7835], [-2.0399, -2.9754], [ 4.4773, -5.0119]], [[-1.5716, 2.7631], [-3.8846, 5.2652], [ 0.2046, -0.7088]], [[ 1.9938, -0.5901], [ 6.5637, 6.4556], [ 2.9865, 4.9318]], [[ 7.0193, 1.1742], [-1.3717, -2.1084], [ 2.0289, 2.9357]]]) >>> # batched 1D FFT >>> torch.fft(x, 1) tensor([[[ 1.8385, 1.2827], [-0.1831, 1.6593], [ 2.4243, 0.5367]], [[-0.9176, -1.5543], [-3.9943, -2.9860], [ 1.2838, -2.9420]], [[-0.8854, -0.6860], [ 2.4450, 0.0808], [ 1.3076, -0.5768]], [[-0.1231, 2.7411], [-0.3075, -1.7295], [-0.5384, -2.0299]]]) >>> # arbitrary number of batch dimensions, 2D FFT >>> x = torch.randn(3, 3, 5, 5, 2) >>> y = torch.fft(x, 2) >>> y.shape torch.Size([3, 3, 5, 5, 2]) torch.ifft(input, signal_ndim, normalized=False) → Tensor 复数到复数的逆离散傅立叶变换 该方法计算复数到复数的离散傅里叶逆变换。忽略批量维度，它计算以下表达式： where = signal_ndim is number of dimensions for the signal, and is the size of signal dimension . 参数规范与 fft() 几乎相同。但是，如果normalized设置为True，则返回结果乘以 ，成为单一运算符。因此，要反转 fft() ，normalized参数应设置为 fft() 相同。 Returns the real and the imaginary parts together as one tensor of the same shape of input. 该函数的反函数是 fft() 。 Note For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same same configuration. Changing torch.backends.cuda.cufft_plan_cache.max_size (default is 4096 on CUDA 10 and newer, and 1023 on older CUDA versions) controls the capacity of this cache. Some cuFFT plans may allocate GPU memory. You can use torch.backends.cuda.cufft_plan_cache.size to query the number of plans currently in cache, and torch.backends.cuda.cufft_plan_cache.clear() to clear the cache. Warning For CPU tensors, this method is currently only available with MKL. Use torch.backends.mkl.is_available() to check if MKL is installed. Parameters: 输入（ Tensor） - 至少signal_ndim + 1维度的输入张量 signal_ndim （ int） - 每个信号中的维数。 signal_ndim只能是1,2或3 归一化（ bool， 任选） - 控制是否返回归一化结果。默认值：False Returns: 包含复数到复数逆傅立叶变换结果的张量 Return type: Tensor Example: >>> x = torch.randn(3, 3, 2) >>> x tensor([[[ 1.2766, 1.3680], [-0.8337, 2.0251], [ 0.9465, -1.4390]], [[-0.1890, 1.6010], [ 1.1034, -1.9230], [-0.9482, 1.0775]], [[-0.7708, -0.8176], [-0.1843, -0.2287], [-1.9034, -0.2196]]]) >>> y = torch.fft(x, 2) >>> torch.ifft(y, 2) # recover x tensor([[[ 1.2766, 1.3680], [-0.8337, 2.0251], [ 0.9465, -1.4390]], [[-0.1890, 1.6010], [ 1.1034, -1.9230], [-0.9482, 1.0775]], [[-0.7708, -0.8176], [-0.1843, -0.2287], [-1.9034, -0.2196]]]) torch.rfft(input, signal_ndim, normalized=False, onesided=True) → Tensor 实对复离散傅立叶变换 该方法计算实数到复数的离散傅立叶变换。它在数学上等同于 fft() ，仅在输入和输出的格式上有所不同。 该方法支持1D，2D和3D实对复变换，由signal_ndim表示。 input必须是具有至少signal_ndim尺寸的张量，可选择任意数量的前导批量。如果normalized设置为True，则通过将其除以 来将结果标准化，以便操作符是单一的，其中 是信号的大小维 。 实对复傅里叶变换结果遵循共轭对称： 计算指数算术的模数是相应维数的大小， 是共轭算子， = signal_ndim。 onesided标志控制是否避免输出结果中的冗余。如果设置为True（默认），输出将不是形状 的完整复杂结果，其中 是input的形状，而是最后一个尺寸将是大小 的一半。 该函数的反函数是 irfft() 。 Note For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same same configuration. Changing torch.backends.cuda.cufft_plan_cache.max_size (default is 4096 on CUDA 10 and newer, and 1023 on older CUDA versions) controls the capacity of this cache. Some cuFFT plans may allocate GPU memory. You can use torch.backends.cuda.cufft_plan_cache.size to query the number of plans currently in cache, and torch.backends.cuda.cufft_plan_cache.clear() to clear the cache. Warning For CPU tensors, this method is currently only available with MKL. Use torch.backends.mkl.is_available() to check if MKL is installed. Parameters: 输入（ Tensor） - 至少signal_ndim维度的输入张量 signal_ndim （ int） - 每个信号中的维数。 signal_ndim只能是1,2或3 归一化（ bool， 任选） - 控制是否返回归一化结果。默认值：False 单独（ bool， 可选） - 控制是否返回一半结果以避免冗余。默认值：True Returns: 包含实数到复数傅立叶变换结果的张量 Return type: Tensor Example: >>> x = torch.randn(5, 5) >>> torch.rfft(x, 2).shape torch.Size([5, 3, 2]) >>> torch.rfft(x, 2, onesided=False).shape torch.Size([5, 5, 2]) torch.irfft(input, signal_ndim, normalized=False, onesided=True, signal_sizes=None) → Tensor 复数到实数的逆离散傅立叶变换 该方法计算复数到实数的逆离散傅里叶变换。它在数学上等同于 ifft() ，仅在输入和输出的格式上有所不同。 参数规范与 ifft() 几乎相同。类似于 ifft() ，如果normalized设置为True，则通过将其与 相乘来使结果归一化，以便运算符是单一的，其中 [] 是信号维 的大小。 由于共轭对称性，input不需要包含完整的复频率值。大约一半的值就足够了， rfft() rfft(signal, onesided=True)给出input的情况就足够了。在这种情况下，将此方法的onesided参数设置为True。此外，原始信号形状信息有时会丢失，可选地将signal_sizes设置为原始信号的大小（如果处于批处理模式，则没有批量维度）以正确的形状恢复它。 因此，要反转 rfft() ，normalized和onesided参数应设置为 irfft() 相同，并且最好给出signal_sizes以避免大小不匹配。有关尺寸不匹配的情况，请参阅下面的示例。 有关共轭对称性的详细信息，请参见 rfft() 。 该函数的反函数是 rfft() 。 Warning 一般而言，此函数的输入应包含共轭对称后的值。请注意，即使onesided为True，仍然需要对某些部分进行对称。当不满足此要求时， irfft() 的行为未定义。由于 torch.autograd.gradcheck() 估计具有点扰动的数值雅可比行列式， irfft() 几乎肯定会失败。 Note For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same same configuration. Changing torch.backends.cuda.cufft_plan_cache.max_size (default is 4096 on CUDA 10 and newer, and 1023 on older CUDA versions) controls the capacity of this cache. Some cuFFT plans may allocate GPU memory. You can use torch.backends.cuda.cufft_plan_cache.size to query the number of plans currently in cache, and torch.backends.cuda.cufft_plan_cache.clear() to clear the cache. Warning For CPU tensors, this method is currently only available with MKL. Use torch.backends.mkl.is_available() to check if MKL is installed. Parameters: 输入（ Tensor） - 至少signal_ndim + 1维度的输入张量 signal_ndim （ int） - 每个信号中的维数。 signal_ndim只能是1,2或3 归一化（ bool， 任选） - 控制是否返回归一化结果。默认值：False 单独（ bool， 任选） - 控制input是否为半数以避免冗余，例如， rfft() 。默认值：True signal_sizes （列表或torch.Size，可选） - 原始信号的大小（无批量维度）。默认值：None Returns: 包含复数到实数逆傅立叶变换结果的张量 Return type: Tensor Example: >>> x = torch.randn(4, 4) >>> torch.rfft(x, 2, onesided=True).shape torch.Size([4, 3, 2]) >>> >>> # notice that with onesided=True, output size does not determine the original signal size >>> x = torch.randn(4, 5) >>> torch.rfft(x, 2, onesided=True).shape torch.Size([4, 3, 2]) >>> >>> # now we use the original shape to recover x >>> x tensor([[-0.8992, 0.6117, -1.6091, -0.4155, -0.8346], [-2.1596, -0.0853, 0.7232, 0.1941, -0.0789], [-2.0329, 1.1031, 0.6869, -0.5042, 0.9895], [-0.1884, 0.2858, -1.5831, 0.9917, -0.8356]]) >>> y = torch.rfft(x, 2, onesided=True) >>> torch.irfft(y, 2, onesided=True, signal_sizes=x.shape) # recover x tensor([[-0.8992, 0.6117, -1.6091, -0.4155, -0.8346], [-2.1596, -0.0853, 0.7232, 0.1941, -0.0789], [-2.0329, 1.1031, 0.6869, -0.5042, 0.9895], [-0.1884, 0.2858, -1.5831, 0.9917, -0.8356]]) torch.stft(input, n_fft, hop_length=None, win_length=None, window=None, center=True, pad_mode='reflect', normalized=False, onesided=True) 短时傅立叶变换（STFT）。 忽略可选批处理维度，此方法计算以下表达式： 其中 是滑动窗口的索引， 是 的频率。当onesided是默认值True时， input必须是1-D时间序列或2-D批时间序列。 如果hop_length为None（默认值），则视为等于floor(n_fft / 4)。 如果win_length为None（默认值），则视为等于n_fft。 window可以是尺寸win_length的1-D张量，例如来自 torch.hann_window() 。如果window是None（默认值），则视为在窗口中的任何地方都有 。如果 ，window将在施加之前在长度n_fft的两侧填充。 如果center为True（默认值），则input将在两侧填充，以便 帧在 时间居中。否则， - 帧在时间 开始。 pad_mode确定center为True时input上使用的填充方法。有关所有可用选项，请参阅 torch.nn.functional.pad() 。默认为\"reflect\"。 如果onesided是True（默认值），则仅返回 中 的值，因为实数到复数的傅里叶变换满足共轭对称性，即， 。 如果normalized是True（默认为False），则该函数返回标准化的STFT结果，即乘以 。 将实部和虚部一起作为一个尺寸 返回，其中 是input， 的可选批量大小是应用STFT的频率的数量， 是使用的帧的总数，并且最后维度中的每对表示作为实部和虚部的复数。 Warning 此功能在0.4.1版本上更改了签名。使用先前的签名调用可能会导致错误或返回错误的结果。 Parameters: 输入（ Tensor） - 输入张量 n_fft （ int） - 傅立叶变换的大小 hop_length （ int， 可选） - 相邻滑动窗口帧之间的距离。默认值：None（视为等于floor(n_fft / 4)） win_length （ int， 任选） - 窗口框架和STFT过滤器的大小。默认值：None（视为等于n_fft） 窗口（ Tensor， 可选） - 可选窗函数。默认值：None（被视为所有 s的窗口） 中心（ bool， 任选） - 是否在两侧垫input使 第一帧以时间 为中心。默认值：True pad_mode （ string ， 可选） - 控制center为True时使用的填充方法。默认值：\"reflect\" 归一化（ bool， 任选） - 控制是否返回归一化STFT结果默认值：False 单独（ bool， 可选） - 控制是否返回一半结果以避免冗余默认：True Returns: 包含具有上述形状的STFT结果的张量 Return type: Tensor torch.bartlett_window(window_length, periodic=True, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor 巴特利特的窗口功能。 其中 是完整的窗口大小。 输入window_length是控制返回窗口大小的正整数。 periodic标志确定返回的窗口是否从对称窗口中删除最后一个重复值，并准备用作具有 torch.stft() 等功能的周期窗口。因此，如果periodic为真，则上式中的 实际上是 。此外，我们总是torch.bartlett_window(L, periodic=True)等于torch.bartlett_window(L + 1, periodic=False)[:-1])。 Note 如果window_length ，则返回的窗口包含单个值1。 Parameters: window_length （ int） - 返回窗口的大小 周期性（ bool， 可选） - 如果为True，则返回一个窗口作为周期函数。如果为False，则返回对称窗口。 dtype （ torch.dtype ，可选） - 返回张量的所需数据类型。默认值：if None，使用全局默认值（参见 torch.set_default_tensor_type() ）。仅支持浮点类型。 布局（ torch.layout ，可选） - 返回窗口张量的理想布局。仅支持torch.strided（密集布局）。 设备（ torch.device ，可选） - 返回张量的所需设备。默认值：如果None，则使用当前设备作为默认张量类型（参见 torch.set_default_tensor_type() ）。 device将是CPU张量类型的CPU和CUDA张量类型的当前CUDA设备。 requires_grad （ bool， 可选） - 如果autograd应该记录对返回张量的操作。默认值：False。 Returns: 含有窗口的1-D张量 Return type: Tensor torch.blackman_window(window_length, periodic=True, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor 布莱克曼窗口功能。 where is the full window size. 输入window_length是控制返回窗口大小的正整数。 periodic标志确定返回的窗口是否从对称窗口中删除最后一个重复值，并准备用作具有 torch.stft() 等功能的周期窗口。因此，如果periodic为真，则上式中的 实际上是 。此外，我们总是torch.blackman_window(L, periodic=True)等于torch.blackman_window(L + 1, periodic=False)[:-1])。 Note If window_length , the returned window contains a single value 1. Parameters: window_length （ int） - 返回窗口的大小 周期性（ bool， 可选） - 如果为True，则返回一个窗口作为周期函数。如果为False，则返回对称窗口。 dtype （ torch.dtype ，可选） - 返回张量的所需数据类型。默认值：if None，使用全局默认值（参见 torch.set_default_tensor_type() ）。仅支持浮点类型。 布局（ torch.layout ，可选） - 返回窗口张量的理想布局。仅支持torch.strided（密集布局）。 设备（ torch.device ，可选） - 返回张量的所需设备。默认值：如果None，则使用当前设备作为默认张量类型（参见 torch.set_default_tensor_type() ）。 device将是CPU张量类型的CPU和CUDA张量类型的当前CUDA设备。 requires_grad （ bool， 可选） - 如果autograd应该记录对返回张量的操作。默认值：False。 Returns: A 1-D tensor of size containing the window Return type: Tensor torch.hamming_window(window_length, periodic=True, alpha=0.54, beta=0.46, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor 汉明窗功能。 where is the full window size. 输入window_length是控制返回窗口大小的正整数。 periodic标志确定返回的窗口是否从对称窗口中删除最后一个重复值，并准备用作具有 torch.stft() 等功能的周期窗口。因此，如果periodic为真，则上式中的 实际上是 。此外，我们总是torch.hamming_window(L, periodic=True)等于torch.hamming_window(L + 1, periodic=False)[:-1])。 Note If window_length , the returned window contains a single value 1. Note 这是 torch.hann_window() 的通用版本。 Parameters: window_length （ int） - 返回窗口的大小 周期性（ bool， 可选） - 如果为True，则返回一个窗口作为周期函数。如果为False，则返回对称窗口。 dtype （ torch.dtype ，可选） - 返回张量的所需数据类型。默认值：if None，使用全局默认值（参见 torch.set_default_tensor_type() ）。仅支持浮点类型。 布局（ torch.layout ，可选） - 返回窗口张量的理想布局。仅支持torch.strided（密集布局）。 设备（ torch.device ，可选） - 返回张量的所需设备。默认值：如果None，则使用当前设备作为默认张量类型（参见 torch.set_default_tensor_type() ）。 device将是CPU张量类型的CPU和CUDA张量类型的当前CUDA设备。 requires_grad （ bool， 可选） - 如果autograd应该记录对返回张量的操作。默认值：False。 Returns: A 1-D tensor of size containing the window Return type: Tensor torch.hann_window(window_length, periodic=True, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor 汉恩窗功能。 where is the full window size. 输入window_length是控制返回窗口大小的正整数。 periodic标志确定返回的窗口是否从对称窗口中删除最后一个重复值，并准备用作具有 torch.stft() 等功能的周期窗口。因此，如果periodic为真，则上式中的 实际上是 。此外，我们总是torch.hann_window(L, periodic=True)等于torch.hann_window(L + 1, periodic=False)[:-1])。 Note If window_length , the returned window contains a single value 1. Parameters: window_length （ int） - 返回窗口的大小 周期性（ bool， 可选） - 如果为True，则返回一个窗口作为周期函数。如果为False，则返回对称窗口。 dtype （ torch.dtype ，可选） - 返回张量的所需数据类型。默认值：if None，使用全局默认值（参见 torch.set_default_tensor_type() ）。仅支持浮点类型。 布局（ torch.layout ，可选） - 返回窗口张量的理想布局。仅支持torch.strided（密集布局）。 设备（ torch.device ，可选） - 返回张量的所需设备。默认值：如果None，则使用当前设备作为默认张量类型（参见 torch.set_default_tensor_type() ）。 device将是CPU张量类型的CPU和CUDA张量类型的当前CUDA设备。 requires_grad （ bool， 可选） - 如果autograd应该记录对返回张量的操作。默认值：False。 Returns: A 1-D tensor of size containing the window Return type: Tensor 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"torch_math_operations_other_ops.html":{"url":"torch_math_operations_other_ops.html","title":"Other Operations","keywords":"","body":"其他行动 译者：ApacheCN torch.bincount(self, weights=None, minlength=0) → Tensor 计算非负的int数组中每个值的频率。 除非input为空，否则箱数（大小为1）比input中的最大值大1，在这种情况下，结果是大小为0.如果指定minlength，则箱数为至少minlength并且如果input为空，则结果是填充零的大小minlength的张量。如果n是位置i的值，out[n] += weights[i]如果指定了weights，则out[n] += 1。 注意 使用CUDA后端时，此操作可能会导致不容易关闭的不确定行为。有关背景，请参阅再现性的注释。 参数： 输入（ Tensor） - 1-d int张量 权重（ Tensor） - 可选，输入张量中每个值的权重。应与输入张量大小相同。 minlength （ int） - 可选的最小二进制数。应该是非负面的。 返回： 如果input非空，则为形状张量Size([max(input) + 1])，否则为Size(0) 返回类型： 输出（ Tensor ） 例： >>> input = torch.randint(0, 8, (5,), dtype=torch.int64) >>> weights = torch.linspace(0, 1, steps=5) >>> input, weights (tensor([4, 3, 6, 3, 4]), tensor([ 0.0000, 0.2500, 0.5000, 0.7500, 1.0000]) >>> torch.bincount(input) tensor([0, 0, 0, 2, 2, 0, 1]) >>> input.bincount(weights) tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000]) torch.broadcast_tensors(*tensors) → List of Tensors 根据_broadcasting-semantics广播给定的张量。 参数： *张量 - 任何数量的相同类型的张量 Example: >>> x = torch.arange(3).view(1, 3) >>> y = torch.arange(2).view(2, 1) >>> a, b = torch.broadcast_tensors(x, y) >>> a.size() torch.Size([2, 3]) >>> a tensor([[0, 1, 2], [0, 1, 2]]) torch.cross(input, other, dim=-1, out=None) → Tensor 返回input和other的维度dim中矢量的叉积。 input和other必须具有相同的尺寸，并且dim尺寸的大小应为3。 如果未给出dim，则默认为找到大小为3的第一个维度。 Parameters: 输入（ Tensor） - 输入张量 其他（ Tensor） - 第二个输入张量 dim （ int， 任选） - 采取交叉积的维度。 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(4, 3) >>> a tensor([[-0.3956, 1.1455, 1.6895], [-0.5849, 1.3672, 0.3599], [-1.1626, 0.7180, -0.0521], [-0.1339, 0.9902, -2.0225]]) >>> b = torch.randn(4, 3) >>> b tensor([[-0.0257, -1.4725, -1.2251], [-1.1479, -0.7005, -1.9757], [-1.3904, 0.3726, -1.1836], [-0.9688, -0.7153, 0.2159]]) >>> torch.cross(a, b, dim=1) tensor([[ 1.0844, -0.5281, 0.6120], [-2.4490, -1.5687, 1.9792], [-0.8304, -1.3037, 0.5650], [-1.2329, 1.9883, 1.0551]]) >>> torch.cross(a, b) tensor([[ 1.0844, -0.5281, 0.6120], [-2.4490, -1.5687, 1.9792], [-0.8304, -1.3037, 0.5650], [-1.2329, 1.9883, 1.0551]]) torch.diag(input, diagonal=0, out=None) → Tensor 如果input是矢量（1-D张量），则返回2-D平方张量，其中input的元素作为对角线。 如果input是矩阵（2-D张量），则返回具有input的对角元素的1-D张量。 参数 diagonal 控制要考虑的对角线： 如果 diagonal = 0，则它是主对角线。 如果 diagonal > 0，它在主对角线上方。 如果 diagonal ＆lt; 0，它在主对角线下面。 Parameters: 输入（ Tensor） - 输入张量 对角线（ int， 可选） - 要考虑的对角线 out （ Tensor， 任选） - 输出张量 也可以看看 torch.diagonal() 始终返回其输入的对角线。 torch.diagflat() 总是构造一个由输入指定的对角元素的张量。 例子： 获取输入向量为对角线的方阵： >>> a = torch.randn(3) >>> a tensor([ 0.5950,-0.0872, 2.3298]) >>> torch.diag(a) tensor([[ 0.5950, 0.0000, 0.0000], [ 0.0000,-0.0872, 0.0000], [ 0.0000, 0.0000, 2.3298]]) >>> torch.diag(a, 1) tensor([[ 0.0000, 0.5950, 0.0000, 0.0000], [ 0.0000, 0.0000,-0.0872, 0.0000], [ 0.0000, 0.0000, 0.0000, 2.3298], [ 0.0000, 0.0000, 0.0000, 0.0000]]) 获取给定矩阵的第k个对角线： >>> a = torch.randn(3, 3) >>> a tensor([[-0.4264, 0.0255,-0.1064], [ 0.8795,-0.2429, 0.1374], [ 0.1029,-0.6482,-1.6300]]) >>> torch.diag(a, 0) tensor([-0.4264,-0.2429,-1.6300]) >>> torch.diag(a, 1) tensor([ 0.0255, 0.1374]) torch.diag_embed(input, offset=0, dim1=-2, dim2=-1) → Tensor 创建一个张量，其某些2D平面的对角线（由dim1和dim2指定）由input填充。为了便于创建批量对角矩阵，默认选择由返回张量的最后两个维度形成的2D平面。 参数offset控制要考虑的对角线： 如果offset = 0，则它是主对角线。 如果offset> 0，它在主对角线上方。 如果offset＆lt; 0，它在主对角线下面。 将计算新矩阵的大小以使得指定的对角线具有最后输入维度的大小。注意，对于 以外的offset，dim1和dim2的顺序很重要。交换它们相当于改变offset的符号。 将 torch.diagonal() 应用于具有相同参数的此函数的输出，将产生与输入相同的矩阵。但是， torch.diagonal() 具有不同的默认尺寸，因此需要明确指定。 Parameters: 输入（ Tensor） - 输入张量。必须至少是一维的。 偏移（ int， 任选） - 对角线考虑。默认值：0（主对角线）。 dim1 （ int， 任选） - 相对于其采取对角线的第一维度。默认值：-2。 dim2 （ int， 任选） - 相对于其采取对角线的第二维度。默认值：-1。 Example: >>> a = torch.randn(2, 3) >>> torch.diag_embed(a) tensor([[[ 1.5410, 0.0000, 0.0000], [ 0.0000, -0.2934, 0.0000], [ 0.0000, 0.0000, -2.1788]], [[ 0.5684, 0.0000, 0.0000], [ 0.0000, -1.0845, 0.0000], [ 0.0000, 0.0000, -1.3986]]]) >>> torch.diag_embed(a, offset=1, dim1=0, dim2=2) tensor([[[ 0.0000, 1.5410, 0.0000, 0.0000], [ 0.0000, 0.5684, 0.0000, 0.0000]], [[ 0.0000, 0.0000, -0.2934, 0.0000], [ 0.0000, 0.0000, -1.0845, 0.0000]], [[ 0.0000, 0.0000, 0.0000, -2.1788], [ 0.0000, 0.0000, 0.0000, -1.3986]], [[ 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000]]]) torch.diagflat(input, diagonal=0) → Tensor 如果input是矢量（1-D张量），则返回2-D平方张量，其中input的元素作为对角线。 如果input是一个具有多个维度的张量，则返回一个二维张量，其对角线元素等于一个展平的input。 The argument offset controls which diagonal to consider: 如果offset = 0，则它是主对角线。 如果offset> 0，它在主对角线上方。 如果offset＆lt; 0，它在主对角线下面。 Parameters: 输入（ Tensor） - 输入张量 偏移（ int， 任选） - 对角线考虑。默认值：0（主对角线）。 Examples: >>> a = torch.randn(3) >>> a tensor([-0.2956, -0.9068, 0.1695]) >>> torch.diagflat(a) tensor([[-0.2956, 0.0000, 0.0000], [ 0.0000, -0.9068, 0.0000], [ 0.0000, 0.0000, 0.1695]]) >>> torch.diagflat(a, 1) tensor([[ 0.0000, -0.2956, 0.0000, 0.0000], [ 0.0000, 0.0000, -0.9068, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.1695], [ 0.0000, 0.0000, 0.0000, 0.0000]]) >>> a = torch.randn(2, 2) >>> a tensor([[ 0.2094, -0.3018], [-0.1516, 1.9342]]) >>> torch.diagflat(a) tensor([[ 0.2094, 0.0000, 0.0000, 0.0000], [ 0.0000, -0.3018, 0.0000, 0.0000], [ 0.0000, 0.0000, -0.1516, 0.0000], [ 0.0000, 0.0000, 0.0000, 1.9342]]) torch.diagonal(input, offset=0, dim1=0, dim2=1) → Tensor 返回input的局部视图，其对角线元素相对于dim1和dim2作为形状末尾的尺寸附加。 The argument offset controls which diagonal to consider: 如果offset = 0，则它是主对角线。 如果offset> 0，它在主对角线上方。 如果offset＆lt; 0，它在主对角线下面。 将 torch.diag_embed() 应用于具有相同参数的此函数的输出，将生成带有输入对角线条目的对角矩阵。但是， torch.diag_embed() 具有不同的默认尺寸，因此需要明确指定。 Parameters: 输入（ Tensor） - 输入张量。必须至少是二维的。 偏移（ int， 任选） - 对角线考虑。默认值：0（主对角线）。 dim1 （ int， 任选） - 相对于其采取对角线的第一维度。默认值：0。 dim2 （ int， 任选） - 相对于其采取对角线的第二维度。默认值：1。 Note 要采用批对角线，传入dim1 = -2，dim2 = -1。 Examples: >>> a = torch.randn(3, 3) >>> a tensor([[-1.0854, 1.1431, -0.1752], [ 0.8536, -0.0905, 0.0360], [ 0.6927, -0.3735, -0.4945]]) >>> torch.diagonal(a, 0) tensor([-1.0854, -0.0905, -0.4945]) >>> torch.diagonal(a, 1) tensor([ 1.1431, 0.0360]) >>> x = torch.randn(2, 5, 4, 2) >>> torch.diagonal(x, offset=-1, dim1=1, dim2=2) tensor([[[-1.2631, 0.3755, -1.5977, -1.8172], [-1.1065, 1.0401, -0.2235, -0.7938]], [[-1.7325, -0.3081, 0.6166, 0.2335], [ 1.0500, 0.7336, -0.3836, -1.1015]]]) torch.einsum(equation, *operands) → Tensor 该函数提供了一种使用爱因斯坦求和约定来计算多线性表达式（即乘积和）的方法。 Parameters: 等式（ string ） - 该等式根据与操作数和结果的每个维度相关联的小写字母（索引）给出。左侧列出了操作数尺寸，以逗号分隔。每个张量维度应该有一个索引字母。右侧跟在-&gt;之后，并给出输出的索引。如果省略-&gt;和右侧，则它隐式地定义为在左侧恰好出现一次的所有索引的按字母顺序排序的列表。在操作数输入之后，将输出中未显示的索引求和。如果索引对同一操作数多次出现，则采用对角线。省略号…表示固定数量的维度。如果推断出右侧，则省略号维度位于输出的开头。 操作数（张量列表） - 计算爱因斯坦和的操作数。请注意，操作数作为列表传递，而不是作为单个参数传递。 Examples: >>> x = torch.randn(5) >>> y = torch.randn(4) >>> torch.einsum('i,j->ij', x, y) # outer product tensor([[-0.0570, -0.0286, -0.0231, 0.0197], [ 1.2616, 0.6335, 0.5113, -0.4351], [ 1.4452, 0.7257, 0.5857, -0.4984], [-0.4647, -0.2333, -0.1883, 0.1603], [-1.1130, -0.5588, -0.4510, 0.3838]]) >>> A = torch.randn(3,5,4) >>> l = torch.randn(2,5) >>> r = torch.randn(2,4) >>> torch.einsum('bn,anm,bm->ba', l, A, r) # compare torch.nn.functional.bilinear tensor([[-0.3430, -5.2405, 0.4494], [ 0.3311, 5.5201, -3.0356]]) >>> As = torch.randn(3,2,5) >>> Bs = torch.randn(3,5,4) >>> torch.einsum('bij,bjk->bik', As, Bs) # batch matrix multiplication tensor([[[-1.0564, -1.5904, 3.2023, 3.1271], [-1.6706, -0.8097, -0.8025, -2.1183]], [[ 4.2239, 0.3107, -0.5756, -0.2354], [-1.4558, -0.3460, 1.5087, -0.8530]], [[ 2.8153, 1.8787, -4.3839, -1.2112], [ 0.3728, -2.1131, 0.0921, 0.8305]]]) >>> A = torch.randn(3, 3) >>> torch.einsum('ii->i', A) # diagonal tensor([-0.7825, 0.8291, -0.1936]) >>> A = torch.randn(4, 3, 3) >>> torch.einsum('...ii->...i', A) # batch diagonal tensor([[-1.0864, 0.7292, 0.0569], [-0.9725, -1.0270, 0.6493], [ 0.5832, -1.1716, -1.5084], [ 0.4041, -1.1690, 0.8570]]) >>> A = torch.randn(2, 3, 4, 5) >>> torch.einsum('...ij->...ji', A).shape # batch permute torch.Size([2, 3, 5, 4]) torch.flatten(input, start_dim=0, end_dim=-1) → Tensor 在张量中展平连续的一系列变暗。 Parameters: 输入（ Tensor） - 输入张量 start_dim （ int） - 第一个暗淡变平 end_dim （ int） - 最后的暗淡变平 Example: >>> t = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) >>> torch.flatten(t) tensor([1, 2, 3, 4, 5, 6, 7, 8]) >>> torch.flatten(t, start_dim=1) tensor([[1, 2, 3, 4], [5, 6, 7, 8]]) torch.flip(input, dims) → Tensor 在dims中沿给定轴反转n-D张量的顺序。 Parameters: 输入（ Tensor） - 输入张量 暗淡（一个列表 或 元组 - 轴要翻转 Example: >>> x = torch.arange(8).view(2, 2, 2) >>> x tensor([[[ 0, 1], [ 2, 3]], [[ 4, 5], [ 6, 7]]]) >>> torch.flip(x, [0, 1]) tensor([[[ 6, 7], [ 4, 5]], [[ 2, 3], [ 0, 1]]]) torch.histc(input, bins=100, min=0, max=0, out=None) → Tensor 计算张量的直方图。 元素在 min 和 max 之间分成相等的宽度区间。如果 min 和 max 都为零，则使用数据的最小值和最大值。 Parameters: 输入（ Tensor） - 输入张量 箱（ int） - 直方图箱数 min （ int） - 范围的下限（含） max （ int） - 范围的上限（含） out （ Tensor， 任选） - 输出张量 Returns: 直方图表示为张量 Return type: Tensor Example: >>> torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3) tensor([ 0., 2., 1., 0.]) torch.meshgrid(*tensors, **kwargs) 取 张量，每个张量可以是标量或1维向量，并创建 N维网格，其中：math：`i通过扩展：math：[](#id4)i` th输入定义由其他输入定义的维度来定义网格。 Args: 张量（Tensor列表）：标量列表或1维张量。标量将被自动视为大小 的张量 Returns: seq（张量序列）：如果输入的 张量大小为 ，那么输出也会有 张量，其中所有张量均为 。 Example: &gt;&gt;&gt; x = torch.tensor([1, 2, 3]) &gt;&gt;&gt; y = torch.tensor([4, 5, 6]) &gt;&gt;&gt; grid_x, grid_y = torch.meshgrid(x, y) &gt;&gt;&gt; grid_x tensor([[1, 1, 1], [2, 2, 2], [3, 3, 3]]) &gt;&gt;&gt; grid_y tensor([[4, 5, 6], [4, 5, 6], [4, 5, 6]]) torch.renorm(input, p, dim, maxnorm, out=None) → Tensor 返回张量，其中沿着维度dim的input的每个子张量被归一化，使得子张量的p - 范数低于值maxnorm Note 如果行的范数低于maxnorm，则该行不变 Parameters: 输入（ Tensor） - 输入张量 p （ float） - 规范计算的动力 dim （ int） - 切片以获得子张量的维数 maxnorm （ float） - 保持每个子张量的最大范数 out （ Tensor， 任选） - 输出张量 Example: >>> x = torch.ones(3, 3) >>> x[1].fill_(2) tensor([ 2., 2., 2.]) >>> x[2].fill_(3) tensor([ 3., 3., 3.]) >>> x tensor([[ 1., 1., 1.], [ 2., 2., 2.], [ 3., 3., 3.]]) >>> torch.renorm(x, 1, 0, 5) tensor([[ 1.0000, 1.0000, 1.0000], [ 1.6667, 1.6667, 1.6667], [ 1.6667, 1.6667, 1.6667]]) torch.tensordot(a, b, dims=2) 返回多维度上a和b的收缩。 tensordot 实现了矩阵乘积的推广。 Parameters: a （ 张量 ） - 左张量收缩 b （ 张量 ） - 右张量收缩 暗淡（ int或 元组的两个python列表：整数） - 要收缩的维数或a和b的明确维度列表 当用整数参数dims = 调用时，a和b的维数是 和 ，它分别计算 当使用列表形式的dims调用时，将收缩给定的维度来代替a的最后 和的第一个 ] 。这些尺寸的尺寸必须匹配，但 tensordot 将处理广播尺寸。 Examples: >>> a = torch.arange(60.).reshape(3, 4, 5) >>> b = torch.arange(24.).reshape(4, 3, 2) >>> torch.tensordot(a, b, dims=([1, 0], [0, 1])) tensor([[4400., 4730.], [4532., 4874.], [4664., 5018.], [4796., 5162.], [4928., 5306.]]) >>> a = torch.randn(3, 4, 5, device='cuda') >>> b = torch.randn(4, 5, 6, device='cuda') >>> c = torch.tensordot(a, b, dims=2).cpu() tensor([[ 8.3504, -2.5436, 6.2922, 2.7556, -1.0732, 3.2741], [ 3.3161, 0.0704, 5.0187, -0.4079, -4.3126, 4.8744], [ 0.8223, 3.9445, 3.2168, -0.2400, 3.4117, 1.7780]]) torch.trace(input) → Tensor 返回输入2-D矩阵的对角线元素的总和。 Example: >>> x = torch.arange(1., 10.).view(3, 3) >>> x tensor([[ 1., 2., 3.], [ 4., 5., 6.], [ 7., 8., 9.]]) >>> torch.trace(x) tensor(15.) torch.tril(input, diagonal=0, out=None) → Tensor 返回矩阵的下三角部分（2-D张量）input，结果张量out的其他元素设置为0。 矩阵的下三角形部分被定义为对角线上和下方的元素。 参数 diagonal 控制要考虑的对角线。如果 diagonal = 0，则保留主对角线上和下方的所有元素。正值包括主对角线上方的对角线数量，同样负值也不包括主对角线下方的对角线数量。主对角线是 的指数 的集合，其中 是基质的维度。 Parameters: 输入（ Tensor） - 输入张量 对角线（ int， 可选） - 要考虑的对角线 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(3, 3) >>> a tensor([[-1.0813, -0.8619, 0.7105], [ 0.0935, 0.1380, 2.2112], [-0.3409, -0.9828, 0.0289]]) >>> torch.tril(a) tensor([[-1.0813, 0.0000, 0.0000], [ 0.0935, 0.1380, 0.0000], [-0.3409, -0.9828, 0.0289]]) >>> b = torch.randn(4, 6) >>> b tensor([[ 1.2219, 0.5653, -0.2521, -0.2345, 1.2544, 0.3461], [ 0.4785, -0.4477, 0.6049, 0.6368, 0.8775, 0.7145], [ 1.1502, 3.2716, -1.1243, -0.5413, 0.3615, 0.6864], [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024, 0.0978]]) >>> torch.tril(b, diagonal=1) tensor([[ 1.2219, 0.5653, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.4785, -0.4477, 0.6049, 0.0000, 0.0000, 0.0000], [ 1.1502, 3.2716, -1.1243, -0.5413, 0.0000, 0.0000], [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024, 0.0000]]) >>> torch.tril(b, diagonal=-1) tensor([[ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.4785, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 1.1502, 3.2716, 0.0000, 0.0000, 0.0000, 0.0000], [-0.0614, -0.7344, -1.3164, 0.0000, 0.0000, 0.0000]]) torch.triu(input, diagonal=0, out=None) → Tensor 返回矩阵的上三角部分（2-D张量）input，结果张量out的其他元素设置为0。 矩阵的上三角形部分被定义为对角线上方和上方的元素。 参数 diagonal 控制要考虑的对角线。如果 diagonal = 0，则保留主对角线上和下方的所有元素。正值排除了主对角线上方的对角线数量，同样负值也包括主对角线下方的对角线数量。主对角线是 的指数 的集合，其中 是基质的维度。 Parameters: 输入（ Tensor） - 输入张量 对角线（ int， 可选） - 要考虑的对角线 out （ Tensor， 任选） - 输出张量 Example: >>> a = torch.randn(3, 3) >>> a tensor([[ 0.2309, 0.5207, 2.0049], [ 0.2072, -1.0680, 0.6602], [ 0.3480, -0.5211, -0.4573]]) >>> torch.triu(a) tensor([[ 0.2309, 0.5207, 2.0049], [ 0.0000, -1.0680, 0.6602], [ 0.0000, 0.0000, -0.4573]]) >>> torch.triu(a, diagonal=1) tensor([[ 0.0000, 0.5207, 2.0049], [ 0.0000, 0.0000, 0.6602], [ 0.0000, 0.0000, 0.0000]]) >>> torch.triu(a, diagonal=-1) tensor([[ 0.2309, 0.5207, 2.0049], [ 0.2072, -1.0680, 0.6602], [ 0.0000, -0.5211, -0.4573]]) >>> b = torch.randn(4, 6) >>> b tensor([[ 0.5876, -0.0794, -1.8373, 0.6654, 0.2604, 1.5235], [-0.2447, 0.9556, -1.2919, 1.3378, -0.1768, -1.0857], [ 0.4333, 0.3146, 0.6576, -1.0432, 0.9348, -0.4410], [-0.9888, 1.0679, -1.3337, -1.6556, 0.4798, 0.2830]]) >>> torch.tril(b, diagonal=1) tensor([[ 0.5876, -0.0794, 0.0000, 0.0000, 0.0000, 0.0000], [-0.2447, 0.9556, -1.2919, 0.0000, 0.0000, 0.0000], [ 0.4333, 0.3146, 0.6576, -1.0432, 0.0000, 0.0000], [-0.9888, 1.0679, -1.3337, -1.6556, 0.4798, 0.0000]]) >>> torch.tril(b, diagonal=-1) tensor([[ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [-0.2447, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.4333, 0.3146, 0.0000, 0.0000, 0.0000, 0.0000], [-0.9888, 1.0679, -1.3337, 0.0000, 0.0000, 0.0000]]) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"torch_math_operations_blas_lapack_ops.html":{"url":"torch_math_operations_blas_lapack_ops.html","title":"BLAS and LAPACK Operations","keywords":"","body":"BLAS和LAPACK操作 译者：ApacheCN torch.addbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) → Tensor 执行存储在batch1和batch2中的矩阵的批量矩阵 - 矩阵乘积，减少加法步骤（所有矩阵乘法沿第一维积累）。 mat被添加到最终结果中。 batch1和batch2必须是3-D张量，每个张量包含相同数量的矩阵。 如果batch1是 张量，batch2是 张量，mat必须是可广播和 张量和out将是 张量。 对于FloatTensor或DoubleTensor类型的输入，参数beta和alpha必须是实数，否则它们应该是整数。 参数： beta （编号 ， 任选） - mat（ ）的乘数 mat （ Tensor） - 要添加的基质 alpha （数 ， 任选） - batch1 @ batch2（ ）的乘数 batch1 （ Tensor） - 第一批要乘的矩阵 batch2 （ Tensor） - 第二批矩阵被乘以 out （ Tensor， 任选） - 输出张量 例： >>> M = torch.randn(3, 5) >>> batch1 = torch.randn(10, 3, 4) >>> batch2 = torch.randn(10, 4, 5) >>> torch.addbmm(M, batch1, batch2) tensor([[ 6.6311, 0.0503, 6.9768, -12.0362, -2.1653], [ -4.8185, -1.4255, -6.6760, 8.9453, 2.5743], [ -3.8202, 4.3691, 1.0943, -1.1109, 5.4730]]) torch.addmm(beta=1, mat, alpha=1, mat1, mat2, out=None) → Tensor 执行矩阵mat1和mat2的矩阵乘法。矩阵mat被添加到最终结果中。 如果mat1是 张量，mat2是 张量，那么mat必须是可广播和 [] 张量和out将是 张量。 alpha和beta分别是mat1和：attr mat2与添加的基质mat之间的基质 - 载体产物的比例因子。 For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers. Parameters: beta （编号 ， 任选） - mat（ ）的乘数 mat （ Tensor） - 要添加的基质 alpha （编号 ， 任选） - （ ）的乘数 mat1 （ Tensor） - 第一个被乘法的矩阵 mat2 （ Tensor） - 要倍增的第二个矩阵 out （ Tensor， 任选） - 输出张量 Example: >>> M = torch.randn(2, 3) >>> mat1 = torch.randn(2, 3) >>> mat2 = torch.randn(3, 3) >>> torch.addmm(M, mat1, mat2) tensor([[-4.8716, 1.4671, -1.3746], [ 0.7573, -3.9555, -2.8681]]) torch.addmv(beta=1, tensor, alpha=1, mat, vec, out=None) → Tensor 执行矩阵mat和向量vec的矩阵向量乘积。将载体 tensor 添加到最终结果中。 如果mat是 张量，vec是大小为m的1-D张量，则 tensor 必须是可广播具有1-D张量的n和out将是1-D张量的大小n。 alpha和beta分别是mat和vec之间的基质 - 载体产物和加入的张量 tensor 的比例因子。 对于FloatTensor或DoubleTensor类型的输入，参数beta和alpha必须是实数，否则它们应该是整数 Parameters: beta （， 任选） - tensor （ ）的乘数 张量（ Tensor） - 要添加的载体 alpha （编号 ， 任选） - （ ）的乘数 mat （ Tensor） - 矩阵成倍增加 vec （ Tensor） - 载体倍增 out （ Tensor， 任选） - 输出张量 Example: >>> M = torch.randn(2) >>> mat = torch.randn(2, 3) >>> vec = torch.randn(3) >>> torch.addmv(M, mat, vec) tensor([-0.3768, -5.5565]) torch.addr(beta=1, mat, alpha=1, vec1, vec2, out=None) → Tensor 执行向量vec1和vec2的外积并将其添加到矩阵mat。 可选值beta和alpha分别是vec1和vec2之间的外积和添加的矩阵mat的缩放因子。 如果vec1是大小为n的矢量而vec2是大小为m的矢量，那么mat必须是可广播的，其大小为矩阵 和out将是大小 的基质。 For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers Parameters: beta （编号 ， 任选） - mat（ ）的乘数 mat （ Tensor） - 要添加的基质 alpha （编号 ， 任选） - （ ）的乘数 vec1 （ Tensor） - 外部产品的第一个载体 vec2 （ Tensor） - 外产品的第二个载体 out （ Tensor， 任选） - 输出张量 Example: >>> vec1 = torch.arange(1., 4.) >>> vec2 = torch.arange(1., 3.) >>> M = torch.zeros(3, 2) >>> torch.addr(M, vec1, vec2) tensor([[ 1., 2.], [ 2., 4.], [ 3., 6.]]) torch.baddbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) → Tensor 在batch1和batch2中执行矩阵的批量矩阵 - 矩阵乘积。 mat被添加到最终结果中。 batch1 and batch2 must be 3-D tensors each containing the same number of matrices. 如果batch1是 张量，batch2是 张量，那么mat必须是可广播和 [] 张量和out将是 张量。 alpha和beta均与 torch.addbmm() 中使用的比例因子相同。 For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers. Parameters: beta （编号 ， 任选） - mat（ ）的乘数 垫（ Tensor） - 要加的张量 alpha （编号 ， 任选） - （ ）的乘数 batch1 （ Tensor） - 第一批要乘的矩阵 batch2 （ Tensor） - 第二批矩阵被乘以 out （ Tensor， 任选） - 输出张量 Example: >>> M = torch.randn(10, 3, 5) >>> batch1 = torch.randn(10, 3, 4) >>> batch2 = torch.randn(10, 4, 5) >>> torch.baddbmm(M, batch1, batch2).size() torch.Size([10, 3, 5]) torch.bmm(batch1, batch2, out=None) → Tensor 执行存储在batch1和batch2中的矩阵的批量矩阵 - 矩阵乘积。 batch1 and batch2 must be 3-D tensors each containing the same number of matrices. 如果batch1是 张量，batch2是 张量，out将是 张量。 注意 此功能不广播。有关广播矩阵产品，请参阅 torch.matmul() 。 Parameters: batch1 （ Tensor） - 第一批要乘的矩阵 batch2 （ Tensor） - 第二批矩阵被乘以 out （ Tensor， 任选） - 输出张量 Example: >>> batch1 = torch.randn(10, 3, 4) >>> batch2 = torch.randn(10, 4, 5) >>> res = torch.bmm(batch1, batch2) >>> res.size() torch.Size([10, 3, 5]) torch.btrifact(A, info=None, pivot=True) 批量LU分解。 返回包含LU分解和枢轴的元组。如果设置了pivot，则完成旋转。 如果每个minibatch示例的分解成功，则可选参数info存储信息。 info作为IntTensor提供，其值将从dgetrf填充，非零值表示发生错误。具体来说，如果使用cuda，则值来自cublas，否则为LAPACK。 警告 info参数不推荐使用 torch.btrifact_with_info() 。 Parameters: A （ 张量 ） - 因子的张量 info （ IntTensor ， 可选） - （弃用）IntTensor存储指示分解是否成功的值 pivot （ bool， 可选） - 控制是否完成旋转 返回： 包含分解和枢轴的元组。 Example: >>> A = torch.randn(2, 3, 3) >>> A_LU, pivots = torch.btrifact(A) >>> A_LU tensor([[[ 1.3506, 2.5558, -0.0816], [ 0.1684, 1.1551, 0.1940], [ 0.1193, 0.6189, -0.5497]], [[ 0.4526, 1.2526, -0.3285], [-0.7988, 0.7175, -0.9701], [ 0.2634, -0.9255, -0.3459]]]) >>> pivots tensor([[ 3, 3, 3], [ 3, 3, 3]], dtype=torch.int32) torch.btrifact_with_info(A, pivot=True) -> (Tensor, IntTensor, IntTensor) 批量LU分解和其他错误信息。 这是 torch.btrifact() 的一个版本，它始终创建一个info IntTensor，并将其作为第三个返回值返回。 Parameters: A （ 张量 ） - 因子的张量 pivot （ bool， 可选） - 控制是否完成旋转 Returns: 包含因式分解，枢轴和IntTensor的元组，其中非零值表示每个小批量样本的分解是否成功。 Example: >>> A = torch.randn(2, 3, 3) >>> A_LU, pivots, info = A.btrifact_with_info() >>> if info.nonzero().size(0) == 0: >>> print('LU factorization succeeded for all samples!') LU factorization succeeded for all samples! torch.btrisolve(b, LU_data, LU_pivots) → Tensor 批量LU解决。 返回线性系统 的LU求解。 Parameters: b （ 张量 ） - RHS张量 LU_data （ Tensor） - 来自 btrifact() 的A的旋转LU分解。 LU_pivots （ IntTensor ） - LU分解的关键点 Example: >>> A = torch.randn(2, 3, 3) >>> b = torch.randn(2, 3) >>> A_LU = torch.btrifact(A) >>> x = torch.btrisolve(b, *A_LU) >>> torch.norm(torch.bmm(A, x.unsqueeze(2)) - b.unsqueeze(2)) tensor(1.00000e-07 * 2.8312) torch.btriunpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True) 从张量的分段LU分解（btrifact）解包数据和枢轴。 返回张量的元组作为(the pivots, the L tensor, the U tensor)。 Parameters: LU_data （ Tensor） - 打包的LU分解数据 LU_pivots （ Tensor） - 打包的LU分解枢轴 unpack_data （ bool） - 指示数据是否应解包的标志 unpack_pivots （ bool） - 指示枢轴是否应解包的标志 Example: >>> A = torch.randn(2, 3, 3) >>> A_LU, pivots = A.btrifact() >>> P, A_L, A_U = torch.btriunpack(A_LU, pivots) >>> >>> # can recover A from factorization >>> A_ = torch.bmm(P, torch.bmm(A_L, A_U)) torch.chain_matmul(*matrices) 返回 2-D张量的矩阵乘积。使用矩阵链序算法有效地计算该乘积，该算法选择在算术运算方面产生最低成本的顺序（ [CLRS] ）。请注意，由于这是计算产品的函数， 需要大于或等于2;如果等于2，则返回一个平凡的矩阵 - 矩阵乘积。如果 为1，那么这是一个无操作 - 原始矩阵按原样返回。 参数： 矩阵（张量... ） - 2个或更多个2-D张量的序列，其产物将被确定。 返回： 如果 张量具有 的维度，则产物的尺寸为 。 返回类型： Tensor Example: >>> a = torch.randn(3, 4) >>> b = torch.randn(4, 5) >>> c = torch.randn(5, 6) >>> d = torch.randn(6, 7) >>> torch.chain_matmul(a, b, c, d) tensor([[ -2.3375, -3.9790, -4.1119, -6.6577, 9.5609, -11.5095, -3.2614], [ 21.4038, 3.3378, -8.4982, -5.2457, -10.2561, -2.4684, 2.7163], [ -0.9647, -5.8917, -2.3213, -5.2284, 12.8615, -12.2816, -2.5095]]) torch.cholesky(A, upper=False, out=None) → Tensor 计算对称正定矩阵 的Cholesky分解或对称批正对称正定矩阵。 如果upper为True，则返回的矩阵U为上三角形，分解的形式为： 如果upper为False，则返回的矩阵L为低三角形，分解的形式为： 如果upper是True，并且A是一批对称正定矩阵，则返回的张量将由每个单独矩阵的上三角形Cholesky因子组成。类似地，当upper是False时，返回的张量将由每个单个矩阵的下三角形Cholesky因子组成。 Parameters: a （ 张量 ） - 输入张量大小（ * ，n，n）其中*为零或更多批由对称正定矩阵组成的维数。 上（ bool， 可选） - 表示是否返回上下三角矩阵的标志。默认值：False out （ Tensor， 可选） - 输出矩阵 Example: >>> a = torch.randn(3, 3) >>> a = torch.mm(a, a.t()) # make symmetric positive-definite >>> l = torch.cholesky(a) >>> a tensor([[ 2.4112, -0.7486, 1.4551], [-0.7486, 1.3544, 0.1294], [ 1.4551, 0.1294, 1.6724]]) >>> l tensor([[ 1.5528, 0.0000, 0.0000], [-0.4821, 1.0592, 0.0000], [ 0.9371, 0.5487, 0.7023]]) >>> torch.mm(l, l.t()) tensor([[ 2.4112, -0.7486, 1.4551], [-0.7486, 1.3544, 0.1294], [ 1.4551, 0.1294, 1.6724]]) >>> a = torch.randn(3, 2, 2) >>> a = torch.matmul(a, a.transpose(-1, -2)) + 1e-03 # make symmetric positive-definite >>> l = torch.cholesky(a) >>> z = torch.matmul(l, l.transpose(-1, -2)) >>> torch.max(torch.abs(z - a)) # Max non-zero tensor(2.3842e-07) torch.dot(tensor1, tensor2) → Tensor 计算两个张量的点积（内积）。 Note 此功能不广播。 Example: >>> torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1])) tensor(7) torch.eig(a, eigenvectors=False, out=None) -> (Tensor, Tensor) 计算实方阵的特征值和特征向量。 Parameters: a （ Tensor） - 形状 的方阵，其特征值和特征向量将被计算 特征向量（ bool） - True计算特征值和特征向量;否则，只计算特征值 out （ 元组 ， 任选） - 输出张量 |返回：|包含元组的元组 ＆GT; e （张量）：形状 。每行是a的特征值，其中第一个元素是实部，第二个元素是虚部。特征值不一定是有序的。 ＆GT; v （ Tensor ）：如果eigenvectors=False，它是一个空张量。否则，该张量形状 可用于计算相应特征值e的归一化（单位长度）特征向量，如下所述。如果对应的e [j]是实数，则列v [：，j]是对应于特征值e [j]的特征向量。如果相应的e [j]和e [j + 1]特征值形成复共轭对，那么真实的特征向量可以被计算为 ， 。 返回类型： （ Tensor ， Tensor ） torch.gels(B, A, out=None) → Tensor 计算大小 的全秩矩阵 和大小的矩阵 的最小二乘和最小范数问题的解决方案 。 如果 ， gels() 解决了最小二乘问题： 如果 ， gels() 解决了最小范数问题： 返回张量 具有 的形状。 的第一 行包含该溶液。如果 ，则每列中溶液的残余平方和由该列的剩余 行中的元素的平方和给出。 Parameters: B （ 张量 ） - 基质 （ 张量 ） - out （ 元组 ， 可选） - 可选目的地张量 |返回：|包含以下内容的元组： ＆GT; X （张量）：最小二乘解＆gt; qr （ Tensor ）：QR分解的细节 Return type: (Tensor, Tensor) Note 无论输入矩阵的步幅如何，返回的矩阵将始终被转置。也就是说，他们将有(1, m)而不是(m, 1)。 Example: >>> A = torch.tensor([[1., 1, 1], [2, 3, 4], [3, 5, 2], [4, 2, 5], [5, 4, 3]]) >>> B = torch.tensor([[-10., -3], [ 12, 14], [ 14, 12], [ 16, 16], [ 18, 16]]) >>> X, _ = torch.gels(B, A) >>> X tensor([[ 2.0000, 1.0000], [ 1.0000, 1.0000], [ 1.0000, 2.0000], [ 10.9635, 4.8501], [ 8.9332, 5.2418]]) torch.geqrf(input, out=None) -> (Tensor, Tensor) 这是一个直接调用LAPACK的低级函数。 您通常希望使用 torch.qr() 。 计算input的QR分解，但不构造 和 作为显式单独的矩阵。 相反，这直接调用底层LAPACK函数?geqrf，它产生一系列“基本反射器”。 有关详细信息，请参阅geqrf 的 LAPACK文档。 Parameters: 输入（ Tensor） - 输入矩阵 out （ 元组 ， 可选） - 输出元组（Tensor，Tensor） torch.ger(vec1, vec2, out=None) → Tensor vec1和vec2的外产物。如果vec1是大小 的载体，vec2是大小 的载体，那么out必须是大小的矩阵 。 Note This function does not broadcast. Parameters: vec1 （ 张量 ） - 1-D输入向量 vec2 （ 张量 ） - 1-D输入向量 out （ Tensor， 可选） - 可选输出矩阵 Example: >>> v1 = torch.arange(1., 5.) >>> v2 = torch.arange(1., 4.) >>> torch.ger(v1, v2) tensor([[ 1., 2., 3.], [ 2., 4., 6.], [ 3., 6., 9.], [ 4., 8., 12.]]) torch.gesv(B, A) -> (Tensor, Tensor) 该函数将解决方案返回到由 表示的线性方程组和A的LU分解，按顺序作为元组X, LU。 LU包含A的LU分解的L和U因子。 torch.gesv(B, A)可以接收2D输入B, A或两批2D矩阵的输入。如果输入是批次，则返回批量输出X, LU。 Note out关键字仅支持2D矩阵输入，即B, A必须是2D矩阵。 Note 不管原始步幅如何，返回的矩阵X和LU将被转置，即分别具有诸如B.contiguous().transpose(-1, -2).strides()和A.contiguous().transpose(-1, -2).strides()的步幅。 Parameters: B （ Tensor） - 大小 的输入矩阵，其中 为零或批量维度更多。 A （ 张量 ） - 输入方形矩阵 ，其中 为零或更多批量维度。 出（（ 张量 ， 张量 ]）__， 可选） - 可选输出元组。 Example: >>> A = torch.tensor([[6.80, -2.11, 5.66, 5.97, 8.23], [-6.05, -3.30, 5.36, -4.44, 1.08], [-0.45, 2.58, -2.70, 0.27, 9.04], [8.32, 2.71, 4.35, -7.17, 2.14], [-9.67, -5.14, -7.26, 6.08, -6.87]]).t() >>> B = torch.tensor([[4.02, 6.19, -8.22, -7.57, -3.03], [-1.56, 4.00, -8.67, 1.75, 2.86], [9.81, -4.09, -4.57, -8.61, 8.99]]).t() >>> X, LU = torch.gesv(B, A) >>> torch.dist(B, torch.mm(A, X)) tensor(1.00000e-06 * 7.0977) >>> # Batched solver example >>> A = torch.randn(2, 3, 1, 4, 4) >>> B = torch.randn(2, 3, 1, 4, 6) >>> X, LU = torch.gesv(B, A) >>> torch.dist(B, A.matmul(X)) tensor(1.00000e-06 * 3.6386) torch.inverse(input, out=None) → Tensor 采用方阵input的倒数。 input可以是2D方形张量的批次，在这种情况下，该函数将返回由单个反转组成的张量。 Note 无论原始步幅如何，返回的张量都将被转置，即像input.contiguous().transpose(-2, -1).strides()这样的步幅 Parameters: 输入（ Tensor） - 输入张量大小（ * ，n，n）其中*为零或更多批尺寸 out （ Tensor， 可选） - 可选输出张量 Example: >>> x = torch.rand(4, 4) >>> y = torch.inverse(x) >>> z = torch.mm(x, y) >>> z tensor([[ 1.0000, -0.0000, -0.0000, 0.0000], [ 0.0000, 1.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 1.0000, 0.0000], [ 0.0000, -0.0000, -0.0000, 1.0000]]) >>> torch.max(torch.abs(z - torch.eye(4))) # Max non-zero tensor(1.1921e-07) >>> # Batched inverse example >>> x = torch.randn(2, 3, 4, 4) >>> y = torch.inverse(x) >>> z = torch.matmul(x, y) >>> torch.max(torch.abs(z - torch.eye(4).expand_as(x))) # Max non-zero tensor(1.9073e-06) torch.det(A) → Tensor 计算2D平方张量的行列式。 Note 当A不可逆时，向后通过 det() 在内部使用SVD结果。在这种情况下，当A没有明显的奇异值时，通过 det() 的双向后将是不稳定的。有关详细信息，请参阅 svd() 。 Parameters: A （ Tensor） - 输入2D平方张量 Example: >>> A = torch.randn(3, 3) >>> torch.det(A) tensor(3.7641) torch.logdet(A) → Tensor 计算2D平方张量的对数行列式。 Note 如果A具有零对数行列式，则结果为-inf，如果A具有负的行列式，则结果为nan。 Note 当A不可逆时，向后通过 logdet() 在内部使用SVD结果。在这种情况下，当A没有明显的奇异值时，通过 logdet() 的双向后将是不稳定的。有关详细信息，请参阅 svd() 。 Parameters: A (Tensor) – The input 2D square tensor Example: >>> A = torch.randn(3, 3) >>> torch.det(A) tensor(0.2611) >>> torch.logdet(A) tensor(-1.3430) torch.slogdet(A) -> (Tensor, Tensor) 计算2D平方张量的行列式的符号和对数值。 Note 如果A的行列式为零，则返回(0, -inf)。 Note 当A不可逆时，向后通过 slogdet() 在内部使用SVD结果。在这种情况下，当A没有明显的奇异值时，通过 slogdet() 的双向后将是不稳定的。有关详细信息，请参阅 svd() 。 Parameters: A (Tensor) – The input 2D square tensor Returns: 包含行列式符号的元组，以及绝对行列式的对数值。 Example: >>> A = torch.randn(3, 3) >>> torch.det(A) tensor(-4.8215) >>> torch.logdet(A) tensor(nan) >>> torch.slogdet(A) (tensor(-1.), tensor(1.5731)) torch.matmul(tensor1, tensor2, out=None) → Tensor 两个张量的矩阵乘积。 行为取决于张量的维度如下： 如果两个张量都是1维的，则返回点积（标量）。 如果两个参数都是二维的，则返回矩阵 - 矩阵乘积。 如果第一个参数是1维且第二个参数是2维，则为了矩阵乘法的目的，在其维度之前加1。在矩阵乘法之后，移除前置维度。 如果第一个参数是2维且第二个参数是1维，则返回矩阵向量乘积。 如果两个参数都是至少一维的并且至少一个参数是N维的（其中N> 2），则返回批量矩阵乘法。如果第一个参数是1维的，则为了批量矩阵的目的，将1加在其维度之前，然后将其删除。如果第二个参数是1维的，则为了批处理矩阵的多个目的，将1附加到其维度，并在之后删除。非矩阵（即批量）维度是广播（因此必须是可广播的）。例如，如果tensor1是 张量而tensor2是 张量，out将是 张量。 Note 此功能的1维点积版本不支持out参数。 Parameters: tensor1 （ Tensor） - 第一个要乘的张量 tensor2 （ Tensor） - 要增加的第二个张量 out （ Tensor， 任选） - 输出张量 Example: >>> # vector x vector >>> tensor1 = torch.randn(3) >>> tensor2 = torch.randn(3) >>> torch.matmul(tensor1, tensor2).size() torch.Size([]) >>> # matrix x vector >>> tensor1 = torch.randn(3, 4) >>> tensor2 = torch.randn(4) >>> torch.matmul(tensor1, tensor2).size() torch.Size([3]) >>> # batched matrix x broadcasted vector >>> tensor1 = torch.randn(10, 3, 4) >>> tensor2 = torch.randn(4) >>> torch.matmul(tensor1, tensor2).size() torch.Size([10, 3]) >>> # batched matrix x batched matrix >>> tensor1 = torch.randn(10, 3, 4) >>> tensor2 = torch.randn(10, 4, 5) >>> torch.matmul(tensor1, tensor2).size() torch.Size([10, 3, 5]) >>> # batched matrix x broadcasted matrix >>> tensor1 = torch.randn(10, 3, 4) >>> tensor2 = torch.randn(4, 5) >>> torch.matmul(tensor1, tensor2).size() torch.Size([10, 3, 5]) torch.matrix_power(input, n) → Tensor 返回为矩形矩阵提升到幂n的矩阵。对于一批矩阵，每个单独的矩阵被提升到功率n。 如果n为负，则矩阵的反转（如果可逆）将升至功率n。对于一批矩阵，批量反转（如果可逆）则上升到功率n。如果n为0，则返回单位矩阵。 Parameters: 输入（ Tensor） - 输入张量 n （ int） - 将矩阵提升到 Example: >>> a = torch.randn(2, 2, 2) >>> a tensor([[[-1.9975, -1.9610], [ 0.9592, -2.3364]], [[-1.2534, -1.3429], [ 0.4153, -1.4664]]]) >>> torch.matrix_power(a, 3) tensor([[[ 3.9392, -23.9916], [ 11.7357, -0.2070]], [[ 0.2468, -6.7168], [ 2.0774, -0.8187]]]) torch.matrix_rank(input, tol=None, bool symmetric=False) → Tensor 返回二维张量的数值等级。默认情况下，使用SVD完成计算矩阵秩的方法。如果symmetric是True，则假设input是对称的，并且通过获得特征值来完成秩的计算。 tol是一个阈值，低于该阈值时，奇异值（或symmetric为True时的特征值）被认为是0.如果未指定tol，则tol设置为S.max() * max(S.size()) * eps，其中S ]是奇异值（或symmetric为True时的特征值），eps是input数据类型的epsilon值。 Parameters: 输入（ Tensor） - 输入2-D张量 tol （ float， 任选） - 耐受值。默认值：None 对称（ bool， 任选） - 表示input是否对称。默认值：False Example: >>> a = torch.eye(10) >>> torch.matrix_rank(a) tensor(10) >>> b = torch.eye(10) >>> b[0, 0] = 0 >>> torch.matrix_rank(b) tensor(9) torch.mm(mat1, mat2, out=None) → Tensor 执行矩阵mat1和mat2的矩阵乘法。 如果mat1是 张量，mat2是 张量，out将是 张量。 Note This function does not broadcast. For broadcasting matrix products, see torch.matmul(). Parameters: mat1 （ Tensor） - 第一个被乘法的矩阵 mat2 （ Tensor） - 要倍增的第二个矩阵 out （ Tensor， 任选） - 输出张量 Example: >>> mat1 = torch.randn(2, 3) >>> mat2 = torch.randn(3, 3) >>> torch.mm(mat1, mat2) tensor([[ 0.4851, 0.5037, -0.3633], [-0.0760, -3.6705, 2.4784]]) torch.mv(mat, vec, out=None) → Tensor 执行矩阵mat和向量vec的矩阵向量乘积。 如果mat是 张量，vec是1-D张量大小 ，out将是1-D大小 [] 。 Note This function does not broadcast. Parameters: mat （ Tensor） - 矩阵成倍增加 vec （ Tensor） - 载体倍增 out （ Tensor， 任选） - 输出张量 Example: >>> mat = torch.randn(2, 3) >>> vec = torch.randn(3) >>> torch.mv(mat, vec) tensor([ 1.0404, -0.6361]) torch.orgqr(a, tau) → Tensor 从 torch.geqrf() 返回的(a, tau)元组计算QR分解的正交矩阵Q。 这直接调用底层LAPACK函数?orgqr。有关详细信息，请参阅orgqr 的 LAPACK文档。 Parameters: a （ 张量 ） - 来自 torch.geqrf() 的a。 tau （ 张量 ） - 来自 torch.geqrf() 的tau。 torch.ormqr(a, tau, mat, left=True, transpose=False) -> (Tensor, Tensor) 将mat乘以由(a, tau)表示的 torch.geqrf() 形成的QR分解的正交Q矩阵。 这直接调用底层LAPACK函数?ormqr。有关详细信息，请参阅ormqr 的 LAPACK文档。 Parameters: a （ 张量 ） - 来自 torch.geqrf() 的a。 tau （ 张量 ） - 来自 torch.geqrf() 的tau。 mat （ Tensor） - 要倍增的矩阵。 torch.pinverse(input, rcond=1e-15) → Tensor 计算2D张量的伪逆（也称为Moore-Penrose逆）。有关详细信息，请查看 Moore-Penrose逆 Note 该方法使用奇异值分解来实现。 Note 伪逆不一定是矩阵 [1] 的元素中的连续函数。因此，衍生物并不总是存在，只存在于恒定等级 [2] 。但是，由于使用SVD结果实现，此方法可以反向使用，并且可能不稳定。由于在内部使用SVD，双向后也将不稳定。有关详细信息，请参阅 svd() 。 Parameters: 输入（ Tensor） - 维度 的输入2D张量 rcond （ float） - 一个浮点值，用于确定小奇异值的截止值。默认值：1e-15 Returns: 维度 的input的伪逆 Example: >>> input = torch.randn(3, 5) >>> input tensor([[ 0.5495, 0.0979, -1.4092, -0.1128, 0.4132], [-1.1143, -0.3662, 0.3042, 1.6374, -0.9294], [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]]) >>> torch.pinverse(input) tensor([[ 0.0600, -0.1933, -0.2090], [-0.0903, -0.0817, -0.4752], [-0.7124, -0.1631, -0.2272], [ 0.1356, 0.3933, -0.5023], [-0.0308, -0.1725, -0.5216]]) torch.potrf(a, upper=True, out=None) 计算对称正定矩阵 的Cholesky分解。 有关 torch.potrf() 的更多信息，请查看 torch.cholesky() 。 Warning torch.potrf不赞成使用torch.cholesky，将在下一个版本中删除。请改用torch.cholesky并注意torch.cholesky中的upper参数默认为False。 torch.potri(u, upper=True, out=None) → Tensor 计算正半定矩阵的倒数，给出其Cholesky因子u：返回矩阵inv 如果upper为True或未提供，则u为上三角形，使得返回的张量为 如果upper为False，则u为下三角形，使得返回的张量为 Parameters: u （ Tensor） - 输入2-D张量，上下三角Cholesky因子 上（ bool， 可选） - 是否返回上限（默认）或下三角矩阵 out （ Tensor， 任选） - inv的输出张量 Example: >>> a = torch.randn(3, 3) >>> a = torch.mm(a, a.t()) # make symmetric positive definite >>> u = torch.cholesky(a) >>> a tensor([[ 0.9935, -0.6353, 1.5806], [ -0.6353, 0.8769, -1.7183], [ 1.5806, -1.7183, 10.6618]]) >>> torch.potri(u) tensor([[ 1.9314, 1.2251, -0.0889], [ 1.2251, 2.4439, 0.2122], [-0.0889, 0.2122, 0.1412]]) >>> a.inverse() tensor([[ 1.9314, 1.2251, -0.0889], [ 1.2251, 2.4439, 0.2122], [-0.0889, 0.2122, 0.1412]]) torch.potrs(b, u, upper=True, out=None) → Tensor 求解具有正半定矩阵的线性方程组，给定其Cholesky因子矩阵u。 如果upper为True或未提供，则u为上三角形并返回c，以便： 如果upper为False，则u为下三角形并返回c，以便： torch.potrs(b, u)可以接收2D输入b, u或两批2D矩阵的输入。如果输入是批次，则返回批量输出c Note out关键字仅支持2D矩阵输入，即b, u必须是2D矩阵。 Parameters: b （ 张量 ） - 大小 的输入矩阵，其中 为零或批量维度更多 u （ Tensor） - 大小为 的输入矩阵，其中 为零更多批量尺寸由上部或下部三角形Cholesky因子组成 上（ bool， 可选） - 是否返回上限（默认）或下三角矩阵 out （ Tensor， 任选） - c的输出张量 Example: >>> a = torch.randn(3, 3) >>> a = torch.mm(a, a.t()) # make symmetric positive definite >>> u = torch.cholesky(a) >>> a tensor([[ 0.7747, -1.9549, 1.3086], [-1.9549, 6.7546, -5.4114], [ 1.3086, -5.4114, 4.8733]]) >>> b = torch.randn(3, 2) >>> b tensor([[-0.6355, 0.9891], [ 0.1974, 1.4706], [-0.4115, -0.6225]]) >>> torch.potrs(b,u) tensor([[ -8.1625, 19.6097], [ -5.8398, 14.2387], [ -4.3771, 10.4173]]) >>> torch.mm(a.inverse(),b) tensor([[ -8.1626, 19.6097], [ -5.8398, 14.2387], [ -4.3771, 10.4173]]) torch.pstrf(a, upper=True, out=None) -> (Tensor, Tensor) 计算正半定矩阵a的旋转Cholesky分解。返回矩阵u和piv。 如果upper为True或未提供，则u为上三角形，使得 ，p为piv给出的置换。 如果upper为False，则u为三角形，使得 。 Parameters: a （ 张量 ） - 输入二维张量 上（ bool， 可选） - 是否返回上限（默认）或下三角矩阵 out （ 元组 ， 任选） - u和piv张量的元组 Example: >>> a = torch.randn(3, 3) >>> a = torch.mm(a, a.t()) # make symmetric positive definite >>> a tensor([[ 3.5405, -0.4577, 0.8342], [-0.4577, 1.8244, -0.1996], [ 0.8342, -0.1996, 3.7493]]) >>> u,piv = torch.pstrf(a) >>> u tensor([[ 1.9363, 0.4308, -0.1031], [ 0.0000, 1.8316, -0.2256], [ 0.0000, 0.0000, 1.3277]]) >>> piv tensor([ 2, 0, 1], dtype=torch.int32) >>> p = torch.eye(3).index_select(0,piv.long()).index_select(0,piv.long()).t() # make pivot permutation >>> torch.mm(torch.mm(p.t(),torch.mm(u.t(),u)),p) # reconstruct tensor([[ 3.5405, -0.4577, 0.8342], [-0.4577, 1.8244, -0.1996], [ 0.8342, -0.1996, 3.7493]]) torch.qr(input, out=None) -> (Tensor, Tensor) 计算矩阵input的QR分解，并返回矩阵Q和R，使 ， 为正交矩阵和 [] 是一个上三角矩阵。 这将返回瘦（减少）QR分解。 Note 如果input的元素的大小很大，则精度可能会丢失 Note 虽然它应该总是给你一个有效的分解，它可能不会跨平台给你相同的 - 它将取决于你的LAPACK实现。 Note 不管原始步幅如何，返回的基质 将被转置，即步幅为(1, m)而不是(m, 1)。 Parameters: 输入（ Tensor） - 输入2-D张量 out （ 元组 ， 任选） - Q和R张量的元组 Example: >>> a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]]) >>> q, r = torch.qr(a) >>> q tensor([[-0.8571, 0.3943, 0.3314], [-0.4286, -0.9029, -0.0343], [ 0.2857, -0.1714, 0.9429]]) >>> r tensor([[ -14.0000, -21.0000, 14.0000], [ 0.0000, -175.0000, 70.0000], [ 0.0000, 0.0000, -35.0000]]) >>> torch.mm(q, r).round() tensor([[ 12., -51., 4.], [ 6., 167., -68.], [ -4., 24., -41.]]) >>> torch.mm(q.t(), q).round() tensor([[ 1., 0., 0.], [ 0., 1., -0.], [ 0., -0., 1.]]) torch.svd(input, some=True, compute_uv=True, out=None) -> (Tensor, Tensor, Tensor) U, S, V = torch.svd(A)返回大小为(n x m)的实矩阵A的奇异值分解，使得 。 U具有 的形状。 S是形状 的对角矩阵，表示为包含非负对角线条目的大小 的向量。 V具有 的形状。 如果some为True（默认值），则返回的U和V矩阵将仅包含 正交列。 如果compute_uv是False，则返回的U和V矩阵将分别为形状 和 的零矩阵。这里将忽略some。 Note 在CPU上实现SVD使用LAPACK例程?gesdd（分而治之算法）而不是?gesvd来提高速度。类似地，GPU上的SVD也使用MAGMA例程gesdd。 Note 不管原始步幅如何，返回的矩阵U将被转置，即用步幅(1, n)代替(n, 1)。 Note 向后通过U和V输出时需要特别小心。当input具有所有不同的奇异值的满秩时，这种操作实际上是稳定的。否则，NaN可能会出现，因为未正确定义渐变。此外，请注意，即使原始后向仅在S上，双向后通常会通过U和V向后进行。 Note 当some = False时，U[:, min(n, m):]和V[:, min(n, m):]上的梯度将被反向忽略，因为这些矢量可以是子空间的任意基数。 Note 当compute_uv = False时，由于后向操作需要前向通道的U和V，因此无法执行后退操作。 Parameters: 输入（ Tensor） - 输入2-D张量 一些（ bool， 任选） - 控制返回U和V的形状 out （ 元组 ， 任选） - 张量的输出元组 Example: >>> a = torch.tensor([[8.79, 6.11, -9.15, 9.57, -3.49, 9.84], [9.93, 6.91, -7.93, 1.64, 4.02, 0.15], [9.83, 5.04, 4.86, 8.83, 9.80, -8.99], [5.45, -0.27, 4.85, 0.74, 10.00, -6.02], [3.16, 7.98, 3.01, 5.80, 4.27, -5.31]]).t() >>> u, s, v = torch.svd(a) >>> u tensor([[-0.5911, 0.2632, 0.3554, 0.3143, 0.2299], [-0.3976, 0.2438, -0.2224, -0.7535, -0.3636], [-0.0335, -0.6003, -0.4508, 0.2334, -0.3055], [-0.4297, 0.2362, -0.6859, 0.3319, 0.1649], [-0.4697, -0.3509, 0.3874, 0.1587, -0.5183], [ 0.2934, 0.5763, -0.0209, 0.3791, -0.6526]]) >>> s tensor([ 27.4687, 22.6432, 8.5584, 5.9857, 2.0149]) >>> v tensor([[-0.2514, 0.8148, -0.2606, 0.3967, -0.2180], [-0.3968, 0.3587, 0.7008, -0.4507, 0.1402], [-0.6922, -0.2489, -0.2208, 0.2513, 0.5891], [-0.3662, -0.3686, 0.3859, 0.4342, -0.6265], [-0.4076, -0.0980, -0.4933, -0.6227, -0.4396]]) >>> torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t())) tensor(1.00000e-06 * 9.3738) torch.symeig(input, eigenvectors=False, upper=True, out=None) -> (Tensor, Tensor) 该函数返回实对称矩阵input的特征值和特征向量，由元组 表示。 input和 是 基质， 是 维向量。 该函数计算input的所有特征值（和向量），使得 。 布尔参数eigenvectors仅定义特征向量或特征值的计算。 如果是False，则仅计算特征值。如果是True，则计算特征值和特征向量。 由于输入矩阵input应该是对称的，因此默认情况下仅使用上三角形部分。 如果upper是False，则使用下三角形部分。 注意：无论原始步幅如何，返回的矩阵V都将被转置，即使用步幅(1, m)而不是(m, 1)。 Parameters: 输入（ Tensor） - 输入对称矩阵 特征向量（布尔 ， 可选） - 控制是否必须计算特征向量 上（布尔 ， 可选） - 控制是否考虑上三角或下三角区域 out （ 元组 ， 可选） - 输出元组（Tensor，Tensor） | Returns: | A tuple containing ＆GT; e （张量）：形状 。每个元素是input的特征值，特征值按升序排列。 ＆GT; V （张量）：形状 。如果eigenvectors=False，它是一个充满零的张量。否则，该张量包含input的标准正交特征向量。 Return type: (Tensor, Tensor) 例子： >>> a = torch.tensor([[ 1.96, 0.00, 0.00, 0.00, 0.00], [-6.49, 3.80, 0.00, 0.00, 0.00], [-0.47, -6.39, 4.17, 0.00, 0.00], [-7.20, 1.50, -1.51, 5.70, 0.00], [-0.65, -6.34, 2.67, 1.80, -7.10]]).t() >>> e, v = torch.symeig(a, eigenvectors=True) >>> e tensor([-11.0656, -6.2287, 0.8640, 8.8655, 16.0948]) >>> v tensor([[-0.2981, -0.6075, 0.4026, -0.3745, 0.4896], [-0.5078, -0.2880, -0.4066, -0.3572, -0.6053], [-0.0816, -0.3843, -0.6600, 0.5008, 0.3991], [-0.0036, -0.4467, 0.4553, 0.6204, -0.4564], [-0.8041, 0.4480, 0.1725, 0.3108, 0.1622]]) torch.trtrs(b, A, upper=True, transpose=False, unitriangular=False) -> (Tensor, Tensor) 求解具有三角系数矩阵 和多个右侧b的方程组。 特别是，解决 并假设 是默认关键字参数的上三角形。 Parameters: A （ Tensor） - 输入三角系数矩阵 b （ Tensor） - 多个右侧。 的每一列是方程组的右侧。 上（ bool， 可选） - 是否解决上三角方程组（默认）或者下三角方程组。默认值：True。 转座（ bool， 任选） - 是否应转置在被送到解算器之前。默认值：False。 三角（ bool， 任选） - 是单位三角形。如果为True，则假定 的对角元素为1，并且未参考 。默认值：False。 Returns: 元组 其中 是 和 的克隆是[的解决方案] （或等式系统的任何变体，取决于关键字参数。） Shape: 答： b： 输出[0]： 输出[1]： Examples: >>> A = torch.randn(2, 2).triu() >>> A tensor([[ 1.1527, -1.0753], [ 0.0000, 0.7986]]) >>> b = torch.randn(2, 3) >>> b tensor([[-0.0210, 2.3513, -1.5492], [ 1.5429, 0.7403, -1.0243]]) >>> torch.trtrs(b, A) (tensor([[ 1.7840, 2.9045, -2.5405], [ 1.9319, 0.9269, -1.2826]]), tensor([[ 1.1527, -1.0753], [ 0.0000, 0.7986]])) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"tensors.html":{"url":"tensors.html","title":"torch.Tensor","keywords":"","body":"torch.Tensor 译者：hijkzzz torch.Tensor 是一种包含单一数据类型元素的多维矩阵. Torch定义了八种CPU张量类型和八种GPU张量类型： Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor torch.Tensor 是默认的tensor类型 (torch.FloatTensor) 的简称. Tensor 可以用torch.tensor()转换Python的 list 或序列​​生成： >>> torch.tensor([[1., -1.], [1., -1.]]) tensor([[ 1.0000, -1.0000], [ 1.0000, -1.0000]]) >>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]])) tensor([[ 1, 2, 3], [ 4, 5, 6]]) 警告 torch.tensor() 总是拷贝 data. 如果你有一个 Tensor data 并且仅仅想改变它的 requires_grad 属性, 可用 requires_grad_() or detach() 来避免拷贝. 如果你有一个 numpy 数组并且想避免拷贝, 请使用 torch.as_tensor(). 指定数据类型的Tensor可以通过传递参数 torch.dtype 和/或者 torch.device 到构造函数生成： >>> torch.zeros([2, 4], dtype=torch.int32) tensor([[ 0, 0, 0, 0], [ 0, 0, 0, 0]], dtype=torch.int32) >>> cuda0 = torch.device('cuda:0') >>> torch.ones([2, 4], dtype=torch.float64, device=cuda0) tensor([[ 1.0000, 1.0000, 1.0000, 1.0000], [ 1.0000, 1.0000, 1.0000, 1.0000]], dtype=torch.float64, device='cuda:0') Tensor的内容可以通过Python索引或者切片访问以及修改： >>> x = torch.tensor([[1, 2, 3], [4, 5, 6]]) >>> print(x[1][2]) tensor(6) >>> x[0][1] = 8 >>> print(x) tensor([[ 1, 8, 3], [ 4, 5, 6]]) 使用 torch.Tensor.item() 从只有一个值的Tensor中获取Python Number： >>> x = torch.tensor([[1]]) >>> x tensor([[ 1]]) >>> x.item() 1 >>> x = torch.tensor(2.5) >>> x tensor(2.5000) >>> x.item() 2.5 Tensor可以通过参数 requires_grad=True 创建, 这样 torch.autograd 会记录相关的运算实现自动求导. >>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True) >>> out = x.pow(2).sum() >>> out.backward() >>> x.grad tensor([[ 2.0000, -2.0000], [ 2.0000, 2.0000]]) 每一个tensor都有一个相应的 torch.Storage 保存其数据. tensor 类提供了一个多维的、strided视图, 并定义了数值操作. 注意 更多关于 torch.dtype, torch.device, 和 torch.layout 等 torch.Tensor的属性, 见 Tensor Attributes. 注意 注意：修改tensor的方法可以用一个下划线后缀来标示.比如, torch.FloatTensor.abs_() 会在原地计算绝对值并返回修改的张量, 而 torch.FloatTensor.abs() 将会在新张量中计算结果. 注意 为了改变已有的 tensor 的 torch.device 和/或者 torch.dtype, 考虑使用 to() 方法. class torch.Tensor 这里有少数几种生成Tensor的方法, 取决于你的实际情况. 从已经存在的数据生成, 用 torch.tensor(). 生成特殊尺寸的Tensor, 用 torch.* creation ops (见 Creation Ops). 生成与其它Tensor尺寸相同的Tensor (并且数据类型相同), 用 torch.*_like creation ops (见 Creation Ops). 生成与其它Tesor数据类型相同但是尺寸不同的Tensor, 用 tensor.new_* creation ops. new_tensor(data, dtype=None, device=None, requires_grad=False) → Tensor 返回一个新的Tensor用 data 作为tensor data.默认情况下, 返回的Tensor有相同的 torch.dtype 和 torch.device . 警告 new_tensor() 总是拷贝 data. 如果 你有一个 Tensor data 并且想避免拷贝, 使用 torch.Tensor.requires_grad_() 或者 torch.Tensor.detach(). 如果你有一个 numpy 数组并且想避免拷贝, 使用 torch.from_numpy(). 警告 当 data 是一个 tensor x, new_tensor() 读取 x 的 ‘data’ 并且创建一个叶子变量. 因此 tensor.new_tensor(x) 等价于 x.clone().detach() 并且 tensor.new_tensor(x, requires_grad=True) 等价于 x.clone().detach().requires_grad_(True). 推荐使用 clone() 和 detach(). 参数: data (array_like) – 返回的 Tensor 拷贝 data. dtype (torch.dtype, 可选) – 期望返回的Tensor的数据类型. 默认值: 如果是 None, 等于 torch.dtype. device (torch.device, 可选) – 期望返回的Tesor所在设备. 默认值: 如果是 None, 等于 torch.device. requires_grad (bool, 可选) – 是否为自动求导记录相关的运算. 默认值: False. 例子: >>> tensor = torch.ones((2,), dtype=torch.int8) >>> data = [[0, 1], [2, 3]] >>> tensor.new_tensor(data) tensor([[ 0, 1], [ 2, 3]], dtype=torch.int8) new_full(size, fill_value, dtype=None, device=None, requires_grad=False) → Tensor 返回一个Tesnor的尺寸等于 size 用 fill_value填充. 默认情况下, 返回的 Tensor 具有与此Tensor相同的 torch.dtype 和 torch.device. 参数: fill_value (scalar) – 用于填充的数值. dtype (torch.dtype, 可选) – 期望返回的Tensor的数据类型. 默认值: 如果是 None, 等于 torch.dtype. device (torch.device, 可选) – 期望返回的Tesor所在设备. 默认值: 如果是 None, 等于 torch.device. requires_grad (bool, 可选) – 是否为自动求导记录相关的运算. 默认值: False. 例子: >>> tensor = torch.ones((2,), dtype=torch.float64) >>> tensor.new_full((3, 4), 3.141592) tensor([[ 3.1416, 3.1416, 3.1416, 3.1416], [ 3.1416, 3.1416, 3.1416, 3.1416], [ 3.1416, 3.1416, 3.1416, 3.1416]], dtype=torch.float64) new_empty(size, dtype=None, device=None, requires_grad=False) → Tensor 返回一个Tesnor的尺寸等于 size 用 未初始化的值填充. 默认情况下, 返回的 Tensor 具有与此Tensor相同的 torch.dtype 和 torch.device. Parameters: dtype (torch.dtype, 可选) – 期望返回的Tensor的数据类型. 默认值: 如果是 None, 等于 torch.dtype. device (torch.device, 可选) – 期望返回的Tesor所在设备. 默认值: 如果是 None, 等于 torch.device. requires_grad (bool, 可选) – 是否为自动求导记录相关的运算. 默认值: False. Example: >>> tensor = torch.ones(()) >>> tensor.new_empty((2, 3)) tensor([[ 5.8182e-18, 4.5765e-41, -1.0545e+30], [ 3.0949e-41, 4.4842e-44, 0.0000e+00]]) new_ones(size, dtype=None, device=None, requires_grad=False) → Tensor 返回一个Tesnor的尺寸等于 size 用 1填充. 默认情况下, 返回的 Tensor 具有与此Tensor相同的 torch.dtype 和 torch.device. Parameters: size (int...) – list, tuple, 或者 torch.Size 定义了输出Tensor的形状. dtype (torch.dtype, 可选) – 期望返回的Tensor的数据类型. 默认值: 如果是 None, 等于 torch.dtype. device (torch.device, 可选) – 期望返回的Tesor所在设备. 默认值: 如果是 None, 等于 torch.device. requires_grad (bool, 可选) – 是否为自动求导记录相关的运算. 默认值: False. 例子: >>> tensor = torch.tensor((), dtype=torch.int32) >>> tensor.new_ones((2, 3)) tensor([[ 1, 1, 1], [ 1, 1, 1]], dtype=torch.int32) new_zeros(size, dtype=None, device=None, requires_grad=False) → Tensor 返回一个Tesnor的尺寸等于 size 用 0填充. 默认情况下, 返回的 Tensor 具有与此Tensor相同的 torch.dtype 和 torch.device. 参数: size (int...) – list, tuple, 或者 torch.Size 定义了输出Tensor的形状. dtype (torch.dtype, 可选) – 期望返回的Tensor的数据类型. 默认值: 如果是 None, 等于 torch.dtype. device (torch.device, 可选) – 期望返回的Tesor所在设备. 默认值: 如果是 None, 等于 torch.device. requires_grad (bool, 可选) – 是否为自动求导记录相关的运算. 默认值: False. 例子: >>> tensor = torch.tensor((), dtype=torch.float64) >>> tensor.new_zeros((2, 3)) tensor([[ 0., 0., 0.], [ 0., 0., 0.]], dtype=torch.float64) is_cuda True 如果 Tensor 在 GPU 上, 否则 False. device torch.device Tensor 所在的设备. abs() → Tensor 见 torch.abs() abs_() → Tensor 原地版本的 abs() acos() → Tensor 见 torch.acos() acos_() → Tensor 原地版本的 acos() add(value) → Tensor add(value=1, other) -> Tensor 见 torch.add() add_(value) → Tensor add_(value=1, other) -> Tensor 原地版本的 add() addbmm(beta=1, mat, alpha=1, batch1, batch2) → Tensor 见 torch.addbmm() addbmm_(beta=1, mat, alpha=1, batch1, batch2) → Tensor 原地版本的 addbmm() addcdiv(value=1, tensor1, tensor2) → Tensor 见 torch.addcdiv() addcdiv_(value=1, tensor1, tensor2) → Tensor 原地版本的 addcdiv() addcmul(value=1, tensor1, tensor2) → Tensor 见 torch.addcmul() addcmul_(value=1, tensor1, tensor2) → Tensor 原地版本的 addcmul() addmm(beta=1, mat, alpha=1, mat1, mat2) → Tensor 见 torch.addmm() addmm_(beta=1, mat, alpha=1, mat1, mat2) → Tensor 原地版本的 addmm() addmv(beta=1, tensor, alpha=1, mat, vec) → Tensor 见 torch.addmv() addmv_(beta=1, tensor, alpha=1, mat, vec) → Tensor 原地版本的 addmv() addr(beta=1, alpha=1, vec1, vec2) → Tensor 见 torch.addr() addr_(beta=1, alpha=1, vec1, vec2) → Tensor 原地版本的 addr() allclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) → Tensor 见 torch.allclose() apply_(callable) → Tensor 应用函数 callable 到Tensor中的每一个元素, 用 callable的返回值替换每一个元素. 注意 这个函数仅仅能在CPU上工作, 并且不要用于需要高性能的代码区域. argmax(dim=None, keepdim=False) 见 torch.argmax() argmin(dim=None, keepdim=False) 见 torch.argmin() asin() → Tensor 见 torch.asin() asin_() → Tensor 原地版本的 asin() atan() → Tensor 见 torch.atan() atan2(other) → Tensor 见 torch.atan2() atan2_(other) → Tensor 原地版本的 atan2() atan_() → Tensor 原地版本的 atan() baddbmm(beta=1, alpha=1, batch1, batch2) → Tensor 见 torch.baddbmm() baddbmm_(beta=1, alpha=1, batch1, batch2) → Tensor 原地版本的 baddbmm() bernoulli(*, generator=None) → Tensor 返回一个Tensor, 每一个 都是独立采样于 . self 必须是浮点型 dtype, 并且返回值有相同的 dtype. 见 torch.bernoulli() bernoulli_() bernoulli_(p=0.5, *, generator=None) → Tensor 从 独立采样填充 self 的每一个位置.self 可以是整型 dtype. bernoulli_(p_tensor, *, generator=None) → Tensor p_tensor 必须是一个包含概率的 Tensor 用于取得二元随机数. self tensor 的 元素将会被设置为采样于 的值. self 可以有整型 dtype, 但是 :attrp_tensor 必须有浮点型 dtype. 可参考 bernoulli() and torch.bernoulli() bmm(batch2) → Tensor 见 torch.bmm() byte() → Tensor self.byte() is equivalent to self.to(torch.uint8). See to(). btrifact(info=None, pivot=True) 见 torch.btrifact() btrifact_with_info(pivot=True) -> (Tensor, Tensor, Tensor) 见 torch.btrifact_with_info() btrisolve(LU_data, LU_pivots) → Tensor 见 torch.btrisolve() cauchy_(median=0, sigma=1, *, generator=None) → Tensor 用取自 Cauchy 分布得值填充Tensor: ceil() → Tensor 见 torch.ceil() ceil_() → Tensor 原地版本的 ceil() char() → Tensor self.char() 等价于 self.to(torch.int8). 见 to(). cholesky(upper=False) → Tensor 见 torch.cholesky() chunk(chunks, dim=0) → List of Tensors 见 torch.chunk() clamp(min, max) → Tensor 见 torch.clamp() clamp_(min, max) → Tensor 原地版本的 clamp() clone() → Tensor 返回一份拷贝的 self tensor. 这份拷贝有 self 相同的数据和类型. 注意 与copy_()不同, 此函数会被记录在计算图中. 传给克隆tensor的梯度将传播到原始tensor. contiguous() → Tensor 返回一个连续的得Tensor, 其data与 self 相同. 如果 self tensor 是连续的, 此函数返回 self tensor 自身. copy_(src, non_blocking=False) → Tensor 从 src 拷贝元素到 self tensor 然后返回 self. src tensor 必须与 self tensor 是 broadcastable. 但数据类型可以不同, 所在的设备也可以不同. 参数: src (Tensor) – 源 tensor non_blocking (bool) – 如果是 True 并且这次复制在 CPU 和 GPU 之间进行, 这次复制将会是异步的. 其他情况则没有影响. cos() → Tensor 见 torch.cos() cos_() → Tensor 原地版本的 cos() cosh() → Tensor 见 torch.cosh() cosh_() → Tensor 原地版本的 cosh() cpu() → Tensor 返回一个拷贝对象于 CPU 内存中. 如果这个对象已经在 CPU 内存中, 并且在者正确的设备上, 那么将会返回其本身. cross(other, dim=-1) → Tensor 见 torch.cross() cuda(device=None, non_blocking=False) → Tensor 返回一个拷贝对象于 CUDA 内存中. 如果这个对象已经在 CUDA 内存中, 并且在者正确的设备上, 那么将会返回其本身. 参数: device (torch.device) –目标GPU设备. 默认值是当前GPU. non_blocking (bool) – 如果是 True 并且源在pinned memory中, 这次拷贝将是异步的.否则此参数没有影响. 默认值: False. cumprod(dim, dtype=None) → Tensor 见 torch.cumprod() cumsum(dim, dtype=None) → Tensor 见 torch.cumsum() data_ptr() → int 返回 self tensor 的第一个元素的指针. det() → Tensor 见 torch.det() diag(diagonal=0) → Tensor 见 torch.diag() diag_embed(offset=0, dim1=-2, dim2=-1) → Tensor 见 torch.diag_embed() dim() → int 返回 self tensor 的维度. dist(other, p=2) → Tensor 见 torch.dist() div(value) → Tensor 见 torch.div() div_(value) → Tensor 原地版本的 div() dot(tensor2) → Tensor 见 torch.dot() double() → Tensor self.double() 等价于 self.to(torch.float64). 见 to(). eig(eigenvectors=False) -> (Tensor, Tensor) 见 torch.eig() element_size() → int 返回每个元素占用的字节数 Example: >>> torch.tensor([]).element_size() 4 >>> torch.tensor([], dtype=torch.uint8).element_size() 1 eq(other) → Tensor 见 torch.eq() eq_(other) → Tensor 原地版本的 eq() equal(other) → bool 见 torch.equal() erf() → Tensor 见 torch.erf() erf_() → Tensor 原地版本的 erf() erfc() → Tensor 见 torch.erfc() erfc_() → Tensor 原地版本的 erfc() erfinv() → Tensor 见 torch.erfinv() erfinv_() → Tensor 原地版本的 erfinv() exp() → Tensor 见 torch.exp() exp_() → Tensor 原地版本的 exp() expm1() → Tensor 见 torch.expm1() expm1_() → Tensor 原地版本的 expm1() expand(*sizes) → Tensor 返回一个新的 self tensor 的视图, 其中单一维度扩展到更大的尺寸. 传递-1意味着不改变该维度的大小. tensor 也可以扩展到更大的维度, 新的维度将会附加在前面.对于新维度, 其大小不能设置为- 1. 扩展张量不会分配新的内存, 但只会在现有张量上创建一个新的视图, 其中通过将stride设置为0, 第一个尺寸的维度会扩展到更大的尺寸.大小为1的任何维度都可以扩展到任意值, 而无需分配新内存. 参数: *sizes (torch.Size or int...) – 期望扩展的尺寸 例子: >>> x = torch.tensor([[1], [2], [3]]) >>> x.size() torch.Size([3, 1]) >>> x.expand(3, 4) tensor([[ 1, 1, 1, 1], [ 2, 2, 2, 2], [ 3, 3, 3, 3]]) >>> x.expand(-1, 4) # -1 意味着不会改变该维度 tensor([[ 1, 1, 1, 1], [ 2, 2, 2, 2], [ 3, 3, 3, 3]]) expand_as(other) → Tensor 扩展这个 tensor 使得其尺寸和 other 相同. self.expand_as(other) 等价于 self.expand(other.size()). 请看 expand() 获得更多关于 expand 的信息. 参数: other (torch.Tensor) – 返回的 tensor 的尺寸和 other. 相同 exponential_(lambd=1, *, generator=None) → Tensor 用取自 exponential 分布 的元素填充 self tensor : fill_(value) → Tensor 用指定的值填充 self. flatten(input, start_dim=0, end_dim=-1) → Tensor 见 torch.flatten() flip(dims) → Tensor 见 torch.flip() float() → Tensor self.float() 等价于 self.to(torch.float32). See to(). floor() → Tensor 见 torch.floor() floor_() → Tensor 原地版本的 floor() fmod(divisor) → Tensor 见 torch.fmod() fmod_(divisor) → Tensor 原地版本的 fmod() frac() → Tensor 见 torch.frac() frac_() → Tensor 原地版本的 frac() gather(dim, index) → Tensor 见 torch.gather() ge(other) → Tensor 见 torch.ge() ge_(other) → Tensor 原地版本的 ge() gels(A) → Tensor 见 torch.gels() geometric_(p, *, generator=None) → Tensor 用取自geometric 分布的值填充 self : geqrf() -> (Tensor, Tensor) 见 torch.geqrf() ger(vec2) → Tensor 见 torch.ger() gesv(A) → Tensor, Tensor 见 torch.gesv() get_device() -> Device ordinal (Integer) 对于 CUDA tensors, 这个函数返回一个 GPU 序号, 对应 tensor 所在的设备. 对于 CPU tensors, 抛出一个错误. Example: >>> x = torch.randn(3, 4, 5, device='cuda:0') >>> x.get_device() 0 >>> x.cpu().get_device() # 运行时错误: get_device 没有在 torch.FloatTensor 上实现 gt(other) → Tensor 见 torch.gt() gt_(other) → Tensor 原地版本的 gt() half() → Tensor self.half() 等价于 self.to(torch.float16). 见 to(). histc(bins=100, min=0, max=0) → Tensor 见 torch.histc() index_add_(dim, index, tensor) → Tensor 根据参数index 中的索引的顺序, 累加 tensor 中的元素到 self tensor, 例如, 如果 dim == 0 并且 index[i] == j, 则第 i 行 tensor 会被加到第 j行. tensor 第 dim 维度 必须和 index(必须是一个向量) 的长度相同, 并且其它维度必须和 self 匹配, 否则将会抛出一个错误. 注意 当使用 CUDA 作为后端, 这个操作可能导致不确定性行为, 且不容易关闭. 请看 Reproducibility. Parameters: dim (int) – 要索引的维度 index (LongTensor) – 从 tensor 中选择的索引 tensor (Tensor) – 用于相加的tensor 例子: >>> x = torch.ones(5, 3) >>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) >>> index = torch.tensor([0, 4, 2]) >>> x.index_add_(0, index, t) tensor([[ 2., 3., 4.], [ 1., 1., 1.], [ 8., 9., 10.], [ 1., 1., 1.], [ 5., 6., 7.]]) index_copy_(dim, index, tensor) → Tensor 根据参数index 中的选择的索引, 复制 tensor 中的元素到 self tensor, 例如, 如果 dim == 0 并且 index[i] == j, 则第 i 行 tensor 会被加到第 j行. tensor 第 dim 维度 必须和 index(必须是一个向量) 的长度相同, 并且其它维度必须和 self 匹配, 否则将会抛出一个错误. Parameters: dim (int) – 要索引的维度 index (LongTensor) – 从 tensor 中选择的索引 tensor (Tensor) – 用于复制的tensor 例子: >>> x = torch.zeros(5, 3) >>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) >>> index = torch.tensor([0, 4, 2]) >>> x.index_copy_(0, index, t) tensor([[ 1., 2., 3.], [ 0., 0., 0.], [ 7., 8., 9.], [ 0., 0., 0.], [ 4., 5., 6.]]) index_fill_(dim, index, val) → Tensor 根据 index 中指定的顺序索引, 用值 val填充 self tensor 中的元素. 参数: dim (int) – 指定索引对应的维度 index (LongTensor) – self tensor 中将被填充的索引值 val (float) – 用于填充的值 例子: >>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) >>> index = torch.tensor([0, 2]) >>> x.index_fill_(1, index, -1) tensor([[-1., 2., -1.], [-1., 5., -1.], [-1., 8., -1.]]) index_put_(indices, value, accumulate=False) → Tensor 根据 indices (是一个 Tensors 的tuple)中指定的索引, 取出 tensor value 中的值放入 tensor self . 表达式 tensor.index_put_(indices, value) 等价于 tensor[indices] = value. 返回 self. 如果 accumulate 等于 True, tensor 中的元素会被加到 self. 如果是 False, 且 indices 中含有重复的元素, 则行为是未定义的. 参数: indices (tuple of LongTensor) – tensors 用于索引 self. value (Tensor) – 与 self 有相同数据类型的 tensor. accumulate (bool) – 是否累加到自身 index_select(dim, index) → Tensor 见 torch.index_select() int() → Tensor self.int() is equivalent to self.to(torch.int32). See to(). inverse() → Tensor 见 torch.inverse() is_contiguous() → bool 返回 True 如果 self tensor 在内存中是连续存储的. is_pinned() 返回 true 如果 tensor 储存在pinned memory is_set_to(tensor) → bool 返回 True 如果此对象在 Torch C API 中引用的 THTensor 对象和给定 tensor 是相同的. is_signed() item() → number 返回 tensor 中的值作为一个标准的 Python number. 仅在只有一个元素的时候有效. 对于其他情况, 见 tolist(). 这个操作是不可微分的. 例子: >>> x = torch.tensor([1.0]) >>> x.item() 1.0 kthvalue(k, dim=None, keepdim=False) -> (Tensor, LongTensor) 见 torch.kthvalue() le(other) → Tensor 见 torch.le() le_(other) → Tensor 原地版本的 le() lerp(start, end, weight) → Tensor 见 torch.lerp() lerp_(start, end, weight) → Tensor 原地版本的 lerp() log() → Tensor 见 torch.log() log_() → Tensor 原地版本的 log() logdet() → Tensor 见 torch.logdet() log10() → Tensor 见 torch.log10() log10_() → Tensor 原地版本的 log10() log1p() → Tensor 见 torch.log1p() log1p_() → Tensor 原地版本的 log1p() log2() → Tensor 见 torch.log2() log2_() → Tensor 原地版本的 log2() log_normal_(mean=1, std=2, *, generator=None) 用 mean 和std 初始化的 log-normal 分布 中取出的值填充 self. 注意 mean 和 std 是下面的 normal 分布的平均值和标准差, 而不是返回的分布: logsumexp(dim, keepdim=False) → Tensor 见 torch.logsumexp() long() → Tensor self.long() is equivalent to self.to(torch.int64). See to(). lt(other) → Tensor 见 torch.lt() lt_(other) → Tensor 原地版本的 lt() map_(tensor, callable) 对 self tensor 和 给定的 tensor 中的每一个元素应用 callable 然后把结果存于 self tensor. self tensor 和给定的 tensor 必须可广播 broadcastable. callable 应该有下面的函数签名: def callable(a, b) -> number masked_scatter_(mask, source) 从 source 复制元素到 self tensor 当对应 mask 对应的值是 1. mask 的形状必须和底层 tensor 可广播 broadcastable. source 的元素数量至少和 mask里面的1一样多 Parameters: mask (ByteTensor) – 二值掩码 source (Tensor) – 源 tensor 注意 mask 操作于 self tensor, 而不是给定的 source tensor. masked_fill_(mask, value) 用value填充 self tensor 中的元素, 当对应位置的 mask 是1. mask 的形状必须和底层 tensor broadcastable. 参数: mask (ByteTensor) – 二值掩码 value (float) – 用于填充的值 masked_select(mask) → Tensor 见 torch.masked_select() matmul(tensor2) → Tensor 见 torch.matmul() matrix_power(n) → Tensor 见 torch.matrix_power() max(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor) 见 torch.max() mean(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor) 见 torch.mean() median(dim=None, keepdim=False) -> (Tensor, LongTensor) 见 torch.median() min(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor) 见 torch.min() mm(mat2) → Tensor 见 torch.mm() mode(dim=None, keepdim=False) -> (Tensor, LongTensor) 见 torch.mode() mul(value) → Tensor 见 torch.mul() mul_(value) 原地版本的 mul() multinomial(num_samples, replacement=False, *, generator=None) → Tensor 见 torch.multinomial() mv(vec) → Tensor 见 torch.mv() mvlgamma(p) → Tensor 见 torch.mvlgamma() mvlgamma_(p) → Tensor 原地版本的 mvlgamma() narrow(dimension, start, length) → Tensor 见 torch.narrow() Example: >>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) >>> x.narrow(0, 0, 2) tensor([[ 1, 2, 3], [ 4, 5, 6]]) >>> x.narrow(1, 1, 2) tensor([[ 2, 3], [ 5, 6], [ 8, 9]]) ndimension() → int Alias for dim() ne(other) → Tensor 见 torch.ne() ne_(other) → Tensor 原地版本的 ne() neg() → Tensor 见 torch.neg() neg_() → Tensor 原地版本的 neg() nelement() → int 别名 numel() nonzero() → LongTensor 见 torch.nonzero() norm(p='fro', dim=None, keepdim=False) 见 :func: torch.norm normal_(mean=0, std=1, *, generator=None) → Tensor 用采样于 normal 分布的元素填充 self tensor, normal 分布使用参数 mean and std初始化. numel() → int 见 torch.numel() numpy() → numpy.ndarray 返回 self tensor 作为一个 NumPy ndarray. 此 tensor 和返回的 ndarray 共享同一个底层存储. 改变self tensor 将会同时改变 ndarray . orgqr(input2) → Tensor 见 torch.orgqr() ormqr(input2, input3, left=True, transpose=False) → Tensor 见 torch.ormqr() permute(*dims) → Tensor 排列 tensor 的维度. 参数: *dims (int...) – 维度的排列顺序 Example >>> x = torch.randn(2, 3, 5) >>> x.size() torch.Size([2, 3, 5]) >>> x.permute(2, 0, 1).size() torch.Size([5, 2, 3]) pin_memory() pinverse() → Tensor 见 torch.pinverse() potrf(upper=True) 见 torch.cholesky() potri(upper=True) → Tensor 见 torch.potri() potrs(input2, upper=True) → Tensor 见 torch.potrs() pow(exponent) → Tensor 见 torch.pow() pow_(exponent) → Tensor 原地版本的 pow() prod(dim=None, keepdim=False, dtype=None) → Tensor 见 torch.prod() pstrf(upper=True, tol=-1) -> (Tensor, IntTensor) 见 torch.pstrf() put_(indices, tensor, accumulate=False) → Tensor 从 tensor 中复制元素到 indices 指定的位置. 对于目的索引, self tensor 被当作一个 1-D tensor. 如果 accumulate 是 True, tensor 中的元素被被加到 self. 如果 accumulate 是 False, 当 indices 中有重复索引时行为未定义. Parameters: indices (LongTensor) – self 的索引位置 tensor (Tensor) – 包含待复制元素的 tensor accumulate (bool) – 是否累加到 self 例子: >>> src = torch.tensor([[4, 3, 5], [6, 7, 8]]) >>> src.put_(torch.tensor([1, 3]), torch.tensor([9, 10])) tensor([[ 4, 9, 5], [ 10, 7, 8]]) qr() -> (Tensor, Tensor) 见 torch.qr() random_(from=0, to=None, *, generator=None) → Tensor 用离散均匀分布介于 [from, to - 1] 采样的数字填充 self tensor. 如果没有特别指定, 这些采样的数值被 self tensor’s 数据类型界定. 然而, 对于浮点型, 如果没有特别指定, 范围将是 [0, 2^mantissa] 来确保每一个值是可表示的. 例如, torch.tensor(1, dtype=torch.double).random_() 将会被设为 [0, 2^53]. reciprocal() → Tensor 见 torch.reciprocal() reciprocal_() → Tensor 原地版本的 reciprocal() remainder(divisor) → Tensor 见 torch.remainder() remainder_(divisor) → Tensor 原地版本的 remainder() renorm(p, dim, maxnorm) → Tensor 见 torch.renorm() renorm_(p, dim, maxnorm) → Tensor 原地版本的 renorm() repeat(*sizes) → Tensor 在指定的维度重复这个 tensor. 不像 expand(), 这个函数会拷贝底层数据. 警告 torch.repeat() 的行为和 numpy.repeat 不一样, 更类似于 numpy.tile. 参数: sizes (torch.Size or int...) – 每个维度重复的次数 例子: >>> x = torch.tensor([1, 2, 3]) >>> x.repeat(4, 2) tensor([[ 1, 2, 3, 1, 2, 3], [ 1, 2, 3, 1, 2, 3], [ 1, 2, 3, 1, 2, 3], [ 1, 2, 3, 1, 2, 3]]) >>> x.repeat(4, 2, 1).size() torch.Size([4, 2, 3]) requires_grad_(requires_grad=True) → Tensor 设置是否应该自动求导: 原地设置这个 tensor 的 requires_grad 属性.返回这个 tensor. require_grad_() 的主要使用情况是告诉自动求导开始记录Tensor tensor上的操作. 如果 tensor 的 requires_grad=False (因为它是通过 DataLoader 获得或者需要预处理或初始化), tensor.requires_grad_() 将会使得自动求导开始生效. 参数: requires_grad (bool) – 是否自动求导应该记录相关操作. Default: True. 例子: >>> # Let's say we want to preprocess some saved weights and use >>> # the result as new weights. >>> saved_weights = [0.1, 0.2, 0.3, 0.25] >>> loaded_weights = torch.tensor(saved_weights) >>> weights = preprocess(loaded_weights) # some function >>> weights tensor([-0.5503, 0.4926, -2.1158, -0.8303]) >>> # Now, start to record operations done to weights >>> weights.requires_grad_() >>> out = weights.pow(2).sum() >>> out.backward() >>> weights.grad tensor([-1.1007, 0.9853, -4.2316, -1.6606]) reshape(*shape) → Tensor 返回一个 tensor, 其data和元素数量与 self 一样, 但是改变成指定的形状. 这个方法返回一个tensor的试图 如果 shape 和当前的形状是兼容的. 见 torch.Tensor.view() 关于是什么时候返回一个 view. 见 torch.reshape() 参数: shape (tuple of python:ints or int...) – 期望变成的形状 reshape_as(other) → Tensor 返回一个tensor形状与 other 相同. self.reshape_as(other) 等价于 self.reshape(other.sizes()). 这个方法返回一个tensor的试图 如果 self.reshape(other.sizes()) 和当前的形状是兼容的. 见 torch.Tensor.view() 关于是什么时候返回一个 view. 请参考 reshape() 获得更多关于 reshape 的信息. 参数: other (torch.Tensor) – 返回的tensor形状与 other 一致. resize_(*sizes) → Tensor 缩放 self tensor到指定的大小. 如果指定的元素数量比当前的要大, 底层的存储结构会缩放到合适的大小. 如果数量更小, 底层存储不变. 当前的元素都会被保留, 没有任何的新的初始化. 警告 这是一个底层的操作. 存储被重新解释为C-contiguous, 忽略当前stride（除非目标大小等于当前大小, 在这种情况下tensor保持不变）.在大多数情况下, 您将要使用 view(), 它会检查连续性, 或者 reshape(), 在必要的时候会拷贝数据. 如果想要改变大小并且自定义stride, 见 set_(). 参数: sizes (torch.Size or int...) – 期望的大小 例子: >>> x = torch.tensor([[1, 2], [3, 4], [5, 6]]) >>> x.resize_(2, 2) tensor([[ 1, 2], [ 3, 4]]) resize_as_(tensor) → Tensor 缩放 self tensor 的大小与参数 tensor 相同. 等价于 self.resize_(tensor.size()). round() → Tensor 见 torch.round() round_() → Tensor 原地版本的 round() rsqrt() → Tensor 见 torch.rsqrt() rsqrt_() → Tensor 原地版本的 rsqrt() scatter_(dim, index, src) → Tensor 根据 index tensor 中指定的索引, 将所有 tensor src 中的值写入self . 对于 src 中的每一个值, 当 dimension != dim, 它的输出的索引由 src 中的索引指定, 当 dimension = dim, 由 index 中对应的值指定. 对于一个 3-D tensor, self 的更新规则如下: self[index[i][j][k]][j][k] = src[i][j][k] # if dim == 0 self[i][index[i][j][k]][k] = src[i][j][k] # if dim == 1 self[i][j][index[i][j][k]] = src[i][j][k] # if dim == 2 这是 gather() 中描述的方式的逆向操作. self, index and src (if it is a Tensor) 应该有相同数量的维度. 同时也要求 index.size(d) 对于每一个维度 d, 而且 index.size(d) 对于每一个维度 d != dim. 此外, 关于 gather(), index 的值必须介于 0 和 self.size(dim) - 1 (包括), 并且沿着指定维度dim的行中的所有值必须是唯一的. 参数: dim (int) – 要索引的轴 index (LongTensor) – 需要 scatter 的元素的索引, 可以是空的，也可以与src大小相同。当为空时，操作返回恒等 src (Tensor or float) – scatter 源 例子: >>> x = torch.rand(2, 5) >>> x tensor([[ 0.3992, 0.2908, 0.9044, 0.4850, 0.6004], [ 0.5735, 0.9006, 0.6797, 0.4152, 0.1732]]) >>> torch.zeros(3, 5).scatter_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x) tensor([[ 0.3992, 0.9006, 0.6797, 0.4850, 0.6004], [ 0.0000, 0.2908, 0.0000, 0.4152, 0.0000], [ 0.5735, 0.0000, 0.9044, 0.0000, 0.1732]]) >>> z = torch.zeros(2, 4).scatter_(1, torch.tensor([[2], [3]]), 1.23) >>> z tensor([[ 0.0000, 0.0000, 1.2300, 0.0000], [ 0.0000, 0.0000, 0.0000, 1.2300]]) scatter_add_(dim, index, other) → Tensor 根据 index tensor 中指定的索引(方式和scatter_()类似), 将所有 tensor other 中的值加到self . 对于 other 中的每一个值, 当 dimension != dim, 它的输出的索引由 other 中的索引指定, 当 dimension = dim, 由 index 中对应的值指定. 对于一个 3-D tensor, self 的更新规则如下: self[index[i][j][k]][j][k] += other[i][j][k] # if dim == 0 self[i][index[i][j][k]][k] += other[i][j][k] # if dim == 1 self[i][j][index[i][j][k]] += other[i][j][k] # if dim == 2 self, index and other 应该有相同数量的维度. 也要求 index.size(d) 对于所有的维度 d, 并且 index.size(d) 对于所有的维度 d != dim. 此外, 关于 gather(), index 的值必须介于 0 和 self.size(dim) - 1 (包括), 并且沿着指定维度dim的行中的所有值必须是唯一的. 注意 当使用 CUDA 作为后端, 这个操作将导致不确定性行为, 并且难以停止. 请参考 Reproducibility 获得相关背景. 参数: dim (int) – 要索引的轴 index (LongTensor) – 需要 scatter add 的元素的索引, 可以是空的，也可以与src大小相同。当为空时，操作返回恒等 src (Tensor or float) – scatter 源 例子: >>> x = torch.rand(2, 5) >>> x tensor([[0.7404, 0.0427, 0.6480, 0.3806, 0.8328], [0.7953, 0.2009, 0.9154, 0.6782, 0.9620]]) >>> torch.ones(3, 5).scatter_add_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x) tensor([[1.7404, 1.2009, 1.9154, 1.3806, 1.8328], [1.0000, 1.0427, 1.0000, 1.6782, 1.0000], [1.7953, 1.0000, 1.6480, 1.0000, 1.9620]]) select(dim, index) → Tensor 沿着选择的维度在给定的索引处切取 self tensor.这个函数返回的 tensor 指定的维度被移除了. 参数: dim (int) – 要切片的维度 index (int) – 选择的索引 注意 select() 等价于切片. 例如, tensor.select(0, index) 等价于 tensor[index] and tensor.select(2, index) 等价于 tensor[:,:,index]. set_(source=None, storage_offset=0, size=None, stride=None) → Tensor 设置底层存储, 大小, 和 strides. 如果 source 是一个 tensor, self tensor 将会和 source 共享底层存储, 并有用一样的大小和 strides. 在一个 tensor 中改变元素将会反应到另一个tensor. 如果 source 是一个 Storage, 此方法设置底层存储, offset, 大小, 和 stride. 参数: source (Tensor or Storage) – 要设置的 tensor 或者 storage storage_offset (int, optional) – storage 的 offset size (torch.Size__, optional) – 期望的大小.默认是 source 的大小. stride (tuple, optional) – 期望的 stride.默认值是 C-contiguous strides. share_memory_() 移动底层存储到共享内存. 这是一个空操作如果底层存储已经在共享内存中或者是 CUDA tensors. 共享内存中的 tensor 不能 resize. short() → Tensor self.short() 等价于 self.to(torch.int16). 见 to(). sigmoid() → Tensor 见 torch.sigmoid() sigmoid_() → Tensor 原地版本的 sigmoid() sign() → Tensor 见 torch.sign() sign_() → Tensor 原地版本的 sign() sin() → Tensor 见 torch.sin() sin_() → Tensor 原地版本的 sin() sinh() → Tensor 见 torch.sinh() sinh_() → Tensor 原地版本的 sinh() size() → torch.Size 返回 self tensor 的尺寸. 返回值是 [tuple] 的子类(https://docs.python.org/3/library/stdtypes.html#tuple \"(in Python v3.7)\"). 例如: >>> torch.empty(3, 4, 5).size() torch.Size([3, 4, 5]) slogdet() -> (Tensor, Tensor) 见 torch.slogdet() sort(dim=None, descending=False) -> (Tensor, LongTensor) 见 torch.sort() split(split_size, dim=0) 见 torch.split() sparse_mask(input, mask) → Tensor 用 mask 的索引过滤 Tensor input, 返回一个新的 SparseTensor. input 和 mask 必须有相同的形状. 参数: input (Tensor) – 输入 Tensor mask (SparseTensor) – SparseTensor 用其索引过滤 input 例子: >>> nnz = 5 >>> dims = [5, 5, 2, 2] >>> I = torch.cat([torch.randint(0, dims[0], size=(nnz,)), torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz) >>> V = torch.randn(nnz, dims[2], dims[3]) >>> size = torch.Size(dims) >>> S = torch.sparse_coo_tensor(I, V, size).coalesce() >>> D = torch.randn(dims) >>> D.sparse_mask(S) tensor(indices=tensor([[0, 0, 0, 2], [0, 1, 4, 3]]), values=tensor([[[ 1.6550, 0.2397], [-0.1611, -0.0779]], [[ 0.2326, -1.0558], [ 1.4711, 1.9678]], [[-0.5138, -0.0411], [ 1.9417, 0.5158]], [[ 0.0793, 0.0036], [-0.2569, -0.1055]]]), size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo) sqrt() → Tensor 见 torch.sqrt() sqrt_() → Tensor 原地版本的 sqrt() squeeze(dim=None) → Tensor 见 torch.squeeze() squeeze_(dim=None) → Tensor 原地版本的 squeeze() std(dim=None, unbiased=True, keepdim=False) → Tensor 见 torch.std() storage() → torch.Storage 返回底层的 storage storage_offset() → int 根据存储元素的数量(而不是字节)，返回底层存储中的tesor偏移量(offset)。 例子: >>> x = torch.tensor([1, 2, 3, 4, 5]) >>> x.storage_offset() 0 >>> x[3:].storage_offset() 3 storage_type() stride(dim) → tuple or int 返回 self tensor 的 stride. stride 是必要的用于在指定的维度 dim 找到下一个元素. 如果传入空, 则返回一个 tuple 包含所有维度的 stride. 否则, 将会返回一个 int 表示指定维度 dim 的 stride. 参数: dim (int, optional) – 需要返回 stride 的维度 例子: >>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) >>> x.stride() (5, 1) >>>x.stride(0) 5 >>> x.stride(-1) 1 sub(value, other) → Tensor self tensor 减去一个 scalar 或者 tensor. 如果 value 和 other 都被指定, 在相减之前, other 的每个元素将会用 value 缩放. 当 other 是一个 tensor, other 的形状必须和底层存储是可广播的 broadcastable . sub_(x) → Tensor 原地版本的 sub() sum(dim=None, keepdim=False, dtype=None) → Tensor 见 torch.sum() svd(some=True, compute_uv=True) -> (Tensor, Tensor, Tensor) 见 torch.svd() symeig(eigenvectors=False, upper=True) -> (Tensor, Tensor) 见 torch.symeig() t() → Tensor 见 torch.t() t_() → Tensor 原地版本的 t() to(*args, **kwargs) → Tensor 执行 tensor 类型或者设备转换. torch.dtype 和 torch.device 是从参数中推断的 self.to(*args, **kwargs). 注意 如果 self Tensor 已经有正确的 torch.dtype 和 torch.device, 则 self 被返回. 否则, 将返回复制的 self 期望的 torch.dtype 和 torch.device. 下面是调用的方法 to: to(dtype, non_blocking=False, copy=False) → Tensor 返回一个 Tensor 指定类型 dtype to(device=None, dtype=None, non_blocking=False, copy=False) → Tensor 返回一个 Tensor 并指定 device 和 (可选的) dtype. 如果 dtype 是 None 则推断为 self.dtype . 当启用 non_blocking, 试图在主机上执行异步转换, 例如, 转换一个 pinned memory 的 CPU Tensor 到 CUDA Tensor. 当 copy 被设置, 一个新的 tensor 被创建. to(other, non_blocking=False, copy=False) → Tensor 返回一个 Tensor 并有和 Tensor other 相同的 torch.dtype 和 torch.device. 当启用 non_blocking, 试图在主机上执行异步转换, 例如, 转换一个 pinned memory 的 CPU Tensor 到 CUDA Tensor. 当 copy 被设置, 一个新的 tensor 被创建. 例子: >>> tensor = torch.randn(2, 2) # Initially dtype=float32, device=cpu >>> tensor.to(torch.float64) tensor([[-0.5044, 0.0005], [ 0.3310, -0.0584]], dtype=torch.float64) >>> cuda0 = torch.device('cuda:0') >>> tensor.to(cuda0) tensor([[-0.5044, 0.0005], [ 0.3310, -0.0584]], device='cuda:0') >>> tensor.to(cuda0, dtype=torch.float64) tensor([[-0.5044, 0.0005], [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0') >>> other = torch.randn((), dtype=torch.float64, device=cuda0) >>> tensor.to(other, non_blocking=True) tensor([[-0.5044, 0.0005], [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0') take(indices) → Tensor 见 torch.take() tan() tan_() → Tensor 原地版本的 tan() tanh() → Tensor 见 torch.tanh() tanh_() → Tensor 原地版本的 tanh() tolist() ” tolist() -> list or number 返回tensor 作为(嵌套的) list. 对于 scalars,一个标准的 Python number 被返回, 就像 item() 一样. Tensors 会自动移动到 CPU 上如果有必要. 这个操作是不可微分的. 例子: >>> a = torch.randn(2, 2) >>> a.tolist() [[0.012766935862600803, 0.5415473580360413], [-0.08909505605697632, 0.7729271650314331]] >>> a[0,0].tolist() 0.012766935862600803 topk(k, dim=None, largest=True, sorted=True) -> (Tensor, LongTensor) 见 torch.topk() to_sparse(sparseDims) → Tensor 返回一个稀疏复制的 tensor. PyTorch 支持 coordinate 格式 的稀疏 tensors. :param sparseDims: 要包含在新稀疏tensor中的稀疏维数 :type sparseDims: int, 可选的 例子:: >>> d = torch.tensor([[0, 0, 0], [9, 0, 10], [0, 0, 0]]) >>> d tensor([[ 0, 0, 0], [ 9, 0, 10], [ 0, 0, 0]]) >>> d.to_sparse() tensor(indices=tensor([[1, 1], [0, 2]]), values=tensor([ 9, 10]), size=(3, 3), nnz=2, layout=torch.sparse_coo) >>> d.to_sparse(1) tensor(indices=tensor([[1]]), values=tensor([[ 9, 0, 10]]), size=(3, 3), nnz=1, layout=torch.sparse_coo) trace() → Tensor 见 torch.trace() transpose(dim0, dim1) → Tensor 见 torch.transpose() transpose_(dim0, dim1) → Tensor 原地版本的 transpose() tril(k=0) → Tensor 见 torch.tril() tril_(k=0) → Tensor 原地版本的 tril() triu(k=0) → Tensor 见 torch.triu() triu_(k=0) → Tensor 原地版本的 triu() trtrs(A, upper=True, transpose=False, unitriangular=False) -> (Tensor, Tensor) 见 torch.trtrs() trunc() → Tensor 见 torch.trunc() trunc_() → Tensor 原地版本的 trunc() type(dtype=None, non_blocking=False, **kwargs) → str or Tensor 返回 type 如果 dtype 没有被设置, 否则将会强制转换成 dtype 类型. 如果这已经是正确的类型，则不执行复制，并返回原始对象. 参数: dtype (type or string) – 期望类型 non_blocking (bool) – 如果 True，并且源在pinned memory中，目的地在GPU上，则拷贝相对于主机异步执行。否则，这个参数没有任何作用。 **kwargs – 为了兼容性, 可能包含 async 用来置换 non_blocking 参数. async 参数被废弃了. type_as(tensor) → Tensor 返回 tensor 强制转换为 tensor 的数据类型. 如果这已经是正确的类型，则是空操作. 等价于: self.type(tensor.type()) Params: tensor (Tensor): 拥有目标数据类型的 tensor unfold(dim, size, step) → Tensor 返回一个 tensor 包含 self tensor 在维度 dim 上的所有切片, 每一个的大小为 size. step 指定每一个切片的间距. 如果 sizedim 是 self dim 维度的大小, 返回的 tensor 的维度 dim 大小是 (sizedim - size) / step + 1. 一个附加的size size的维度追加于返回的 tensor. 参数: dim (int) – 指定 unfold 的维度 size (int) – 指定每个slice的大小 step (int) – 指定步长 例子: >>> x = torch.arange(1., 8) >>> x tensor([ 1., 2., 3., 4., 5., 6., 7.]) >>> x.unfold(0, 2, 1) tensor([[ 1., 2.], [ 2., 3.], [ 3., 4.], [ 4., 5.], [ 5., 6.], [ 6., 7.]]) >>> x.unfold(0, 2, 2) tensor([[ 1., 2.], [ 3., 4.], [ 5., 6.]]) uniform_(from=0, to=1) → Tensor 用连续均匀分布的采样值填充 self tensor: unique(sorted=False, return_inverse=False, dim=None) 返回 tensor 中唯一的标量作为 1-D tensor. 见 torch.unique() unsqueeze(dim) → Tensor 见 torch.unsqueeze() unsqueeze_(dim) → Tensor 原地版本的 unsqueeze() var(dim=None, unbiased=True, keepdim=False) → Tensor 见 torch.var() view(*shape) → Tensor 返回一个新的 tersor, 和 self 有相同的数据, 但是有不同的 shape. 返回的 tensor 共享相同的数据，并且具有相同数量的元素，但是可能有不同的大小。要 view() 一个tensor，新视图大小必须与其原始大小和 stride 兼容, 例如, 每个新视图维度必须是原始维度的子空间，或者仅跨越原始维度 满足以下连续性条件 , 否则在 view() 之前, contiguous() 需要被调用. 可参考: reshape(), 返回一个view 当形状是兼容的, 否则复制 (等价于调用 contiguous()). 参数: shape (torch.Size or int...) – the desired size 例子: >>> x = torch.randn(4, 4) >>> x.size() torch.Size([4, 4]) >>> y = x.view(16) >>> y.size() torch.Size([16]) >>> z = x.view(-1, 8) # the size -1 is inferred from other dimensions >>> z.size() torch.Size([2, 8]) view_as(other) → Tensor 使用 other 的大小 View tensor . self.view_as(other) 等价于 self.view(other.size()). 请参考 view() 获得更多信息关于 view. 参数: other (torch.Tensor) – 返回的tensor 和 other 大小相同. zero_() → Tensor 用 0 填充 self tensor. class torch.ByteTensor 下面的方法是 torch.ByteTensor 独占. all() all() → bool 返回 True 如果所有的元素非零, 否则 False. 例子: >>> a = torch.randn(1, 3).byte() % 2 >>> a tensor([[1, 0, 0]], dtype=torch.uint8) >>> a.all() tensor(0, dtype=torch.uint8) all(dim, keepdim=False, out=None) → Tensor 返回 True 如果 tensor 在指定维度dim每一行的所有的元素非零, 否则 False. 如果 keepdim 是 True, 则输出 tensor 的大小与 input相同, 但尺寸为1的维度dim除外. 否则, dim 会被压缩 (见 torch.squeeze()), 导致输出张量比input少1维. Parameters: dim (int) – 要reduce的维度 keepdim (bool) – output tensor 是否保留 dim out (Tensor, 可选的) – output tensor 例子: >>> a = torch.randn(4, 2).byte() % 2 >>> a tensor([[0, 0], [0, 0], [0, 1], [1, 1]], dtype=torch.uint8) >>> a.all(dim=1) tensor([0, 0, 0, 1], dtype=torch.uint8) any() any() → bool 返回 True 如果任意元素非零, 否则 False. 例子: >>> a = torch.randn(1, 3).byte() % 2 >>> a tensor([[0, 0, 1]], dtype=torch.uint8) >>> a.any() tensor(1, dtype=torch.uint8) any(dim, keepdim=False, out=None) → Tensor 返回 True 如果 tensor 在指定维度dim每一行的任意的元素非零, 否则 False. 如果 keepdim 是 True, 则输出 tensor 的大小与 input相同, 但尺寸为1的维度dim除外. 否则, dim 会被压缩 (见 torch.squeeze()), 导致输出张量比input少1维. 参数: dim (int) – 要减少的维度 keepdim (bool) – output tensor 是否保留 dim out (Tensor, 可选的) – output tensor Example: >>> a = torch.randn(4, 2).byte() % 2 >>> a tensor([[1, 0], [0, 0], [0, 1], [0, 0]], dtype=torch.uint8) >>> a.any(dim=1) tensor([1, 0, 1, 0], dtype=torch.uint8) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"tensor_attributes.html":{"url":"tensor_attributes.html","title":"Tensor Attributes","keywords":"","body":"Tensor（张量）的属性 译者：阿远 每个 torch.Tensor 对象都有以下几个属性： torch.dtype, torch.device， 和 torch.layout。 torch.dtype class torch.dtype torch.dtype 属性标识了 torch.Tensor的数据类型。PyTorch 有八种不同的数据类型： Data type dtype Tensor types 32-bit floating point torch.float32 or torch.float torch.*.FloatTensor 64-bit floating point torch.float64 or torch.double torch.*.DoubleTensor 16-bit floating point torch.float16 or torch.half torch.*.HalfTensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.*.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.*.LongTensor torch.device class torch.device torch.device 属性标识了torch.Tensor对象在创建之后所存储在的设备名称，而在对象创建之前此属性标识了即将为此对象申请存储空间的设备名称。 torch.device 包含了两种设备类型 ('cpu' 或者 'cuda') ，分别标识将Tensor对象储存于cpu内存或者gpu内存中，同时支持指定设备编号，比如多张gpu，可以通过gpu编号指定某一块gpu。 如果没有指定设备编号，则默认将对象存储于current_device()当前设备中； 举个例子， 一个torch.Tensor 对象构造函数中的设备字段如果填写'cuda'，那等价于填写了'cuda:X'，其中X是函数 torch.cuda.current_device()的返回值。 在torch.Tensor对象创建之后，可以通过访问Tensor.device属性实时访问当前对象所存储在的设备名称。 torch.device 对象支持使用字符串或者字符串加设备编号这两种方式来创建： 通过字符串创建： >>> torch.device('cuda:0') device(type='cuda', index=0) # 编号为0的cuda设备 >>> torch.device('cpu') # cpu内存 device(type='cpu') >>> torch.device('cuda') # 当前cuda设备 device(type='cuda') 通过字符串加设备编号创建： >>> torch.device('cuda', 0) device(type='cuda', index=0) >>> torch.device('cpu', 0) device(type='cpu', index=0) Note 当torch.device作为函数的参数的时候， 可以直接用字符串替换。 这样有助于加快代码创建原型的速度。 >>> # 一个接受torch.device对象为参数的函数例子 >>> cuda1 = torch.device('cuda:1') >>> torch.randn((2,3), device=cuda1) >>> # 可以用一个字符串替换掉torch.device对象，一样的效果 >>> torch.randn((2,3), 'cuda:1') Note 由于一些历史遗留问题, device对象还可以仅通过一个设备编号来创建，这些设备编号对应的都是相应的cuda设备。 这正好对应了 Tensor.get_device()函数, 这个仅支持cuda Tensor的函数返回的就是当前tensor所在的cuda设备编号，cpu Tensor不支持这个函数。 >>> torch.device(1) device(type='cuda', index=1) Note 接受device参数的函数同时也可以接受一个正确格式的字符串或者正确代表设备编号的数字（数字这个是历史遗留问题）作为参数，以下的操作是等价的： >>> torch.randn((2,3), device=torch.device('cuda:1')) >>> torch.randn((2,3), device='cuda:1') >>> torch.randn((2,3), device=1) # 历史遗留做法 torch.layout class torch.layout torch.layout 属性标识了torch.Tensor 在内存中的布局模式。 现在， 我们支持了两种内存布局模式 torch.strided (dense Tensors) 和尚处试验阶段的torch.sparse_coo (sparse COO Tensors， 一种经典的稀疏矩阵存储方式). torch.strided 跨步存储代表了密集张量的存储布局方式，当然也是最常用最经典的一种布局方式。 每一个strided tensor都有一个与之相连的torch.Storage对象, 这个对象存储着tensor的数据. 这些Storage对象为tensor提供了一种多维的， 跨步的(strided)数据视图. 这一视图中的strides是一个interger整形列表：这个列表的主要作用是给出当前张量的各个维度的所占内存大小，严格的定义就是，strides中的第k个元素代表了在第k维度下，从一个元素跳转到下一个元素所需要跨越的内存大小。 跨步这个概念有助于提高多种张量运算的效率。 例子: >>> x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) >>> x.stride() (5, 1) # 此时在这个二维张量中，在第0维度下，从一个元素到下一个元素需要跨越的内存大小是5，比如x[0] 到x[1]需要跨越x[0]这5个元素, 在第1维度下，是1，如x[0, 0]到x[0, 1]需要跨越1个元素 >>> x.t().stride() (1, 5) 更多关于 torch.sparse_coo tensors的信息, 请看torch.sparse. 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"type_info.html":{"url":"type_info.html","title":"数据类型信息","keywords":"","body":"数据类型信息 译者：冯宝宝 可以通过torch.finfo 或 torch.iinfo访问torch.dtype的数字属性。 torch.finfo class torch.finfo torch.finfo 是一个用来表示浮点torch.dtype的数字属性的对象（即torch.float32，torch.float64和torch.float16）。 这类似于 numpy.finfo。 torch.finfo 提供以下属性: 名称 类型 描述 bits 整型　int 数据类型占用的位数 eps 浮点型float 可表示的最小数字，使得1.0 + eps！= 1.0 max 浮点型float 可表示的最大数字 tiny 浮点型float 可表示的最小正数 注意 在使用pytorch默认dtype创建类（由torch.get_default_dtype（）返回）的情况下，构造的 torch.finfo 函数可以不带参数被调用。 torch.iinfo class torch.iinfo torch.iinfo是一个用来表示整数torch.dtype 的数字属性的对象，（即torch.uint8，torch.int8，torch.int16，torch.int32和torch.int64）。 这与numpy.iinfo类似。 torch.iinfo 提供以下属性： 名称 类型 描述 bits 整型 数据类型占用的位数 max 整型 可表示的最大数字 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"sparse.html":{"url":"sparse.html","title":"torch.sparse","keywords":"","body":"torch.sparse 译者：hijkzzz 警告 这个API目前还处于试验阶段, 可能在不久的将来会发生变化. Torch支持COO(rdinate )格式的稀疏张量, 这可以有效地存储和处理大多数元素为零的张量. 稀疏张量表示为一对稠密张量:一个值张量和一个二维指标张量. 一个稀疏张量可以通过提供这两个张量, 以及稀疏张量的大小来构造(从这些张量是无法推导出来的!)假设我们要定义一个稀疏张量, 它的分量3在(0,2)处, 分量4在(1,0)处, 分量5在(1,2)处, 然后我们可以这样写 >>> i = torch.LongTensor([[0, 1, 1], [2, 0, 2]]) >>> v = torch.FloatTensor([3, 4, 5]) >>> torch.sparse.FloatTensor(i, v, torch.Size([2,3])).to_dense() 0 0 3 4 0 5 [torch.FloatTensor of size 2x3] 注意, LongTensor的输入不是索引元组的列表. 如果你想这样写你的指标, 你应该在把它们传递给稀疏构造函数之前进行转置: >>> i = torch.LongTensor([[0, 2], [1, 0], [1, 2]]) >>> v = torch.FloatTensor([3, 4, 5 ]) >>> torch.sparse.FloatTensor(i.t(), v, torch.Size([2,3])).to_dense() 0 0 3 4 0 5 [torch.FloatTensor of size 2x3] 也可以构造混合稀疏张量, 其中只有前n个维度是稀疏的, 其余维度是密集的. >>> i = torch.LongTensor([[2, 4]]) >>> v = torch.FloatTensor([[1, 3], [5, 7]]) >>> torch.sparse.FloatTensor(i, v).to_dense() 0 0 0 0 1 3 0 0 5 7 [torch.FloatTensor of size 5x2] 可以通过指定其大小来构造空的稀疏张量： >>> torch.sparse.FloatTensor(2, 3) SparseFloatTensor of size 2x3 with indices: [torch.LongTensor with no dimension] and values: [torch.FloatTensor with no dimension] SparseTensor 具有以下不变量: sparse_dim + dense_dim = len(SparseTensor.shape) SparseTensor._indices().shape = (sparse_dim, nnz) SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:]) 因为SparseTensor._indices()总是一个二维张量, 最小的sparse_dim = 1. 因此, sparse_dim = 0的稀疏张量的表示就是一个稠密张量. 注意 我们的稀疏张量格式允许uncoalesced(未合并) 的稀疏张量, 其中索引中可能有重复的坐标;在这种情况下, 解释是索引处的值是所有重复值项的和. uncoalesced 张量允许我们更有效地实现某些运算符. 在大多数情况下, 你不需要关心一个稀疏张量是否coalesced(合并), 因为大多数操作在给出一个coalesced或uncoalesced稀疏张量的情况下都是一样的. 然而, 有两种情况您可能需要注意. 第一, 如果您重复执行可以产生重复项的操作 (例如, torch.sparse.FloatTensor.add()), 你应该偶尔将稀疏张量coalesced一起, 以防止它们变得太大. 第二, 一些运算符将根据它们是否coalesced产生不同的值 (例如, torch.sparse.FloatTensor._values() and torch.sparse.FloatTensor._indices(), 以及 torch.Tensor.sparse_mask()). 这些操作符以下划线作为前缀, 表示它们揭示了内部实现细节, 应该小心使用, 因为使用合并稀疏张量的代码可能无法使用未合并稀疏张量;一般来说, 在使用这些操作符之前显式地合并是最安全的. 例如, 假设我们想通过直接操作torch.sparse.FloatTensor._values().来实现一个操作符.标量乘法可以用很明显的方法实现, 因为乘法分布于加法之上;但是, 平方根不能直接实现, 因为sqrt(a + b) != sqrt(a) + sqrt(b)(如果给定一个uncoalesced的张量, 就会计算出这个结果). class torch.sparse.FloatTensor add() add_() clone() dim() div() div_() get_device() hspmm() mm() mul() mul_() narrow_copy() resizeAs_() size() spadd() spmm() sspaddmm() sspmm() sub() sub_() t_() toDense() transpose() transpose_() zero_() coalesce() is_coalesced() _indices() _values() _nnz() 函数 torch.sparse.addmm(mat, mat1, mat2, beta=1, alpha=1) 这个函数和 torch.addmm() 在forward中做同样的事情, 除了它支持稀疏矩阵mat1 的 backward. mat1应具有 sparse_dim = 2. 请注意, mat1的梯度是一个合并的稀疏张量. 参数: mat (Tensor) – 被相加的稠密矩阵 mat1 (SparseTensor) – 被相乘的稀疏矩阵 mat2 (Tensor) – 被相乘的稠密矩阵 beta (Number__, optional) – 乘数 mat () alpha (Number__, optional) – 乘数 () torch.sparse.mm(mat1, mat2) 执行稀疏矩阵mat1 和 稠密矩阵 mat2的矩阵乘法. 类似于 torch.mm(), 如果 mat1 是一个 tensor, mat2 是一个 tensor, 输出将会是 稠密的 tensor. mat1 应具有 sparse_dim = 2. 此函数也支持两个矩阵的向后. 请注意, mat1的梯度是一个合并的稀疏张量 参数: mat1 (SparseTensor) – 第一个要相乘的稀疏矩阵 mat2 (Tensor) – 第二个要相乘的稠密矩阵 例子: >>> a = torch.randn(2, 3).to_sparse().requires_grad_(True) >>> a tensor(indices=tensor([[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]]), values=tensor([ 1.5901, 0.0183, -0.6146, 1.8061, -0.0112, 0.6302]), size=(2, 3), nnz=6, layout=torch.sparse_coo, requires_grad=True) >>> b = torch.randn(3, 2, requires_grad=True) >>> b tensor([[-0.6479, 0.7874], [-1.2056, 0.5641], [-1.1716, -0.9923]], requires_grad=True) >>> y = torch.sparse.mm(a, b) >>> y tensor([[-0.3323, 1.8723], [-1.8951, 0.7904]], grad_fn=) >>> y.sum().backward() >>> a.grad tensor(indices=tensor([[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]]), values=tensor([ 0.1394, -0.6415, -2.1639, 0.1394, -0.6415, -2.1639]), size=(2, 3), nnz=6, layout=torch.sparse_coo) torch.sparse.sum(input, dim=None, dtype=None) 返回给定维度dim中每行SparseTensor input的总和. 如果 :attr::dim 是一个维度的list, reduce将在全部给定维度进行.如果包括全部的 sparse_dim, 此方法将返回 Tensor 代替 SparseTensor. 所有被求和的 dim 将被 squeezed (see torch.squeeze()),导致速出 tensor 的 :attr::dim 小于 input. backward 过程中, 仅仅 input 的 nnz 位置被反向传播. 请注意, input的梯度是合并的. 参数: input (Tensor) – t输入 SparseTensor dim (int or tuple of python:ints) – 维度或者维度列表. Default: 所有维度. dtype (torch.dtype, optional) – 返回 Tensor 的数据类型. 默认值: dtype 和 input 一致. 例子: >>> nnz = 3 >>> dims = [5, 5, 2, 3] >>> I = torch.cat([torch.randint(0, dims[0], size=(nnz,)), torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz) >>> V = torch.randn(nnz, dims[2], dims[3]) >>> size = torch.Size(dims) >>> S = torch.sparse_coo_tensor(I, V, size) >>> S tensor(indices=tensor([[2, 0, 3], [2, 4, 1]]), values=tensor([[[-0.6438, -1.6467, 1.4004], [ 0.3411, 0.0918, -0.2312]], [[ 0.5348, 0.0634, -2.0494], [-0.7125, -1.0646, 2.1844]], [[ 0.1276, 0.1874, -0.6334], [-1.9682, -0.5340, 0.7483]]]), size=(5, 5, 2, 3), nnz=3, layout=torch.sparse_coo) # when sum over only part of sparse_dims, return a SparseTensor >>> torch.sparse.sum(S, [1, 3]) tensor(indices=tensor([[0, 2, 3]]), values=tensor([[-1.4512, 0.4073], [-0.8901, 0.2017], [-0.3183, -1.7539]]), size=(5, 2), nnz=3, layout=torch.sparse_coo) # when sum over all sparse dim, return a dense Tensor # with summed dims squeezed >>> torch.sparse.sum(S, [0, 1, 3]) tensor([-2.6596, -1.1450]) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"cuda.html":{"url":"cuda.html","title":"torch.cuda","keywords":"","body":"torch.cuda 译者：bdqfork 这个包添加了对CUDA张量类型的支持，它实现了与CPU张量同样的功能，但是它使用GPU进计算。 它是懒加载的，所以你可以随时导入它，并使用 is_available() 来决定是否让你的系统支持CUDA。 CUDA semantics 有关于使用CUDA更详细的信息。 torch.cuda.current_blas_handle() 返回一个cublasHandle_t指针给当前的cuBLAS处理。 torch.cuda.current_device() 返回当前选择地设备索引。 torch.cuda.current_stream() 返回当前选择地 Stream。 class torch.cuda.device(device) Context-manager 用来改变选择的设备。 参数: device (torch.device 或者 int) – 要选择的设备索引。如果这个参数是负数或者是 None，那么它不会起任何作用。 torch.cuda.device_count() 返回可用的GPU数量。 torch.cuda.device_ctx_manager torch.cuda.device 的别名。 class torch.cuda.device_of(obj) Context-manager 将当前的设备改变成传入的对象。. 你可以使用张量或者存储作为参数。如果传入的对象没有分配在GPU上，这个操作是无效的。 参数: obj (Tensor 或者 Storage) – 分配在已选择的设备上的对象。 torch.cuda.empty_cache() 释放缓存分配器当前持有的所有未占用的缓存显存，使其可以用在其他GPU应用且可以在 nvidia-smi可视化。 注意 empty_cache() 并不会增加PyTorch可以使用的GPU显存的大小。 查看 显存管理 来获取更多的GPU显存管理的信息。 torch.cuda.get_device_capability(device) 获取一个设备的cuda容量。 参数: device (torch.device 或者 int, 可选的) – 需要返回容量的设备。如果这个参数传入的是负数，那么这个方法不会起任何作用。如果device是None（默认值），会通过 current_device()传入当前设备。 返回: 设备的最大和最小的cuda容量。 --- --- 返回 类型: tuple(int, int) --- --- torch.cuda.get_device_name(device) 获取设备名称。 参数: device (torch.device 或者 int, 可选的) – 需要返回名称的设备。如果参数是负数，那么将不起作用。如果device是None（默认值），会通过 current_device()传入当前设备。 torch.cuda.init() 初始化PyTorch的CUDA状态。如果你通过C API与PyTorch进行交互，你可能需要显式调用这个方法。只有CUDA的初始化完成，CUDA的功能才会绑定到Python。用户一般不应该需要这个，因为所有PyTorch的CUDA方法都会自动在需要的时候初始化CUDA。 如果CUDA的状态已经初始化了，将不起任何作用。 torch.cuda.is_available() 返回一个bool值，表示当前CUDA是否可用。 torch.cuda.max_memory_allocated(device=None) 返回给定设备的张量的最大GPU显存使用量（以字节为单位）。 参数: device (torch.device or int, optional) – 选择的设备。如果 device 是None（默认的），将返回 current_device()返回的当前设备的数据。 注意 查看 显存管理 部分了解更多关于GPU显存管理部分的详细信息。 torch.cuda.max_memory_cached(device=None) 返回给定设备的缓存分配器管理的最大GPU显存（以字节为单位）。 参数: device (torch.device 或者 int, 可选的) – 选择的设备。如果 device 是None（默认的），将返回 current_device()返回的当前设备的数据。 注意 查看 显存管理 部分了解更多关于GPU显存管理部分的详细信息。 torch.cuda.memory_allocated(device=None) 返回给定设备的当前GPU显存使用量（以字节为单位）。 参数: device (torch.device 或者 int, 可选的) – 选择的设备。如果 device 是None（默认的），将返回 current_device()返回的当前设备的数据。 注意 这可能比 nvidia-smi 显示的数量少，因为一些没有使用的显存会被缓存分配器持有，且一些上下文需要在GPU中创建。查看 显存管理 部分了解更多关于GPU显存管理部分的详细信息。 torch.cuda.memory_cached(device=None) 返回由缓存分配器管理的当前GPU显存（以字节为单位）。 参数: device (torch.device 或者 int, 可选的) – 选择的设备。如果 device 是None（默认的），将返回 current_device()返回的当前设备的数据。 注意 查看 显存管理 部分了解更多关于GPU显存管理部分的详细信息。 torch.cuda.set_device(device) 设置当前设备。 不鼓励使用此功能以支持 device.。在多数情况下，最好使用CUDA_VISIBLE_DEVICES环境变量。 参数: device (torch.device 或者 int) – 选择的设备。如果参数是负数，将不会起任何作用。 torch.cuda.stream(stream) 给定流的上下文管理器。 所有CUDA在上下文中排队的内核将会被添加到选择的流中。 参数: stream (Stream) – 选择的流。如果为None，这个管理器将不起任何作用。 注意 流是针对每个设备的，这个方法只更改当前选择设备的“当前流”。选择一个不同的设备流是不允许的。 torch.cuda.synchronize() 等待所有当前设备的所有流完成。 随机数生成器 torch.cuda.get_rng_state(device=-1) 以ByteTensor的形式返回当前GPU的随机数生成器的状态。 参数: device (int, 可选的) – 需要返回RNG状态的目标设备。默认：-1 (例如，使用当前设备)。 警告 此函数会立即初始化CUDA。 torch.cuda.set_rng_state(new_state, device=-1) 设置当前GPU的随机数生成器状态。 参数: new_state (torch.ByteTensor) – 目标状态 torch.cuda.manual_seed(seed) 设置为当前GPU生成随机数的种子。如果CUDA不可用，可以安全地调用此函数；在这种情况下，它将被静默地忽略。 参数: seed (int) – 目标种子。 警告 如果您使用的是多GPU模型，那么这个函数不具有确定性。设置用于在所有GPU上生成随机数的种子，使用 manual_seed_all(). torch.cuda.manual_seed_all(seed) 设置用于在所有GPU上生成随机数的种子。 如果CUDA不可用，可以安全地调用此函数；在这种情况下，它将被静默地忽略。 参数: seed (int) – 目标种子。 torch.cuda.seed() 将用于生成随机数的种子设置为当前GPU的随机数。 如果CUDA不可用，可以安全地调用此函数；在这种情况下，它将被静默地忽略。 警告 如果您使用的是多GPU模型，此函数将只初始化一个GPU上的种子。在所有GPU上将用于生成随机数的种子设置为随机数， 使用 seed_all(). torch.cuda.seed_all() 在所有GPU上将用于生成随机数的种子设置为随机数。 如果CUDA不可用，可以安全地调用此函数；在这种情况下，它将被静默地忽略。 torch.cuda.initial_seed() 返回当前GPU的当前随机种子。 警告 此函数会立即初始化CUDA。 通信集合 torch.cuda.comm.broadcast(tensor, devices) 将张量广播到多个GPU。 | 参数: | tensor (Tensor) – 需要广播的张量。 devices (Iterable) – 一个要被广播的可迭代的张量集合。注意，它应该是这样的形式 (src, dst1, dst2, …)，其中第一个元素是广播的源设备。 返回: 一个包含tensor副本的元组，放置在与设备索引相对应的设备上。 torch.cuda.comm.broadcast_coalesced(tensors, devices, buffer_size=10485760) 将序列张量广播到指定的GPU。 首先将小型张量合并到缓冲区中以减少同步次数。 | 参数: | tensors (sequence) – 要被广播的张量。 devices (Iterable) – 一个要被广播的可迭代的张量集合。注意，它应该是这样的形式 (src, dst1, dst2, …)，其中第一个元素是广播的源设备。 buffer_size (int) – 用于合并的缓冲区的最大大小 返回: 一个包含tensor副本的元组，放置在与设备索引相对应的设备上。 torch.cuda.comm.reduce_add(inputs, destination=None) 从多个GPU上对张量进行求和。 所有输入必须有相同的形状。 | 参数: | inputs (Iterable__[Tensor]) – 一个可迭代的要添加的张量集合。 destination (int, 可选的) – 输出所在的设备。(默认值: 当前设备)。 返回: 一个包含按元素相加的所有输入的和的张量，在 destination 设备上。 torch.cuda.comm.scatter(tensor, devices, chunk_sizes=None, dim=0, streams=None) 将张量分散在多个GPU上。 | 参数: | tensor (Tensor) – 要分散的张量. devices (Iterable__[int]) – 可迭代的数字集合，指明在哪个设备上的张量要被分散。 chunk_sizes (Iterable__[int]__, 可选的) – 每个设备上放置的块的大小。它应该和devices的长度相等，并相加等于tensor.size(dim)。如果没有指定，张量将会被分散成相同的块。 dim (int, 可选的) – 分块张量所在的维度。 返回: 一个包含tensor块的元组，分散在给定的devices上。 torch.cuda.comm.gather(tensors, dim=0, destination=None) 从多个GPU收集张量。 在所有维度中与dim不同的张量尺寸必须匹配。 | 参数: | tensors (Iterable__[Tensor]) – 可迭代的张量集合。 dim (int) – 纬度，张量将会在这个维度上被连接。 destination (int, 可选的) – 输出设备(-1 表示 CPU, 默认值: 当前设备) 返回: 在destination 设备上的张量，这是沿着dim连接张量的结果。 流和事件 class torch.cuda.Stream 围绕CUDA流的包装器。 CUDA流是属于特定设备的线性执行序列，独立于其他流。 查看 CUDA semantics 获取更详细的信息。 参数: device (torch.device 或者 int, 可选的) – 要在其上分配流的设备。 如果 device 为None（默认值）或负整数，则将使用当前设备。 priority (int, 可选的) – 流的优先级。数字越小，优先级越高。 query() 检查提交的所有工作是否已完成。 返回: 一个布尔值，表示此流中的所有内核是否都已完成。 record_event(event=None) 记录一个事件。 参数: event (Event, 可选的) – 需要记录的事件。如果没有给出，将分配一个新的。 返回: 记录的事件。 --- --- synchronize() 等待此流中的所有内核完成。 注意 这是一个围绕 cudaStreamSynchronize()的包装： 查看 CUDA 文档 获取更详细的信息。 wait_event(event) 使提交给流的所有未来工作等待事件。 参数: event (Event) – 需要等待的事件。 注意 这是一个围绕 cudaStreamWaitEvent()的包装： 查看 CUDA 文档 获取更详细的信息。 此函数返回时无需等待event： 只有未来的操作受到影响。 wait_stream(stream) 与另一个流同步。 提交给此流的所有未来工作将等到所有内核在呼叫完成时提交给给定流。 参数: stream (Stream) – 要同步的流。 注意 此函数返回时不等待stream中当前排队的内核 ： 只有未来的操作受到影响。 class torch.cuda.Event(enable_timing=False, blocking=False, interprocess=False, _handle=None) 围绕CUDA事件的包装。 参数: enable_timing (bool) – 表示事件是否应该测量时间（默认值：False） blocking (bool) – 如果是True， wait() 将会阻塞 (默认值: False) interprocess (bool) – 如果是 True，事件将会在进程中共享 (默认值: False) elapsed_time(end_event) 返回记录事件之前经过的时间。 ipc_handle() 返回此事件的IPC句柄。 query() 检测事件是否被记录。 返回: 一个布尔值，表示事件是否被记录。 record(stream=None) 记录给定流的一个事件。 synchronize() 和一个事件同步。 wait(stream=None) 使给定的流等待一个事件。 显存管理 torch.cuda.empty_cache() 释放当前由缓存分配器保存的所有未占用的缓存显存，以便可以在其他GPU应用程序中使用这些缓存并在nvidia-smi中可见。 注意 empty_cache() 不会增加PyTorch可用的GPU显存量。 查看 显存管理 以了解更多GPU显存管理的详细信息。 torch.cuda.memory_allocated(device=None) 返回给定设备的当前GPU显存使用量（以字节为单位）。 参数: device (torch.device 或者 int, 可选的) – 选定的设备。如果 device 是None（默认的），将返回 current_device()返回的当前设备的数据。 注意 这可能比 nvidia-smi 显示的数量少，因为一些没有使用的显存会被缓存分配器持有，且一些上下文需要在GPU中创建。查看 显存管理 部分了解更多关于GPU显存管理部分的详细信息。 torch.cuda.max_memory_allocated(device=None) 返回给定设备的张量的最大GPU显存使用量（以字节为单位）。 参数: device (torch.device 或者 int, 可选的) – 选择的设备。如果 device 是None（默认的），将返回 current_device()返回的当前设备的数据。 注意 查看 显存管理 部分了解更多关于GPU显存管理部分的详细信息。 torch.cuda.memory_cached(device=None) 返回由缓存分配器管理的当前GPU显存（以字节为单位）。 参数: device (torch.device or int, 可选的) – 选择的设备。如果 device 是None（默认的），将返回 current_device()返回的当前设备的数据。 注意 查看 显存管理 部分了解更多关于GPU显存管理部分的详细信息。 torch.cuda.max_memory_cached(device=None) 返回给定设备的缓存分配器管理的最大GPU显存（以字节为单位）。 参数: device (torch.device 或者 int, 可选的) – 选择的设备。如果 device 是None（默认的），将返回 current_device()返回的当前设备的数据。 注意 查看 显存管理 部分了解更多关于GPU显存管理部分的详细信息。 NVIDIA Tools Extension (NVTX) torch.cuda.nvtx.mark(msg) 描述某个时刻发生的瞬时事件。 参数: msg (string) – 与时间相关的ASCII信息。 torch.cuda.nvtx.range_push(msg) 将范围推到嵌套范围跨度的堆栈上。 返回启动范围的从零开始的深度。 参数: msg (string) – 与时间相关的ASCII信息。 torch.cuda.nvtx.range_pop() 从一堆嵌套范围跨度中弹出一个范围。 返回结束范围的从零开始的深度。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"storage.html":{"url":"storage.html","title":"torch.Storage","keywords":"","body":"torch.Storage 译者：yuange250 torch.Storage 跟绝大部分基于连续存储的数据结构类似，本质上是一个单一数据类型的一维连续数组(array)。 每一个 torch.Tensor 都有一个与之相对应的torch.Storage对象，两者存储数据的数据类型(data type)保持一致。 下面以数据类型为float的torch.FloatStorage 为例介绍一下torch.Storage的成员函数。 class torch.FloatStorage byte() byte()函数可以将此storage对象的数据类型转换为byte char() char()函数可以将此storage对象的数据类型转换为char clone() clone()函数可以返回一个此storage对象的复制 copy_() cpu() 如果此storage对象一开始不在cpu设备上，调用cpu()函数返回此storage对象的一个cpu上的复制 cuda(device=None, non_blocking=False, **kwargs) cuda()函数返回一个存储在CUDA内存中的复制，其中device可以指定cuda设备。 但如果此storage对象早已在CUDA内存中存储，并且其所在的设备编号与cuda()函数传入的device参数一致，则不会发生复制操作，返回原对象。 cuda()函数的参数信息: device (int) – 指定的GPU设备id. 默认为当前设备，即 torch.cuda.current_device()的返回值。 non_blocking (bool) – 如果此参数被设置为True, 并且此对象的资源存储在固定内存上(pinned memory)，那么此cuda()函数产生的复制将与host端的原storage对象保持同步。否则此参数不起作用。 **kwargs – 为了保证兼容性，也支持async参数，此参数的作用与no_blocking参数的作用完全相同，旧版本的遗留问题之一。 data_ptr() double() double()函数可以将此storage对象的数据类型转换为double element_size() fill_() float() float()函数可以将此storage对象的数据类型转换为float static from_buffer() static from_file(filename, shared=False, size=0) → Storage 对于from_file()函数，如果shared参数被设置为True， 那么此部分内存可以在进程间共享，任何对storage对象的更改都会被写入存储文件。 如果 shared 被置为 False, 那么在内存中对storage对象的更改则不会影响到储存文件中的数据。 size 参数是此storage对象中的元素个数。 如果shared被置为False, 那么此存储文件必须要包含size * sizeof(Type)字节大小的数据 (Type是此storage对象的数据类型)。 如果 shared 被置为 True，那么此存储文件只有在需要的时候才会被创建。 from_file()函数的参数： filename (str) – 对应的存储文件名 shared (bool) – 是否共享内存 size (int) – 此storage对象中的元素个数 half() half()函数可以将此storage对象的数据类型转换为half int() int()函数可以将此storage对象的数据类型转换为int is_cuda = False is_pinned() is_shared() is_sparse = False long() long()函数可以将此storage对象的数据类型转换为long new() pin_memory() 如果此storage对象还没有被存储在固定内存中，则pin_memory()函数可以将此storage对象存储到固定内存中 resize_() share_memory_() sharememory()函数可以将此storage对象转移到共享内存中。 对于早已在共享内存中的storage对象，这个操作无效；对于存储在CUDA设备上的storage对象，无需移动即可实现此类对象在进程间的共享，所以此操作对于它们来说也无效。 在共享内存中存储的storage对象无法被更改大小。 sharememory()函数返回值: self short() short()函数可以将此storage对象的数据类型转换为short size() tolist() tolist()函数可以返回一个包含此storage对象所有元素的列表 type(dtype=None, non_blocking=False, **kwargs) 如果函数调用时没有提供dtype参数，则type()函数的调用结果是返回此storage对象的数据类型。如果提供了此参数，则将此storage对象转化为此参数指定的数据类型。如果所提供参数所指定的数据类型与当前storage对象的数据类型一致，则不会进行复制操作，将原对象返回。 type()函数的参数信息: dtype (type or string) – 想要转化为的数据类型 non_blocking (bool) – 如果此参数被设置为True, 并且此对象的资源存储在固定内存上(pinned memory)，那么此cuda()函数产生的复制将与host端的原storage对象保持同步。否则此参数不起作用。 **kwargs – 为了保证兼容性，也支持async参数，此参数的作用与no_blocking参数的作用完全相同，旧版本的遗留问题之一 (已经被deprecated)。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"nn.html":{"url":"nn.html","title":"torch.nn","keywords":"","body":"torch.nn Parameters（参数） class torch.nn.Parameter Parameters对象是一种会被视为模块参数（module parameter）的Tensor张量。 Parameters类是Tensor 的子类, 不过相对于它的父类，Parameters类有一个很重要的特性就是当其在 Module类中被使用并被当做这个Module类的模块属性的时候，那么这个Parameters对象会被自动地添加到这个Module类的参数列表(list of parameters)之中，同时也就会被添加入此Module类的 parameters()方法所返回的参数迭代器中。而Parameters类的父类Tensor类也可以被用为构建模块的属性，但不会被加入参数列表。这样主要是因为，有时可能需要在模型中存储一些非模型参数的临时状态，比如RNN中的最后一个隐状态。而通过使用非Parameter的Tensor类，可以将这些临时变量注册(register)为模型的属性的同时使其不被加入参数列表。 Parameters: data (Tensor) – 参数张量(parameter tensor). requires_grad (bool, optional) – 参数是否需要梯度， 默认为 True。更多细节请看 如何将子图踢出反向传播过程。 Containers（容器） Module（模块） class torch.nn.Module 模块（Module）是所有神经网络模型的基类。 你创建模型的时候也应该继承这个类哦。 模块(Module)中还可以包含其他的模块，你可以将一个模块赋值成为另一个模块的属性，从而成为这个模块的一个子模块。而通过不断的赋值，你可以将不同的模块组织成一个树结构: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) # 当前的nn.Conv2d模块就被赋值成为Model模块的一个子模块，成为“树结构”的叶子 self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) 通过赋值这种方式添加的子模块将会被模型注册(register)，而后当调用模块的一些参数转换函数（to()）的时候，子模块的参数也会一并转换。 add_module(name, module) 向当前模块添加一个子模块。 此子模块可以作为当前模块的属性被访问到，而属性名就是add_module()函数中的name参数。 add_module()函数参数: name (string) – 子模块的名字. 函数调用完成后，可以通过访问当前模块的此字段来访问该子模块。 parameter (Module) – 要添加到当前模块的子模块。 apply(fn) apply()函数的主要作用是将 fn 递归地应用于模块的所有子模块（.children()函数的返回值）以及模块自身。此函数的一个经典应用就是初始化模型的所有参数这一过程(同样参见于 torch-nn-init)。 Parameters: fn (Module -> None) – 要应用于所有子模型的函数 Returns: self --- --- Return type: Module --- --- 例子: >>> def init_weights(m): print(m) if type(m) == nn.Linear: m.weight.data.fill_(1.0) print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) # 将init_weights()函数应用于模块的所有子模块 Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) buffers(recurse=True) 返回模块的缓冲区的迭代器 Parameters: recurse (bool) – 如果设置为True，产生的缓冲区迭代器会遍历模块自己与所有子模块，否则只会遍历模块的直连的成员。 Yields: torch.Tensor – 模型缓冲区 --- --- 举例: >>> for buf in model.buffers(): >>> print(type(buf.data), buf.size()) (20L,) (20L, 1L, 5L, 5L) children() 返回一个当前所有子模块的迭代器 Returns an iterator over immediate children modules. Yields: Module – 子模块 cpu() 将模型的所有参数(parameter)和缓冲区(buffer)都转移到CPU内存中。 Returns: self Return type: Module --- --- cuda(device=None) 将模型的所有参数和缓冲区都转移到CUDA设备内存中。 因为cuda()函数同时会将处理模块中的所有参数并缓存这些参数的对象。所以如果想让模块在GPU上进行优化操作，一定要在构建优化器之前调用模块的cuda()函数。 Parameters: device (int, optional) – 如果设备编号被指定，所有的参数都会被拷贝到编号指定设备上 Returns: self --- --- Return type: Module --- --- double() 将所有的浮点数类型的参数(parameters)和缓冲区(buffers)转换为double数据类型。 Returns: self Return type: Module --- --- dump_patches = False 这个字段可以为load_state_dict()提供 BC 支持（BC support实在不懂是什么意思-.-）。 在 state_dict()函数返回的状态字典（state dict）中， 有一个名为_metadata的属性中存储了这个state_dict的版本号。_metadata是一个遵从了状态字典（state dict）的命名规范的关键字字典， 要想了解这个_metadata在加载状态（loading state dict）的时候是怎么用的，可以看一下 _load_from_state_dict部分的文档。 如果新的参数/缓冲区被添加于/移除自这个模块之中时，这个版本号数字会随之发生变化。同时模块的_load_from_state_dict方法会比较版本号的信息并依据此状态词典（state dict）的变化做出一些适当的调整。 eval() 将模块转换为测试模式。 这个函数只对特定的模块类型有效，如 Dropout和BatchNorm等等。如果想了解这些特定模块在训练/测试模式下各自的运作细节，可以看一下这些特殊模块的文档部分。 extra_repr() 为模块设置额外的展示信息(extra representation)。 如果想要打印展示(print)你的模块的一些定制的额外信息，那你应该在你的模块中复现这个函数。单行和多行的字符串都可以被接受。 float() 将所有浮点数类型的参数(parameters)和缓冲区(buffers)转换为float数据类型。 Returns: self Return type: Module --- --- forward(*input) 定义了每次模块被调用之后所进行的计算过程。 应该被Module类的所有子类重写。 Note 尽管模块的前向操作都被定义在这个函数里面，但是当你要进行模块的前向操作的时候，还是要直接调用模块Module 的实例函数，而不是直接调用这个forward()函数。这主要是因为前者会照顾到注册在此模块之上的钩子函数（the registered hooks）的运行，而后者则不会。 half() 将所有的浮点数类型的参数(parameters)和缓冲区(buffers)转换为half数据类型。 Returns: self Return type: Module --- --- load_state_dict(state_dict, strict=True) 将state_dict中的参数（parameters）和缓冲区（buffers）拷贝到模块和其子模块之中。如果strict被设置为True，那么state_dict中的键值（keys）必须与模型的[state_dict()]函数所返回的键值（keys）信息保持完全的一致。 load_state_dict()函数参数： state_dict (dict) – 一个包含了参数和持久缓冲区的字典。 strict (bool, optional) – 是否严格要求 state_dict 中的键值（keys）与模型 state_dict() 函数返回的键值（keys）信息保持完全一致。 默认： True modules() 返回一个当前模块内所有模块（包括自身）的迭代器。 Yields: Module – a module in the network Note 注意重复的模块只会被返回一次。比在下面这个例子中，l就只会被返回一次。 例子: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential ( (0): Linear (2 -> 2) (1): Linear (2 -> 2) ) 1 -> Linear (2 -> 2) named_buffers(prefix='', recurse=True) 返回一个模块缓冲区的迭代器，每次返回的元素是由缓冲区的名字和缓冲区自身组成的元组。 named_buffers()函数的参数: prefix (str) – 要添加在所有缓冲区名字之前的前缀。 recurse (bool) – 如果设置为True，那样迭代器中不光会返回这个模块自身直连成员的缓冲区，同时也会递归返回其子模块的缓冲区。否则，只返回这个模块直连成员的缓冲区。 Yields: (string, torch.Tensor) – 包含了缓冲区的名字和缓冲区自身的元组 例子: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) named_children() 返回一个当前模型直连的子模块的迭代器，每次返回的元素是由子模块的名字和子模块自身组成的元组。 Yields: (string, Module) – 包含了子模块的名字和子模块自身的元组 例子： >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) named_modules(memo=None, prefix='') 返回一个当前模块内所有模块（包括自身）的迭代器，每次返回的元素是由模块的名字和模块自身组成的元组。 Yields: (string, Module) – 模块名字和模块自身组成的元组 Note 重复的模块只会被返回一次。在下面的例子中，l只被返回了一次。 例子： >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential ( (0): Linear (2 -> 2) (1): Linear (2 -> 2) )) 1 -> ('0', Linear (2 -> 2)) named_parameters(prefix='', recurse=True) 返回一个当前模块内所有参数的迭代器，每次返回的元素是由参数的名字和参数自身组成的元组。 named_parameters()函数参数： prefix (str) – 要在所有参数名字前面添加的前缀。 recurse (bool) – 如果设置为True，那样迭代器中不光会返回这个模块自身直连成员的参数，同时也会返回其子模块的参数。否则，只返回这个模块直连成员的参数。 Yields: (string, Parameter) – 参数名字和参数自身组成的元组 例子: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) parameters(recurse=True) 返回一个遍历模块所有参数的迭代器。 parameters()函数一个经典的应用就是实践中经常将此函数的返回值传入优化器。 Parameters: recurse (bool) – 如果设置为True，那样迭代器中不光会返回这个模块自身直连成员的参数，同时也会递归返回其子模块的参数。否则，只返回这个模块直连成员的参数。 Yields: Parameter – 模块参数 --- --- 例子: >>> for param in model.parameters(): >>> print(type(param.data), param.size()) (20L,) (20L, 1L, 5L, 5L) register_backward_hook(hook) 在模块上注册一个挂载在反向操作之后的钩子函数。（挂载在backward之后这个点上的钩子函数） 对于每次输入，当模块关于此次输入的反向梯度的计算过程完成，该钩子函数都会被调用一次。此钩子函数需要遵从以下函数签名： hook(module, grad_input, grad_output) -> Tensor or None 如果模块的输入或输出是多重的（multiple inputs or outputs），那 grad_input 和 grad_output 应当是元组数据。 钩子函数不能对输入的参数grad_input 和 grad_output进行任何更改，但是可以选择性地根据输入的参数返回一个新的梯度回去，而这个新的梯度在后续的计算中会替换掉grad_input。 Returns: 一个句柄（handle），这个handle的特点就是通过调用handle.remove()函数就可以将这个添加于模块之上的钩子移除掉。 Return type: torch.utils.hooks.RemovableHandle --- --- Warning 对于一些具有很多复杂操作的Module，当前的hook实现版本还不能达到完全理想的效果。举个例子，有些错误的情况下，函数的输入参数grad_input 和 grad_output中可能只是真正的输入和输出变量的一个子集。对于此类的Module，你应该使用[torch.Tensor.register_hook()]直接将钩子挂载到某个特定的输入输出的变量上，而不是当前的模块。 register_buffer(name, tensor) 往模块上添加一个持久缓冲区。 这个函数的经常会被用于向模块添加不会被认为是模块参数（model parameter）的缓冲区。举个栗子，BatchNorm的running_mean就不是一个参数，但却属于持久状态。 所添加的缓冲区可以通过给定的名字(name参数)以访问模块的属性的方式进行访问。 register_buffer()函数的参数: name (string) – 要添加的缓冲区的名字。所添加的缓冲区可以通过此名字以访问模块的属性的方式进行访问。 tensor (Tensor) – 需要注册到模块上的缓冲区。 例子: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook(hook) 在模块上注册一个挂载在前向操作之后的钩子函数。（挂载在forward操作结束之后这个点） 此钩子函数在每次模块的 forward()函数运行结束产生output之后就会被触发。此钩子函数需要遵从以下函数签名： hook(module, input, output) -> None 此钩子函数不能进行会修改 input 和 output 这两个参数的操作。 Returns: 一个句柄（handle），这个handle的特点就是通过调用handle.remove()函数就可以将这个添加于模块之上的钩子移除掉。 Return type: torch.utils.hooks.RemovableHandle --- --- register_forward_pre_hook(hook) 在模块上注册一个挂载在前向操作之前的钩子函数。（挂载在forward操作开始之前这个点） 此钩子函数在每次模块的 forward()函数运行开始之前会被触发。此钩子函数需要遵从以下函数签名： The hook will be called every time before forward() is invoked. It should have the following signature: hook(module, input) -> None 此钩子函数不能进行会修改 input 这个参数的操作。 Returns: 一个句柄（handle），这个handle的特点就是通过调用handle.remove()函数就可以将这个添加于模块之上的钩子移除掉。 Return type: torch.utils.hooks.RemovableHandle --- --- register_parameter(name, param) 向模块添加一个参数（parameter）。 所添加的参数（parameter）可以通过给定的名字(name参数)以访问模块的属性的方式进行访问。 register_parameter()函数的参数： name (string) – 所添加的参数的名字. 所添加的参数（parameter）可以通过此名字以访问模块的属性的方式进行访问 parameter (Parameter) – 要添加到模块之上的参数。 state_dict(destination=None, prefix='', keep_vars=False) 返回一个包含了模块当前所有状态(state)的字典(dictionary)。 所有的参数和持久缓冲区都被囊括在其中。字典的键值就是响应的参数和缓冲区的名字(name)。 Returns: 一个包含了模块当前所有状态的字典 Return type: dict --- --- 例子: >>> module.state_dict().keys() ['bias', 'weight'] to(*args, **kwargs) 移动 并且/或者（and/or）转换所有的参数和缓冲区。 这个函数可以这样调用： to(device=None, dtype=None, non_blocking=False) to(dtype, non_blocking=False) to(tensor, non_blocking=False) 此函数的函数签名跟torch.Tensor.to()函数的函数签名很相似，只不过这个函数dtype参数只接受浮点数类型的dtype，如float， double， half（ floating point desired dtype s）。同时，这个方法只会将浮点数类型的参数和缓冲区（the floating point parameters and buffers）转化为dtype（如果输入参数中给定的话）的数据类型。而对于整数类型的参数和缓冲区（the integral parameters and buffers），即便输入参数中给定了dtype，也不会进行转换操作，而如果给定了 device参数，移动操作则会正常进行。当non_blocking参数被设置为True之后，此函数会尽可能地相对于 host 进行异步的 转换/移动 操作，比如，将存储在固定内存（pinned memory）上的CPU Tensors移动到CUDA设备上这一过程既是如此。 例子在下面。 Note 这个方法对模块的修改都是in-place操作。 to()函数的参数: device (torch.device) – 想要将这个模块中的参数和缓冲区转移到的设备。 dtype (torch.dtype) – 想要将这个模块中浮点数的参数和缓冲区转化为的浮点数数据类型。 tensor (torch.Tensor) – 一个Tensor，如果被指定，其dtype和device信息，将分别起到上面两个参数的作用，也就是说，这个模块的浮点数的参数和缓冲区的数据类型将会被转化为这个Tensor的dtype类型，同时被转移到此Tensor所处的设备device上去。 Returns: self Return type: Module --- --- 例子: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) train(mode=True) 将模块转换成训练模式。 这个函数只对特定的模块类型有效，如 Dropout和BatchNorm等等。如果想了解这些特定模块在训练/测试模式下各自的运作细节，可以看一下这些特殊模块的文档部分。 Returns: self Return type: Module --- --- type(dst_type) 将所有的参数和缓冲区转化为 dst_type的数据类型。 Parameters: dst_type (type or string) – 要转化的数据类型 Returns: self --- --- Return type: Module --- --- zero_grad() 讲模块所有参数的梯度设置为0。 Sequential class torch.nn.Sequential(*args) 一种顺序容器。传入Sequential构造器中的模块会被按照他们传入的顺序依次添加到Sequential之上。相应的，一个由模块组成的顺序词典也可以被传入到Sequential的构造器中。 为了方便大家理解，举个简单的例子： # 构建Sequential的例子 model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # 利用OrderedDict构建Sequential的例子 model = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ])) ModuleList (模块列表) class torch.nn.ModuleList(modules=None) ModuleList的作用是将一堆模块（module）存储在一个列表之中。 ModuleList 可以按一般的python列表的索引方式进行索引，但ModuleList中的模块都已被正确注册，并且对所有的Module method可见。 Parameters: modules (iterable__, optional) – 一个要添加到ModuleList中的由模块组成的可迭代结构(an iterable of modules) 例子: class MyModule(nn.Module): def __init__(self): super(MyModule, self).__init__() self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)]) def forward(self, x): # ModuleList可以被当作一个迭代器，同时也可以使用index索引 for i, l in enumerate(self.linears): x = self.linears[i // 2](x) + l(x) return x append(module) 将一个模块添加到ModuleList的末尾，与python list的append()一致。 Parameters: module (nn.Module) – 要添加的模块 extend(modules) 将一个由模块组成的可迭代结构添加到ModuleList的末尾，与python list的extend()一致。 Parameters: modules (iterable) – 要添加到ModuleList末尾的由模块组成的可迭代结构 insert(index, module) 将给定的module插入到ModuleList的index位置。 insert()函数的参数: index (int) – 要插入的位置 module (nn.Module) – 要插入的模块 ModuleDict (模块词典) class torch.nn.ModuleDict(modules=None) ModuleDict的作用是将一堆模块（module）存储在一个词典之中。 ModuleDict 可以按一般的python词典的索引方式进行索引，但ModuleDict中的模块都已被正确注册，并且对所有的Module method可见。 Parameters: modules (iterable__, optional) – 一个由(string: module)映射组成的映射集合（词典）或者 一个由(string, module)键/值对组成的可迭代结构 Example: class MyModule(nn.Module): def __init__(self): super(MyModule, self).__init__() self.choices = nn.ModuleDict({ 'conv': nn.Conv2d(10, 10, 3), 'pool': nn.MaxPool2d(3) }) self.activations = nn.ModuleDict([ ['lrelu', nn.LeakyReLU()], ['prelu', nn.PReLU()] ]) def forward(self, x, choice, act): x = self.choices[choice](x) x = self.activations[act](x) return x clear() 移除ModuleDict中所有的元素。 items() 返回一个由ModuleDict中的键/值对组成的可迭代结构。 keys() 返回一个由ModuleDict中的键组成的可迭代结构。 pop(key) 将key这个键从ModuleDict中删除，并将其对应的模块返回。 Parameters: key (string) – 要从ModuleDict中弹出的键 update(modules) 通过传入的映射或者由键/值对组成的可迭代结构对当前的ModuleDict进行更新，如果传入对象与当前ModuleDict中存在键重复，当前ModuleDict中这些重复的键所对应的值将被覆盖。 Parameters: modules (iterable) – 一个由(string: Module)映射组成的映射集合（词典）或者 一个由(string: Module)键/值对组成的可迭代结构 values() 返回一个由ModuleDict中的值组成的可迭代结构。 ParameterList (参数列表) class torch.nn.ParameterList(parameters=None) ParameterList的作用是将一堆参数（parameter）存储到一个列表中。 ParameterList 可以按一般的python列表的索引方式进行索引，但ParameterList中的参数（parameter）都已被正确注册，并且对所有的Module method可见。 Parameters: parameters (iterable__, optional) – 要添加到ParameterList之上的由parameter组成的可迭代结构 例子: class MyModule(nn.Module): def __init__(self): super(MyModule, self).__init__() self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)]) def forward(self, x): # ParameterList可以被当作一个迭代器，同时也可以使用index索引 for i, p in enumerate(self.params): x = self.params[i // 2].mm(x) + p.mm(x) return x append(parameter) 将一个parameter添加到ParameterList的末尾。 Parameters: parameter (nn.Parameter) – 要添加的参数 extend(parameters) 将一个由parameter组成的Python可迭代结构添加到ParameterList的末尾。 Parameters: parameters (iterable) – 要添加到ParameterList的末尾的由parameter组成的Python可迭代结构 ParameterDict (参数词典) class torch.nn.ParameterDict(parameters=None) ParameterDict的作用是将一堆参数（Parameter）存储在一个词典之中。 ParameterDict 可以按一般的python词典的索引方式进行索引，但ParameterDictt中的参数都已被正确注册，并且对所有的Module method可见。 Parameters: parameters (iterable__, optional) – 一个由(string:Parameter)映射组成的映射集合（词典）或者 一个由(string, Parameter)键/值对组成的可迭代结构 例子: class MyModule(nn.Module): def __init__(self): super(MyModule, self).__init__() self.params = nn.ParameterDict({ 'left': nn.Parameter(torch.randn(5, 10)), 'right': nn.Parameter(torch.randn(5, 10)) }) def forward(self, x, choice): x = self.params[choice].mm(x) return x clear() 移除ParameterDict中所有的元素。 items() 返回一个由ParameterDict中的键/值对组成的可迭代结构。 keys() 返回一个由 ParameterDict中的键组成的可迭代结构。 pop(key) 将key这个键从ParameterDict中删除，并将其对应的模块返回。 Parameters: key (string) – 要从ParameterDict中弹出的键 update(parameters) 通过传入的映射或者由键/值对组成的可迭代结构对当前的ParameterDict进行更新，如果传入对象与当前ParameterDict中存在键重复，当前ParameterDict中这些重复的键所对应的值将被覆盖。 Parameters: parameters (iterable) – modules (iterable) – 一个由(string: Parameter)映射组成的映射集合（词典）或者 一个由(string: Parameter)键/值对组成的可迭代结构 values() 返回一个由ParameterDict中的值组成的可迭代结构。 Convolution layers (卷积层) Conv1d class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) 利用指定大小的一维卷积核对输入的多通道一维输入信号进行一维卷积操作的卷积层。 在最简单的情况下，对于输入大小为，输出大小为的一维卷积层，其卷积计算过程可以如下表述： 这里的符号实际上是一个互相关（cross-correlation） 操作符（大家可以自己查一下互相关和真卷积的区别，互相关因为实现起来很简单，所以一般的深度学习框架都是用互相关操作取代真卷积）, is a batch size, 代表通道的数量, 代表信号序列的长度。 stride 参数控制了互相关操作（伪卷积）的步长，参数的数据类型一般是单个数字或者一个只有一个元素的元组。 padding 参数控制了要在一维卷积核的输入信号的各维度各边上要补齐0的层数。 dilation 参数控制了卷积核中各元素之间的距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 groups 控制了输入输出之间的连接（connections）的数量。in_channels 和 out_channels 必须能被 groups 整除。举个栗子， > * 当 groups=1, 此Conv1d层会使用一个卷积层进行所有输入到输出的卷积操作。 > * 当 groups=2, 此时Conv1d层会产生两个并列的卷积层。同时，输入通道被分为两半，两个卷积层分别处理一半的输入通道，同时各自产生一半的输出通道。最后这两个卷积层的输出会被concatenated一起，作为此Conv1d层的输出。 > * 当 groups= in_channels, 每个输入通道都会被单独的一组卷积层处理，这个组的大小是 Note 取决于你卷积核的大小，有些时候输入数据中某些列（最后几列）可能不会参与计算（比如列数整除卷积核大小有余数，而又没有padding，那最后的余数列一般不会参与卷积计算），这主要是因为pytorch中的互相关操作cross-correlation是保证计算正确的操作(valid operation)， 而不是满操作(full operation)。所以实际操作中，还是要亲尽量选择好合适的padding参数哦。 Note 当groups == in_channels 并且 out_channels == K * in_channels（其中K是正整数）的时候，这个操作也被称为深度卷积。 举个创建深度卷积层的例子，对于一个大小为 的输入，要构建一个深度乘数为K的深度卷积层，可以通过以下参数来创建：。 Note 当程序的运行环境是使用了CuDNN的CUDA环境的时候，一些非确定性的算法（nondeterministic algorithm）可能会被采用以提高整个计算的性能。如果不想使用这些非确定性的算法，你可以通过设置torch.backends.cudnn.deterministic = True来让整个计算过程保持确定性（可能会损失一定的计算性能）。对于后端(background)，你可以看一下这一部分Reproducibility了解其相关信息。 Conv1d的参数: in_channels (int) – 输入通道个数 out_channels (int) – 输出通道个数 kernel_size (int or tuple) – 卷积核大小 stride (int or tuple, optional) – 卷积操作的步长。 默认： 1 padding (int or tuple, optional) – 输入数据各维度各边上要补齐0的层数。 默认： 0 dilation (int or tuple, optional) – 卷积核各元素之间的距离。 默认： 1 groups (int, optional) – 输入通道与输出通道之间相互隔离的连接的个数。 默认：1 bias (bool, optional) – 如果被置为 True，向输出增加一个偏差量，此偏差是可学习参数。 默认：True Shape: 输入: 输出: 其中 | 内部Variables： | weight (Tensor) – Conv1d模块中的一个大小为(out_channels, in_channels, kernel_size)的权重张量，这些权重可训练学习(learnable)。这些权重的初始值的采样空间是， 其中。 bias (Tensor) – 模块的偏差项，大小为(out_channels)，可训练学习。如果构造Conv1d时构造函数中的bias 被置为 True，那么这些权重的初始值的采样空间是， 其中 。 例子: >>> m = nn.Conv1d(16, 33, 3, stride=2) >>> input = torch.randn(20, 16, 50) >>> output = m(input) Conv2d class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) 利用指定大小的二维卷积核对输入的多通道二维输入信号进行二维卷积操作的卷积层。 在最简单的情况下，对于输入大小为，输出大小为的二维维卷积层，其卷积计算过程可以如下表述： 这里的符号实际上是一个二维互相关（cross-correlation） 操作符（大家可以自己查一下互相关和真卷积的区别，互相关因为实现起来很简单，所以一般的深度学习框架都是用互相关操作取代真卷积）, is a batch size, 代表通道的数量, 是输入的二维数据的像素高度， 是输入的二维数据的像素宽度。 stride 参数控制了互相关操作（伪卷积）的步长，参数的数据类型一般是单个数字或者一个只有一个元素的元组。 padding 参数控制了要在二维卷积核的输入信号的各维度各边上要补齐0的层数。 dilation 参数控制了卷积核中各元素之间的距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 groups 控制了输入输出之间的连接（connections）的数量。in_channels 和 out_channels 必须能被 groups 整除。举个栗子， > * 当 groups=1, 此Conv1d层会使用一个卷积层进行所有输入到输出的卷积操作。 > * 当 groups=2, 此时Conv1d层会产生两个并列的卷积层。同时，输入通道被分为两半，两个卷积层分别处理一半的输入通道，同时各自产生一半的输出通道。最后这两个卷积层的输出会被concatenated一起，作为此Conv1d层的输出。 > * 当 groups= in_channels, 每个输入通道都会被单独的一组卷积层处理，这个组的大小是 kernel_size, stride, padding, dilation这几个参数均支持一下输入形式： 一个 int 数字 – 二维数据的高和宽这两个维度都会采用这一个数字。 一个由两个int数字组成的tuple– 这种情况下，二维数据的高这一维度会采用元组中的第一个int数字，宽这一维度会采用第二个int数字。 Note 取决于你卷积核的大小，有些时候输入数据中某些列（最后几列）可能不会参与计算（比如列数整除卷积核大小有余数，而又没有padding，那最后的余数列一般不会参与卷积计算），这主要是因为pytorch中的互相关操作cross-correlation是保证计算正确的操作(valid operation)， 而不是满操作(full operation)。所以实际操作中，还是要亲尽量选择好合适的padding参数哦。 Note 当groups == in_channels 并且 out_channels == K * in_channels（其中K是正整数）的时候，这个操作也被称为深度卷积。 换句话说，对于一个大小为的输入，要构建一个深度乘数为K的深度卷积层，可以通过以下参数来创建：。 Note 当程序的运行环境是使用了CuDNN的CUDA环境的时候，一些非确定性的算法（nondeterministic algorithm）可能会被采用以提高整个计算的性能。如果不想使用这些非确定性的算法，你可以通过设置torch.backends.cudnn.deterministic = True来让整个计算过程保持确定性（可能会损失一定的计算性能）。对于后端(background)，你可以看一下这一部分Reproducibility了解其相关信息。 Conv2d的参数: in_channels (int) – 输入通道个数 out_channels (int) – 输出通道个数 kernel_size (int or tuple) – 卷积核大小 stride (int or tuple, optional) –卷积操作的步长。 默认： 1 padding (int or tuple, optional) – 输入数据各维度各边上要补齐0的层数。 默认： 0 dilation (int or tuple, optional) –卷积核各元素之间的距离。 默认： 1 groups (int, optional) – 输入通道与输出通道之间相互隔离的连接的个数。 默认：1 bias (bool, optional) – 如果被置为 True，向输出增加一个偏差量，此偏差是可学习参数。 默认：True Shape: 输入: 输出: 其中 | 内部Variables: | weight (Tensor) – Conv2d模块中的一个大小为 (out_channels, in_channels, kernel_size[0], kernel_size[1])的权重张量，这些权重可训练学习(learnable)。这些权重的初始值的采样空间是 ， 其中。 bias (Tensor) – 块的偏差项，大小为(out_channels)，可训练学习。如果构造Conv2d时构造函数中的bias 被置为 True，那么这些权重的初始值的采样空间是，其中。 例子: >>> # With square kernels and equal stride >>> m = nn.Conv2d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) >>> # non-square kernels and unequal stride and with padding and dilation >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)) >>> input = torch.randn(20, 16, 50, 100) >>> output = m(input) Conv3d class torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) 利用指定大小的三维卷积核对输入的多通道三维输入信号进行三维卷积操作的卷积层。 最简单的情况下，对于输入大小为，输出大小为 的三维卷积层，其卷积计算过程可以如下表述： 这里的 符号实际上是一个三维互相关 cross-correlation 操作符。 stride 数控制了互相关操作（伪卷积）的步长。 padding 参数控制了要在三维卷积核的输入信号的各维度各边上要补齐0的层数。 dilation 参数控制了卷积核中各元素之间的距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 groups 控制了输入输出之间的连接（connections）的数量。in_channels 和 out_channels 必须能被 groups 整除。举个栗子， > * 当 groups=1, 此Conv3d层会使用一个卷积层进行对所有输入到输出的卷积操作。 > * 当 groups=2, 此时Conv3d层会产生两个并列的卷积层。同时，输入通道被分为两半，两个卷积层分别处理一半的输入通道，同时各自产生一半的输出通道。最后这两个卷积层的输出会被concatenated一起，作为此Conv3d层的输出。 > * 当 groups= in_channels, 每个输入通道都会被单独的一组卷积层处理，这个组的大小是 . kernel_size, stride, padding, dilation这几个参数均支持一下输入形式： 一个 int 数字 – 三维维数据的深度，高和宽这三个维度都会采用这一个数字。 一个由三个int数字组成的tuple– 这种情况下，三维数据的深度这一维度会采用元组中的第一个int数字，高这一维度会采用元组中的第二个int数字，宽这一维度会采用第三个int数字。 Note 取决于你卷积核的大小，有些时候输入数据中某些列（最后几列）可能不会参与计算（比如列数整除卷积核大小有余数，而又没有padding，那最后的余数列一般不会参与卷积计算），这主要是因为pytorch中的互相关操作cross-correlation是保证计算正确的操作(valid operation)， 而不是满操作(full operation)。所以实际操作中，还是要亲尽量选择好合适的padding参数哦。 Note 当groups == in_channels 并且 out_channels == K * in_channels（其中K是正整数）的时候，这个操作也被称为深度卷积。 换句话说，对于一个大小为 的输入，要构建一个深度乘数为K的深度卷积层，可以通过以下参数来创建：。 Note 当程序的运行环境是使用了CuDNN的CUDA环境的时候，一些非确定性的算法（nondeterministic algorithm）可能会被采用以提高整个计算的性能。如果不想使用这些非确定性的算法，你可以通过设置torch.backends.cudnn.deterministic = True来让整个计算过程保持确定性（可能会损失一定的计算性能）。对于后端(background)，你可以看一下这一部分Reproducibility了解其相关信息。 Parameters: in_channels (int) – 输入通道的个数 out_channels (int) – 卷积操作输出通道的个数 kernel_size (int or tuple) – 卷积核大小 stride (int or tuple, optional) – 卷积操作的步长。 默认： 1 padding (int or tuple, optional) – 输入数据各维度各边上要补齐0的层数。 默认： 0 dilation (int or tuple, optional) – 卷积核各元素之间的距离。 默认： 1 groups (int, optional) – 输入通道与输出通道之间相互隔离的连接的个数。 默认：1 bias (bool, optional) – 如果被置为 True，向输出增加一个偏差量，此偏差是可学习参数。 默认：True Shape: 输入: 输出: where | 内部Variables: | weight (Tensor) – Conv3d模块中的一个大小为 (out_channels, in_channels, kernel_size[0], kernel_size[1], kernel_size[2]) 的权重张量，这些权重可训练学习(learnable)。这些权重的初始值的采样空间是，其中。 bias (Tensor) – 模块的偏差项，大小为(out_channels)，可训练学习。如果构造Conv1d时构造函数中的bias 被置为 True，那么这些权重的初始值的采样空间是 ，其中 。 例子: >>> # With square kernels and equal stride >>> m = nn.Conv3d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0)) >>> input = torch.randn(20, 16, 10, 50, 100) >>> output = m(input) ConvTranspose1d class torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1) 利用指定大小的一维转置卷积核对输入的多通道一维输入信号进行转置卷积（当然此卷积也是互相关操作，cross-correlation）操作的模块。 该模块可以看作是Conv1d相对于其输入的梯度(the gradient of Conv1d with respect to its input， 直译)， 转置卷积又被称为小数步长卷积或是反卷积（尽管这不是一个真正意义上的反卷积）。 stride 控制了转置卷积操作的步长 padding 控制了要在输入的各维度的各边上补齐0的层数，与Conv1d不同的地方，此padding参数与实际补齐0的层数的关系为层数 = kernel_size - 1 - padding，详情请见下面的note。 output_padding 控制了转置卷积操作输出的各维度的长度增量，但注意这个参数不是说要往转置卷积的输出上pad 0，而是直接控制转置卷积的输出大小为根据此参数pad后的大小。更多的详情请见下面的note。 dilation 控制了卷积核中各点之间的空间距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 groups 控制了输入输出之间的连接（connections）的数量。in_channels 和 out_channels 必须能被 groups 整除。举个栗子， > * 当 groups=1, 此Conv1d层会使用一个卷积层进行所有输入到输出的卷积操作。 > * 当 groups=2, 此时Conv1d层会产生两个并列的卷积层。同时，输入通道被分为两半，两个卷积层分别处理一半的输入通道，同时各自产生一半的输出通道。最后这两个卷积层的输出会被concatenated一起，作为此Conv1d层的输出。 > * 当 groups= in_channels, 每个输入通道都会被单独的一组卷积层处理，这个组的大小是。 Note 取决于你卷积核的大小，有些时候输入数据中某些列（最后几列）可能不会参与计算（比如列数整除卷积核大小有余数，而又没有padding，那最后的余数列一般不会参与卷积计算），这主要是因为pytorch中的互相关操作cross-correlation是保证计算正确的操作(valid operation)， 而不是满操作(full operation)。所以实际操作中，还是要亲尽量选择好合适的padding参数哦。 Note padding 参数控制了要在输入的各维度各边上补齐0的层数，与在Conv1d中不同的是，在转置卷积操作过程中，此padding参数与实际补齐0的层数的关系为层数 = kernel_size - 1 - padding， 这样设置的主要原因是当使用相同的参数构建Conv1d 和ConvTranspose1d模块的时候，这种设置能够实现两个模块有正好相反的输入输出的大小，即Conv1d的输出大小是其对应的ConvTranspose1d模块的输入大小，而ConvTranspose1d的输出大小又恰好是其对应的Conv1d模块的输入大小。然而，当stride > 1的时候，Conv1d 的一个输出大小可能会对应多个输入大小，上一个note中就详细的介绍了这种情况，这样的情况下要保持前面提到两种模块的输入输出保持反向一致，那就要用到 output_padding参数了，这个参数可以增加转置卷积输出的某一维度的大小，以此来达到前面提到的同参数构建的Conv1d 和ConvTranspose1d模块的输入输出方向一致。 但注意这个参数不是说要往转置卷积的输出上pad 0，而是直接控制转置卷积的输出各维度的大小为根据此参数pad后的大小。 Note 当程序的运行环境是使用了CuDNN的CUDA环境的时候，一些非确定性的算法（nondeterministic algorithm）可能会被采用以提高整个计算的性能。如果不想使用这些非确定性的算法，你可以通过设置torch.backends.cudnn.deterministic = True来让整个计算过程保持确定性（可能会损失一定的计算性能）。对于后端(background)，你可以看一下这一部分Reproducibility了解其相关信息。 Parameters: in_channels (int) – 输入通道的个数 out_channels (int) – 卷积操作输出通道的个数 kernel_size (int or tuple) – 卷积核大小 stride (int or tuple, optional) – 卷积操作的步长。 默认： 1 padding (int or tuple, optional) – kernel_size - 1 - padding 层 0 会被补齐到输入数据的各边上。 默认： 0 output_padding (int or tuple, optional) – 输出的各维度要增加的大小。默认：0 groups (int, optional) – 输入通道与输出通道之间相互隔离的连接的个数。 默认：1 bias (bool, optional) – 如果被置为 True，向输出增加一个偏差量，此偏差是可学习参数。 默认：True dilation (int or tuple, optional) – 卷积核各元素之间的距离。 默认： 1 Shape: 输入: 输出: 其中， | Variables: | weight (Tensor) – 模块中的一个大小为 (in_channels, out_channels, kernel_size[0])的权重张量，这些权重可训练学习(learnable)。这些权重的初始值的采样空间是，其中 。 bias (Tensor) – 模块的偏差项，大小为 (out_channels)， 如果构造函数中的 bias 被置为 True，那么这些权重的初始值的采样空间是 ，其中 。 ConvTranspose2d class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1) 利用指定大小的二维转置卷积核对输入的多通道二维输入信号进行转置卷积（当然此卷积也是互相关操作，cross-correlation）操作的模块。 该模块可以看作是Conv2d相对于其输入的梯度(the gradient of Conv2d with respect to its input， 直译)， 转置卷积又被称为小数步长卷积或是反卷积（尽管这不是一个真正意义上的反卷积）。 stride 控制了转置卷积操作的步长 padding 控制了要在输入的各维度的各边上补齐0的层数，与Conv1d不同的地方，此padding参数与实际补齐0的层数的关系为层数 = kernel_size - 1 - padding，详情请见下面的note。 output_padding 控制了转置卷积操作输出的各维度的长度增量，但注意这个参数不是说要往转置卷积的输出上pad 0，而是直接控制转置卷积的输出大小为根据此参数pad后的大小。更多的详情请见下面的note。 dilation 控制了卷积核中各点之间的空间距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 groups 控制了输入输出之间的连接（connections）的数量。in_channels 和 out_channels 必须能被 groups 整除。举个栗子， > * 当 groups=1, 此Conv1d层会使用一个卷积层进行所有输入到输出的卷积操作。 > * 当 groups=2, 此时Conv1d层会产生两个并列的卷积层。同时，输入通道被分为两半，两个卷积层分别处理一半的输入通道，同时各自产生一半的输出通道。最后这两个卷积层的输出会被concatenated一起，作为此Conv1d层的输出。 > * 当 groups= in_channels, 每个输入通道都会被单独的一组卷积层处理，这个组的大小是。 kernel_size, stride, padding, output_padding 这几个参数均支持一下输入形式： 一个 int 数字 – 二维维数据的高和宽这两个维度都会采用这一个数字。 一个由两个int数字组成的tuple– 这种情况下，二维数据的高这一维度会采用元组中的第一个int数字，宽这一维度会采用第二个int数字。 Note 取决于你卷积核的大小，有些时候输入数据中某些列（最后几列）可能不会参与计算（比如列数整除卷积核大小有余数，而又没有padding，那最后的余数列一般不会参与卷积计算），这主要是因为pytorch中的互相关操作cross-correlation是保证计算正确的操作(valid operation)， 而不是满操作(full operation)。所以实际操作中，还是要亲尽量选择好合适的padding参数哦。 Note padding 参数控制了要在输入的各维度各边上补齐0的层数，与在Conv1d中不同的是，在转置卷积操作过程中，此padding参数与实际补齐0的层数的关系为层数 = kernel_size - 1 - padding， 这样设置的主要原因是当使用相同的参数构建Conv2d 和ConvTranspose2d模块的时候，这种设置能够实现两个模块有正好相反的输入输出的大小，即Conv2d的输出大小是其对应的ConvTranspose2d模块的输入大小，而ConvTranspose2d的输出大小又恰好是其对应的Conv2d模块的输入大小。然而，当stride > 1的时候，Conv2d 的一个输出大小可能会对应多个输入大小，上一个note中就详细的介绍了这种情况，这样的情况下要保持前面提到两种模块的输入输出保持反向一致，那就要用到 output_padding参数了，这个参数可以增加转置卷积输出的某一维度的大小，以此来达到前面提到的同参数构建的Conv2d 和ConvTranspose2d模块的输入输出方向一致。 但注意这个参数不是说要往转置卷积的输出上pad 0，而是直接控制转置卷积的输出各维度的大小为根据此参数pad后的大小。 Note 当程序的运行环境是使用了CuDNN的CUDA环境的时候，一些非确定性的算法（nondeterministic algorithm）可能会被采用以提高整个计算的性能。如果不想使用这些非确定性的算法，你可以通过设置torch.backends.cudnn.deterministic = True来让整个计算过程保持确定性（可能会损失一定的计算性能）。对于后端(background)，你可以看一下这一部分Reproducibility了解其相关信息。 Parameters: in_channels (int) – 输入通道的个数 out_channels (int) – 卷积操作输出通道的个数 kernel_size (int or tuple) – 卷积核大小 stride (int or tuple, optional) – 卷积操作的步长。 默认： 1 padding (int or tuple, optional) – kernel_size - 1 - padding 层 0 会被补齐到输入数据的各边上。 默认： 0 output_padding (int or tuple, optional) – 输出的各维度要增加的大小。默认：0 groups (int, optional) – 输入通道与输出通道之间相互隔离的连接的个数。 默认：1 bias (bool, optional) – 如果被置为 True，向输出增加一个偏差量，此偏差是可学习参数。 默认：True dilation (int or tuple, optional) – 卷积核各元素之间的距离。 默认： 1 Shape: 输入: 输出: 其中 | Variables: | weight (Tensor) – 模块中的一个大小为 (in_channels, out_channels, kernel_size[0], kernel_size[1])的权重张量，这些权重可训练学习(learnable)。这些权重的初始值的采样空间是，其中 。 bias (Tensor) – 模块的偏差项，大小为 (out_channels)， 如果构造函数中的 bias 被置为 True，那么这些权重的初始值的采样空间是 ，其中 。 例子: >>> # With square kernels and equal stride >>> m = nn.ConvTranspose2d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) >>> input = torch.randn(20, 16, 50, 100) >>> output = m(input) >>> # exact output size can be also specified as an argument >>> input = torch.randn(1, 16, 12, 12) >>> downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1) >>> upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1) >>> h = downsample(input) >>> h.size() torch.Size([1, 16, 6, 6]) >>> output = upsample(h, output_size=input.size()) >>> output.size() torch.Size([1, 16, 12, 12]) ConvTranspose3d class torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1) 利用指定大小的三维转置卷积核对输入的多通道三维输入信号进行转置卷积（当然此卷积也是互相关操作，cross-correlation）操作的模块。转置卷积的操作本质是将各通道输入与卷积核做乘法，然后返回各通道与此卷积核乘积结果之和（卷积的定义）。 该模块可以看作是Conv3d相对于其输入的梯度(the gradient of Conv3d with respect to its input， 直译)， 转置卷积又被称为小数步长卷积或是反卷积（尽管这不是一个真正意义上的反卷积）。 stride 控制了转置卷积操作的步长 padding 控制了要在输入的各维度的各边上补齐0的层数，与Conv1d不同的地方，此padding参数与实际补齐0的层数的关系为层数 = kernel_size - 1 - padding，详情请见下面的note。 output_padding 控制了转置卷积操作输出的各维度的长度增量，但注意这个参数不是说要往转置卷积的输出上pad 0，而是直接控制转置卷积的输出大小为根据此参数pad后的大小。更多的详情请见下面的note。 dilation 控制了卷积核中各点之间的空间距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 groups 控制了输入输出之间的连接（connections）的数量。in_channels 和 out_channels 必须能被 groups 整除。举个栗子， > * 当 groups=1, 此Conv1d层会使用一个卷积层进行所有输入到输出的卷积操作。 > * 当 groups=2, 此时Conv1d层会产生两个并列的卷积层。同时，输入通道被分为两半，两个卷积层分别处理一半的输入通道，同时各自产生一半的输出通道。最后这两个卷积层的输出会被concatenated一起，作为此Conv1d层的输出。 > * 当 groups= in_channels, 每个输入通道都会被单独的一组卷积层处理，这个组的大小是。 kernel_size, stride, padding, output_padding 这几个参数均支持一下输入形式： 一个 int 数字 – 三维维数据的深度，高和宽这两个维度都会采用这一个数字。 一个由三个int数字组成的tuple– 这种情况下，三维数据的深度这一维度会采用元组中的第一个int数字，高这一维度会采用元组中的第二个int数字，宽这一维度会采用第三个int数字。 Note 取决于你卷积核的大小，有些时候输入数据中某些列（最后几列）可能不会参与计算（比如列数整除卷积核大小有余数，而又没有padding，那最后的余数列一般不会参与卷积计算），这主要是因为pytorch中的互相关操作cross-correlation是保证计算正确的操作(valid operation)， 而不是满操作(full operation)。所以实际操作中，还是要亲尽量选择好合适的padding参数哦。 Note padding 参数控制了要在输入的各维度各边上补齐0的层数，与在Conv3d中不同的是，在转置卷积操作过程中，此padding参数与实际补齐0的层数的关系为层数 = kernel_size - 1 - padding， 这样设置的主要原因是当使用相同的参数构建Conv3d 和ConvTranspose3d模块的时候，这种设置能够实现两个模块有正好相反的输入输出的大小，即Conv3d的输出大小是其对应的ConvTranspose3d模块的输入大小，而ConvTranspose3d的输出大小又恰好是其对应的Conv3d模块的输入大小。然而，当stride > 1的时候，Conv3d 的一个输出大小可能会对应多个输入大小，上一个note中就详细的介绍了这种情况，这样的情况下要保持前面提到两种模块的输入输出保持反向一致，那就要用到 output_padding参数了，这个参数可以增加转置卷积输出的某一维度的大小，以此来达到前面提到的同参数构建的Conv3d 和ConvTranspose3d模块的输入输出方向一致。 但注意这个参数不是说要往转置卷积的输出上pad 0，而是直接控制转置卷积的输出各维度的大小为根据此参数pad后的大小。 Note 当程序的运行环境是使用了CuDNN的CUDA环境的时候，一些非确定性的算法（nondeterministic algorithm）可能会被采用以提高整个计算的性能。如果不想使用这些非确定性的算法，你可以通过设置torch.backends.cudnn.deterministic = True来让整个计算过程保持确定性（可能会损失一定的计算性能）。对于后端(background)，你可以看一下这一部分Reproducibility了解其相关信息。 Parameters: in_channels (int) – 输入通道的个数 out_channels (int) – 卷积操作输出通道的个数 kernel_size (int or tuple) – 卷积核大小 stride (int or tuple, optional) – 卷积操作的步长。 默认： 1 padding (int or tuple, optional) – kernel_size - 1 - padding 层 0 会被补齐到输入数据的各边上。 默认： 0 output_padding (int or tuple, optional) – 输出的各维度要增加的大小。默认：0 groups (int, optional) – 输入通道与输出通道之间相互隔离的连接的个数。 默认：1 bias (bool, optional) – 如果被置为 True，向输出增加一个偏差量，此偏差是可学习参数。 默认：True dilation (int or tuple, optional) – 卷积核各元素之间的距离。 默认： 1 Shape: 输入: 输出: 其中 | Variables: | weight (Tensor) – 模块中的一个大小为 (in_channels, out_channels, kernel_size[0], kernel_size[1], kernel_size[2])的权重张量，这些权重可训练学习(learnable)。这些权重的初始值的采样空间是，其中 。 bias (Tensor) – 模块的偏差项，大小为 (out_channels)， 如果构造函数中的 bias 被置为 True，那么这些权重的初始值的采样空间是 ，其中 。 例子: >>> # With square kernels and equal stride >>> m = nn.ConvTranspose3d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2)) >>> input = torch.randn(20, 16, 10, 50, 100) >>> output = m(input) Unfold class torch.nn.Unfold(kernel_size, dilation=1, padding=0, stride=1) 将一个batch的输入张量展开成由多个滑动局部块组成的形式。（im2col的扩展模块，起到基本类似im2col的作用） 以一个大小为的批次化(batched)输入张量为例，其中是batch的大小，是通道数量，代表了任意空间维度。那Unfold这个操作在此张量上的操作就是，将这个张量展开成由多个kernel_size大小的滑动块组成的大小为的三维张量，其中是每个块中数的个数（每个块有个空间位置，每个空间位置存储一个通道大小为的向量），是块的个数： （这张图有问题啊，编辑整理的时候注意修正一下） 其中 是由上面例子中的input各空间维度组成的，遍历了各个空间维度。 因此，索引Fold操作的output的最后一个维度等价于索引某一个block，而索引操作的返回值是这个索引到的block中的所有值。 padding, stride 和 dilation 参数指明了滑动块的相关性质。 stride 控制了滑动块的步长。 padding 控制了在变形之前要向input的各维度各边上补齐的0的层数。 dilation 控制了卷积核中各点之间的空间距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 Parameters: kernel_size (int or tuple) – 滑动块的大小 stride (int or tuple, optional) – 滑动块在输入各维度上的步长。默认: 1 padding (int or tuple, optional) – 在输入各维度各边上补齐0的层数。 dilation (int or tuple, optional) – 控制了各元素之间的距离（没有指明元素具体指的是谁的元素，猜测是输出的）。默认：1 如果 kernel_size, dilation, padding 或者 stride的值是一个int，或是一个长度为1的int元组，在相关操作的时候各个空间维度上都会使用这同一个值。 如果输出向量有两个空间维度，那么此Fold操作有时又被称为im2col。 Note Fold在执行类col2im的操作的时候，主要是是通过集成此im（输出张量）分裂出所有对应位置的col（输入的滑动块）来复原原im。而Unfold则是通过从输入张量中不断拷贝数值到相应的block中来生成由滑动块组成的输出张量。所以，如果滑动块之间如果有数值重叠，那这些滑动块之间并不是互逆的。 Warning 目前，只有四维张量（比如批次化的图像张量）支持这个操作。 Shape: 输入: 输出: Examples: >>> unfold = nn.Unfold(kernel_size=(2, 3)) >>> input = torch.randn(2, 5, 3, 4) >>> output = unfold(input) >>> # each patch contains 30 values (2x3=6 vectors, each of 5 channels) >>> # 4 blocks (2x3 kernels) in total in the 3x4 input >>> output.size() torch.Size([2, 30, 4]) >>> # Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape) >>> inp = torch.randn(1, 3, 10, 12) >>> w = torch.randn(2, 3, 4, 5) >>> inp_unf = torch.nn.functional.unfold(inp, (4, 5)) >>> out_unf = inp_unf.transpose(1, 2).matmul(w.view(w.size(0), -1).t()).transpose(1, 2) >>> out = torch.nn.functional.fold(out_unf, (7, 8), (1, 1)) >>> # or equivalently (and avoiding a copy), >>> # out = out_unf.view(1, 2, 7, 8) >>> (torch.nn.functional.conv2d(inp, w) - out).abs().max() tensor(1.9073e-06) Fold class torch.nn.Fold(output_size, kernel_size, dilation=1, padding=0, stride=1) 将由滑动局部块组成的数组集合为一个大张量。(类col2im) 考虑一个包含了很多个滑动局部块的输入张量，比如，一批图像分割块(patches of images)的集合，大小为，其中是batch大小， 是一个块中的数值个数（每个块有个空间位置，每个空间位置存储一个通道大小为的向量），是滑动块的个数。（这些大小参数严格遵循了Unfold操作的输出向量的大小规定。）Fold操作通过求和重叠值的方式来将这些局部块集合为一个大小为的output张量。与 Unfold类似，这些参数必须满足： 其中遍历了各个空间维度。 output_size 描述了要生成的output的各空间维度的大小。有时，同样数量的滑动块，可能会产生多种input的形状，比如，当stride > 0的时候，这时候，设置output_size参数就会显得极为重要。 padding, stride 和 dilation 参数指明了滑动块的相关性质。 stride 控制了滑动块的步长。 padding 控制了在变形之前要向input的各维度各边上补齐的0的层数。 dilation 控制了卷积核中各点之间的空间距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 Parameters: output_size (int or tuple) – 输出向量的各空间维度的大小 (i.e., input.sizes()[2:]) kernel_size (int or tuple) – 滑动块的大小 stride (int or tuple, optional) – 滑动块在输入各维度上的步长。默认: 1 padding (int or tuple, optional) – 在输入各维度各边上补齐0的层数。 dilation (int or tuple, optional) – 控制了各元素之间的距离（没有指明元素具体指的是谁的元素，猜测是输出的）。默认：1 如果output_size， kernel_size, dilation, padding 或者 stride是一个int或者长度为1的int元组，在相关操作的时候各个空间维度上都会使用这同一个值。 如果此输出向量的空间维度数为2，那么此Fold操作有时又被称为col2im。 Note Fold在执行类col2im的操作的时候，主要是是通过集成此im（输出张量）分裂出所有对应位置的col（输入的滑动块）来复原原im。而Unfold则是通过从输入张量中不断拷贝数值到相应的block中来生成由滑动块组成的输出张量。所以，如果滑动块之间如果有数值重叠，那这些滑动块之间并不是互逆的。 Warning 目前，只有四维张量（比如批次化的图像张量）支持这个操作。 Shape: 输入: 输出: 举例: >>> fold = nn.Fold(output_size=(4, 5), kernel_size=(2, 2)) >>> input = torch.randn(1, 3 * 2 * 2, 1) >>> output = fold(input) >>> output.size() 卷积层部分Fold 与 Unfold 是1.0新增的内容，猜测其主要目的是开放col2im和im2col这两个通过矩阵乘法实现卷积操作的前序接口，要好好理解这部分可能要了解一下现在主流框架通过大矩阵乘法来实现卷积操作这一通用做法了，这一篇文章就介绍的很好[Implementing convolution as a matrix multiplication](https://buptldy.github.io/2016/10/01/2016-10-01-im2col/)，这一段如果感觉我的直译晦涩难懂，那我深感抱歉并建议看一下英文原版，虽然我觉得英文原版介绍的也是晦涩难懂 池化层（Pooling layers） MaxPool1d class torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) 对输入的多通道信号执行一维最大池化操作。 最简单的情况下，对于输入大小为 ，输出大小为的池化操作，此池化过程可表述如下： padding 参数控制了要在输入信号的各维度各边上要补齐0的层数。 dilation 参数控制了池化核中各元素之间的距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 Parameters: kernel_size – 最大池化操作的滑动窗大小 stride – 滑动窗的步长，默认值是 kernel_size padding – 要在输入信号的各维度各边上要补齐0的层数 dilation – 滑动窗中各元素之间的距离 return_indices – 如果此参数被设置为True， 那么此池化层在返回输出信号的同时还会返回一连串滑动窗最大值的索引位置，即每个滑动窗的最大值位置信息。这些信息可以在后面的上采样torch.nn.MaxUnpool1d中被用到。 ceil_mode – 如果此参数被设置为True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作 Shape: 输入: 输出: 其中 例子: >>> # pool of size=3, stride=2 >>> m = nn.MaxPool1d(3, stride=2) >>> input = torch.randn(20, 16, 50) >>> output = m(input) MaxPool2d class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) 对输入的多通道信号执行二维最大池化操作。 最简单的情况下，对于输入大小为 ，输出大小为，kernel_size为的池化操作，此池化过程可表述如下： padding 参数控制了要在输入信号的各维度各边上要补齐0的层数。 dilation 参数控制了池化核中各元素之间的距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 kernel_size, stride, padding, dilation 等参数均支持以下类型输入： 一个单独的 int – 此时这个int会同时控制池化滑动窗的宽和高这两个维度的大小 一个由两个int组成的tuple – 这种情况下，高这一维度会采用元组中的第一个int数字，宽这一维度会采用第二个int数字。 Parameters: kernel_size – 最大池化操作的滑动窗大小 stride – 滑动窗的步长，默认值是 kernel_size padding – 要在输入信号的各维度各边上要补齐0的层数 dilation – 滑动窗中各元素之间的距离 return_indices – 如果此参数被设置为True， 那么此池化层在返回输出信号的同时还会返回一连串滑动窗最大值的索引位置，即每个滑动窗的最大值位置信息。这些信息可以在后面的上采样torch.nn.MaxUnpool2d中被用到。 ceil_mode – 如果此参数被设置为True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作 Shape: 输入: 输出: , 其中 例子: >>> # pool of square window of size=3, stride=2 >>> m = nn.MaxPool2d(3, stride=2) >>> # pool of non-square window >>> m = nn.MaxPool2d((3, 2), stride=(2, 1)) >>> input = torch.randn(20, 16, 50, 32) >>> output = m(input) MaxPool3d class torch.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) 对输入的多通道信号执行三维最大池化操作。 最简单的情况下，对于输入大小为 ，输出大小为，kernel_size为 的池化操作，此池化过程可表述如下： padding 参数控制了要在输入信号的各维度各边上要补齐0的层数。 dilation 参数控制了池化核中各元素之间的距离；这也被称为多孔算法(à trous algorithm)。这个概念有点难解释，这个链接link用可视化的方法很好地解释了dilation的作用。 kernel_size, stride, padding, dilation 等参数均支持以下类型输入： 一个单独的 int – 此时这个int会同时控制池化滑动窗的深度，宽和高这三个维度的大小 一个由三个int组成的tuple – 这种情况下，深度这一维度会采用元组中的第一个int数字，高这一维度会采用元组中的第二个int数字，宽这一维度会采用第三个int数字。 Parameters: kernel_size – 最大池化操作的滑动窗大小 stride – 滑动窗的步长，默认值是 kernel_size padding – 要在输入信号的各维度各边上要补齐0的层数 dilation – 滑动窗中各元素之间的距离 return_indices – 如果此参数被设置为True， 那么此池化层在返回输出信号的同时还会返回一连串滑动窗最大值的索引位置，即每个滑动窗的最大值位置信息。这些信息可以在后面的上采样torch.nn.MaxUnpool3d中被用到。 ceil_mode – 如果此参数被设置为True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作 Shape: 输入: 输出: , 其中 例子: >>> # pool of square window of size=3, stride=2 >>> m = nn.MaxPool3d(3, stride=2) >>> # pool of non-square window >>> m = nn.MaxPool3d((3, 2, 2), stride=(2, 1, 2)) >>> input = torch.randn(20, 16, 50,44, 31) >>> output = m(input) MaxUnpool1d class torch.nn.MaxUnpool1d(kernel_size, stride=None, padding=0) MaxPool1d的逆过程，不过并不是完全的逆过程，因为在MaxPool1d的过程中，池化窗区域内的非最大值都已经丢失。 MaxUnpool1d的输入是MaxPool1d的输出，其中也包括包括滑动窗最大值的索引（即return_indices所控制的输出），逆池化操作的过程就是将MaxPool1d过程中产生的最大值插回到原来的位置，并将非最大值区域置为0。 Note MaxPool1d操作可以将多个大小不同的输入映射到相同的输出大小。因此，池化操作的反过程，MaxUnpool1d的上采样过程的输出大小就不唯一了。为了适应这一点，可以在设置控制上采样输出大小的（output_size）参数。 具体用法，请参阅下面的输入和示例 Parameters: kernel_size (int or tuple) – 最大池化窗的大小 stride (int or tuple) – 最大池化窗的步长。默认kernel_size padding (int or tuple) – 输入信号的各维度各边要补齐0的层数 Inputs: input: 要执行上采样操作的张量 indices: MaxPool1d池化过程中输出的池化窗最大值的位置索引 output_size (选填): 指定的输出大小 Shape: 输入: 输出: , 其中 也可以使用output_size指定输出的大小 例子: >>> pool = nn.MaxPool1d(2, stride=2, return_indices=True) >>> unpool = nn.MaxUnpool1d(2, stride=2) >>> input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]]) >>> output, indices = pool(input) >>> unpool(output, indices) tensor([[[ 0., 2., 0., 4., 0., 6., 0., 8.]]]) >>> # Example showcasing the use of output_size >>> input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8, 9]]]) >>> output, indices = pool(input) >>> unpool(output, indices, output_size=input.size()) tensor([[[ 0., 2., 0., 4., 0., 6., 0., 8., 0.]]]) >>> unpool(output, indices) tensor([[[ 0., 2., 0., 4., 0., 6., 0., 8.]]]) MaxUnpool2d class torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0) MaxPool2d的逆过程，不过并不是完全的逆过程，因为在MaxPool2d的过程中，池化窗区域内的非最大值都已经丢失。 MaxUnpool2d的输入是MaxPool2d的输出，其中也包括包括滑动窗最大值的索引（即return_indices所控制的输出），逆池化操作的过程就是将MaxPool2d过程中产生的最大值插回到原来的位置，并将非最大值区域置为0。 Note MaxPool2d操作可以将多个大小不同的输入映射到相同的输出大小。因此，池化操作的反过程，MaxUnpool2d的上采样过程的输出大小就不唯一了。为了适应这一点，可以在设置控制上采样输出大小的（output_size）参数。 具体用法，请参阅下面的输入和示例 Parameters: kernel_size (int or tuple) – 最大池化窗的大小 stride (int or tuple) – 最大池化窗的步长。默认kernel_size padding (int or tuple) – 输入信号的各维度各边要补齐0的层数 Inputs: input: 要执行上采样操作的张量 indices: MaxPool2d池化过程中输出的池化窗最大值的位置索引 output_size (选填): 指定的输出大小 Shape: 输入: 输出: , 其中 也可以使用output_size指定输出的大小 例子: >>> pool = nn.MaxPool2d(2, stride=2, return_indices=True) >>> unpool = nn.MaxUnpool2d(2, stride=2) >>> input = torch.tensor([[[[ 1., 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12], [13, 14, 15, 16]]]]) >>> output, indices = pool(input) >>> unpool(output, indices) tensor([[[[ 0., 0., 0., 0.], [ 0., 6., 0., 8.], [ 0., 0., 0., 0.], [ 0., 14., 0., 16.]]]]) >>> # specify a different output size than input size >>> unpool(output, indices, output_size=torch.Size([1, 1, 5, 5])) tensor([[[[ 0., 0., 0., 0., 0.], [ 6., 0., 8., 0., 0.], [ 0., 0., 0., 14., 0.], [ 16., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0.]]]]) MaxUnpool3d class torch.nn.MaxUnpool3d(kernel_size, stride=None, padding=0) MaxPool3d的逆过程，不过并不是完全的逆过程，因为在MaxPool3d的过程中，池化窗区域内的非最大值都已经丢失。 MaxUnpool3d的输入是MaxPool3d的输出，其中也包括包括滑动窗最大值的索引（即return_indices所控制的输出），逆池化操作的过程就是将MaxPool3d过程中产生的最大值插回到原来的位置，并将非最大值区域置为0。 Note MaxPool3d操作可以将多个大小不同的输入映射到相同的输出大小。因此，池化操作的反过程，MaxUnpool3d的上采样过程的输出大小就不唯一了。为了适应这一点，可以在设置控制上采样输出大小的（output_size）参数。 具体用法，请参阅下面的输入和示例 Parameters: kernel_size (int or tuple) – 最大池化窗的大小 stride (int or tuple) – 最大池化窗的步长。默认kernel_size padding (int or tuple) – 输入信号的各维度各边要补齐0的层数 Inputs: input: 要执行上采样操作的张量 indices: MaxPool3d池化过程中输出的池化窗最大值的位置索引 output_size (选填): 指定的输出大小 Shape: 输入: 输出: , 其中 也可以使用output_size指定输出的大小 例子: >>> # pool of square window of size=3, stride=2 >>> pool = nn.MaxPool3d(3, stride=2, return_indices=True) >>> unpool = nn.MaxUnpool3d(3, stride=2) >>> output, indices = pool(torch.randn(20, 16, 51, 33, 15)) >>> unpooled_output = unpool(output, indices) >>> unpooled_output.size() torch.Size([20, 16, 51, 33, 15]) AvgPool1d class torch.nn.AvgPool1d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) 对输入的多通道信号执行一维平均池化操作。 最简单的情况下，对于输入大小为 ，输出大小为，kernel_size为的池化操作，此池化过程可表述如下： padding 参数控制了要在输入信号的各维度各边上要补齐0的层数。 kernel_size, stride, padding, dilation 等参数均支持输入一个int或者由一个int组成的tuple。 Parameters: kernel_size – 平均池化操作的滑动窗大小 stride – 滑动窗的步长，默认值是 kernel_size padding – 要在输入信号的各维度各边上要补齐0的层数 ceil_mode – 如果此参数被设置为True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作 count_include_pad – 如果被设置为True, 那么在进行平均运算的时候也会将用于补齐的0加入运算。 Shape: 输入: 输出: , 其中 例子: >>> # pool with window of size=3, stride=2 >>> m = nn.AvgPool1d(3, stride=2) >>> m(torch.tensor([[[1.,2,3,4,5,6,7]]])) tensor([[[ 2., 4., 6.]]]) AvgPool2d class torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) 对输入的多通道信号执行二维平均池化操作。 最简单的情况下，对于输入大小为 ，输出大小为，kernel_size为的池化操作，此池化过程可表述如下： padding 参数控制了要在输入信号的各维度各边上要补齐0的层数。 kernel_size, stride, padding等参数均支持以下类型输入： 一个单独的 int – 此时这个int会同时控制池化滑动窗的宽和高这两个维度的大小 一个由两个int组成的tuple – 这种情况下，高这一维度会采用元组中的第一个int数字，宽这一维度会采用第二个int数字。 Parameters: kernel_size – 平均池化操作的滑动窗大小 stride – 滑动窗的步长，默认值是 kernel_size padding – 要在输入信号的各维度各边上要补齐0的层数 ceil_mode – 如果此参数被设置为True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作 count_include_pad – 如果被设置为True, 那么在进行平均运算的时候也会将用于补齐的0加入运算。 Shape: 输入: 输出: , 其中 例子: >>> # pool of square window of size=3, stride=2 >>> m = nn.AvgPool2d(3, stride=2) >>> # pool of non-square window >>> m = nn.AvgPool2d((3, 2), stride=(2, 1)) >>> input = torch.randn(20, 16, 50, 32) >>> output = m(input) AvgPool3d class torch.nn.AvgPool3d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) 对输入的多通道信号执行三维平均池化操作。 最简单的情况下，对于输入大小为，输出大小为，kernel_size为的池化操作，此池化过程可表述如下： padding 参数控制了要在输入信号的各维度各边上要补齐0的层数。 kernel_size, stride, padding等参数均支持以下类型输入： 一个单独的 int – 此时这个int会同时控制池化滑动窗的深度，宽和高这两个维度的大小 一个由三个int组成的tuple – 这种情况下，深度这一维度会采用元组中的第一个int数字，高这一维度会采用元组中的第二个int数字，宽这一维度会采用第三个int数字。 Parameters: kernel_size – 平均池化操作的滑动窗大小 stride – 滑动窗的步长，默认值是 kernel_size padding – 要在输入信号的各维度各边上要补齐0的层数 ceil_mode – 如果此参数被设置为True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作 count_include_pad – 如果被设置为True, 那么在进行平均运算的时候也会将用于补齐的0加入运算。 Shape: 输入: 输出: , 其中 例子: >>> # pool of square window of size=3, stride=2 >>> m = nn.AvgPool3d(3, stride=2) >>> # pool of non-square window >>> m = nn.AvgPool3d((3, 2, 2), stride=(2, 1, 2)) >>> input = torch.randn(20, 16, 50,44, 31) >>> output = m(input) FractionalMaxPool2d class torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None) 对输入的多通道信号执行小数级二维最大池化操作。小数级指的是此操作的输出大小与输入大小成指定的小数倍数关系。 Ben Graham的这篇文章Fractional MaxPooling中详细地介绍了小数级二维最大池化的基本思想和技术细节。 小数级二维最大池化的基本思想就是将最大池化操作应用于个由随机步长大小采集的区域中，这些步长大小是由输出目标的大小决定的。小数级二维最大池化的输出特征的数量等于输入通道的数量。 Parameters: kernel_size – 执行最大操作的窗口大小。支持的数据类型包括一个单独的数字k(生成一个大小为k x k的正方形kernal)，或者一个元组 (kh x kw) output_size – 池化输出目标大小，具体形式是 oH x oW。支持的数据类型包括一个单独的数字oH，或者一个元组 (oH, oW)，注意此处oH x oW与kernal_size中的kh x ow相呼应，两者成一定的小数级倍数关系 output_ratio – 如果想让输出目标的大小是输入目标大小的ratio倍，可以通过设置此参数来实现。此参数可以是一个小数数字或者小数元组，数字范围是(0, 1) return_indices – 如果此参数设置为True, 那么在池化操作结束后，返回池化输出结果的同时也会返回每个池化区域中，最大值的位置信息。这些信息在nn.MaxUnpool2d()可以被用到。此参数默认为False 例子 >>> # pool of square window of size=3, and target output size 13x12 >>> m = nn.FractionalMaxPool2d(3, output_size=(13, 12)) >>> # pool of square window and target output size being half of input image size >>> m = nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5)) >>> input = torch.randn(20, 16, 50, 32) >>> output = m(input) LPPool1d class torch.nn.LPPool1d(norm_type, kernel_size, stride=None, ceil_mode=False) 对输入的多通道信号执行一维幂平均池化操作。 对于每个池化窗口，此池化操作的计算方式如下： 当p为无穷大的时候时，等价于最大池化操作 当p=1时，等价于求和池化操作（一定程度上等价于平均池化） Note 如果某个特殊的输入导致这个输入关于幂指数p的求和是0，那上述池化函数在这一点是没有意义的。在实际实现过程中，此点的梯度被设置为0。 Parameters: kernel_size: 池化窗口的大小 stride：池化窗口移动的步长。默认值是kernel_size ceil_mode: 当此参数被设置为True时，在计算输出大小的时候将使用向下取整代替向上取整 Shape: 输入: 输出: ，其中 例子: >>> # power-2 pool of window of length 3, with stride 2. >>> m = nn.LPPool1d(2, 3, stride=2) >>> input = torch.randn(20, 16, 50) >>> output = m(input) LPPool2d class torch.nn.LPPool2d(norm_type, kernel_size, stride=None, ceil_mode=False) 对输入的多通道信号执行二维幂平均池化操作。 对于每个池化窗口，此池化操作的计算方式如下： 当p等于时候时，等价于最大池化操作 当p=1时，等价于求和池化操作（一定程度上等价于平均池化） 参数kernel_size, stride支持的数据类型： int，池化窗口的宽和高相等 tuple数组（两个数字的），第一个元素是池化窗口的高，第二个是宽 Note 如果某个特殊的输入导致这个输入关于幂指数p的求和是0，那上述池化函数在这一点是没有意义的。在实际实现过程中，此点的梯度被设置为0。 Parameters: kernel_size: 池化窗口的大小 stride：池化窗口移动的步长。默认值是kernel_size ceil_mode: 当此参数被设置为True时，在计算输出大小的时候将使用向下取整代替向上取整 Shape: 输入: 输出: , 其中 例子: >>> # power-2 pool of square window of size=3, stride=2 >>> m = nn.LPPool2d(2, 3, stride=2) >>> # pool of non-square window of power 1.2 >>> m = nn.LPPool2d(1.2, (3, 2), stride=(2, 1)) >>> input = torch.randn(20, 16, 50, 32) >>> output = m(input) AdaptiveMaxPool1d class torch.nn.AdaptiveMaxPool1d(output_size, return_indices=False) 对输入的多通道信号进行1维的自适应最大池化操作。 此池化层可以通过指定输出大小H，将任意输入大小的输入强行的池化到指定的输出大小。不过输入和输出特征的通道数不会变化。 Parameters: output_size – 指定的输出大小H return_indices – 如果此参数设置为True, 那么在池化操作结束后，返回池化输出结果的同时也会返回每个池化区域中，最大值的位置信息。这些信息在nn.MaxUnpool1d()可以被用到。此参数默认为False 例子 >>> # target output size of 5 >>> m = nn.AdaptiveMaxPool1d(5) >>> input = torch.randn(1, 64, 8) >>> output = m(input) AdaptiveMaxPool2d class torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False) 对输入的多通道信号进行2维的自适应最大池化操作。 此池化层可以通过指定输出大小H x W，将任意输入大小的输入强行的池化到指定的输出大小。不过输入和输出特征的通道数不会变化。 Parameters: output_size – 指定的输出大小H x W。此参数支持的数据类型可以是一个元组(H, W)，又或者是一个单独的int H（等价于H x H）。H 和 W这两个参数支持输入一个int又或者是None, None表示此输出维度的大小等价于输入数据此维度的大小 return_indices – 如果此参数设置为True, 那么在池化操作结束后，返回池化输出结果的同时也会返回每个池化区域中，最大值的位置信息。这些信息在nn.MaxUnpool2d()可以被用到。此参数默认为False 例子 >>> # target output size of 5x7 >>> m = nn.AdaptiveMaxPool2d((5,7)) >>> input = torch.randn(1, 64, 8, 9) >>> output = m(input) >>> # target output size of 7x7 (square) >>> m = nn.AdaptiveMaxPool2d(7) >>> input = torch.randn(1, 64, 10, 9) >>> output = m(input) >>> # target output size of 10x7 >>> m = nn.AdaptiveMaxPool2d((None, 7)) >>> input = torch.randn(1, 64, 10, 9) >>> output = m(input) AdaptiveMaxPool3d class torch.nn.AdaptiveMaxPool3d(output_size, return_indices=False) 对输入的多通道信号进行3维的自适应最大池化操作。 此池化层可以通过指定输出大小D x H x W，将任意输入大小的输入强行的池化到指定的输出大小。不过输入和输出特征的通道数不会变化。 Parameters: output_size – 指定的输出大小D x H x W。此参数支持的数据类型可以是一个元组(D, H, W)，又或者是一个单独的int D（等价于D x D x D)。D, H 和 W这三个参数支持输入一个int又或者是None, None表示此输出维度的大小等价于输入数据此维度的大小 return_indices – 如果此参数设置为True, 那么在池化操作结束后，返回池化输出结果的同时也会返回每个池化区域中，最大值的位置信息。这些信息在nn.MaxUnpool3d()可以被用到。此参数默认为False 例子 >>> # target output size of 5x7x9 >>> m = nn.AdaptiveMaxPool3d((5,7,9)) >>> input = torch.randn(1, 64, 8, 9, 10) >>> output = m(input) >>> # target output size of 7x7x7 (cube) >>> m = nn.AdaptiveMaxPool3d(7) >>> input = torch.randn(1, 64, 10, 9, 8) >>> output = m(input) >>> # target output size of 7x9x8 >>> m = nn.AdaptiveMaxPool3d((7, None, None)) >>> input = torch.randn(1, 64, 10, 9, 8) >>> output = m(input) AdaptiveAvgPool1d class torch.nn.AdaptiveAvgPool1d(output_size) 对输入的多通道信号进行1维的自适应平均池化操作。 此池化层可以通过指定输出大小H，将任意输入大小的输入强行的池化到指定的输出大小。不过输入和输出特征的通道数不会变化。 Parameters: output_size – 指定的输出大小H 例子 >>> # target output size of 5 >>> m = nn.AdaptiveAvgPool1d(5) >>> input = torch.randn(1, 64, 8) >>> output = m(input) AdaptiveAvgPool2d class torch.nn.AdaptiveAvgPool2d(output_size) 对输入的多通道信号进行2维的自适应平均池化操作。 此池化层可以通过指定输出大小H x W，将任意输入大小的输入强行的池化到指定的输出大小。不过输入和输出特征的通道数不会变化。 Parameters: output_size – 指定的输出大小H x W。此参数支持的数据类型可以是一个元组(H, W)，又或者是一个单独的int H（等价于H x H）。H 和 W这两个参数支持输入一个int又或者是None, None表示此输出维度的大小等价于输入数据此维度的大小 例子 >>> # target output size of 5x7 >>> m = nn.AdaptiveAvgPool2d((5,7)) >>> input = torch.randn(1, 64, 8, 9) >>> output = m(input) >>> # target output size of 7x7 (square) >>> m = nn.AdaptiveAvgPool2d(7) >>> input = torch.randn(1, 64, 10, 9) >>> output = m(input) >>> # target output size of 10x7 >>> m = nn.AdaptiveMaxPool2d((None, 7)) >>> input = torch.randn(1, 64, 10, 9) >>> output = m(input) AdaptiveAvgPool3d class torch.nn.AdaptiveAvgPool3d(output_size) 对输入的多通道信号进行3维的自适应平均池化操作。 此池化层可以通过指定输出大小D x H x W，将任意输入大小的输入强行的池化到指定的输出大小。不过输入和输出特征的通道数不会变化。 Parameters: output_size – 指定的输出大小D x H x W。此参数支持的数据类型可以是一个元组(D, H, W)，又或者是一个单独的int D（等价于D x D x D)。D, H 和 W这三个参数支持输入一个int又或者是None, None表示此输出维度的大小等价于输入数据此维度的大小 例子 >>> # target output size of 5x7x9 >>> m = nn.AdaptiveAvgPool3d((5,7,9)) >>> input = torch.randn(1, 64, 8, 9, 10) >>> output = m(input) >>> # target output size of 7x7x7 (cube) >>> m = nn.AdaptiveAvgPool3d(7) >>> input = torch.randn(1, 64, 10, 9, 8) >>> output = m(input) >>> # target output size of 7x9x8 >>> m = nn.AdaptiveMaxPool3d((7, None, None)) >>> input = torch.randn(1, 64, 10, 9, 8) >>> output = m(input) 填充层（Padding layers） ReflectionPad1d class torch.nn.ReflectionPad1d(padding) 以输入张量的各边界为轴，通过对输入张量数据的进行镜像复制的方式来对输入张量进行填充操作。 对于N维的填充操作，调用torch.nn.functional.pad()。 Parameters: padding (int, tuple) – 要填充的范围大小。如果输入数据是一个int, 那各个边界上都会填充同样大小的数据。如果是一个两个元素的元组，那么按照 (, )的大小设定来在各边上填充。 Shape: 输入: 输出: 其中 示例: >>> m = nn.ReflectionPad1d(2) >>> input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4) >>> input tensor([[[0., 1., 2., 3.], [4., 5., 6., 7.]]]) >>> m(input) tensor([[[2., 1., 0., 1., 2., 3., 2., 1.], [6., 5., 4., 5., 6., 7., 6., 5.]]]) >>> m(input) tensor([[[2., 1., 0., 1., 2., 3., 2., 1.], [6., 5., 4., 5., 6., 7., 6., 5.]]]) >>> # using different paddings for different sides >>> m = nn.ReflectionPad1d((3, 1)) >>> m(input) tensor([[[3., 2., 1., 0., 1., 2., 3., 2.], [7., 6., 5., 4., 5., 6., 7., 6.]]]) ReflectionPad2d class torch.nn.ReflectionPad2d(padding) 以输入张量的各边界为轴，通过对输入张量数据的进行镜像复制的方式来对输入张量进行填充操作。 对于N维的填充操作，调用torch.nn.functional.pad()。 Parameters: padding (int, tuple) – 要填充的范围大小。如果输入数据是一个int, 那各个边界上都会填充同样大小的数据。如果是一个四个元素的元组，那么按照(, , , )的大小设定来在各边上填充。 Shape: 输入: 输出: 其中 示例: >>> m = nn.ReflectionPad2d(2) >>> input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3) >>> input tensor([[[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]]]) >>> m(input) tensor([[[[8., 7., 6., 7., 8., 7., 6.], [5., 4., 3., 4., 5., 4., 3.], [2., 1., 0., 1., 2., 1., 0.], [5., 4., 3., 4., 5., 4., 3.], [8., 7., 6., 7., 8., 7., 6.], [5., 4., 3., 4., 5., 4., 3.], [2., 1., 0., 1., 2., 1., 0.]]]]) >>> # using different paddings for different sides >>> m = nn.ReflectionPad2d((1, 1, 2, 0)) >>> m(input) tensor([[[[7., 6., 7., 8., 7.], [4., 3., 4., 5., 4.], [1., 0., 1., 2., 1.], [4., 3., 4., 5., 4.], [7., 6., 7., 8., 7.]]]]) ReplicationPad1d class torch.nn.ReplicationPad1d(padding) 通过复制输入张量边界元素的方式对输入张量进行填充操作。 对于N维的填充操作，调用torch.nn.functional.pad()。 Parameters: padding (int, tuple) – 要填充的范围大小。如果输入数据是一个int, 那各个边界上都会填充同样大小的数据。如果是一个两个元素的元组，那么按照 (, )的大小设定来在各边上填充。 Shape: 输入: 输出: 其中 示例: >>> m = nn.ReplicationPad1d(2) >>> input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4) >>> input tensor([[[0., 1., 2., 3.], [4., 5., 6., 7.]]]) >>> m(input) tensor([[[0., 0., 0., 1., 2., 3., 3., 3.], [4., 4., 4., 5., 6., 7., 7., 7.]]]) >>> # using different paddings for different sides >>> m = nn.ReplicationPad1d((3, 1)) >>> m(input) tensor([[[0., 0., 0., 0., 1., 2., 3., 3.], [4., 4., 4., 4., 5., 6., 7., 7.]]]) ReplicationPad2d class torch.nn.ReplicationPad2d(padding) 通过复制输入张量边界元素的方式对输入张量进行填充操作。 对于N维的填充操作，调用torch.nn.functional.pad()。 Parameters: padding (int, tuple) – 要填充的范围大小。如果输入数据是一个int, 那各个边界上都会填充同样大小的数据。如果是一个四个元素的元组，那么按照(, , , )的大小设定来在各边上填充。 Shape: 输入: 输出: 其中 示例: >>> m = nn.ReplicationPad2d(2) >>> input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3) >>> input tensor([[[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]]]) >>> m(input) tensor([[[[0., 0., 0., 1., 2., 2., 2.], [0., 0., 0., 1., 2., 2., 2.], [0., 0., 0., 1., 2., 2., 2.], [3., 3., 3., 4., 5., 5., 5.], [6., 6., 6., 7., 8., 8., 8.], [6., 6., 6., 7., 8., 8., 8.], [6., 6., 6., 7., 8., 8., 8.]]]]) >>> # using different paddings for different sides >>> m = nn.ReplicationPad2d((1, 1, 2, 0)) >>> m(input) tensor([[[[0., 0., 1., 2., 2.], [0., 0., 1., 2., 2.], [0., 0., 1., 2., 2.], [3., 3., 4., 5., 5.], [6., 6., 7., 8., 8.]]]]) ReplicationPad3d class torch.nn.ReplicationPad3d(padding) 通过复制输入张量边界元素的方式对输入张量进行填充操作。 对于N维的填充操作，调用torch.nn.functional.pad()。 Parameters: padding (int, tuple) – 要填充的范围大小。如果输入数据是一个int, 那各个边界上都会填充同样大小的数据。如果是一个六个元素的元组，那么按照(, , , , , )的大小设定来在各边上填充。 Shape: 输入: 输出: 其中 示例: >>> m = nn.ReplicationPad3d(3) >>> input = torch.randn(16, 3, 8, 320, 480) >>> output = m(input) >>> # using different paddings for different sides >>> m = nn.ReplicationPad3d((3, 3, 6, 6, 1, 1)) >>> output = m(input) ZeroPad2d class torch.nn.ZeroPad2d(padding) 通过在各边上填充0的方式对输入张量进行填充操作。 对于N维的填充操作，调用torch.nn.functional.pad()。 Parameters: padding (int, tuple) – 要填充的范围大小。如果输入数据是一个int, 那各个边界上都会填充同样大小的数据。如果是一个四个元素的元组，那么按照(, , , )的大小设定来在各边上填充。 Shape: 输入: 输出: 其中 示例: >>> m = nn.ZeroPad2d(2) >>> input = torch.randn(1, 1, 3, 3) >>> input tensor([[[[-0.1678, -0.4418, 1.9466], [ 0.9604, -0.4219, -0.5241], [-0.9162, -0.5436, -0.6446]]]]) >>> m(input) tensor([[[[ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, -0.1678, -0.4418, 1.9466, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.9604, -0.4219, -0.5241, 0.0000, 0.0000], [ 0.0000, 0.0000, -0.9162, -0.5436, -0.6446, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]]) >>> # using different paddings for different sides >>> m = nn.ZeroPad2d((1, 1, 2, 0)) >>> m(input) tensor([[[[ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, -0.1678, -0.4418, 1.9466, 0.0000], [ 0.0000, 0.9604, -0.4219, -0.5241, 0.0000], [ 0.0000, -0.9162, -0.5436, -0.6446, 0.0000]]]]) ConstantPad1d class torch.nn.ConstantPad1d(padding, value) 通过在各边上填充固定数字的方式对输入张量进行填充操作 对于N维的填充操作，调用torch.nn.functional.pad()。 Parameters: padding (int, tuple) – 要填充的范围大小。如果输入数据是一个int, 那各个边界上都会填充同样大小的数据。如果是一个两个元素的元组，那么按照 (, )的大小设定来在各边上填充。 Shape: 输入: 输出: 其中 示例: >>> m = nn.ConstantPad1d(2, 3.5) >>> input = torch.randn(1, 2, 4) >>> input tensor([[[-1.0491, -0.7152, -0.0749, 0.8530], [-1.3287, 1.8966, 0.1466, -0.2771]]]) >>> m(input) tensor([[[ 3.5000, 3.5000, -1.0491, -0.7152, -0.0749, 0.8530, 3.5000, 3.5000], [ 3.5000, 3.5000, -1.3287, 1.8966, 0.1466, -0.2771, 3.5000, 3.5000]]]) >>> m = nn.ConstantPad1d(2, 3.5) >>> input = torch.randn(1, 2, 3) >>> input tensor([[[ 1.6616, 1.4523, -1.1255], [-3.6372, 0.1182, -1.8652]]]) >>> m(input) tensor([[[ 3.5000, 3.5000, 1.6616, 1.4523, -1.1255, 3.5000, 3.5000], [ 3.5000, 3.5000, -3.6372, 0.1182, -1.8652, 3.5000, 3.5000]]]) >>> # using different paddings for different sides >>> m = nn.ConstantPad1d((3, 1), 3.5) >>> m(input) tensor([[[ 3.5000, 3.5000, 3.5000, 1.6616, 1.4523, -1.1255, 3.5000], [ 3.5000, 3.5000, 3.5000, -3.6372, 0.1182, -1.8652, 3.5000]]]) ConstantPad2d class torch.nn.ConstantPad2d(padding, value) 通过在各边上填充固定数字的方式对输入张量进行填充操作 对于N维的填充操作，调用torch.nn.functional.pad()。 Parameters: padding (int, tuple) – 要填充的范围大小。如果输入数据是一个int, 那各个边界上都会填充同样大小的数据。如果是一个四个元素的元组，那么按照(, , , )的大小设定来在各边上填充。 Shape: 输入: 输出: 其中 示例: >>> m = nn.ConstantPad2d(2, 3.5) >>> input = torch.randn(1, 2, 2) >>> input tensor([[[ 1.6585, 0.4320], [-0.8701, -0.4649]]]) >>> m(input) tensor([[[ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], [ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], [ 3.5000, 3.5000, 1.6585, 0.4320, 3.5000, 3.5000], [ 3.5000, 3.5000, -0.8701, -0.4649, 3.5000, 3.5000], [ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], [ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]]]) >>> m(input) tensor([[[ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], [ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], [ 3.5000, 3.5000, 1.6585, 0.4320, 3.5000, 3.5000], [ 3.5000, 3.5000, -0.8701, -0.4649, 3.5000, 3.5000], [ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], [ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]]]) >>> # using different paddings for different sides >>> m = nn.ConstantPad2d((3, 0, 2, 1), 3.5) >>> m(input) tensor([[[ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], [ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], [ 3.5000, 3.5000, 3.5000, 1.6585, 0.4320], [ 3.5000, 3.5000, 3.5000, -0.8701, -0.4649], [ 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]]]) ConstantPad3d class torch.nn.ConstantPad3d(padding, value) 通过在各边上填充固定数字的方式对输入张量进行填充操作 对于N维的填充操作，调用torch.nn.functional.pad()。 Parameters: padding (int, tuple) – 要填充的范围大小。如果输入数据是一个int, 那各个边界上都会填充同样大小的数据。如果是一个六个元素的元组，那么按照(, , , , , )的大小设定来在各边上填充。 Shape: 输入: 输出: 其中 示例: >>> m = nn.ConstantPad3d(3, 3.5) >>> input = torch.randn(16, 3, 10, 20, 30) >>> output = m(input) >>> # using different paddings for different sides >>> m = nn.ConstantPad3d((3, 3, 6, 6, 0, 1), 3.5) >>> output = m(input) 非线性激活(加权求和，非线性) ( Non-linear activations (weighted sum, nonlinearity) ) ELU class torch.nn.ELU(alpha=1.0, inplace=False) 将下面的元素级函数应用到输入张量上： Parameters: alpha – ELU操作的值。 默认： 1.0 inplace – 是否进行原位操作。 默认： False Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.ELU() >>> input = torch.randn(2) >>> output = m(input) Hardshrink class torch.nn.Hardshrink(lambd=0.5) 将下面的元素级hard shrinkage函数应用到输入张量上： Parameters: lambd – Hardshrink运算中的 值。 默认: 0.5 Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.Hardshrink() >>> input = torch.randn(2) >>> output = m(input) Hardtanh class torch.nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False, min_value=None, max_value=None) 将下面的元素级HardTanh函数应用到输入张量上。 HardTanh函数定义如下: 线性区域 的大小可以通过设置min_val 参数 max_val来进行调整。 参数: min_val – 线性区域的下限. 默认: -1 max_val – 线性区域的上限. 默认: 1 inplace – 是否进行原位操作。 默认： False 之前版本的min_value 和 max_value 参数已经被废弃掉了，改为min_val 和 max_val参数。 Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.Hardtanh(-2, 2) >>> input = torch.randn(2) >>> output = m(input) LeakyReLU class torch.nn.LeakyReLU(negative_slope=0.01, inplace=False) 将下面的元素级函数应用到输入张量上： 或 Parameters: negative_slope – 控制负数范围函数的斜率。 默认: 1e-2 inplace – 是否进行原位操作。 默认： False Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.LeakyReLU(0.1) >>> input = torch.randn(2) >>> output = m(input) LogSigmoid class torch.nn.LogSigmoid 将下面的元素级函数LogSigmoid应用到输入张量上：(此处漏了一张图，后期补一下) Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.LogSigmoid() >>> input = torch.randn(2) >>> output = m(input) PReLU class torch.nn.PReLU(num_parameters=1, init=0.25) 将下面的元素级函数应用到输入张量上： 或 此处 是一个可学习的参数。 如果在调用nn.PReLU()函数的时候没有传入参数，那么会默认在所有的输入通道上应用同一个 参数。 如果以nn.PReLU(nChannels)这种方式调用， 每个输入通道都会有一个单独的 参数。 Note 想要学一个好的参数，最好不要用weight decay。 Note 通道维度是输入张量的第二个维度。当输入张量的维度数 Parameters: num_parameters (int) – 要进行训练学习的 参数的数量。尽管此函数的输入是一个整形，但此函数要求输入的整形只能为两个值，1或者输入张量的通道数。默认：1 init (float) – 的初始值，默认: 0.25 Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 Variables: weight (Tensor) – 大小为num_parameters的可学习参数。The attr:dtype is default to（这句话有点问题， to后面漏掉了） 示例: >>> m = nn.PReLU() >>> input = torch.randn(2) >>> output = m(input) ReLU class torch.nn.ReLU(inplace=False) 将元素级线性整流函数函数应用到输入张量上 Applies the rectified linear unit function element-wise Parameters: inplace – 是否进行原位操作。 默认： False Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.ReLU() >>> input = torch.randn(2) >>> output = m(input) ReLU6 class torch.nn.ReLU6(inplace=False) 将下面的元素级函数应用到输入张量上: Parameters: inplace – 是否进行原位操作。 默认： False Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.ReLU6() >>> input = torch.randn(2) >>> output = m(input) RReLU class torch.nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=False) 将元素级随机线性整流函数函数应用到输入张量上，详情此文章： Empirical Evaluation of Rectified Activations in Convolutional Network. 此函数定义如下: 其中 是从此均匀分布中采样而来：. 详见: https://arxiv.org/pdf/1505.00853.pdf Parameters: lower – 均匀分布下限， 默认: upper – 均匀分布上限，默认: inplace – 是否进行原位操作。 默认： False Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.RReLU(0.1, 0.3) >>> input = torch.randn(2) >>> output = m(input) SELU class torch.nn.SELU(inplace=False) 将下面的元素级函数应用到输入张量上: 其中 而且 . More details can be found in the paper Self-Normalizing Neural Networks . Parameters: inplace (bool, optional) – 是否进行原位操作。 默认： False Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.SELU() >>> input = torch.randn(2) >>> output = m(input) CELU class torch.nn.CELU(alpha=1.0, inplace=False) 将下面的元素级函数应用到输入张量上： 更多细节请见paper: Continuously Differentiable Exponential Linear Units . Parameters: alpha – CELU操作中的 值，默认: 1.0 inplace – 是否进行原位操作。 默认： False Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.CELU() >>> input = torch.randn(2) >>> output = m(input) Sigmoid class torch.nn.Sigmoid 将下面的元素级函数应用到输入张量上： Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.Sigmoid() >>> input = torch.randn(2) >>> output = m(input) Softplus class torch.nn.Softplus(beta=1, threshold=20) 将下面的元素级函数应用到输入张量上: SoftPlus是一个平滑的类ReLU函数，可以用于将输出结果规范到全正。 为了数值稳定性，在实现此函数的过程中，当 x 超过某个特定值之后，我们会将此函数转化为一个线性函数。 Parameters: beta – Softplus操作的 值，默认: 1 threshold – 将函数转化为线性函数的阈值， 默认: 20 Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.Softplus() >>> input = torch.randn(2) >>> output = m(input) Softshrink class torch.nn.Softshrink(lambd=0.5) 将下面的元素级软收缩函数应用到输入张量上: Parameters: lambd – 软收缩运算的值，默认: 0.5 Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.Softshrink() >>> input = torch.randn(2) >>> output = m(input) Softsign class torch.nn.Softsign 将下面的元素级函数应用到输入张量上: Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.Softsign() >>> input = torch.randn(2) >>> output = m(input) Tanh class torch.nn.Tanh 将下面的元素级函数应用到输入张量上: Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.Tanh() >>> input = torch.randn(2) >>> output = m(input) Tanhshrink class torch.nn.Tanhshrink 将下面的元素级函数应用到输入张量上: Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.Tanhshrink() >>> input = torch.randn(2) >>> output = m(input) Threshold class torch.nn.Threshold(threshold, value, inplace=False) 使用阈值过滤输入张量的每个元素 阈值被定义如下： Parameters: threshold – 阈值大小 value – 小于阈值的元素的替换值 inplace – 是否进行原位操作。 默认： False Shape: 输入: 其中 * 代表支持任意大小的附加维度 输出: , 与输入向量保持一样的形状大小 示例: >>> m = nn.Threshold(0.1, 20) >>> input = torch.randn(2) >>> output = m(input) Non-linear activations (other) Softmin class torch.nn.Softmin(dim=None) Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range (0, 1) and sum to 1 Shape: Input: any shape Output: same as input Parameters: dim (int) – A dimension along which Softmin will be computed (so every slice along dim will sum to 1). Returns: a Tensor of the same dimension and shape as the input, with values in the range [0, 1] --- --- Examples: >>> m = nn.Softmin() >>> input = torch.randn(2, 3) >>> output = m(input) Softmax class torch.nn.Softmax(dim=None) Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range (0,1) and sum to 1 Softmax is defined as: Shape: Input: any shape Output: same as input Returns: a Tensor of the same dimension and shape as the input with values in the range [0, 1] Parameters: dim (int) – A dimension along which Softmax will be computed (so every slice along dim will sum to 1). --- --- Note This module doesn’t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use LogSoftmax instead (it’s faster and has better numerical properties). Examples: >>> m = nn.Softmax() >>> input = torch.randn(2, 3) >>> output = m(input) Softmax2d class torch.nn.Softmax2d Applies SoftMax over features to each spatial location. When given an image of Channels x Height x Width, it will apply Softmax to each location Shape: Input: Output: (same shape as input) Returns: a Tensor of the same dimension and shape as the input with values in the range [0, 1] Examples: >>> m = nn.Softmax2d() >>> # you softmax over the 2nd dimension >>> input = torch.randn(2, 3, 12, 13) >>> output = m(input) LogSoftmax class torch.nn.LogSoftmax(dim=None) Applies the function to an n-dimensional input Tensor. The LogSoftmax formulation can be simplified as: Shape: Input: any shape Output: same as input Parameters: dim (int) – A dimension along which Softmax will be computed (so every slice along dim will sum to 1). Returns: a Tensor of the same dimension and shape as the input with values in the range [-inf, 0) --- --- Examples: >>> m = nn.LogSoftmax() >>> input = torch.randn(2, 3) >>> output = m(input) AdaptiveLogSoftmaxWithLoss class torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, div_value=4.0, head_bias=False) Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Cissé, David Grangier, and Hervé Jégou. Adaptive softmax is an approximate strategy for training models with large output spaces. It is most effective when the label distribution is highly imbalanced, for example in natural language modelling, where the word frequency distribution approximately follows the Zipf’s law. Adaptive softmax partitions the labels into several clusters, according to their frequency. These clusters may contain different number of targets each. Additionally, clusters containing less frequent labels assign lower dimensional embeddings to those labels, which speeds up the computation. For each minibatch, only clusters for which at least one target is present are evaluated. The idea is that the clusters which are accessed frequently (like the first one, containing most frequent labels), should also be cheap to compute – that is, contain a small number of assigned labels. We highly recommend taking a look at the original paper for more details. cutoffs should be an ordered Sequence of integers sorted in the increasing order. It controls number of clusters and the partitioning of targets into clusters. For example setting cutoffs = [10, 100, 1000] means that first 10 targets will be assigned to the ‘head’ of the adaptive softmax, targets 11, 12, …, 100 will be assigned to the first cluster, and targets 101, 102, …, 1000 will be assigned to the second cluster, while targets 1001, 1002, …, n_classes - 1 will be assigned to the last, third cluster div_value is used to compute the size of each additional cluster, which is given as , where is the cluster index (with clusters for less frequent words having larger indices, and indices starting from ). head_bias if set to True, adds a bias term to the ‘head’ of the adaptive softmax. See paper for details. Set to False in the official implementation. Warning Labels passed as inputs to this module should be sorted accoridng to their frequency. This means that the most frequent label should be represented by the index 0, and the least frequent label should be represented by the index n_classes - 1. Note This module returns a NamedTuple with output and loss fields. See further documentation for details. Note To compute log-probabilities for all classes, the log_prob method can be used. Parameters: in_features (int) – Number of features in the input tensor n_classes (int) – Number of classes in the dataset. cutoffs (Sequence) – Cutoffs used to assign targets to their buckets. div_value (float, optional) – value used as an exponent to compute sizes of the clusters. Default: 4.0 | Returns: | output is a Tensor of size N containing computed target log probabilities for each example loss is a Scalar representing the computed negative log likelihood loss Return type: NamedTuple with output and loss fields Shape: input: target: where each value satisfies output: loss: Scalar log_prob(input) Computes log probabilities for all Parameters: input (Tensor) – a minibatch of examples Returns: log-probabilities of for each class in range , where is a parameter passed to AdaptiveLogSoftmaxWithLoss constructor. --- --- Shape: Input: Output: predict(input) This is equivalent to self.log_pob(input).argmax(dim=1), but is more efficient in some cases. Parameters: input (Tensor) – a minibatch of examples Returns: a class with the highest probability for each example --- --- Return type: output (Tensor) --- --- Shape: Input: Output: Normalization layers BatchNorm1d class torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift . The mean and standard-deviation are calculated per-dimension over the mini-batches and and are learnable parameter vectors of size C (where C is the input size). By default, the elements of are sampled from and the elements of are set to 0. Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well. Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is , where is the estimated statistic and is the new observed value. Because the Batch Normalization is done over the C dimension, computing statistics on (N, L) slices, it’s common terminology to call this Temporal Batch Normalization. Parameters: num_features – from an expected input of size or from input of size eps – a value added to the denominator for numerical stability. Default: 1e-5 momentum – the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1 affine – a boolean value that when set to True, this module has learnable affine parameters. Default: True track_running_stats – a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: True Shape: Input: or Output: or (same shape as input) Examples: >>> # With Learnable Parameters >>> m = nn.BatchNorm1d(100) >>> # Without Learnable Parameters >>> m = nn.BatchNorm1d(100, affine=False) >>> input = torch.randn(20, 100) >>> output = m(input) BatchNorm2d class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift . The mean and standard-deviation are calculated per-dimension over the mini-batches and and are learnable parameter vectors of size C (where C is the input size). By default, the elements of are sampled from and the elements of are set to 0. Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well. Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is , where is the estimated statistic and is the new observed value. Because the Batch Normalization is done over the C dimension, computing statistics on (N, H, W) slices, it’s common terminology to call this Spatial Batch Normalization. Parameters: num_features – from an expected input of size eps – a value added to the denominator for numerical stability. Default: 1e-5 momentum – the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1 affine – a boolean value that when set to True, this module has learnable affine parameters. Default: True track_running_stats – a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: True Shape: Input: Output: (same shape as input) Examples: >>> # With Learnable Parameters >>> m = nn.BatchNorm2d(100) >>> # Without Learnable Parameters >>> m = nn.BatchNorm2d(100, affine=False) >>> input = torch.randn(20, 100, 35, 45) >>> output = m(input) BatchNorm3d class torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift . The mean and standard-deviation are calculated per-dimension over the mini-batches and and are learnable parameter vectors of size C (where C is the input size). By default, the elements of are sampled from and the elements of are set to 0. Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well. Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is , where is the estimated statistic and is the new observed value. Because the Batch Normalization is done over the C dimension, computing statistics on (N, D, H, W) slices, it’s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization. Parameters: num_features – from an expected input of size eps – a value added to the denominator for numerical stability. Default: 1e-5 momentum – the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1 affine – a boolean value that when set to True, this module has learnable affine parameters. Default: True track_running_stats – a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: True Shape: Input: Output: (same shape as input) Examples: >>> # With Learnable Parameters >>> m = nn.BatchNorm3d(100) >>> # Without Learnable Parameters >>> m = nn.BatchNorm3d(100, affine=False) >>> input = torch.randn(20, 100, 35, 45, 10) >>> output = m(input) GroupNorm class torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True) Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization . The input channels are separated into num_groups groups, each containing num_channels / num_groups channels. The mean and standard-deviation are calculated separately over the each group. and are learnable per-channel affine transform parameter vectorss of size num_channels if affine is True. This layer uses statistics computed from input data in both training and evaluation modes. Parameters: num_groups (int) – number of groups to separate the channels into num_channels (int) – number of channels expected in input eps – a value added to the denominator for numerical stability. Default: 1e-5 affine – a boolean value that when set to True, this module has learnable per-channel affine parameters initialized to ones (for weights) and zeros (for biases). Default: True. Shape: Input: Output: (same shape as input) Examples: >>> input = torch.randn(20, 6, 10, 10) >>> # Separate 6 channels into 3 groups >>> m = nn.GroupNorm(3, 6) >>> # Separate 6 channels into 6 groups (equivalent with InstanceNorm) >>> m = nn.GroupNorm(6, 6) >>> # Put all 6 channels into a single group (equivalent with LayerNorm) >>> m = nn.GroupNorm(1, 6) >>> # Activating the module >>> output = m(input) InstanceNorm1d class torch.nn.InstanceNorm1d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False) Applies Instance Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization . The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. and are learnable parameter vectors of size C (where C is the input size) if affine is True. By default, this layer uses instance statistics computed from input data in both training and evaluation modes. If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is , where is the estimated statistic and is the new observed value. Note InstanceNorm1d and LayerNorm are very similar, but have some subtle differences. InstanceNorm1d is applied on each channel of channeled data like multidimensional time series, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionaly, LayerNorm applies elementwise affine transform, while InstanceNorm1d usually don’t apply affine transform. Parameters: num_features – from an expected input of size or from input of size eps – a value added to the denominator for numerical stability. Default: 1e-5 momentum – the value used for the running_mean and running_var computation. Default: 0.1 affine – a boolean value that when set to True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: False. track_running_stats – a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: False Shape: Input: Output: (same shape as input) Examples: >>> # Without Learnable Parameters >>> m = nn.InstanceNorm1d(100) >>> # With Learnable Parameters >>> m = nn.InstanceNorm1d(100, affine=True) >>> input = torch.randn(20, 100, 40) >>> output = m(input) InstanceNorm2d class torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False) Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization . The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. and are learnable parameter vectors of size C (where C is the input size) if affine is True. By default, this layer uses instance statistics computed from input data in both training and evaluation modes. If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is , where is the estimated statistic and is the new observed value. Note InstanceNorm2d and LayerNorm are very similar, but have some subtle differences. InstanceNorm2d is applied on each channel of channeled data like RGB images, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionaly, LayerNorm applies elementwise affine transform, while InstanceNorm2d usually don’t apply affine transform. Parameters: num_features – from an expected input of size eps – a value added to the denominator for numerical stability. Default: 1e-5 momentum – the value used for the running_mean and running_var computation. Default: 0.1 affine – a boolean value that when set to True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: False. track_running_stats – a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: False Shape: Input: Output: (same shape as input) Examples: >>> # Without Learnable Parameters >>> m = nn.InstanceNorm2d(100) >>> # With Learnable Parameters >>> m = nn.InstanceNorm2d(100, affine=True) >>> input = torch.randn(20, 100, 35, 45) >>> output = m(input) InstanceNorm3d class torch.nn.InstanceNorm3d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False) Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization . The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. and are learnable parameter vectors of size C (where C is the input size) if affine is True. By default, this layer uses instance statistics computed from input data in both training and evaluation modes. If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is , where is the estimated statistic and is the new observed value. Note InstanceNorm3d and LayerNorm are very similar, but have some subtle differences. InstanceNorm3d is applied on each channel of channeled data like 3D models with RGB color, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionaly, LayerNorm applies elementwise affine transform, while InstanceNorm3d usually don’t apply affine transform. Parameters: num_features – from an expected input of size eps – a value added to the denominator for numerical stability. Default: 1e-5 momentum – the value used for the running_mean and running_var computation. Default: 0.1 affine – a boolean value that when set to True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: False. track_running_stats – a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: False Shape: Input: Output: (same shape as input) Examples: >>> # Without Learnable Parameters >>> m = nn.InstanceNorm3d(100) >>> # With Learnable Parameters >>> m = nn.InstanceNorm3d(100, affine=True) >>> input = torch.randn(20, 100, 35, 45, 10) >>> output = m(input) LayerNorm class torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True) Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization . The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by normalized_shape. and are learnable affine transform parameters of normalized_shape if elementwise_affine is True. Note Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the affine option, Layer Normalization applies per-element scale and bias with elementwise_affine. This layer uses statistics computed from input data in both training and evaluation modes. Parameters: normalized_shape (int or list or torch.Size) – input shape from an expected input of size If a single integer is used, it is treated as a singleton list, and this module will normalize over the last dimension which is expected to be of that specific size. eps – a value added to the denominator for numerical stability. Default: 1e-5 elementwise_affine – a boolean value that when set to True, this module has learnable per-element affine parameters initialized to ones (for weights) and zeros (for biases). Default: True. Shape: Input: Output: (same shape as input) Examples: >>> input = torch.randn(20, 5, 10, 10) >>> # With Learnable Parameters >>> m = nn.LayerNorm(input.size()[1:]) >>> # Without Learnable Parameters >>> m = nn.LayerNorm(input.size()[1:], elementwise_affine=False) >>> # Normalize over last two dimensions >>> m = nn.LayerNorm([10, 10]) >>> # Normalize over last dimension of size 10 >>> m = nn.LayerNorm(10) >>> # Activating the module >>> output = m(input) LocalResponseNorm class torch.nn.LocalResponseNorm(size, alpha=0.0001, beta=0.75, k=1.0) Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels. Parameters: size – amount of neighbouring channels used for normalization alpha – multiplicative factor. Default: 0.0001 beta – exponent. Default: 0.75 k – additive factor. Default: 1 Shape: Input: Output: (same shape as input) Examples: >>> lrn = nn.LocalResponseNorm(2) >>> signal_2d = torch.randn(32, 5, 24, 24) >>> signal_4d = torch.randn(16, 5, 7, 7, 7, 7) >>> output_2d = lrn(signal_2d) >>> output_4d = lrn(signal_4d) Recurrent layers RNN class torch.nn.RNN(*args, **kwargs) Applies a multi-layer Elman RNN with or non-linearity to an input sequence. For each element in the input sequence, each layer computes the following function: where is the hidden state at time t, is the input at time t, and is the hidden state of the previous layer at time t-1 or the initial hidden state at time 0. If nonlinearity is ‘relu’, then ReLU is used instead of tanh. Parameters: input_size – The number of expected features in the input x hidden_size – The number of features in the hidden state h num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1 nonlinearity – The non-linearity to use. Can be either ‘tanh’ or ‘relu’. Default: ‘tanh’ bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True batch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False dropout – If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0 bidirectional – If True, becomes a bidirectional RNN. Default: False Inputs: input, h_0 input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details. h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1. Outputs: output, h_n output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_k) from the last layer of the RNN, for each k. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case. h_n (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for k = seq_len. Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size). | Variables: | weight_ih_l[k] – the learnable input-hidden weights of the k-th layer, of shape (hidden_size * input_size) for k = 0. Otherwise, the shape is (hidden_size * hidden_size) weight_hh_l[k] – the learnable hidden-hidden weights of the k-th layer, of shape (hidden_size * hidden_size) bias_ih_l[k] – the learnable input-hidden bias of the k-th layer, of shape (hidden_size) bias_hh_l[k] – the learnable hidden-hidden bias of the k-th layer, of shape (hidden_size) Note All the weights and biases are initialized from where Note If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance. Examples: >>> rnn = nn.RNN(10, 20, 2) >>> input = torch.randn(5, 3, 10) >>> h0 = torch.randn(2, 3, 20) >>> output, hn = rnn(input, h0) LSTM class torch.nn.LSTM(*args, **kwargs) Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence. For each element in the input sequence, each layer computes the following function: where is the hidden state at time t, is the cell state at time t, is the input at time t, is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and , , , are the input, forget, cell, and output gates, respectively. is the sigmoid function. In a multilayer LSTM, the input of the -th layer () is the hidden state of the previous layer multiplied by dropout where each is a Bernoulli random variable which is with probability dropout. Parameters: input_size – The number of expected features in the input x hidden_size – The number of features in the hidden state h num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1 bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True batch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False dropout – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0 bidirectional – If True, becomes a bidirectional LSTM. Default: False Inputs: input, (h_0, c_0) input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details. h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. If the RNN is bidirectional, num_directions should be 2, else it should be 1. c_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial cell state for each element in the batch. If (h_0, c_0) is not provided, both h_0 and c_0 default to zero. Outputs: output, (h_n, c_n) output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the LSTM, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case. h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len. Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size) and similarly for c_n. c_n (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for t = seq_len | Variables: | weight_ih_l[k] – the learnable input-hidden weights of the layer (W_ii&#124;W_if&#124;W_ig&#124;W_io), of shape (4*hidden_size x input_size) weight_hh_l[k] – the learnable hidden-hidden weights of the layer (W_hi&#124;W_hf&#124;W_hg&#124;W_ho), of shape (4*hidden_size x hidden_size) bias_ih_l[k] – the learnable input-hidden bias of the layer (b_ii&#124;b_if&#124;b_ig&#124;b_io), of shape (4*hidden_size) bias_hh_l[k] – the learnable hidden-hidden bias of the layer (b_hi&#124;b_hf&#124;b_hg&#124;b_ho), of shape (4*hidden_size) Note All the weights and biases are initialized from where Note If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance. Examples: >>> rnn = nn.LSTM(10, 20, 2) >>> input = torch.randn(5, 3, 10) >>> h0 = torch.randn(2, 3, 20) >>> c0 = torch.randn(2, 3, 20) >>> output, (hn, cn) = rnn(input, (h0, c0)) GRU class torch.nn.GRU(*args, **kwargs) Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence. For each element in the input sequence, each layer computes the following function: where is the hidden state at time t, is the input at time t, is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and , , are the reset, update, and new gates, respectively. is the sigmoid function. In a multilayer GRU, the input of the -th layer () is the hidden state of the previous layer multiplied by dropout where each is a Bernoulli random variable which is with probability dropout. Parameters: input_size – The number of expected features in the input x hidden_size – The number of features in the hidden state h num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1 bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True batch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False dropout – If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0 bidirectional – If True, becomes a bidirectional GRU. Default: False Inputs: input, h_0 input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() for details. h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1. Outputs: output, h_n output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case. h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size). | Variables: | weight_ih_l[k] – the learnable input-hidden weights of the layer (W_ir|W_iz|W_in), of shape (3*hidden_size x input_size) weight_hh_l[k] – the learnable hidden-hidden weights of the layer (W_hr|W_hz|W_hn), of shape (3*hidden_size x hidden_size) bias_ih_l[k] – the learnable input-hidden bias of the layer (b_ir|b_iz|b_in), of shape (3*hidden_size) bias_hh_l[k] – the learnable hidden-hidden bias of the layer (b_hr|b_hz|b_hn), of shape (3*hidden_size) Note All the weights and biases are initialized from where Note If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance. Examples: >>> rnn = nn.GRU(10, 20, 2) >>> input = torch.randn(5, 3, 10) >>> h0 = torch.randn(2, 3, 20) >>> output, hn = rnn(input, h0) RNNCell class torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh') An Elman RNN cell with tanh or ReLU non-linearity. If nonlinearity is ‘relu’, then ReLU is used in place of tanh. Parameters: input_size – The number of expected features in the input x hidden_size – The number of features in the hidden state h bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True nonlinearity – The non-linearity to use. Can be either ‘tanh’ or ‘relu’. Default: ‘tanh’ Inputs: input, hidden input of shape (batch, input_size): tensor containing input features hidden of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. Outputs: h’ h’ of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch | Variables: | weight_ih – the learnable input-hidden weights, of shape (hidden_size x input_size) weight_hh – the learnable hidden-hidden weights, of shape (hidden_size x hidden_size) bias_ih – the learnable input-hidden bias, of shape (hidden_size) bias_hh – the learnable hidden-hidden bias, of shape (hidden_size) Note All the weights and biases are initialized from where Examples: >>> rnn = nn.RNNCell(10, 20) >>> input = torch.randn(6, 3, 10) >>> hx = torch.randn(3, 20) >>> output = [] >>> for i in range(6): hx = rnn(input[i], hx) output.append(hx) LSTMCell class torch.nn.LSTMCell(input_size, hidden_size, bias=True) A long short-term memory (LSTM) cell. where is the sigmoid function. Parameters: input_size – The number of expected features in the input x hidden_size – The number of features in the hidden state h bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True Inputs: input, (h_0, c_0) input of shape (batch, input_size): tensor containing input features h_0 of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. c_0 of shape (batch, hidden_size): tensor containing the initial cell state for each element in the batch. If (h_0, c_0) is not provided, both h_0 and c_0 default to zero. Outputs: h_1, c_1 h_1 of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch c_1 of shape (batch, hidden_size): tensor containing the next cell state for each element in the batch | Variables: | weight_ih – the learnable input-hidden weights, of shape (4*hidden_size x input_size) weight_hh – the learnable hidden-hidden weights, of shape (4*hidden_size x hidden_size) bias_ih – the learnable input-hidden bias, of shape (4*hidden_size) bias_hh – the learnable hidden-hidden bias, of shape (4*hidden_size) Note All the weights and biases are initialized from where Examples: >>> rnn = nn.LSTMCell(10, 20) >>> input = torch.randn(6, 3, 10) >>> hx = torch.randn(3, 20) >>> cx = torch.randn(3, 20) >>> output = [] >>> for i in range(6): hx, cx = rnn(input[i], (hx, cx)) output.append(hx) GRUCell class torch.nn.GRUCell(input_size, hidden_size, bias=True) A gated recurrent unit (GRU) cell where is the sigmoid function. Parameters: input_size – The number of expected features in the input x hidden_size – The number of features in the hidden state h bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True Inputs: input, hidden input of shape (batch, input_size): tensor containing input features hidden of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. Outputs: h’ h’ of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch | Variables: | weight_ih – the learnable input-hidden weights, of shape (3*hidden_size x input_size) weight_hh – the learnable hidden-hidden weights, of shape (3*hidden_size x hidden_size) bias_ih – the learnable input-hidden bias, of shape (3*hidden_size) bias_hh – the learnable hidden-hidden bias, of shape (3*hidden_size) Note All the weights and biases are initialized from where Examples: >>> rnn = nn.GRUCell(10, 20) >>> input = torch.randn(6, 3, 10) >>> hx = torch.randn(3, 20) >>> output = [] >>> for i in range(6): hx = rnn(input[i], hx) output.append(hx) Linear layers Linear class torch.nn.Linear(in_features, out_features, bias=True) Applies a linear transformation to the incoming data: Parameters: in_features – size of each input sample out_features – size of each output sample bias – If set to False, the layer will not learn an additive bias. Default: True Shape: Input: where means any number of additional dimensions Output: where all but the last dimension are the same shape as the input. | Variables: | weight – the learnable weights of the module of shape . The values are initialized from , where bias – the learnable bias of the module of shape . If bias is True, the values are initialized from where Examples: >>> m = nn.Linear(20, 30) >>> input = torch.randn(128, 20) >>> output = m(input) >>> print(output.size()) torch.Size([128, 30]) Bilinear class torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True) Applies a bilinear transformation to the incoming data: Parameters: in1_features – size of each first input sample in2_features – size of each second input sample out_features – size of each output sample bias – If set to False, the layer will not learn an additive bias. Default: True Shape: Input: , where means any number of additional dimensions. All but the last dimension of the inputs should be the same. Output: where all but the last dimension are the same shape as the input. | Variables: | weight – the learnable weights of the module of shape . The values are initialized from , where bias – the learnable bias of the module of shape If bias is True, the values are initialized from , where Examples: >>> m = nn.Bilinear(20, 30, 40) >>> input1 = torch.randn(128, 20) >>> input2 = torch.randn(128, 30) >>> output = m(input1, input2) >>> print(output.size()) torch.Size([128, 40]) Dropout layers Dropout class torch.nn.Dropout(p=0.5, inplace=False) During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call. This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper Improving neural networks by preventing co-adaptation of feature detectors . Furthermore, the outputs are scaled by a factor of during training. This means that during evaluation the module simply computes an identity function. Parameters: p – probability of an element to be zeroed. Default: 0.5 inplace – If set to True, will do this operation in-place. Default: False Shape: Input: Any. Input can be of any shape Output: Same. Output is of the same shape as input Examples: >>> m = nn.Dropout(p=0.2) >>> input = torch.randn(20, 16) >>> output = m(input) Dropout2d class torch.nn.Dropout2d(p=0.5, inplace=False) Randomly zero out entire channels (a channel is a 2D feature map, e.g., the -th channel of the -th sample in the batched input is a 2D tensor ) of the input tensor). Each channel will be zeroed out independently on every forward call. with probability p using samples from a Bernoulli distribution. Usually the input comes from nn.Conv2d modules. As described in the paper Efficient Object Localization Using Convolutional Networks , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, nn.Dropout2d() will help promote independence between feature maps and should be used instead. Parameters: p (float, optional) – probability of an element to be zero-ed. inplace (bool, optional) – If set to True, will do this operation in-place Shape: Input: Output: (same shape as input) Examples: >>> m = nn.Dropout2d(p=0.2) >>> input = torch.randn(20, 16, 32, 32) >>> output = m(input) Dropout3d class torch.nn.Dropout3d(p=0.5, inplace=False) Randomly zero out entire channels (a channel is a 3D feature map, e.g., the -th channel of the -th sample in the batched input is a 3D tensor ) of the input tensor). Each channel will be zeroed out independently on every forward call. with probability p using samples from a Bernoulli distribution. Usually the input comes from nn.Conv3d modules. As described in the paper Efficient Object Localization Using Convolutional Networks , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, nn.Dropout3d() will help promote independence between feature maps and should be used instead. Parameters: p (float, optional) – probability of an element to be zeroed. inplace (bool, optional) – If set to True, will do this operation in-place Shape: Input: Output: (same shape as input) Examples: >>> m = nn.Dropout3d(p=0.2) >>> input = torch.randn(20, 16, 4, 32, 32) >>> output = m(input) AlphaDropout class torch.nn.AlphaDropout(p=0.5, inplace=False) Applies Alpha Dropout over the input. Alpha Dropout is a type of Dropout that maintains the self-normalizing property. For an input with zero mean and unit standard deviation, the output of Alpha Dropout maintains the original mean and standard deviation of the input. Alpha Dropout goes hand-in-hand with SELU activation function, which ensures that the outputs have zero mean and unit standard deviation. During training, it randomly masks some of the elements of the input tensor with probability p using samples from a bernoulli distribution. The elements to masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit standard deviation. During evaluation the module simply computes an identity function. More details can be found in the paper Self-Normalizing Neural Networks . Parameters: p (float) – probability of an element to be dropped. Default: 0.5 inplace (bool, optional) – If set to True, will do this operation in-place Shape: Input: Any. Input can be of any shape Output: Same. Output is of the same shape as input Examples: >>> m = nn.AlphaDropout(p=0.2) >>> input = torch.randn(20, 16) >>> output = m(input) Sparse layers Embedding class torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None) A simple lookup table that stores embeddings of a fixed dictionary and size. This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings. Parameters: num_embeddings (int) – size of the dictionary of embeddings embedding_dim (int) – the size of each embedding vector padding_idx (int, optional) – If given, pads the output with the embedding vector at padding_idx (initialized to zeros) whenever it encounters the index. max_norm (float, optional) – If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm. norm_type (float, optional) – The p of the p-norm to compute for the max_norm option. Default 2. scale_grad_by_freq (boolean__, optional) – If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False. sparse (bool, optional) – If True, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients. Variables: weight (Tensor) – the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from Shape: Input: LongTensor of arbitrary shape containing the indices to extract Output: (*, embedding_dim), where * is the input shape Note Keep in mind that only a limited number of optimizers support sparse gradients: currently it’s optim.SGD (CUDA and CPU), optim.SparseAdam (CUDA and CPU) and optim.Adagrad (CPU) Note With padding_idx set, the embedding vector at padding_idx is initialized to all zeros. However, note that this vector can be modified afterwards, e.g., using a customized initialization method, and thus changing the vector used to pad the output. The gradient for this vector from Embedding is always zero. Examples: >>> # an Embedding module containing 10 tensors of size 3 >>> embedding = nn.Embedding(10, 3) >>> # a batch of 2 samples of 4 indices each >>> input = torch.LongTensor([[1,2,4,5],[4,3,2,9]]) >>> embedding(input) tensor([[[-0.0251, -1.6902, 0.7172], [-0.6431, 0.0748, 0.6969], [ 1.4970, 1.3448, -0.9685], [-0.3677, -2.7265, -0.1685]], [[ 1.4970, 1.3448, -0.9685], [ 0.4362, -0.4004, 0.9400], [-0.6431, 0.0748, 0.6969], [ 0.9124, -2.3616, 1.1151]]]) >>> # example with padding_idx >>> embedding = nn.Embedding(10, 3, padding_idx=0) >>> input = torch.LongTensor([[0,2,0,5]]) >>> embedding(input) tensor([[[ 0.0000, 0.0000, 0.0000], [ 0.1535, -2.0309, 0.9315], [ 0.0000, 0.0000, 0.0000], [-0.1655, 0.9897, 0.0635]]]) classmethod from_pretrained(embeddings, freeze=True, sparse=False) Creates Embedding instance from given 2-dimensional FloatTensor. Parameters: embeddings (Tensor) – FloatTensor containing weights for the Embedding. First dimension is being passed to Embedding as ‘num_embeddings’, second as ‘embedding_dim’. freeze (boolean__, optional) – If True, the tensor does not get updated in the learning process. Equivalent to embedding.weight.requires_grad = False. Default: True sparse (bool, optional) – if True, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients. Examples: >>> # FloatTensor containing pretrained weights >>> weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]]) >>> embedding = nn.Embedding.from_pretrained(weight) >>> # Get embeddings for index 1 >>> input = torch.LongTensor([1]) >>> embedding(input) tensor([[ 4.0000, 5.1000, 6.3000]]) EmbeddingBag class torch.nn.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, mode='mean', sparse=False) Computes sums or means of ‘bags’ of embeddings, without instantiating the intermediate embeddings. For bags of constant length, this class with mode=\"sum\" is equivalent to Embedding followed by torch.sum(dim=1), with mode=\"mean\" is equivalent to Embedding followed by torch.mean(dim=1), with mode=\"max\" is equivalent to Embedding followed by torch.max(dim=1). However, EmbeddingBag is much more time and memory efficient than using a chain of these operations. Parameters: num_embeddings (int) – size of the dictionary of embeddings embedding_dim (int) – the size of each embedding vector max_norm (float, optional) – If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm. norm_type (float, optional) – The p of the p-norm to compute for the max_norm option. Default 2. scale_grad_by_freq (boolean__, optional) – if given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False. Note: this option is not supported when mode=\"max\". mode (string__, optional) – \"sum\", \"mean\" or \"max\". Specifies the way to reduce the bag. Default: \"mean\" sparse (bool, optional) – if True, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients. Note: this option is not supported when mode=\"max\". Variables: weight (Tensor) – the learnable weights of the module of shape (num_embeddings x embedding_dim) initialized from . Inputs: input (LongTensor) and offsets (LongTensor, optional) If input is 2D of shape B x N, it will be treated as `B` bags (sequences) each of fixed length `N`, and this will return `B` values aggregated in a way depending on the `mode`. `offsets` is ignored and required to be `None` in this case. If input is 1D of shape N, it will be treated as a concatenation of multiple bags (sequences). `offsets` is required to be a 1D tensor containing the starting index positions of each bag in `input`. Therefore, for `offsets` of shape `B`, `input` will be viewed as having `B` bags. Empty bags (i.e., having 0-length) will have returned vectors filled by zeros. Output shape: B x embedding_dim Examples: >>> # an Embedding module containing 10 tensors of size 3 >>> embedding_sum = nn.EmbeddingBag(10, 3, mode='sum') >>> # a batch of 2 samples of 4 indices each >>> input = torch.LongTensor([1,2,4,5,4,3,2,9]) >>> offsets = torch.LongTensor([0,4]) >>> embedding_sum(input, offsets) tensor([[-0.8861, -5.4350, -0.0523], [ 1.1306, -2.5798, -1.0044]]) Distance functions CosineSimilarity class torch.nn.CosineSimilarity(dim=1, eps=1e-08) Returns cosine similarity between and , computed along dim. Parameters: dim (int, optional) – Dimension where cosine similarity is computed. Default: 1 eps (float, optional) – Small value to avoid division by zero. Default: 1e-8 Shape: Input1: where D is at position dim Input2: , same shape as the Input1 Output: Examples: >>> input1 = torch.randn(100, 128) >>> input2 = torch.randn(100, 128) >>> cos = nn.CosineSimilarity(dim=1, eps=1e-6) >>> output = cos(input1, input2) PairwiseDistance class torch.nn.PairwiseDistance(p=2.0, eps=1e-06, keepdim=False) Computes the batchwise pairwise distance between vectors , using the p-norm: Parameters: p (real) – the norm degree. Default: 2 eps (float, optional) – Small value to avoid division by zero. Default: 1e-6 keepdim (bool, optional) – Determines whether or not to keep the batch dimension. Default: False Shape: Input1: where D = vector dimension Input2: , same shape as the Input1 Output: . If keepdim is False, then . Examples: >>> pdist = nn.PairwiseDistance(p=2) >>> input1 = torch.randn(100, 128) >>> input2 = torch.randn(100, 128) >>> output = pdist(input1, input2) Loss functions L1Loss class torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean') Creates a criterion that measures the mean absolute error (MAE) between each element in the input x and target y. The loss can be described as: where is the batch size. If reduce is True, then: x and y are tensors of arbitrary shapes with a total of n elements each. The sum operation still operates over all the elements, and divides by n. The division by n can be avoided if one sets the constructor argument size_average=False. Parameters: size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: where * means, any number of additional dimensions Target: , same shape as the input Output: scalar. If reduce is False, then , same shape as the input Examples: >>> loss = nn.L1Loss() >>> input = torch.randn(3, 5, requires_grad=True) >>> target = torch.randn(3, 5) >>> output = loss(input, target) >>> output.backward() MSELoss class torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean') Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input x and target y. The loss can be described as: where is the batch size. If reduce is True, then: The sum operation still operates over all the elements, and divides by n. The division by n can be avoided if one sets size_average to False. To get a batch of losses, a loss per batch element, set reduce to False. These losses are not averaged and are not affected by size_average. Parameters: size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: where * means, any number of additional dimensions Target: , same shape as the input Examples: >>> loss = nn.MSELoss() >>> input = torch.randn(3, 5, requires_grad=True) >>> target = torch.randn(3, 5) >>> output = loss(input, target) >>> output.backward() CrossEntropyLoss class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class. It is useful when training a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set. The input is expected to contain scores for each class. input has to be a Tensor of size either or with for the K-dimensional case (described later). This criterion expects a class index (0 to C-1) as the target for each value of a 1D tensor of size minibatch The loss can be described as: or in the case of the weight argument being specified: The losses are averaged across observations for each minibatch. Can also be used for higher dimension inputs, such as 2D images, by providing an input of size with , where is the number of dimensions, and a target of appropriate shape (see below). Parameters: weight (Tensor, optional) – a manual rescaling weight given to each class. If given, has to be a Tensor of size C size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True ignore_index (int, optional) – Specifies a target value that is ignored and does not contribute to the input gradient. When size_average is True, the loss is averaged over non-ignored targets. reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: \\((N, C)\\) where C = number of classes, or with in the case of K-dimensional loss. Target: \\((N)\\) where each value is \\(0 \\leq \\text{targets}[i] \\leq C-1\\), or with in the case of K-dimensional loss. Output: scalar. If reduce is False, then the same size as the target: , or with in the case of K-dimensional loss. Examples: >>> loss = nn.CrossEntropyLoss() >>> input = torch.randn(3, 5, requires_grad=True) >>> target = torch.empty(3, dtype=torch.long).random_(5) >>> output = loss(input, target) >>> output.backward() CTCLoss class torch.nn.CTCLoss(blank=0, reduction='mean') The Connectionist Temporal Classification loss. Parameters: blank (int, optional) – blank label. Default . reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the output losses will be divided by the target lengths and then the mean over the batch is taken. Default: ‘mean’ Inputs: log_probs: Tensor of size \\((T, N, C)\\) where C = number of characters in alphabet including blank, T = input length, and N = batch size. The logarithmized probabilities of the outputs (e.g. obtained with torch.nn.functional.log_softmax()). targets: Tensor of size \\((N, S)\\) or (sum(target_lengths)). Targets (cannot be blank). In the second form, the targets are assumed to be concatenated. input_lengths: Tuple or tensor of size \\((N)\\). Lengths of the inputs (must each be ) target_lengths: Tuple or tensor of size \\((N)\\). Lengths of the targets Example: >>> ctc_loss = nn.CTCLoss() >>> log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_() >>> targets = torch.randint(1, 20, (16, 30), dtype=torch.long) >>> input_lengths = torch.full((16,), 50, dtype=torch.long) >>> target_lengths = torch.randint(10,30,(16,), dtype=torch.long) >>> loss = ctc_loss(log_probs, targets, input_lengths, target_lengths) >>> loss.backward() Reference: A. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks: https://www.cs.toronto.edu/~graves/icml_2006.pdf Note In order to use CuDNN, the following must be satisfied: targets must be in concatenated format, all input_lengths must be T. , target_lengths , the integer arguments must be of dtype torch.int32. The regular implementation uses the (more common in PyTorch) torch.long dtype. Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background. NLLLoss class torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') The negative log likelihood loss. It is useful to train a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set. The input given through a forward call is expected to contain log-probabilities of each class. input has to be a Tensor of size either or with for the K-dimensional case (described later). Obtaining log-probabilities in a neural network is easily achieved by adding a LogSoftmax layer in the last layer of your network. You may use CrossEntropyLoss instead, if you prefer not to add an extra layer. The target that this loss expects is a class index (0 to C-1, where C = number of classes) If reduce is False, the loss can be described as: where is the batch size. If reduce is True (default), then Can also be used for higher dimension inputs, such as 2D images, by providing an input of size with , where is the number of dimensions, and a target of appropriate shape (see below). In the case of images, it computes NLL loss per-pixel. Parameters: weight (Tensor, optional) – a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones. size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True ignore_index (int, optional) – Specifies a target value that is ignored and does not contribute to the input gradient. When size_average is True, the loss is averaged over non-ignored targets. reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: \\((N, C)\\) where C = number of classes, or with in the case of K-dimensional loss. Target: \\((N)\\) where each value is \\(0 \\leq \\text{targets}[i] \\leq C-1\\), or with in the case of K-dimensional loss. Output: scalar. If reduce is False, then the same size as the target: , or with in the case of K-dimensional loss. Examples: >>> m = nn.LogSoftmax() >>> loss = nn.NLLLoss() >>> # input is of size N x C = 3 x 5 >>> input = torch.randn(3, 5, requires_grad=True) >>> # each element in target has to have 0 >> target = torch.tensor([1, 0, 4]) >>> output = loss(m(input), target) >>> output.backward() >>> >>> >>> # 2D loss example (used, for example, with image inputs) >>> N, C = 5, 4 >>> loss = nn.NLLLoss() >>> # input is of size N x C x height x width >>> data = torch.randn(N, 16, 10, 10) >>> conv = nn.Conv2d(16, C, (3, 3)) >>> m = nn.LogSoftmax() >>> # each element in target has to have 0 >> target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C) >>> output = loss(m(conv(data)), target) >>> output.backward() PoissonNLLLoss class torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean') Negative log likelihood loss with Poisson distribution of target. The loss can be described as: The last term can be omitted or approximated with Stirling formula. The approximation is used for target values more than 1. For targets less or equal to 1 zeros are added to the loss. Parameters: log_input (bool, optional) – if True the loss is computed as , if False the loss is . full (bool, optional) – whether to compute full loss, i. e. to add the Stirling approximation term size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True eps (float, optional) – Small value to avoid evaluation of when log_input == False. Default: 1e-8 reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Examples: >>> loss = nn.PoissonNLLLoss() >>> log_input = torch.randn(5, 2, requires_grad=True) >>> target = torch.randn(5, 2) >>> output = loss(log_input, target) >>> output.backward() KLDivLoss class torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean') The Kullback-Leibler divergence Loss KL divergence is a useful distance measure for continuous distributions and is often useful when performing direct regression over the space of (discretely sampled) continuous output distributions. As with NLLLoss, the input given is expected to contain log-probabilities. However, unlike NLLLoss, input is not restricted to a 2D Tensor. The targets are given as probabilities (i.e. without taking the logarithm). This criterion expects a target Tensor of the same size as the input Tensor. The unreduced (i.e. with reduce set to False) loss can be described as: where the index spans all dimensions of input and has the same shape as input. If reduce is True (the default), then: In default reduction mode ‘mean’, the losses are averaged for each minibatch over observations as well as over dimensions. ‘batchmean’ mode gives the correct KL divergence where losses are averaged over batch dimension only. ‘mean’ mode’s behavior will be changed to the same as ‘batchmean’ in the next major release. Parameters: size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘batchmean’ | ‘sum’ | ‘mean’. ‘none’: no reduction will be applied. ‘batchmean’: the sum of the output will be divided by batchsize. ‘sum’: the output will be summed. ‘mean’: the output will be divided by the number of elements in the output. Default: ‘mean’ :param .. note:: size_average and reduce are in the process of being deprecated,: and in the meantime, specifying either of those two args will override reduction. :param .. note:: reduction=’mean’ doesn’t return the true kl divergence value, please use: reduction=’batchmean’ which aligns with KL math definition. In the next major release, ‘mean’ will be changed to be the same as ‘batchmean’. Shape: input: where * means, any number of additional dimensions target: , same shape as the input output: scalar by default. If reduce is False, then \\((N, *)\\), the same shape as the input BCELoss class torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean') Creates a criterion that measures the Binary Cross Entropy between the target and the output: The loss can be described as: where is the batch size. If reduce is True, then This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets y should be numbers between 0 and 1. Parameters: weight (Tensor, optional) – a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size “nbatch”. size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: where * means, any number of additional dimensions Target: , same shape as the input Output: scalar. If reduce is False, then (N, *), same shape as input. Examples: >>> m = nn.Sigmoid() >>> loss = nn.BCELoss() >>> input = torch.randn(3, requires_grad=True) >>> target = torch.empty(3).random_(2) >>> output = loss(m(input), target) >>> output.backward() BCEWithLogitsLoss class torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None) This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability. The loss can be described as: where is the batch size. If reduce is True, then This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets t[i] should be numbers between 0 and 1. It’s possible to trade off recall and precision by adding weights to positive examples. In this case the loss can be described as: where is the positive weight of class . increases the recall, increases the precision. For example, if a dataset contains 100 positive and 300 negative examples of a single class, then pos_weight for the class should be equal to . The loss would act as if the dataset contains positive examples. Parameters: weight (Tensor, optional) – a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size “nbatch”. size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ pos_weight – a weight of positive examples. Must be a vector with length equal to the number of classes. MarginRankingLoss class torch.nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean') Creates a criterion that measures the loss given inputs x1, x2, two 1D mini-batch Tensors, and a label 1D mini-batch tensor y with values (1 or -1). If y == 1 then it assumed the first input should be ranked higher (have a larger value) than the second input, and vice-versa for y == -1. The loss function for each sample in the mini-batch is: Parameters: margin (float, optional) – Has a default value of 0. size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: where N is the batch size and D is the size of a sample. Target: Output: scalar. If reduce is False, then (N). HingeEmbeddingLoss class torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean') Measures the loss given an input tensor x and a labels tensor y containing values (1 or -1). This is usually used for measuring whether two inputs are similar or dissimilar, e.g. using the L1 pairwise distance as x, and is typically used for learning nonlinear embeddings or semi-supervised learning. The loss function for -th sample in the mini-batch is and the total loss functions is where . Parameters: margin (float, optional) – Has a default value of 1. size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: Tensor of arbitrary shape. The sum operation operates over all the elements. Target: Same shape as input. Output: scalar. If reduce is False, then same shape as the input MultiLabelMarginLoss class torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction='mean') Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input x (a 2D mini-batch Tensor) and output y (which is a 2D Tensor of target class indices). For each sample in the mini-batch: where to , to , , and for all and . y and x must have the same size. The criterion only considers a contiguous block of non-negative targets that starts at the front. This allows for different samples to have variable amounts of target classes Parameters: size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: or where N is the batch size and C is the number of classes. Target: or , same shape as the input. Output: scalar. If reduce is False, then (N). SmoothL1Loss class torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean') Creates a criterion that uses a squared term if the absolute element-wise error falls below 1 and an L1 term otherwise. It is less sensitive to outliers than the MSELoss and in some cases prevents exploding gradients (e.g. see “Fast R-CNN” paper by Ross Girshick). Also known as the Huber loss: where is given by: x and y arbitrary shapes with a total of n elements each the sum operation still operates over all the elements, and divides by n. The division by n can be avoided if one sets size_average to False Parameters: size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: where * means, any number of additional dimensions Target: , same shape as the input Output: scalar. If reduce is False, then , same shape as the input SoftMarginLoss class torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction='mean') Creates a criterion that optimizes a two-class classification logistic loss between input tensor x and target tensor y (containing 1 or -1). Parameters: size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: Tensor of arbitrary shape. Target: Same shape as input. Output: scalar. If reduce is False, then same shape as the input MultiLabelSoftMarginLoss class torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=None, reduce=None, reduction='mean') Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input x and target y of size (N, C). For each sample in the minibatch: where i == 0 to x.nElement()-1, y[i] in {0,1}. Parameters: weight (Tensor, optional) – a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones. size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: where N is the batch size and C is the number of classes. Target: , same shape as the input. Output: scalar. If reduce is False, then (N). CosineEmbeddingLoss class torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean') Creates a criterion that measures the loss given input tensors , and a Tensor label y with values 1 or -1. This is used for measuring whether two inputs are similar or dissimilar, using the cosine distance, and is typically used for learning nonlinear embeddings or semi-supervised learning. The loss function for each sample is: Parameters: margin (float, optional) – Should be a number from -1 to 1, 0 to 0.5 is suggested. If margin is missing, the default value is 0. size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ MultiMarginLoss class torch.nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean') Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input x (a 2D mini-batch Tensor) and output y (which is a 1D tensor of target class indices, ): For each mini-batch sample, the loss in terms of the 1D input x and scalar output y is: where i == 0 to x.size(0) and . Optionally, you can give non-equal weighting on the classes by passing a 1D weight tensor into the constructor. The loss function then becomes: Parameters: p (int, optional) – Has a default value of 1. 1 and 2 are the only supported values margin (float, optional) – Has a default value of 1. weight (Tensor, optional) – a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones. size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ TripletMarginLoss class torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean') Creates a criterion that measures the triplet loss given an input tensors x1, x2, x3 and a margin with a value greater than 0. This is used for measuring a relative similarity between samples. A triplet is composed by a, p and n: anchor, positive examples and negative example respectively. The shapes of all input tensors should be . The distance swap is described in detail in the paper Learning shallow convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al. The loss function for each sample in the mini-batch is: where Parameters: margin (float, optional) – Default: 1. p (int, optional) – The norm degree for pairwise distance. Default: 2. swap (float, optional) – The distance swap is described in detail in the paper Learning shallow convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al. Default: False. size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string__, optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the sum of the output will be divided by the number of elements in the output, ‘sum’: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: ‘mean’ Shape: Input: where D is the vector dimension. Output: scalar. If reduce is False, then (N). >>> triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2) >>> input1 = torch.randn(100, 128, requires_grad=True) >>> input2 = torch.randn(100, 128, requires_grad=True) >>> input3 = torch.randn(100, 128, requires_grad=True) >>> output = triplet_loss(input1, input2, input3) >>> output.backward() Vision layers PixelShuffle class torch.nn.PixelShuffle(upscale_factor) Rearranges elements in a tensor of shape to a tensor of shape . This is useful for implementing efficient sub-pixel convolution with a stride of . Look at the paper: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network by Shi et. al (2016) for more details. Parameters: upscale_factor (int) – factor to increase spatial resolution by Shape: Input: Output: Examples: >>> pixel_shuffle = nn.PixelShuffle(3) >>> input = torch.randn(1, 9, 4, 4) >>> output = pixel_shuffle(input) >>> print(output.size()) torch.Size([1, 1, 12, 12]) Upsample class torch.nn.Upsample(size=None, scale_factor=None, mode='nearest', align_corners=None) Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. The input data is assumed to be of the form minibatch x channels x [optional depth] x [optional height] x width. Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor. The algorithms available for upsampling are nearest neighbor and linear, bilinear and trilinear for 3D, 4D and 5D input Tensor, respectively. One can either give a scale_factor or the target output size to calculate the output size. (You cannot give both, as it is ambiguous) Parameters: size (tuple, optional) – a tuple of ints ([optional D_out], [optional H_out], W_out) output sizes scale_factor (int / tuple of python:ints__, optional) – the multiplier for the image height / width / depth mode (string__, optional) – the upsampling algorithm: one of nearest, linear, bilinear and trilinear. Default: nearest align_corners (bool, optional) – if True, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when mode is linear, bilinear, or trilinear. Default: False Shape: Input: , or Output: , or , where Warning With align_corners = True, the linearly interpolating modes (linear, bilinear, and trilinear) don’t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See below for concrete examples on how this affects the outputs. Note If you want downsampling/general resizing, you should use interpolate(). Examples: >>> input = torch.arange(1, 5).view(1, 1, 2, 2).float() >>> input tensor([[[[ 1., 2.], [ 3., 4.]]]]) >>> m = nn.Upsample(scale_factor=2, mode='nearest') >>> m(input) tensor([[[[ 1., 1., 2., 2.], [ 1., 1., 2., 2.], [ 3., 3., 4., 4.], [ 3., 3., 4., 4.]]]]) >>> m = nn.Upsample(scale_factor=2, mode='bilinear') # align_corners=False >>> m(input) tensor([[[[ 1.0000, 1.2500, 1.7500, 2.0000], [ 1.5000, 1.7500, 2.2500, 2.5000], [ 2.5000, 2.7500, 3.2500, 3.5000], [ 3.0000, 3.2500, 3.7500, 4.0000]]]]) >>> m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) >>> m(input) tensor([[[[ 1.0000, 1.3333, 1.6667, 2.0000], [ 1.6667, 2.0000, 2.3333, 2.6667], [ 2.3333, 2.6667, 3.0000, 3.3333], [ 3.0000, 3.3333, 3.6667, 4.0000]]]]) >>> # Try scaling the same data in a larger tensor >>> >>> input_3x3 = torch.zeros(3, 3).view(1, 1, 3, 3) >>> input_3x3[:, :, :2, :2].copy_(input) tensor([[[[ 1., 2.], [ 3., 4.]]]]) >>> input_3x3 tensor([[[[ 1., 2., 0.], [ 3., 4., 0.], [ 0., 0., 0.]]]]) >>> m = nn.Upsample(scale_factor=2, mode='bilinear') # align_corners=False >>> # Notice that values in top left corner are the same with the small input (except at boundary) >>> m(input_3x3) tensor([[[[ 1.0000, 1.2500, 1.7500, 1.5000, 0.5000, 0.0000], [ 1.5000, 1.7500, 2.2500, 1.8750, 0.6250, 0.0000], [ 2.5000, 2.7500, 3.2500, 2.6250, 0.8750, 0.0000], [ 2.2500, 2.4375, 2.8125, 2.2500, 0.7500, 0.0000], [ 0.7500, 0.8125, 0.9375, 0.7500, 0.2500, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]]) >>> m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) >>> # Notice that values in top left corner are now changed >>> m(input_3x3) tensor([[[[ 1.0000, 1.4000, 1.8000, 1.6000, 0.8000, 0.0000], [ 1.8000, 2.2000, 2.6000, 2.2400, 1.1200, 0.0000], [ 2.6000, 3.0000, 3.4000, 2.8800, 1.4400, 0.0000], [ 2.4000, 2.7200, 3.0400, 2.5600, 1.2800, 0.0000], [ 1.2000, 1.3600, 1.5200, 1.2800, 0.6400, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]]) UpsamplingNearest2d class torch.nn.UpsamplingNearest2d(size=None, scale_factor=None) Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. To specify the scale, it takes either the size or the scale_factor as it’s constructor argument. When size is given, it is the output size of the image (h, w). Parameters: size (tuple, optional) – a tuple of ints (H_out, W_out) output sizes scale_factor (int, optional) – the multiplier for the image height or width Warning This class is deprecated in favor of interpolate(). Shape: Input: Output: where Examples: >>> input = torch.arange(1, 5).view(1, 1, 2, 2) >>> input tensor([[[[ 1., 2.], [ 3., 4.]]]]) >>> m = nn.UpsamplingNearest2d(scale_factor=2) >>> m(input) tensor([[[[ 1., 1., 2., 2.], [ 1., 1., 2., 2.], [ 3., 3., 4., 4.], [ 3., 3., 4., 4.]]]]) UpsamplingBilinear2d class torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None) Applies a 2D bilinear upsampling to an input signal composed of several input channels. To specify the scale, it takes either the size or the scale_factor as it’s constructor argument. When size is given, it is the output size of the image (h, w). Parameters: size (tuple, optional) – a tuple of ints (H_out, W_out) output sizes scale_factor (int, optional) – the multiplier for the image height or width Warning This class is deprecated in favor of interpolate(). It is equivalent to nn.functional.interpolate(..., mode='bilinear', align_corners=True). Shape: Input: Output: where Examples: >>> input = torch.arange(1, 5).view(1, 1, 2, 2) >>> input tensor([[[[ 1., 2.], [ 3., 4.]]]]) >>> m = nn.UpsamplingBilinear2d(scale_factor=2) >>> m(input) tensor([[[[ 1.0000, 1.3333, 1.6667, 2.0000], [ 1.6667, 2.0000, 2.3333, 2.6667], [ 2.3333, 2.6667, 3.0000, 3.3333], [ 3.0000, 3.3333, 3.6667, 4.0000]]]]) DataParallel layers (multi-GPU, distributed) DataParallel class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0) Implements data parallelism at the module level. This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension (other objects will be copied once per device). In the forward pass, the module is replicated on each device, and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module. The batch size should be larger than the number of GPUs used. See also: Use nn.DataParallel instead of multiprocessing Arbitrary positional and keyword inputs are allowed to be passed into DataParallel EXCEPT Tensors. All tensors will be scattered on dim specified (default 0). Primitive types will be broadcasted, but all other types will be a shallow copy and can be corrupted if written to in the model’s forward pass. The parallelized module must have its parameters and buffers on device_ids[0] before running this DataParallel module. Warning In each forward, module is replicated on each device, so any updates to the runing module in forward will be lost. For example, if module has a counter attribute that is incremented in each forward, it will always stay at the initial value becasue the update is done on the replicas which are destroyed after forward. However, DataParallel guarantees that the replica on device[0] will have its parameters and buffers sharing storage with the base parallelized module. So in-place updates to the parameters or buffers on device[0] will be recorded. E.g., BatchNorm2d and spectral_norm() rely on this behavior to update the buffers. Warning Forward and backward hooks defined on module and its submodules will be invoked len(device_ids) times, each with inputs located on a particular device. Particularly, the hooks are only guaranteed to be executed in correct order with respect to operations on corresponding devices. For example, it is not guaranteed that hooks set via register_forward_pre_hook() be executed before all len(device_ids) forward() calls, but that each such hook be executed before the corresponding forward() call of that device. Warning When module returns a scalar (i.e., 0-dimensional tensor) in forward(), this wrapper will return a vector of length equal to number of devices used in data parallelism, containing the result from each device. Note There is a subtlety in using the pack sequence -&gt; recurrent network -&gt; unpack sequence pattern in a Module wrapped in DataParallel. See My recurrent network doesn’t work with data parallelism section in FAQ for details. Parameters: module (Module) – module to be parallelized device_ids (list of python:int or torch.device) – CUDA devices (default: all devices) output_device (int or torch.device) – device location of output (default: device_ids[0]) Variables: module (Module) – the module to be parallelized Example: >>> net = torch.nn.DataParallel(model, device_ids=[0, 1, 2]) >>> output = net(input_var) DistributedDataParallel class torch.nn.parallel.DistributedDataParallel(module, device_ids=None, output_device=None, dim=0, broadcast_buffers=True, process_group=None, bucket_cap_mb=25, check_reduction=False) Implements distributed data parallelism that is based on torch.distributed package at the module level. This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine and each device, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged. The batch size should be larger than the number of GPUs used locally. It should also be an integer multiple of the number of GPUs so that each chunk is the same size (so that each GPU processes the same number of samples). See also: Basics and Use nn.DataParallel instead of multiprocessing. The same constraints on input as in torch.nn.DataParallel apply. Creation of this class requires that torch.distributed to be already initialized, by calling torch.distributed.init_process_group() DistributedDataParallel can be used in the following two ways: Single-Process Multi-GPU In this case, a single process will be spawned on each host/node and each process will operate on all the GPUs of the node where it’s running. To use DistributedDataParallel in this way, you can simply construct the model as the following: >>> torch.distributed.init_process_group(backend=\"nccl\") >>> model = DistributedDataParallel(model) # device_ids will include all GPU devices be default Multi-Process Single-GPU This is the highly recommended way to use DistributedDataParallel, with multiple processes, each of which operates on a single GPU. This is currently the fastest approach to do data parallel training using PyTorch and applies to both single-node(multi-GPU) and multi-node data parallel training. It is proven to be significantly faster than torch.nn.DataParallel for single-node multi-GPU data parallel training. Here is how to use it: on each host with N GPUs, you should spawn up N processes, while ensuring that each process invidually works on a single GPU from 0 to N-1. Therefore, it is your job to ensure that your training script operates on a single given GPU by calling: >>> torch.cuda.set_device(i) where i is from 0 to N-1. In each process, you should refer the following to construct this module: >>> torch.distributed.init_process_group(backend='nccl', world_size=4, init_method='...') >>> model = DistributedDataParallel(model, device_ids=[i], output_device=i) In order to spawn up multiple processes per node, you can use either torch.distributed.launch or torch.multiprocessing.spawn Note nccl backend is currently the fastest and highly recommended backend to be used with Multi-Process Single-GPU distributed training and this applies to both single-node and multi-node distributed training Warning This module works only with the gloo and nccl backends. Warning Constructor, forward method, and differentiation of the output (or a function of the output of this module) is a distributed synchronization point. Take that into account in case different processes might be executing different code. Warning This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Same applies to buffers. Warning This module assumes all parameters are registered in the model of each distributed processes are in the same order. The module itself will conduct gradient all-reduction following the reverse order of the registered parameters of the model. In other wise, it is users’ responsibility to ensure that each distributed process has the exact same model and thus the exact parameter registeration order. Warning This module assumes all buffers and gradients are dense. Warning This module doesn’t work with torch.autograd.grad() (i.e. it will only work if gradients are to be accumulated in .grad attributes of parameters). Warning If you plan on using this module with a nccl backend or a gloo backend (that uses Infiniband), together with a DataLoader that uses multiple workers, please change the multiprocessing start method to forkserver (Python 3 only) or spawn. Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will likely experience deadlocks if you don’t change this setting. Warning Forward and backward hooks defined on module and its submodules won’t be invoked anymore, unless the hooks are initialized in the forward() method. Warning You should never try to change your model’s parameters after wrapping up your model with DistributedDataParallel. In other words, when wrapping up your model with DistributedDataParallel, the constructor of DistributedDataParallel will register the additional gradient reduction functions on all the parameters of the model itself at the time of construction. If you change the model’s parameters after the DistributedDataParallel construction, this is not supported and unexpected behaviors can happen, since some parameters’ gradient reduction functions might not get called. Note Parameters are never broadcast between processes. The module performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all processes in the same way. Buffers (e.g. BatchNorm stats) are broadcast from the module in process of rank 0, to all other replicas in the system in every iteration. Parameters: module (Module) – module to be parallelized device_ids (list of python:int or torch.device) – CUDA devices (default: all devices) output_device (int or torch.device) – device location of output (default: device_ids[0]) broadcast_buffers (bool) – flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function. (default: True) process_group – the process group to be used for distributed data all-reduction. If None, the default process group, which is created by torch.distributed.init_process_group, will be used. (default: None) bucket_cap_mb – DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB) (default: 25) check_reduction – when setting to True, it enables DistributedDataParallel to automatically check if the previous iteration’s backward reductions were successfully issued at the beginning of every iteration’s forward function. You normally don’t need this option enabled unless you are observing weird behaviors such as different ranks are getting different gradients, which should not happen if DistributedDataParallel is corrected used. (default: False) Variables: module (Module) – the module to be parallelized Example:: >>> torch.distributed.init_process_group(backend='nccl', world_size=4, init_method='...') >>> net = torch.nn.DistributedDataParallel(model, pg) DistributedDataParallelCPU class torch.nn.parallel.DistributedDataParallelCPU(module) Implements distributed data parallelism for CPU at the module level. This module supports the mpi and gloo backends. This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged. This module could be used in conjunction with the DistributedSampler, (see :class torch.utils.data.distributed.DistributedSampler) which will load a subset of the original datset for each node with the same batch size. So strong scaling should be configured like this: n = 1, batch size = 12 n = 2, batch size = 64 n = 4, batch size = 32 n = 8, batch size = 16 Creation of this class requires the distributed package to be already initialized in the process group mode (see torch.distributed.init_process_group()). Warning Constructor, forward method, and differentiation of the output (or a function of the output of this module) is a distributed synchronization point. Take that into account in case different node might be executing different code. Warning This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Warning This module assumes all gradients are dense. Warning This module doesn’t work with torch.autograd.grad() (i.e. it will only work if gradients are to be accumulated in .grad attributes of parameters). Warning Forward and backward hooks defined on module and its submodules won’t be invoked anymore, unless the hooks are initialized in the forward() method. Note Parameters are broadcast between nodes in the init() function. The module performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all nodes in the same way. Parameters: module – module to be parallelized Example: >>> torch.distributed.init_process_group(world_size=4, init_method='...') >>> net = torch.nn.DistributedDataParallelCPU(model) Utilities clipgrad_norm torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2) Clips gradient norm of an iterable of parameters. The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place. Parameters: parameters (Iterable__[Tensor] or Tensor) – an iterable of Tensors or a single Tensor that will have gradients normalized max_norm (float or int) – max norm of the gradients norm_type (float or int) – type of the used p-norm. Can be 'inf' for infinity norm. Returns: Total norm of the parameters (viewed as a single vector). clipgrad_value torch.nn.utils.clip_grad_value_(parameters, clip_value) Clips gradient of an iterable of parameters at specified value. Gradients are modified in-place. Parameters: parameters (Iterable__[Tensor] or Tensor) – an iterable of Tensors or a single Tensor that will have gradients normalized clip_value (float or int) – maximum allowed value of the gradients The gradients are clipped in the range [-clip_value, clip_value] parameters_to_vector torch.nn.utils.parameters_to_vector(parameters) Convert parameters to one vector Parameters: parameters (Iterable__[Tensor]) – an iterator of Tensors that are the parameters of a model. Returns: The parameters represented by a single vector --- --- vector_to_parameters torch.nn.utils.vector_to_parameters(vec, parameters) Convert one vector to the parameters Parameters: vec (Tensor) – a single vector represents the parameters of a model. parameters (Iterable__[Tensor]) – an iterator of Tensors that are the parameters of a model. weight_norm torch.nn.utils.weight_norm(module, name='weight', dim=0) Applies weight normalization to a parameter in the given module. Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This replaces the parameter specified by name (e.g. “weight”) with two parameters: one specifying the magnitude (e.g. “weight_g”) and one specifying the direction (e.g. “weight_v”). Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every forward() call. By default, with dim=0, the norm is computed independently per output channel/plane. To compute a norm over the entire weight tensor, use dim=None. See https://arxiv.org/abs/1602.07868 Parameters: module (nn.Module) – containing module name (str, optional) – name of weight parameter dim (int, optional) – dimension over which to compute the norm Returns: The original module with the weight norm hook Example: >>> m = weight_norm(nn.Linear(20, 40), name='weight') Linear (20 -> 40) >>> m.weight_g.size() torch.Size([40, 1]) >>> m.weight_v.size() torch.Size([40, 20]) remove_weight_norm torch.nn.utils.remove_weight_norm(module, name='weight') Removes the weight normalization reparameterization from a module. Parameters: module (nn.Module) – containing module name (str, optional) – name of weight parameter Example >>> m = weight_norm(nn.Linear(20, 40)) >>> remove_weight_norm(m) spectral_norm torch.nn.utils.spectral_norm(module, name='weight', n_power_iterations=1, eps=1e-12, dim=None) Applies spectral normalization to a parameter in the given module. Spectral normalization stabilizes the training of discriminators (critics) in Generaive Adversarial Networks (GANs) by rescaling the weight tensor with spectral norm of the weight matrix calculated using power iteration method. If the dimension of the weight tensor is greater than 2, it is reshaped to 2D in power iteration method to get spectral norm. This is implemented via a hook that calculates spectral norm and rescales weight before every forward() call. See Spectral Normalization for Generative Adversarial Networks . Parameters: module (nn.Module) – containing module name (str, optional) – name of weight parameter n_power_iterations (int, optional) – number of power iterations to calculate spectal norm eps (float, optional) – epsilon for numerical stability in calculating norms dim (int, optional) – dimension corresponding to number of outputs, the default is 0, except for modules that are instances of ConvTranspose1/2/3d, when it is 1 Returns: The original module with the spectal norm hook Example: >>> m = spectral_norm(nn.Linear(20, 40)) Linear (20 -> 40) >>> m.weight_u.size() torch.Size([20]) remove_spectral_norm torch.nn.utils.remove_spectral_norm(module, name='weight') Removes the spectral normalization reparameterization from a module. Parameters: module (nn.Module) – containing module name (str, optional) – name of weight parameter Example >>> m = spectral_norm(nn.Linear(40, 10)) >>> remove_spectral_norm(m) PackedSequence torch.nn.utils.rnn.PackedSequence(data, batch_sizes=None) Holds the data and list of batch_sizes of a packed sequence. All RNN modules accept packed sequences as inputs. Note Instances of this class should never be created manually. They are meant to be instantiated by functions like pack_padded_sequence(). Batch sizes represent the number elements at each sequence step in the batch, not the varying sequence lengths passed to pack_padded_sequence(). For instance, given data abc and x the PackedSequence would contain data axbc with batch_sizes=[2,1,1]. | Variables: | data (Tensor) – Tensor containing packed sequence batch_sizes (Tensor) – Tensor of integers holding information about the batch size at each sequence step pack_padded_sequence torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False) Packs a Tensor containing padded sequences of variable length. Input can be of size T x B x * where T is the length of the longest sequence (equal to lengths[0]), B is the batch size, and * is any number of dimensions (including 0). If batch_first is True B x T x * inputs are expected. The sequences should be sorted by length in a decreasing order, i.e. input[:,0] should be the longest sequence, and input[:,B-1] the shortest one. Note This function accepts any input that has at least two dimensions. You can apply it to pack the labels, and use the output of the RNN with them to compute the loss directly. A Tensor can be retrieved from a PackedSequence object by accessing its .data attribute. Parameters: input (Tensor) – padded batch of variable length sequences. lengths (Tensor) – list of sequences lengths of each batch element. batch_first (bool, optional) – if True, the input is expected in B x T x * format. Returns: a PackedSequence object pad_packed_sequence torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0, total_length=None) Pads a packed batch of variable length sequences. It is an inverse operation to pack_padded_sequence(). The returned Tensor’s data will be of size T x B x *, where T is the length of the longest sequence and B is the batch size. If batch_first is True, the data will be transposed into B x T x * format. Batch elements will be ordered decreasingly by their length. Note total_length is useful to implement the pack sequence -&gt; recurrent network -&gt; unpack sequence pattern in a Module wrapped in DataParallel. See this FAQ section for details. Parameters: sequence (PackedSequence) – batch to pad batch_first (bool, optional) – if True, the output will be in B x T x * format. padding_value (float, optional) – values for padded elements. total_length (int, optional) – if not None, the output will be padded to have length total_length. This method will throw ValueError if total_length is less than the max sequence length in sequence. Returns: Tuple of Tensor containing the padded sequence, and a Tensor containing the list of lengths of each sequence in the batch. pad_sequence torch.nn.utils.rnn.pad_sequence(sequences, batch_first=False, padding_value=0) Pad a list of variable length Tensors with zero pad_sequence stacks a list of Tensors along a new dimension, and pads them to equal length. For example, if the input is list of sequences with size L x * and if batch_first is False, and T x B x * otherwise. B is batch size. It is equal to the number of elements in sequences. T is length of the longest sequence. L is length of the sequence. * is any number of trailing dimensions, including none. Example >>> from torch.nn.utils.rnn import pad_sequence >>> a = torch.ones(25, 300) >>> b = torch.ones(22, 300) >>> c = torch.ones(15, 300) >>> pad_sequence([a, b, c]).size() torch.Size([25, 3, 300]) Note This function returns a Tensor of size T x B x * or B x T x * where T is the length of the longest sequence. This function assumes trailing dimensions and type of all the Tensors in sequences are same. Parameters: sequences (list[Tensor]) – list of variable length sequences. batch_first (bool, optional) – output will be in B x T x * if True, or in T x B x * otherwise padding_value (float, optional) – value for padded elements. Default: 0. Returns: Tensor of size T x B x * if batch_first is False. Tensor of size B x T x * otherwise pack_sequence torch.nn.utils.rnn.pack_sequence(sequences) Packs a list of variable length Tensors sequences should be a list of Tensors of size L x *, where L is the length of a sequence and * is any number of trailing dimensions, including zero. They should be sorted in the order of decreasing length. Example >>> from torch.nn.utils.rnn import pack_sequence >>> a = torch.tensor([1,2,3]) >>> b = torch.tensor([4,5]) >>> c = torch.tensor([6]) >>> pack_sequence([a, b, c]) PackedSequence(data=tensor([ 1, 4, 6, 2, 5, 3]), batch_sizes=tensor([ 3, 2, 1])) Parameters: sequences (list[Tensor]) – A list of sequences of decreasing length. Returns: a PackedSequence object --- --- 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"nn_functional.html":{"url":"nn_functional.html","title":"torch.nn.functional","keywords":"","body":"﻿# torch.nn.functional 译者：hijkzzz 卷积函数 conv1d torch.nn.functional.conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor 对由多个输入平面组成的输入信号进行一维卷积. 有关详细信息和输出形状, 请参见Conv1d. 注意 在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置torch.backends.cudn .deterministic = True来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 Reproducibility 了解背景. 参数: input – 输入张量, 形状为 ) weight – 卷积核, 形状为 ) bias – 可选的偏置, 形状为 ). 默认值: None stride – 卷积核的步幅, 可以是单个数字或一个元素元组(sW,). 默认值: 1 padding – 在输入的两边隐式加零. 可以是单个数字或一个元素元组(padW, ). 默认值: 0 dilation – 核元素之间的空洞. 可以是单个数字或单元素元组(dW,). 默认值: 1 groups – 将输入分组, 应该可以被组的数目整除. 默认值: 1 例子: >>> filters = torch.randn(33, 16, 3) >>> inputs = torch.randn(20, 16, 50) >>> F.conv1d(inputs, filters) conv2d torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor 对由多个输入平面组成的输入图像应用二维卷积. 有关详细信息和输出形状, 请参见Conv2d. 注意 在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置torch.backends.cudn .deterministic = True来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 Reproducibility 了解背景. 参数: input – 输入张量, 形状为 ) weight – 卷积核, 形状为 ) bias – 可选的偏置, 形状为 ). 默认值: None stride – 卷积核的步幅, 可以是单个数字或一个元素元组 (sH, sW). 默认值: 1 padding – 在输入的两边隐式加零. 可以是单个数字或一个元素元组 (padH, padW). 默认值: 0 dilation – 核元素之间的空洞. 可以是单个数字或单元素元组 (dH, dW). 默认值: 1 groups – 将输入分组, 应该可以被组的数目整除. 默认值: 1 例子: >>> # With square kernels and equal stride >>> filters = torch.randn(8,4,3,3) >>> inputs = torch.randn(1,4,5,5) >>> F.conv2d(inputs, filters, padding=1) conv3d torch.nn.functional.conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor 对由多个输入平面组成的输入图像应用三维卷积. 有关详细信息和输出形状, 请参见 Conv3d. 注意 在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置torch.backends.cudn .deterministic = True来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 Reproducibility 了解背景. 参数: input – 输入张量, 形状为 ) weight – 卷积核, 形状为 ) bias – 可选的偏置, 形状为 ). 默认值: None stride – 卷积核的步幅, 可以是单个数字或一个元素元组 (sT, sH, sW). 默认值: 1 padding – 在输入的两边隐式加零. 可以是单个数字或一个元素元组 (padT, padH, padW). 默认值: 0 dilation – 核元素之间的空洞. 可以是单个数字或单元素元组 (dT, dH, dW). 默认值: 1 groups – 将输入分组, 应该可以被组的数目整除. 默认值: 1 例子: >>> filters = torch.randn(33, 16, 3, 3, 3) >>> inputs = torch.randn(20, 16, 50, 10, 20) >>> F.conv3d(inputs, filters) conv_transpose1d torch.nn.functional.conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) → Tensor 对由多个输入平面组成的输入信号应用一维转置卷积操作, 有时也称为反卷积. 有关详细信息和输出形状, 请参见 ConvTranspose1d 注意 在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置torch.backends.cudn .deterministic = True来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 Reproducibility 了解背景. 参数: input – 输入张量, 形状为 ) weight – 卷积核, 形状为 ) bias – 可选的偏置, 形状为 ). 默认值: None stride – 卷积核的步幅, 可以是单个数字或一个元素元组 (sW,). 默认值: 1 padding – 输入中的每个维度的两边都将添加零填充kernel_size - 1 - padding. 可以是单个数字或元组 (padW,). 默认值: 0 output_padding – 添加到输出形状中每个维度的一侧的额外大小. 可以是单个数字或元组 (out_padW). 默认值: 0 groups – 将输入分组, 应该可以被组的数目整除. 默认值: 1 dilation – 核元素之间的空洞. 可以是单个数字或单元素元组 (dW,). 默认值: 1 例子: >>> inputs = torch.randn(20, 16, 50) >>> weights = torch.randn(16, 33, 5) >>> F.conv_transpose1d(inputs, weights) conv_transpose2d torch.nn.functional.conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) → Tensor 对由多个输入平面组成的输入图像应用二维转置卷积操作, 有时也称为反卷积. 有关详细信息和输出形状, 请参见 ConvTranspose2d. 注意 在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置torch.backends.cudn .deterministic = True来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 Reproducibility 了解背景. 参数: input – 输入张量, 形状为 ) weight – 卷积核, 形状为 ) bias –可选的偏置, 形状为 ). 默认值: None stride – 卷积核的步幅, 可以是单个数字或一个元素元组 (sH, sW). 默认值: 1 padding – 输入中的每个维度的两边都将添加零填充kernel_size - 1 - padding. 可以是单个数字或元组 (padH, padW). 默认值: 0 output_padding – 添加到输出形状中每个维度的一侧的额外大小. 可以是单个数字或元组 (out_padH, out_padW). 默认值: 0 groups – 将输入分组, 应该可以被组的数目整除. 默认值: 1 dilation – 核元素之间的空洞. 可以是单个数字或单元素元组 (dH, dW). 默认值: 1 例子: >>> # With square kernels and equal stride >>> inputs = torch.randn(1, 4, 5, 5) >>> weights = torch.randn(4, 8, 3, 3) >>> F.conv_transpose2d(inputs, weights, padding=1) conv_transpose3d torch.nn.functional.conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) → Tensor 对由多个输入平面组成的输入图像应用一个三维转置卷积操作, 有时也称为反卷积 有关详细信息和输出形状, 请参见 ConvTranspose3d. 注意 在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置torch.backends.cudn .deterministic = True来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 Reproducibility 了解背景. 参数: input – 输入张量, 形状为 ) weight – 卷积核, 形状为 ) bias –可选的偏置, 形状为 ). 默认值: None stride – 卷积核的步幅, 可以是单个数字或一个元素元组 (sT, sH, sW). 默认值: 1 padding – 输入中的每个维度的两边都将添加零填充kernel_size - 1 - padding. 可以是单个数字或元组 (padT, padH, padW). 默认值: 0 output_padding – 添加到输出形状中每个维度的一侧的额外大小. 可以是单个数字或元组 (out_padT, out_padH, out_padW). 默认值: 0 groups – 将输入分组, 应该可以被组的数目整除. 默认值: 1 dilation – 核元素之间的空洞. 可以是单个数字或单元素元组 (dT, dH, dW). 默认值: 1 例子: >>> inputs = torch.randn(20, 16, 50, 10, 20) >>> weights = torch.randn(16, 33, 3, 3, 3) >>> F.conv_transpose3d(inputs, weights) unfold torch.nn.functional.unfold(input, kernel_size, dilation=1, padding=0, stride=1) 从批量的输入张量中提取滑动局部块. 警告 目前, 仅支持四维（4D）的输入张量(批量的类似图像的张量). 细节请参阅 torch.nn.Unfold fold torch.nn.functional.fold(input, output_size, kernel_size, dilation=1, padding=0, stride=1) 将一组滑动局部块数组合成一个大的张量. 警告 目前, 仅支持四维（4D）的输入张量(批量的类似图像的张量). 细节请参阅 torch.nn.Fold 池化函数 avg_pool1d torch.nn.functional.avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) → Tensor 对由多个输入平面组成的输入信号应用一维平均池化. 有关详细信息和输出形状, 请参见 AvgPool1d. 参数: input – 输入张量, 形状为 ) kernel_size – 窗口的大小. 可以是单个数字或元组 ) stride – 窗户的步幅. 可以是单个数字或元组 (sW,). 默认值: kernel_size padding – 在输入的两边隐式加零. 可以是单个数字或一个元素元组 (padW,). 默认值: 0 ceil_mode – 如果 True, 将用 ceil 代替 floor计算输出形状. 默认值: False count_include_pad – 如果 True, 将在平均计算中包括零填充. 默认值: True 例子: >>> # pool of square window of size=3, stride=2 >>> input = torch.tensor([[[1,2,3,4,5,6,7]]]) >>> F.avg_pool1d(input, kernel_size=3, stride=2) tensor([[[ 2., 4., 6.]]]) avg_pool2d torch.nn.functional.avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) → Tensor 在 区域应用二维平均池化, 步幅为 . 输出特征的数量等于输入平面的数量. 有关详细信息和输出形状, 请参见 AvgPool2d. 参数: input – input tensor ) kernel_size – 池化区域的大小, 可以是一个数字或者元组 ) stride – 池化步幅, 可以是一个数字或者元组 (sH, sW). 默认值: kernel_size padding – 在输入的两边隐式加零. 可以是单个数字或一个元素元组 (padH, padW). 默认值: 0 ceil_mode – 如果 True, 将用 ceil 代替 floor计算输出形状. 默认值: False count_include_pad – 如果 True, 将在平均计算中包括零填充. 默认值: True avg_pool3d torch.nn.functional.avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) → Tensor 应 区域应用三维平均池化, 步幅为 . 输出特征的数量等于 . 有关详细信息和输出形状, 请参见 AvgPool3d. 参数: input – 输入张量 ) kernel_size – 池化区域的大小, 可以是一个数字或者元组 ) stride – 池化步幅, 可以是一个数字或者元组 (sT, sH, sW). 默认值: kernel_size padding – 在输入的两边隐式加零. 可以是单个数字或一个元素元组 (padT, padH, padW), 默认值: 0 ceil_mode – 如果 True, 将用 ceil 代替 floor计算输出形状. 默认值: False count_include_pad – 如果 True, 将在平均计算中包括零填充. 默认值: True max_pool1d torch.nn.functional.max_pool1d(*args, **kwargs) 对由多个输入平面组成的输入信号应用一维最大池化. 详情见 MaxPool1d. max_pool2d torch.nn.functional.max_pool2d(*args, **kwargs) 对由多个输入平面组成的输入信号应用二维最大池化. 详情见 MaxPool2d. max_pool3d torch.nn.functional.max_pool3d(*args, **kwargs) 对由多个输入平面组成的输入信号上应用三维最大池化. 详情见 MaxPool3d. max_unpool1d torch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, padding=0, output_size=None) 计算MaxPool1d的偏逆. 请参见 MaxUnpool1d. max_unpool2d torch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, padding=0, output_size=None) 计算MaxPool2d的偏逆. 详情见 MaxUnpool2d. max_unpool3d torch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, padding=0, output_size=None) 计算的MaxPool3d偏逆. 详情见 MaxUnpool3d. lp_pool1d torch.nn.functional.lp_pool1d(input, norm_type, kernel_size, stride=None, ceil_mode=False) 在由多个输入平面组成的输入信号上应用一维幂平均池化. 如果所有输入的p次方的和为零, 梯度也为零. 详情见 LPPool1d. lp_pool2d torch.nn.functional.lp_pool2d(input, norm_type, kernel_size, stride=None, ceil_mode=False) 在由多个输入平面组成的输入信号上应用二维幂平均池化. 如果所有输入的p次方的和为零, 梯度也为零. 详情见 LPPool2d. adaptive_max_pool1d torch.nn.functional.adaptive_max_pool1d(*args, **kwargs) 在由多个输入平面组成的输入信号上应用一维自适应最大池化. 请参见 AdaptiveMaxPool1d和输出形状. 参数: output_size – 目标输出的大小(单个整数) return_indices – 是否返回池化索引. 默认值: False adaptive_max_pool2d torch.nn.functional.adaptive_max_pool2d(*args, **kwargs) 在由多个输入平面组成的输入信号上应用二维自适应最大池. 请参见 AdaptiveMaxPool2d 和输出形状. 参数: output_size – 目标输出的大小(单个整数 或者 双整数元组) return_indices – 是否返回池化索引. 默认值: False adaptive_max_pool3d torch.nn.functional.adaptive_max_pool3d(*args, **kwargs) 在由多个输入平面组成的输入信号上应用三维自适应最大池. 请参见 AdaptiveMaxPool3d和输出形状. 参数: output_size – 目标输出的大小(单个整数 或者 三整数元组) return_indices – 是否返回池化索引. 默认值: False adaptive_avg_pool1d torch.nn.functional.adaptive_avg_pool1d(input, output_size) → Tensor 在由多个输入平面组成的输入信号上应用一维自适应平均池化. 请参见 AdaptiveAvgPool1d 了解详情和输出的形状. 参数: output_size – 输出目标大小(单个整数) adaptive_avg_pool2d torch.nn.functional.adaptive_avg_pool2d(input, output_size) 在由多个输入平面组成的输入信号上应用二维自适应平均池化. 请参见 AdaptiveAvgPool2d 了解详情和输出的形状. 参数: output_size – 输出目标大小(单个整数 或者 双整数元组) adaptive_avg_pool3d torch.nn.functional.adaptive_avg_pool3d(input, output_size) 在由多个输入平面组成的输入信号上应用三维自适应平均池化. 请参见 AdaptiveAvgPool3d 了解详情和输出的形状. 参数: output_size – 输出目标大小(单个整数 或者 三整数元组) 非线性激活函数 threshold torch.nn.functional.threshold(input, threshold, value, inplace=False) 为输入元素的每个元素设置阈值. 请参见 Threshold. torch.nn.functional.threshold_(input, threshold, value) → Tensor 就地版的 threshold(). relu torch.nn.functional.relu(input, inplace=False) → Tensor 逐元素应用整流线性单元函数. 请参见 ReLU. torch.nn.functional.relu_(input) → Tensor 就地版的 relu(). hardtanh torch.nn.functional.hardtanh(input, min_val=-1., max_val=1., inplace=False) → Tensor 逐元素应用hardtanh函数. 请参见 Hardtanh. torch.nn.functional.hardtanh_(input, min_val=-1., max_val=1.) → Tensor 原地版的 hardtanh(). relu6 torch.nn.functional.relu6(input, inplace=False) → Tensor 逐元素应用函数 %20%3D%20%5Cmin(%5Cmax(0%2Cx)%2C%206)). 请参见 ReLU6. elu torch.nn.functional.elu(input, alpha=1.0, inplace=False) 逐元素应用 %20%3D%20%5Cmax(0%2Cx)%20%2B%20%5Cmin(0%2C%20%5Calpha%20*%20(%5Cexp(x)%20-%201))). 请参见 ELU. torch.nn.functional.elu_(input, alpha=1.) → Tensor 就地版的 elu(). selu torch.nn.functional.selu(input, inplace=False) → Tensor 逐元素应用 %20%3D%20scale%20%20(%5Cmax(0%2Cx)%20%2B%20%5Cmin(0%2C%20%5Calpha%20%20(%5Cexp(x)%20-%201)))), 其中 并且 . 请参见 SELU. celu torch.nn.functional.celu(input, alpha=1., inplace=False) → Tensor 逐元素应用 %20%3D%20%5Cmax(0%2Cx)%20%2B%20%5Cmin(0%2C%20%5Calpha%20*%20(%5Cexp(x%2F%5Calpha)%20-%201))). 请参见 CELU. leaky_relu torch.nn.functional.leaky_relu(input, negative_slope=0.01, inplace=False) → Tensor 逐元素应用 %20%3D%20%5Cmax(0%2C%20x)%20%2B%20%5Ctext%7Bnegative%5C_slope%7D%20*%20%5Cmin(0%2C%20x)) 请参见 LeakyReLU. torch.nn.functional.leaky_relu_(input, negative_slope=0.01) → Tensor 就地版的 leaky_relu(). prelu torch.nn.functional.prelu(input, weight) → Tensor 逐元素应用函数 %20%3D%20%5Cmax(0%2Cx)%20%2B%20%5Ctext%7Bweight%7D%20*%20%5Cmin(0%2Cx)) 其中，权重是可学习的参数. 请参见 PReLU. rrelu torch.nn.functional.rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) → Tensor 随机的 leaky ReLU. 请参见 RReLU. torch.nn.functional.rrelu_(input, lower=1./8, upper=1./3, training=False) → Tensor 就地版的 rrelu(). glu torch.nn.functional.glu(input, dim=-1) → Tensor 门控线性单元. 计算: ) 其中inpuy沿dim分成两半, 形成A和B. 见 Language Modeling with Gated Convolutional Networks. 参数: input (Tensor) – 输入张量 dim (int) – 用于分割输入的维度 logsigmoid torch.nn.functional.logsigmoid(input) → Tensor 逐元素应用 %20%3D%20%5Clog%20%5Cleft(%5Cfrac%7B1%7D%7B1%20%2B%20%5Cexp(-x_i)%7D%5Cright)) 请参见 LogSigmoid. hardshrink torch.nn.functional.hardshrink(input, lambd=0.5) → Tensor 逐元素应用hardshrink函数 请参见 Hardshrink. tanhshrink torch.nn.functional.tanhshrink(input) → Tensor 逐元素应用, %20%3D%20x%20-%20%5Ctext%7BTanh%7D(x)) 请参见 Tanhshrink. softsign torch.nn.functional.softsign(input) → Tensor 逐元素应用, the function %20%3D%20%5Cfrac%7Bx%7D%7B1%20%2B%20%7Cx%7C%7D) 请参见 Softsign. softplus torch.nn.functional.softplus(input, beta=1, threshold=20) → Tensor softmin torch.nn.functional.softmin(input, dim=None, _stacklevel=3, dtype=None) 应用 softmin 函数. 注意 %20%3D%20%5Ctext%7BSoftmax%7D(-x)). 数学公式见softmax定义 请参见 Softmin. 参数: input (Tensor) – 输入 dim (int) – 计算softmin的维度(因此dim上每个切片的和为1). dtype (torch.dtype, 可选的) – 返回tenosr的期望数据类型. 如果指定了参数, 输入张量在执行::param操作之前被转换为dtype. 这对于防止数据类型溢出非常有用. 默认值: None. softmax torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None) 应用 softmax 函数. Softmax定义为: %20%3D%20%5Cfrac%7Bexp(x_i)%7D%7B%5Csum_j%20exp(x_j)%7D) 它应用于dim上的所有切片, 并将对它们进行重新缩放, 使元素位于(0,1)范围内, 和为1. 请参见 Softmax. 参数: input (Tensor) – 输入 dim (int) – 将计算softmax的维度. dtype (torch.dtype, 可选的) – 返回tenosr的期望数据类型. :如果指定了参数, 输入张量在执行::param操作之前被转换为dtype. 这对于防止数据类型溢出非常有用. 默认值: None. 注意 这个函数不能直接处理NLLLoss, NLLLoss要求日志在Softmax和它自己之间计算. 使用log_softmax来代替(它更快，并且具有更好的数值属性). softshrink torch.nn.functional.softshrink(input, lambd=0.5) → Tensor 逐元素应用 soft shrinkage 函数 请参见 Softshrink. gumbel_softmax torch.nn.functional.gumbel_softmax(logits, tau=1.0, hard=False, eps=1e-10) 采样自Gumbel-Softmax分布, 并可选择离散化. 参数: logits – [batch_size, num_features] 非规范化对数概率 tau – 非负的对抗强度 hard – 如果 True, 返回的样本将会离散为 one-hot 向量, 但将会是可微分的，就像是在自动求导的soft样本一样 返回值: 从 Gumbel-Softmax 分布采样的 tensor, 形状为 batch_size x num_features . 如果 hard=True, 返回值是 one-hot 编码, 否则, 它们就是特征和为1的概率分布 约束: 目前仅支持二维的 logits 输入张量, 形状为 batch_size x num_features 基于 https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb , (MIT license) log_softmax torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None) 应用 softmax 和对数运算. 虽然在数学上等价于log(softmax(x)), 但分开执行这两个操作比较慢, 而且在数值上不稳定. 这个函数使用另一种公式来正确计算输出和梯度. 请参见 LogSoftmax. 参数: input (Tensor) – 输入 dim (int) – 计算log_softmax的维度. dtype (torch.dtype, 可选的) – 返回张量的期望数据类型. :如果指定了参数, 输入张量在执行::param操作之前被转换为dtype. 这对于防止数据类型溢出非常有用. 默认值: None. tanh torch.nn.functional.tanh(input) → Tensor 逐元素应用 %20%3D%20%5Ctanh(x)%20%3D%20%5Cfrac%7B%5Cexp(x)%20-%20%5Cexp(-x)%7D%7B%5Cexp(x)%20%2B%20%5Cexp(-x)%7D) 请参见 Tanh. sigmoid torch.nn.functional.sigmoid(input) → Tensor 逐元素应用函数 %20%3D%20%5Cfrac%7B1%7D%7B1%20%2B%20%5Cexp(-x)%7D) 请参见 Sigmoid. 规范化函数 batch_norm torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, bias=None, training=False, momentum=0.1, eps=1e-05) 对一批数据中的每个通道应用批量标准化. 请参见 BatchNorm1d, BatchNorm2d, BatchNorm3d. instance_norm torch.nn.functional.instance_norm(input, running_mean=None, running_var=None, weight=None, bias=None, use_input_stats=True, momentum=0.1, eps=1e-05) 对批中每个数据样本中的每个通道应用实例规范化. 请参见 InstanceNorm1d, InstanceNorm2d, InstanceNorm3d. layer_norm torch.nn.functional.layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-05) 对最后特定数量的维度应用layer规范化. 请参见 LayerNorm. local_response_norm torch.nn.functional.local_response_norm(input, size, alpha=0.0001, beta=0.75, k=1.0) 对由多个输入平面组成的输入信号进行局部响应归一化, 其中通道占据第二维. 跨通道应用标准化. 请参见 LocalResponseNorm. normalize torch.nn.functional.normalize(input, p=2, dim=1, eps=1e-12, out=None) 对指定维度执行 规范化. 对于一个尺寸为 )的输入张量, 每一 -元素向量 沿着维度 dim 被转换为 %7D.%0D%0A%0D%0A) 对于默认参数, 它使用沿维度的欧几里得范数进行标准化. 参数: input – 任意形状的输入张量 p (float) – 范数公式中的指数值. 默认值: 2 dim (int) – 进行规约的维度. 默认值: 1 eps (float) – 避免除以零的小值. 默认值: 1e-12 out (Tensor, 可选的) – 输出张量. 如果 out 被设置, 此操作不可微分. 线性函数 linear torch.nn.functional.linear(input, weight, bias=None) 对传入数据应用线性转换: . 形状: Input: ) * 表示任意数量的附加维度 Weight: ) Bias: ) Output: ) bilinear torch.nn.functional.bilinear(input1, input2, weight, bias=None) Dropout 函数 dropout torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False) 在训练过程中, 使用伯努利分布的样本, 随机地用概率p将输入张量的一些元素归零. 请参见 Dropout. 参数: p – 清零概率. 默认值: 0.5 training – 如果 True 使用 dropout. 默认值: True inplace – 如果设置为 True, 将会原地操作. 默认值: False alpha_dropout torch.nn.functional.alpha_dropout(input, p=0.5, training=False, inplace=False) 对输入应用 alpha dropout. 请参见 AlphaDropout. dropout2d torch.nn.functional.dropout2d(input, p=0.5, training=True, inplace=False) 随机归零输入张量的整个通道 (一个通道是一个二维特征图, 例如, 在批量输入中第j个通道的第i个样本是一个二维张量的输入[i,j]). 每次前向传递时, 每个信道都将被独立清零. 用概率 p 从 Bernoulli 分布采样. 请参见 Dropout2d. 参数: p – 通道清零的概率. 默认值: 0.5 training – 使用 dropout 如果设为 True. 默认值: True inplace – 如果设置为 True, 将会做原地操作. 默认值: False dropout3d torch.nn.functional.dropout3d(input, p=0.5, training=True, inplace=False) 随机归零输入张量的整个通道 (一个通道是一个三维特征图, 例如, 在批量输入中第j个通道的第i个样本是一个三维张量的输入[i,j]). 每次前向传递时, 每个信道都将被独立清零. 用概率 p 从 Bernoulli 分布采样. 请参见 Dropout3d. 参数: p – 通道清零的概率. 默认值: 0.5 training – 使用 dropout 如果设为 True. 默认值: True inplace – 如果设置为 True, 将会做原地操作. 默认值: False 稀疏函数 embedding torch.nn.functional.embedding(input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False) 一个简单的查找表, 查找固定字典中的embedding(嵌入)内容和大小. 这个模块通常用于使用索引检索单词嵌入. 模块的输入是索引列表和嵌入矩阵, 输出是相应的单词嵌入. 请参见 torch.nn.Embedding. 参数: input (LongTensor) – 包含嵌入矩阵中的索引的张量 weight (Tensor) – 嵌入矩阵的行数等于可能的最大索引数+ 1, 列数等于嵌入大小 padding_idx (int, 可选的) – 如果给定, 每当遇到索引时, 在padding_idx (初始化为零)用嵌入向量填充输出. max_norm (float, 可选的) – 如果给定, 则将范数大于max_norm的每个嵌入向量重新规范化, 得到范数max_norm. 注意:这将修改适当的weight. norm_type (float, 可选的) – 用于计算max_norm选项的p范数的p. 默认 2. scale_grad_by_freq (boolean__, 可选的) – 如果给定, 这将通过小批处理中单词频率的倒数来缩放梯度. 默认 False. sparse (bool, 可选的) – 如果值为 True, 梯度 w.r.t. weight 将会是一个稀疏 tensor. 请看 torch.nn.Embedding有关稀疏梯度的更多详细信息. 形状: Input: 包含要提取的索引的任意形状的长张量 Weight: 浮点型嵌入矩阵, 形状为 (V, embedding_dim), V = maximum index + 1 并且 embedding_dim = the embedding size Output: (*, embedding_dim), * 是输入形状 例子: >>> # a batch of 2 samples of 4 indices each >>> input = torch.tensor([[1,2,4,5],[4,3,2,9]]) >>> # an embedding matrix containing 10 tensors of size 3 >>> embedding_matrix = torch.rand(10, 3) >>> F.embedding(input, embedding_matrix) tensor([[[ 0.8490, 0.9625, 0.6753], [ 0.9666, 0.7761, 0.6108], [ 0.6246, 0.9751, 0.3618], [ 0.4161, 0.2419, 0.7383]], [[ 0.6246, 0.9751, 0.3618], [ 0.0237, 0.7794, 0.0528], [ 0.9666, 0.7761, 0.6108], [ 0.3385, 0.8612, 0.1867]]]) >>> # example with padding_idx >>> weights = torch.rand(10, 3) >>> weights[0, :].zero_() >>> embedding_matrix = weights >>> input = torch.tensor([[0,2,0,5]]) >>> F.embedding(input, embedding_matrix, padding_idx=0) tensor([[[ 0.0000, 0.0000, 0.0000], [ 0.5609, 0.5384, 0.8720], [ 0.0000, 0.0000, 0.0000], [ 0.6262, 0.2438, 0.7471]]]) embedding_bag torch.nn.functional.embedding_bag(input, weight, offsets=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, mode='mean', sparse=False) 计算嵌入bags的和、平均值或最大值, 而不实例化中间嵌入. 请参见 torch.nn.EmbeddingBag 参数: input (LongTensor) – 包含嵌入矩阵的索引的bags张量 weight (Tensor) – 嵌入矩阵的行数等于可能的最大索引数+ 1, 列数等于嵌入大小 offsets (LongTensor__, 可选的) – 仅当input为一维时使用. offsets确定输入中每个bag(序列)的起始索引位置 max_norm (float, 可选的) – 如果给定此参数, 范数大于max_norm的每个嵌入向量将被重新规格化为范数max_norm. 注意:这将就地修改weight norm_type (float, 可选的) – The p in the p-norm to compute for the max_norm option. 默认 2. scale_grad_by_freq (boolean__, 可选的) – 如果给定此参数, 这将通过小批处理中单词频率的倒数来缩放梯度. 默认值 False. 注意:当mode=\"max\"时不支持此选项. mode (string__, 可选的) – \"sum\", \"mean\" or \"max\". 指定减少bag的方法. 默认值: \"mean\" sparse (bool, 可选的) – 如果True, 梯度w.r.t.权值就是一个稀疏张量.请参见 torch.nn.Embedding 关于稀疏梯度. 注意: 此选项不支持 mode=\"max\". 形状: input (LongTensor) 和 offsets (LongTensor, 可选的) 如果 input 是二维的, 形状为 B x N,它将被视为每个固定长度N的B个bag(序列), 这将根据模式以某种方式返回B个聚合值. 在本例中, offsets被忽略, 并且要求为None 如果 input 是一维的, 形状为 N 它将被视为多个bag(序列)的串联. offsets必须是一个一维tensor, 其中包含input中每个bag的起始索引位置. 因此, 对于形状B的偏移量, 输入将被视为有B个bag. 空bags( 即, 具有0长度)将返回由0填充的向量 weight (Tensor): 模块的可学习权重, 形状 (num_embeddings x embedding_dim) output: 聚合的嵌入值, 形状 B x embedding_dim 例子: >>> # an Embedding module containing 10 tensors of size 3 >>> embedding_matrix = torch.rand(10, 3) >>> # a batch of 2 samples of 4 indices each >>> input = torch.tensor([1,2,4,5,4,3,2,9]) >>> offsets = torch.tensor([0,4]) >>> F.embedding_bag(embedding_matrix, input, offsets) tensor([[ 0.3397, 0.3552, 0.5545], [ 0.5893, 0.4386, 0.5882]]) 距离函数 pairwise_distance torch.nn.functional.pairwise_distance(x1, x2, p=2.0, eps=1e-06, keepdim=False) 请参见 torch.nn.PairwiseDistance cosine_similarity torch.nn.functional.cosine_similarity(x1, x2, dim=1, eps=1e-8) → Tensor 返回x1和x2之间的余弦相似度, 沿dim计算 %7D%0D%0A%0D%0A) 参数: x1 (Tensor) – 第一个输入. x2 (Tensor) – 第二个输入(大小和 x1 匹配). dim (int, 可选的) – 维度. 默认值: 1 eps (float, 可选的) – 非常小的值避免除以0. 默认值: 1e-8 形状: Input: ) 其中D在dim位置. Output: ) 其中1在dim位置. 例子: >>> input1 = torch.randn(100, 128) >>> input2 = torch.randn(100, 128) >>> output = F.cosine_similarity(input1, input2) >>> print(output) pdist torch.nn.functional.pdist(input, p=2) → Tensor 计算输入中每对行向量之间的p范数距离. 这与torch.norm(input[:, None] - input, dim=2, p=p)的上三角形部分（不包括对角线）相同. 如果行是连续的, 则此函数将更快 如果输入具有形状 则输出将具有形状 ). 这个函数相当于 scipy.spatial.distance.pdist(input, ‘minkowski’, p=p) 如果 ). 当 它等价于 scipy.spatial.distance.pdist(input, ‘hamming’) * M. 当 , 最相近的scipy函数是 scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max()). 参数: input – 输入张量, 形状为 . p – 计算每个向量对之间的p范数距离的p值 . 损失函数 binary_cross_entropy torch.nn.functional.binary_cross_entropy(input, target, weight=None, size_average=None, reduce=None, reduction='mean') 计算目标和输出之间二进制交叉熵的函数. 请参见 BCELoss. 参数: input – 任意形状的张量 target – 与输入形状相同的张量 weight (Tensor, 可选的) – 手动重新调整权重, 如果提供, 它重复来匹配输入张量的形状 size_average (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 批处理中的每个损失元素的平均损失. 注意, 对于某些损失, 每个样本有多个元素. 如果size_average设置为False, 则对每个小批的损失进行汇总. reduce为False时忽略. 默认值: True reduce (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 根据size_average, 对每个小批量的观察结果的损失进行平均或求和. 当reduce为False时, 返回每批元素的损失并忽略size_average. 默认值: True reduction (string__, 可选的) – 指定要应用于输出的reduction：'none'| 'mean'| 'sum'. 'none'：没有reduction, 'mean'：输出的总和将除以输出中的元素数量 'sum'：输出将被求和. 注意：size_average和reduce正在被弃用, 同时, 指定这两个args中的任何一个都将覆盖reduce. 默认值：'mean', 默认值: ‘mean’ 例子: >>> input = torch.randn((3, 2), requires_grad=True) >>> target = torch.rand((3, 2), requires_grad=False) >>> loss = F.binary_cross_entropy(F.sigmoid(input), target) >>> loss.backward() binary_cross_entropy_with_logits torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None) 计算目标和输出logits之间的二进制交叉熵的函数. 请参见 BCEWithLogitsLoss. 参数: input – 任意形状的张量 target – 与输入形状相同的张量 weight (Tensor, 可选的) – 手动重新调整权重, 如果提供, 它重复来匹配输入张量的形状 size_average (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 批处理中的每个损失元素的平均损失. 注意, 对于某些损失, 每个样本有多个元素. 如果size_average设置为False, 则对每个小批的损失进行汇总. reduce为False时忽略. 默认值: True reduce (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 根据size_average, 对每个小批量的观察结果的损失进行平均或求和. 当reduce为False时, 返回每批元素的损失并忽略size_average. 默认值: True reduction (string__, 可选的) – 指定要应用于输出的reduction：'none'| 'mean'| 'sum'. 'none'：没有reduction, 'mean'：输出的总和将除以输出中的元素数量 'sum'：输出将被求和. 注意：size_average和reduce正在被弃用, 同时, 指定这两个args中的任何一个都将覆盖reduce. 默认值：'mean', 默认值: ‘mean’ pos_weight (Tensor, 可选的) – 正例样本的权重. 必须是长度等于类数的向量. 例子: >>> input = torch.randn(3, requires_grad=True) >>> target = torch.empty(3).random_(2) >>> loss = F.binary_cross_entropy_with_logits(input, target) >>> loss.backward() poisson_nll_loss torch.nn.functional.poisson_nll_loss(input, target, log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean') 泊松负对数似然损失. 请参见 PoissonNLLLoss. 参数: input – 潜在泊松分布的期望. target – 随机抽样 ). log_input – 如果为True, 则损失计算为 %20-%20%5Ctext%7Btarget%7D%20%20%5Ctext%7Binput%7D), 如果为False, 则损失计算为 ![](http://latex.codecogs.com/gif.latex?%5Ctext%7Binput%7D%20-%20%5Ctext%7Btarget%7D%20%20%5Clog(%5Ctext%7Binput%7D%2B%5Ctext%7Beps%7D)). 默认值: True full – 是否计算全部损失, 即. 加入Stirling近似项. 默认值: False %20-%20%5Ctext%7Btarget%7D%20%2B%200.5%20%20%5Clog(2%20%20%5Cpi%20*%20%5Ctext%7Btarget%7D)). size_average (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 批处理中的每个损失元素的平均损失. 注意, 对于某些损失, 每个样本有多个元素. 如果size_average设置为False, 则对每个小批的损失进行汇总. reduce为False时忽略. 默认值: True eps (float, 可选的) – 一个小值避免求值 ) 当 log_input=False. 默认值: 1e-8 reduce (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 根据size_average, 对每个小批量的观察结果的损失进行平均或求和. 当reduce为False时, 返回每批元素的损失并忽略size_average. 默认值: True reduction (string__, 可选的) – 指定要应用于输出的reduction：'none'| 'mean'| 'sum'. 'none'：没有reduction, 'mean'：输出的总和将除以输出中的元素数量 'sum'：输出将被求和. 注意：size_average和reduce正在被弃用, 同时, 指定这两个args中的任何一个都将覆盖reduce. 默认值：'mean', 默认值: ‘mean’ cosine_embedding_loss torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') → Tensor 请参见 CosineEmbeddingLoss. cross_entropy torch.nn.functional.cross_entropy(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') 此函数结合了 log_softmax 和 nll_loss. 请参见 CrossEntropyLoss. 参数: input (Tensor) – ) 其中 C = 类别数 或者在二维损失的情况下为 ), 或者 ) 当 在k维损失的情况下 target (Tensor) – ) 其中每个值都在 范围内, 或者 ) 其中 在k维损失情况下. weight (Tensor, 可选的) – 给每个类别的手动重定权重. 如果给定, 必须是大小为C的张量 size_average (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 批处理中的每个损失元素的平均损失. 注意, 对于某些损失, 每个样本有多个元素. 如果size_average设置为False, 则对每个小批的损失进行汇总. reduce为False时忽略. 默认值: True ignore_index (int, 可选的) – 指定一个被忽略的目标值，该目标值不影响输入梯度。当 size_average 取值为 True, 损失平均在不可忽略的目标上. 默认值: -100 reduce (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 根据size_average, 对每个小批量的观察结果的损失进行平均或求和. 当reduce为False时, 返回每批元素的损失并忽略size_average. 默认值: True reduction (string__, 可选的) – 指定要应用于输出的reduction：'none'| 'mean'| 'sum'. 'none'：没有reduction, 'mean'：输出的总和将除以输出中的元素数量 'sum'：输出将被求和. 注意：size_average和reduce正在被弃用, 同时, 指定这两个args中的任何一个都将覆盖reduce. 默认值：'mean', 默认值: ‘mean’ 例子: >>> input = torch.randn(3, 5, requires_grad=True) >>> target = torch.randint(5, (3,), dtype=torch.int64) >>> loss = F.cross_entropy(input, target) >>> loss.backward() ctc_loss torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean') 联结主义时间分类损失. 请参见 CTCLoss. 注意 在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置torch.backends.cudn .deterministic = True来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 Reproducibility 了解背景. 注意 当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于Reproducibility的注释. 参数: log_probs – ) 其中 C = 字母表中包括空格在内的字符数, T = 输入长度, and N = 批次数量. 输出的对数概率(e.g. 获得于torch.nn.functional.log_softmax()). targets – ) or (sum(target_lengths)). 目标（不能为空）. 在第二种形式中，假定目标是串联的。 input_lengths – ). 输入的长度 (必须 ) target_lengths – ). 目标的长度 blank (int, 可选的) – 空白的标签. 默认 . reduction (string__, 可选的) - 指定要应用于输出的reduction：'none'| 'mean'| 'sum'. 'none'：不会应用reduce, 'mean'：输出损失将除以目标长度, 然后得到批次的平均值. 默认值：'mean' 例子: >>> log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_() >>> targets = torch.randint(1, 20, (16, 30), dtype=torch.long) >>> input_lengths = torch.full((16,), 50, dtype=torch.long) >>> target_lengths = torch.randint(10,30,(16,), dtype=torch.long) >>> loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths) >>> loss.backward() hinge_embedding_loss torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean') → Tensor 请参见 HingeEmbeddingLoss. kl_div torch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction='mean') Kullback-Leibler divergence 损失. 请参见 KLDivLoss 参数: input – 任意形状的张量 target – 和输入形状相同的张量 size_average (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 批处理中的每个损失元素的平均损失. 注意, 对于某些损失, 每个样本有多个元素. 如果size_average设置为False, 则对每个小批的损失进行汇总. reduce为False时忽略. 默认值: True reduce (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 根据size_average, 对每个小批量的观察结果的损失进行平均或求和. 当reduce为False时, 返回每批元素的损失并忽略size_average. 默认值: True reduction (string__, 可选的) – 指定要应用于输出的缩减：'none'| 'batchmean'| 'sum'| 'mean'. 'none'：不会应用reduction 'batchmean'：输出的总和将除以batchsize 'sum'：输出将被加总 'mean'：输出将除以输出中的元素数 默认值：'mean' :param 注::size average和reduce正在被弃用, 同时, 指定这两个arg中的一个将覆盖reduce. :param 注::reduce = mean不返回真实的kl散度值, 请使用:reduce = batchmean, 它符合kl的数学定义. 在下一个主要版本中, “mean”将被修改为与“batchmean”相同. l1_loss torch.nn.functional.l1_loss(input, target, size_average=None, reduce=None, reduction='mean') → Tensor 该函数取元素的绝对值差的平均值。 请参见 L1Loss. mse_loss torch.nn.functional.mse_loss(input, target, size_average=None, reduce=None, reduction='mean') → Tensor 计算元素的均方误差. 请参见 MSELoss. margin_ranking_loss torch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') → Tensor 请参见 MarginRankingLoss. multilabel_margin_loss torch.nn.functional.multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') → Tensor 请参见 MultiLabelMarginLoss. multilabel_soft_margin_loss torch.nn.functional.multilabel_soft_margin_loss(input, target, weight=None, size_average=None) → Tensor 请参见 MultiLabelSoftMarginLoss. multi_margin_loss torch.nn.functional.multi_margin_loss(input, target, p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean') multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None, reduce=None, reduction=’mean’) -> Tensor 请参见 MultiMarginLoss. nll_loss torch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') 负的对数似然函数. 请参见 NLLLoss. 参数: input – ) C = 类别的数量 或者 ) 在二维损失的情况下, 或者 ) 在K维损失的情况下. target – ) 每个值是 , 或者 ) K维损失. weight (Tensor, 可选的) – 给每个类别的手动重定权重. 如果给定, 必须是大小为C的张量 size_average (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 批处理中的每个损失元素的平均损失. 注意, 对于某些损失, 每个样本有多个元素. 如果size_average设置为False, 则对每个小批的损失进行汇总. reduce为False时忽略. 默认值: True ignore_index (int, 可选的) – 指定一个被忽略的目标值, 该值不会影响输入梯度. 当size_average为True时, 损耗在未忽略的目标上平均. 默认值: -100 reduce (bool, 可选的) – 废弃的 (见 reduction). 默认情况下, 根据size_average, 对每个小批量的观察结果的损失进行平均或求和. 当reduce为False时, 返回每批元素的损失并忽略size_average. 默认值: True reduction (string__, 可选的) – 指定要应用于输出的reduction：'none'| 'mean'| 'sum'. 'none'：没有reduction, 'mean'：输出的总和将除以输出中的元素数量 'sum'：输出将被求和. 注意：size_average和reduce正在被弃用, 同时, 指定这两个args中的任何一个都将覆盖reduce. 默认值：'mean', 默认值: ‘mean’ 例子: >>> # input is of size N x C = 3 x 5 >>> input = torch.randn(3, 5, requires_grad=True) >>> # each element in target has to have 0 >> target = torch.tensor([1, 0, 4]) >>> output = F.nll_loss(F.log_softmax(input), target) >>> output.backward() smooth_l1_loss torch.nn.functional.smooth_l1_loss(input, target, size_average=None, reduce=None, reduction='mean') 如果绝对元素误差低于1, 则使用平方项, 否则使用L1项的函数. 请参见 SmoothL1Loss. soft_margin_loss torch.nn.functional.soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') → Tensor 请参见 SoftMarginLoss. triplet_margin_loss torch.nn.functional.triplet_margin_loss(anchor, positive, negative, margin=1.0, p=2, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean') 请参见 TripletMarginLoss 视觉函数 pixel_shuffle torch.nn.functional.pixel_shuffle() 重新排列张量中的元素, 从形状 ) 到 ). 请参见 PixelShuffle. 参数: input (Tensor) – 输入张量 upscale_factor (int) – 提高空间解析度的参数 例子: >>> input = torch.randn(1, 9, 4, 4) >>> output = torch.nn.functional.pixel_shuffle(input, 3) >>> print(output.size()) torch.Size([1, 1, 12, 12]) pad torch.nn.functional.pad(input, pad, mode='constant', value=0) 用于填充张量. Pading size: 要填充的维度数为 %7D%7D%7B2%7D%5Cright%5Crfloor)填充的维度从最后一个维度开始向前移动. 例如, 填充输入tensor的最后一个维度, 所以 pad 形如 (padLeft, padRight); 填充最后 2 个维度, 使用 (padLeft, padRight, padTop, padBottom); 填充最后 3 个维度, 使用 (padLeft, padRight, padTop, padBottom, padFront, padBack). Padding mode: 请参见 torch.nn.ConstantPad2d, torch.nn.ReflectionPad2d, and torch.nn.ReplicationPad2d 有关每个填充模式如何工作的具体示例. Constant padding 已经实现于任意维度. 复制填充用于填充5D输入张量的最后3个维度, 或4D输入张量的最后2个维度, 或3D输入张量的最后一个维度. 反射填充仅用于填充4D输入张量的最后两个维度, 或者3D输入张量的最后一个维度. 注意 当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于Reproducibility的注释. 参数: input (Tensor) – N维张量 pad (tuple) – m个元素的元组, 其中 输入维数，且m是偶数 mode – ‘constant’, ‘reflect’ or ‘replicate’. 默认值: ‘constant’ value – 用“常量”填充来填充值. 默认值: 0 例子: >>> t4d = torch.empty(3, 3, 4, 2) >>> p1d = (1, 1) # pad last dim by 1 on each side >>> out = F.pad(t4d, p1d, \"constant\", 0) # effectively zero padding >>> print(out.data.size()) torch.Size([3, 3, 4, 4]) >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2) >>> out = F.pad(t4d, p2d, \"constant\", 0) >>> print(out.data.size()) torch.Size([3, 3, 8, 4]) >>> t4d = torch.empty(3, 3, 4, 2) >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3) >>> out = F.pad(t4d, p3d, \"constant\", 0) >>> print(out.data.size()) torch.Size([3, 9, 7, 3]) interpolate torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None) 向下/向上采样输入到给定的size或给定的scale_factor 由 mode 指定插值的算法. 目前支持时间, 空间和体积上采样, 即预期输入为三维、四维或五维形状. 输入维度形式: mini-batch x channels x [可选的 depth] x [可选的 height] x width. 可用于上采样的模式是: nearest, linear (仅三维), bilinear (仅四维), trilinear (仅五维), area 参数: input (Tensor) – 输入张量 size (int or Tuple__[int] or Tuple__[int, int] or Tuple__[int, int, int]) – 输出尺寸. scale_factor (float or Tuple__[float]) – 空间大小的乘数. 如果是元组, 则必须匹配输入大小. mode (string) – 上采样算法: ‘nearest’ | ‘linear’ | ‘bilinear’ | ‘trilinear’ | ‘area’. 默认值: ‘nearest’ align_corners (bool, 可选的) – 如果为True, 则输入和输出张量的角像素对齐, 从而保留这些像素的值. 仅在 mode 是 linear, bilinear, 或者 trilinear 时生效. 默认值: False 警告 align_corners = True时, 线性插值模式(linear, bilinear, and trilinear)不会按比例对齐输出和输入像素, 因此输出值可能取决于输入大小. 这是0.3.1版之前这些模式的默认行为.此后, 默认行为为align_corners = False. 有关这如何影响输出的具体示例, 请参见上例. 注意 当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于Reproducibility的注释. upsample torch.nn.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None) 将输入采样到给定size或给定的scale_factor 警告 此函数已被弃用, 取而代之的是 torch.nn.functional.interpolate(). 等价于 nn.functional.interpolate(...). 注意 当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于Reproducibility的注释. 用于上采样的算法由 mode 确定. 目前支持时间, 空间和体积上采样, 即预期输入为三维、四维或五维形状. 输入维度形式: mini-batch x channels x [可选的 depth] x [可选的 height] x width. 可用于上采样的模式是: nearest, linear (仅三维), bilinear (仅四维), trilinear (仅五维), area 参数: input (Tensor) – 输入张量 size (int or Tuple__[int] or Tuple__[int, int] or Tuple__[int, int, int]) – 输出尺寸. scale_factor (int) – 空间大小的乘数. 必须是整数. mode (string) – 上采样算法: ‘nearest’ | ‘linear’| ‘bilinear’ | ‘trilinear’. 默认值: ‘nearest’ align_corners (bool, 可选的) – 如果为True, 则输入和输出张量的角像素对齐, 从而保留这些像素的值. 仅在 mode 是 linear, bilinear, 或者 trilinear 时生效. 默认值: False 警告 align_corners = True时, 线性插值模式(linear, bilinear, and trilinear)不会按比例对齐输出和输入像素, 因此输出值可能取决于输入大小. 这是0.3.1版之前这些模式的默认行为.此后, 默认行为为align_corners = False. 有关这如何影响输出的具体示例, 请参见 Upsample upsample_nearest torch.nn.functional.upsample_nearest(input, size=None, scale_factor=None) 使用最近邻的像素值对输入进行上采样. 警告 不推荐使用此函数, 而使用 torch.nn.functional.interpolate(). 等价于h nn.functional.interpolate(..., mode='nearest'). 目前支持空间和体积上采样 (即 inputs 是 4 或者 5 维的). 参数: input (Tensor) – 输入 size (int or Tuple__[int, int] or Tuple__[int, int, int]) – 输出空间大小. scale_factor (int) – 空间大小乘法器。必须是整数。 注意 当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于Reproducibility的注释. upsample_bilinear torch.nn.functional.upsample_bilinear(input, size=None, scale_factor=None) 使用双线性上采样对输入进行上采样. 警告 不推荐使用此函数, 而使用 torch.nn.functional.interpolate(). 等价于 nn.functional.interpolate(..., mode='bilinear', align_corners=True). 期望输入是空间的 (四维). 用 upsample_trilinear 对体积 (五维) 输入. 参数: input (Tensor) – 输入 size (int or Tuple__[int, int] or Tuple__[int, int, int]) – 输出空间大小. scale_factor (int) – 空间大小乘法器。 注意 当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于Reproducibility的注释. grid_sample torch.nn.functional.grid_sample(input, grid, mode='bilinear', padding_mode='zeros') 给定input 和流场 grid, 使用 input 和 grid 中的像素位置计算output. 目前, 仅支持 spatial (四维) 和 volumetric (五维) input. 在 spatial (4四维) 的情况下, 对于 input 形如 ) 和 grid 形如 ), 输出的形状为 ). 对于每个输出位置 output[n, :, h, w], 大小为2的向量 grid[n, h, w] 指定 input 的像素位置 x 和 y, 用于插值输出值 output[n, :, h, w]. 对于 5D 的 inputs, grid[n, d, h, w] 指定 x, y, z 像素位置用于插值 output[n, :, d, h, w]. mode 参数指定 nearest or bilinear 插值方法. grid 大多数值应该处于 [-1, 1]. 这是因为像素位置由input 空间维度标准化.例如, 值 x = -1, y = -1 是 input 的左上角, 值 x = 1, y = 1 是 input 的右下角. 如果 grid 有 [-1, 1] 之外的值, 那些坐标将由 padding_mode 定义. 选项如下 padding_mode=\"zeros\": 用 0 代替边界外的值, padding_mode=\"border\": 用 border 值代替, padding_mode=\"reflection\": 对于超出边界的值, 用反射的值. 对于距离边界较远的位置, 它会一直被反射, 直到到达边界, 例如(归一化)像素位置x = -3.5被-1反射, 变成x' = 2.5, 然后被边界1反射, 变成x'' = -0.5. 注意 该功能常用于空间变换网络的构建. 注意 当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于Reproducibility的注释. 参数: input (Tensor) – 形状为 )的输入 (四维情形) 或形状为) 的输入（五维情形） grid (Tensor) – 形状为) 的流场(四维情形) 或者 ) （五维情形） mode (str) – 插值模式计算输出值'双线性' | '最接近'. 默认值: ‘bilinear’ padding_mode (str) – 外部网格值' zeros ' | ' border ' | ' reflection '的填充模式. 默认值: ‘zeros’ 返回值: 输出张量 返回类型: 输出 (Tensor) affine_grid torch.nn.functional.affine_grid(theta, size) 在给定一批仿射矩阵theta的情况下生成二维流场. 通常与grid_sample()一起使用以实现空间变换器网络. 参数: theta (Tensor) – 输入的仿射矩阵 () size (torch.Size) – 目标图像输出的大小 () 例子: torch.Size((32, 3, 24, 24)) 返回值: 输出tensor, 形状为 () 返回类型: output (Tensor) 数据并行函数 (multi-GPU, distributed) data_parallel torch.nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None) 在设备id中给定的gpu上并行计算模块(输入). 这是DataParallel模块的函数版本. 参数: module (Module) – 要并行评估的模块 inputs (tensor) – 模块的输入 device_ids (list of python:int or torch.device) – 用于复制模块的GPU id output_device (list of python:int or torch.device) – 输出的GPU位置使用 -1表示CPU. (默认值: device_ids[0]) 返回值: 一个张量, 包含位于输出设备上的模块(输入)的结果 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"nn_init.html":{"url":"nn_init.html","title":"torch.nn.init","keywords":"","body":"torch.nn.init 译者：GeneZC torch.nn.init.calculate_gain(nonlinearity, param=None) 返回给定非线性函数的推荐的增益值。对应关系如下表： 非线性函数 增益 Linear / Identity Conv{1,2,3}D Sigmoid Tanh ReLU Leaky Relu 参数： nonlinearity – 非线性函数 (nn.functional 中的名字) param – 对应非线性函数的可选参数 例子 >>> gain = nn.init.calculate_gain('leaky_relu') torch.nn.init.uniform_(tensor, a=0, b=1) 用均匀分布 ) 初始化输入 Tensor。 参数： tensor – n 维 torch.Tensor a – 均匀分布的下界 b – 均匀分布的上界 例子 >>> w = torch.empty(3, 5) >>> nn.init.uniform_(w) torch.nn.init.normal_(tensor, mean=0, std=1) 用正态分布 ) 初始化输入 Tensor。 参数： tensor – n 维 torch.Tensor mean – 正态分布的均值 std – 正态分布的标准差 例子 >>> w = torch.empty(3, 5) >>> nn.init.normal_(w) torch.nn.init.constant_(tensor, val) 用常数 初始化输入 Tensor。 参数： tensor – n 维 torch.Tensor val – 用以填入张量的常数 例子 >>> w = torch.empty(3, 5) >>> nn.init.constant_(w, 0.3) torch.nn.init.eye_(tensor) 用单位矩阵初始化 2 维输入 Tensor。 保持输入张量输入 Linear 时的独一性，并且越多越好. 参数： tensor – 2 维 torch.Tensor 例子 >>> w = torch.empty(3, 5) >>> nn.init.eye_(w) torch.nn.init.dirac_(tensor) 用狄拉克δ函数初始化 {3, 4, 5} 维输入 Tensor。 保持输入张量输入 Convolutional 时的独一性，并且越多通道越好。 参数： tensor – {3, 4, 5} 维 torch.Tensor 例子 >>> w = torch.empty(3, 16, 5, 5) >>> nn.init.dirac_(w) torch.nn.init.xavier_uniform_(tensor, gain=1) 用论文 “Understanding the difficulty of training deep feedforward neural networks” - Glorot, X. & Bengio, Y. (2010) 中提及的均匀分布初始化输入 Tensor。初始化后的张量中的值采样自 ) 且 也被称作 Glorot 初始化。 参数： tensor – n 维 torch.Tensor gain – 可选缩放因子 例子 >>> w = torch.empty(3, 5) >>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu')) torch.nn.init.xavier_normal_(tensor, gain=1) 用论文 “Understanding the difficulty of training deep feedforward neural networks” - Glorot, X. & Bengio, Y. (2010) 中提及的正态分布初始化输入 Tensor。初始化后的张量中的值采样自 ) 且 也被称作 Glorot initialization。 参数： tensor – n 维 torch.Tensor gain – 可选缩放因子 例子 >>> w = torch.empty(3, 5) >>> nn.init.xavier_normal_(w) torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu') 用论文 “Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification” - He, K. et al. (2015) 中提及的均匀分布初始化输入 Tensor。初始化后的张量中的值采样自 ) 且 %20%5Ctimes%20%5Ctext%7Bfan%5C_in%7D%7D%7D%0D%0A%0D%0A) 也被称作 He initialization。 参数： tensor – n 维 torch.Tensor a – 该层后面一层的整流函数中负的斜率 (默认为 0，此时为 Relu) mode – ‘fan_in’ (default) 或者 ‘fan_out’。使用fan_in保持weights的方差在前向传播中不变；使用fan_out保持weights的方差在反向传播中不变。 nonlinearity – 非线性函数 (nn.functional 中的名字)，推荐只使用 ‘relu’ 或 ‘leaky_relu’ (default)。 例子 >>> w = torch.empty(3, 5) >>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu') torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu') 用论文 “Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification” - He, K. et al. (2015) 中提及的正态分布初始化输入 Tensor。初始化后的张量中的值采样 ) 且 %20%5Ctimes%20%5Ctext%7Bfan%5C_in%7D%7D%7D%0D%0A%0D%0A) 也被称作 He initialization。 参数： tensor – n 维 torch.Tensor a – 该层后面一层的整流函数中负的斜率 (默认为 0，此时为 Relu) mode – ‘fan_in’ (default) 或者 ‘fan_out’。使用fan_in保持weights的方差在前向传播中不变；使用fan_out保持weights的方差在反向传播中不变。 nonlinearity – 非线性函数 (nn.functional 中的名字)，推荐只使用 ‘relu’ 或 ‘leaky_relu’ (default)。 例子 >>> w = torch.empty(3, 5) >>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu') torch.nn.init.orthogonal_(tensor, gain=1) 用论文 “Exact solutions to the nonlinear dynamics of learning in deep linear neural networks” - Saxe, A. et al. (2013) 中描述的（半）正定矩阵初始化输入 Tensor。输入张量必须至少有 2 维，如果输入张量的维度大于 2， 则对后续维度进行放平操作。 参数： tensor – n 维 torch.Tensor，且 gain – 可选缩放因子 例子 >>> w = torch.empty(3, 5) >>> nn.init.orthogonal_(w) torch.nn.init.sparse_(tensor, sparsity, std=0.01) 用论文 “Deep learning via Hessian-free optimization” - Martens, J. (2010). 提及的稀疏矩阵初始化 2 维输入 Tensor，且使用正态分布 ) 初始化非零元素。 参数： tensor – n 维 torch.Tensor sparsity – 每一行置零元素的比例 std – 初始化非零元素时使用正态分布的标准差 例子 >>> w = torch.empty(3, 5) >>> nn.init.sparse_(w, sparsity=0.1) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"optim.html":{"url":"optim.html","title":"torch.optim","keywords":"","body":"torch.optim 译者：ApacheCN 是一个实现各种优化算法的包。已经支持最常用的方法，并且界面足够通用，因此将来可以轻松集成更复杂的方法。 如何使用优化器 要使用，您必须构造一个优化器对象，该对象将保持当前状态并将根据计算的渐变更新参数。 构建它 要构造一个你必须给它一个包含参数的迭代（所有应该是Variable s）来优化。然后，您可以指定特定于优化程序的选项，例如学习率，重量衰减等。 注意 如果您需要通过.cuda()将模型移动到GPU，请在为其构建优化器之前执行此操作。 .cuda()之后的模型参数与调用之前的参数不同。 通常，在构造和使用优化程序时，应确保优化参数位于一致的位置。 例： optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9) optimizer = optim.Adam([var1, var2], lr = 0.0001) 每个参数选项 s还支持指定每个参数选项。要做到这一点，不要传递一个可迭代的Variable，而是传递一个可迭代的s。它们中的每一个都将定义一个单独的参数组，并且应包含params键，其中包含属于它的参数列表。其他键应与优化程序接受的关键字参数匹配，并将用作此组的优化选项。 Note 您仍然可以将选项作为关键字参数传递。它们将在未覆盖它们的组中用作默认值。当您只想改变单个选项，同时保持参数组之间的所有其他选项保持一致时，这非常有用。 例如，当想要指定每层学习速率时，这非常有用： optim.SGD([ {'params': model.base.parameters()}, {'params': model.classifier.parameters(), 'lr': 1e-3} ], lr=1e-2, momentum=0.9) 这意味着model.base的参数将使用1e-2的默认学习速率，model.classifier的参数将使用1e-3的学习速率，0.9的动量将用于所有参数 采取优化步骤 所有优化器都实现了一个更新参数的方法。它可以以两种方式使用： optimizer.step() 这是大多数优化器支持的简化版本。一旦使用例如计算梯度，就可以调用该函数。 backward()。 Example: for input, target in dataset: optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() optimizer.step() optimizer.step(closure) 一些优化算法，例如Conjugate Gradient和LBFGS需要多次重新评估函数，因此您必须传入一个允许它们重新计算模型的闭包。闭合应清除梯度，计算损失并返回。 Example: for input, target in dataset: def closure(): optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() return loss optimizer.step(closure) 算法 class torch.optim.Optimizer(params, defaults) 所有优化器的基类。 警告 需要将参数指定为具有在运行之间一致的确定性排序的集合。不满足这些属性的对象的示例是字典值的集合和迭代器。 参数： params （ iterable ） - s或s的可迭代。指定应优化的张量。 默认值 - （dict）：包含优化选项默认值的dict（当参数组未指定它们时使用）。 add_param_group(param_group) 将参数组添加到s param_groups。 当微调预先训练的网络时，这可以是有用的，因为冻结层可以被训练并且被添加到训练进展中。 Parameters: param_group （） - 指定应该与组一起优化的张量 优化选项。 （特异性） - load_state_dict(state_dict) 加载优化器状态。 参数： state_dict （） - 优化器状态。应该是从调用返回的对象。 state_dict() 以...格式返回优化程序的状态。 它包含两个条目： state - a dict holding current optimization state. Its content 优化器类之间有所不同。 param_groups - 包含所有参数组的dict step(closure) 执行单个优化步骤（参数更新）。 Parameters: 闭包（可调用） - 一个重新评估模型并返回损失的闭包。大多数优化器都是可选的。 zero_grad() 清除所有优化s的渐变。 class torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0) 实现Adadelta算法。 已在 ADADELTA中提出：自适应学习速率方法。 Parameters: params （ iterable ） - 可迭代参数以优化或决定参数组 rho （， 可选） - 用于计算平方梯度运行平均值的系数（默认值：0.9） eps （， 可选） - 术语加入分母以提高数值稳定性（默认值：1e-6） lr （， 可选） - 在应用于参数之前缩放增量的系数（默认值：1.0） weight_decay （， 可选） - 体重衰减（L2惩罚）（默认值：0） step(closure=None) 执行单个优化步骤。 Parameters: 关闭（可调用 ， 可选） - 一个重新评估模型并返回损失的闭包。 class torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0) 实现Adagrad算法。 已经在自适应子梯度方法中提出了在线学习和随机优化。 Parameters: params （ iterable ） - 可迭代参数以优化或决定参数组 lr （， 可选） - 学习率（默认值：1e-2） lr_decay （， 可选） - 学习率衰减（默认值：0） weight_decay （， 可选） - 体重衰减（L2惩罚）（默认值：0） step(closure=None) Performs a single optimization step. Parameters: closure (callable__, optional) – A closure that reevaluates the model and returns the loss. class torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False) 实现Adam算法。 已在 Adam中提出：随机优化方法。 Parameters: params （ iterable ） - 可迭代参数以优化或决定参数组 lr （， 可选） - 学习率（默认值：1e-3） beta （元组 [， ] __， 任选） - 用于计算运行平均值的系数渐变及其方形（默认值：（0.9,0.999）） eps （， 可选） - 术语加入分母以提高数值稳定性（默认值：1e-8） weight_decay （， 可选） - 体重衰减（L2惩罚）（默认值：0） amsgrad （布尔 ， 可选） - 是否使用该算法的AMSGrad变体关于亚当及其后的收敛（默认值：False） step(closure=None) Performs a single optimization step. Parameters: closure (callable__, optional) – A closure that reevaluates the model and returns the loss. class torch.optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08) 实现适用于稀疏张量的懒惰版Adam算法。 在此变体中，只有渐变中显示的时刻才会更新，并且只有渐变的那些部分才会应用于参数。 Parameters: params （ iterable ） - 可迭代参数以优化或决定参数组 lr （， 可选） - 学习率（默认值：1e-3） beta （元组 [， ] __， 任选） - 用于计算运行平均值的系数渐变及其方形（默认值：（0.9,0.999）） eps （， 可选） - 术语加入分母以提高数值稳定性（默认值：1e-8） step(closure=None) Performs a single optimization step. Parameters: closure (callable__, optional) – A closure that reevaluates the model and returns the loss. class torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0) 实现Adamax算法（基于无穷大规范的Adam的变体）。 It has been proposed in Adam: A Method for Stochastic Optimization. Parameters: params （ iterable ） - 可迭代参数以优化或决定参数组 lr （， 可选） - 学习率（默认值：2e-3） beta （元组 [， ] __， 任选） - 用于计算运行平均值的系数渐变和它的正方形 eps （， 可选） - 术语加入分母以提高数值稳定性（默认值：1e-8） weight_decay （， 可选） - 体重衰减（L2惩罚）（默认值：0） step(closure=None) Performs a single optimization step. Parameters: closure (callable__, optional) – A closure that reevaluates the model and returns the loss. class torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0) 实现平均随机梯度下降。 已经在中通过平均来加速随机近似。 Parameters: params （ iterable ） - 可迭代参数以优化或决定参数组 lr （， 可选） - 学习率（默认值：1e-2） lambd （， 可选） - 衰变期限（默认值：1e-4） alpha （， 可选） - eta更新的权力（默认值：0.75） t0 （， 可选） - 开始平均的点（默认值：1e6） weight_decay （， 可选） - 体重衰减（L2惩罚）（默认值：0） step(closure=None) Performs a single optimization step. Parameters: closure (callable__, optional) – A closure that reevaluates the model and returns the loss. class torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-05, tolerance_change=1e-09, history_size=100, line_search_fn=None) 实现L-BFGS算法。 Warning 此优化器不支持每个参数选项和参数组（只能有一个）。 Warning 现在所有参数都必须在一台设备上。这将在未来得到改善。 Note 这是一个内存密集型优化器（它需要额外的param_bytes * (history_size + 1)字节）。如果它不适合内存尝试减少历史记录大小，或使用不同的算法。 Parameters: lr （） - 学习率（默认值：1） max_iter （） - 每个优化步骤的最大迭代次数（默认值：20） max_eval （） - 每个优化步骤的最大函数评估数（默认值：max_iter * 1.25）。 tolerance_grad （） - 一阶最优性的终止容差（默认值：1e-5）。 tolerance_change （） - 功能值/参数更改的终止容差（默认值：1e-9）。 history_size （） - 更新历史记录大小（默认值：100）。 step(closure) Performs a single optimization step. Parameters: 闭包（可调用） - 一个重新评估模型并返回损失的闭包。 class torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False) 实现RMSprop算法。 G. Hinton在他的课程中提出的建议。 中心版本首先出现在生成具有回归神经网络的序列中。 Parameters: params （ iterable ） - 可迭代参数以优化或决定参数组 lr （， 可选） - 学习率（默认值：1e-2） 动量（， 可选） - 动量因子（默认值：0） alpha （， 可选） - 平滑常数（默认值：0.99） eps （， 可选） - 术语加入分母以提高数值稳定性（默认值：1e-8） 居中（， 可选） - 如果True计算居中的RMSProp，则通过估计其方差对梯度进行归一化 weight_decay （， 可选） - 体重衰减（L2惩罚）（默认值：0） step(closure=None) Performs a single optimization step. Parameters: closure (callable__, optional) – A closure that reevaluates the model and returns the loss. class torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50)) 实现弹性反向传播算法。 Parameters: params （ iterable ） - 可迭代参数以优化或决定参数组 lr （， 可选） - 学习率（默认值：1e-2） etas （ Tuple [， ] __， 任选） - 对（etaminus，etaplis） ，这是乘法增加和减少因子（默认值：（0.5,1.2）） step_sizes （ Tuple [， ] __， 任选） - 一对最小和最大允许步长（默认值：（1e-6,50）） step(closure=None) Performs a single optimization step. Parameters: closure (callable__, optional) – A closure that reevaluates the model and returns the loss. class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False) 实现随机梯度下降（可选择带动量）。 Nesterov动量是基于关于初始化和动量在深度学习中的重要性的公式。 Parameters: params （ iterable ） - 可迭代参数以优化或决定参数组 lr （） - 学习率 动量（， 可选） - 动量因子（默认值：0） weight_decay （， 可选） - 体重衰减（L2惩罚）（默认值：0） 阻尼（， 可选） - 抑制动量（默认值：0） nesterov （， 可选） - 启用Nesterov动量（默认值：False） 例 >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9) >>> optimizer.zero_grad() >>> loss_fn(model(input), target).backward() >>> optimizer.step() Note 使用Momentum / Nesterov实施SGD与Sutskever等有所不同。人。和其他一些框架中的实现。 考虑到Momentum的具体情况，更新可以写成 其中p，g，v分别表示参数，梯度，速度和动量。 这与Sutskever等人形成鲜明对比。人。和其他采用表格更新的框架 Nesterov版本经过类似修改。 step(closure=None) Performs a single optimization step. Parameters: closure (callable__, optional) – A closure that reevaluates the model and returns the loss. 如何调整学习率 torch.optim.lr_scheduler提供了几种根据时期数调整学习率的方法。允许基于一些验证测量来降低动态学习速率。 class torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1) 将每个参数组的学习速率设置为给定函数的初始lr倍。当last_epoch = -1时，将初始lr设置为lr。 Parameters: 优化器（） - 包装优化器。 lr_lambda （函数 或） - 一个函数，它计算给定整数参数时期的乘法因子，或这些函数的列表，优化器中每个组一个.param_groups。 last_epoch （） - 最后一个纪元的索引。默认值：-1。 Example >>> # Assuming optimizer has two groups. >>> lambda1 = lambda epoch: epoch // 30 >>> lambda2 = lambda epoch: 0.95 ** epoch >>> scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2]) >>> for epoch in range(100): >>> scheduler.step() >>> train(...) >>> validate(...) load_state_dict(state_dict) 加载调度程序状态。 Parameters: state_dict （） - 调度程序状态。应该是从调用返回的对象。 state_dict() 将调度程序的状态作为a返回。 它包含自我中每个变量的条目。 dict 不是优化器。学习率lambda函数只有在它们是可调用对象时才会被保存，而不是它们是函数或lambdas。 class torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1) 将每个参数组的学习速率设置为每个step_size epochs由gamma衰减的初始lr。当last_epoch = -1时，将初始lr设置为lr。 Parameters: 优化器（） - 包装优化器。 step_size （） - 学习率衰减的时期。 gamma （） - 学习率衰减的乘法因子。默认值：0.1。 last_epoch （） - 最后一个纪元的索引。默认值：-1。 Example >>> # Assuming optimizer uses lr = 0.05 for all groups >>> # lr = 0.05 if epoch >> # lr = 0.005 if 30 >> # lr = 0.0005 if 60 >> # ... >>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1) >>> for epoch in range(100): >>> scheduler.step() >>> train(...) >>> validate(...) class torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1) 一旦纪元数达到其中一个里程碑，将每个参数组的学习速率设置为由伽玛衰减的初始lr。当last_epoch = -1时，将初始lr设置为lr。 Parameters: 优化器（） - 包装优化器。 里程碑（） - 时代指数列表。必须增加。 gamma （） - 学习率衰减的乘法因子。默认值：0.1。 last_epoch （） - 最后一个纪元的索引。默认值：-1。 Example >>> # Assuming optimizer uses lr = 0.05 for all groups >>> # lr = 0.05 if epoch >> # lr = 0.005 if 30 >> # lr = 0.0005 if epoch >= 80 >>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1) >>> for epoch in range(100): >>> scheduler.step() >>> train(...) >>> validate(...) class torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1) 将每个参数组的学习率设置为每个时期由伽玛衰减的初始lr。当last_epoch = -1时，将初始lr设置为lr。 Parameters: 优化器（） - 包装优化器。 gamma （） - 学习率衰减的乘法因子。 last_epoch （） - 最后一个纪元的索引。默认值：-1。 class torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1) 使用余弦退火计划设置每个参数组的学习速率，其中设置为初始lr，并且是自SGDR上次重启以来的纪元数： 当last_epoch = -1时，将初始lr设置为lr。 已在 SGDR中提出：具有暖启动的随机梯度下降。请注意，这仅实现SGDR的余弦退火部分，而不是重启。 Parameters: 优化器（） - 包装优化器。 T_max （） - 最大迭代次数。 eta_min （） - 最低学习率。默认值：0。 last_epoch （） - 最后一个纪元的索引。默认值：-1。 class torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08) 当指标停止改进时降低学习率。一旦学习停滞，模型通常会将学习率降低2-10倍。该调度程序读取度量数量，如果“耐心”数量的时期没有看到改善，则学习速率降低。 Parameters: 优化器（） - 包装优化器。 模式（） - min，max之一。在min模式下，当监控量停止下降时，lr将减少;在max模式下，当监控量停止增加时，它将减少。默认值：'min'。 factor （） - 学习率降低的因素。 new_lr = lr * factor。默认值：0.1。 耐心（） - 没有改善的时期数，之后学习率会降低。例如，如果patience = 2，那么我们将忽略没有改进的前2个时期，并且如果损失仍然没有改善那么将仅在第3个时期之后减少LR。默认值：10。 verbose （） - 如果True，每次更新都会向stdout输出一条消息。默认值：False。 阈值（） - 测量新最佳值的阈值，仅关注重大变化。默认值：1e-4。 threshold_mode （） - rel，abs之一。在rel模式下，dynamic_threshold ='max'模式下的最佳（1 +阈值）或min模式下的最佳（1 - 阈值）。在abs模式下，dynamic_threshold = max模式下的最佳+阈值或min模式下的最佳阈值。默认值：'rel'。 冷却时间（） - 在减少lr之后恢复正常操作之前要等待的时期数。默认值：0。 min_lr （或） - 标量或标量列表。所有参数组或每组的学习率的下限。默认值：0。 eps （） - 应用于lr的最小衰减。如果新旧lr之间的差异小于eps，则忽略更新。默认值：1e-8。 Example >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9) >>> scheduler = ReduceLROnPlateau(optimizer, 'min') >>> for epoch in range(10): >>> train(...) >>> val_loss = validate(...) >>> # Note that step should be called after validate() >>> scheduler.step(val_loss) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"autograd.html":{"url":"autograd.html","title":"Automatic differentiation package - torch.autograd","keywords":"","body":"Automatic differentiation package - torch.autograd 译者：gfjiangly torch.autograd 提供类和函数，实现任意标量值函数的自动微分。 它要求对已有代码的最小改变---你仅需要用requires_grad=True关键字为需要计算梯度的声明Tensor。 torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None) 计算被给张量关于图的叶节点的梯度和。 图使用链式法则微分。如何任何tensors是非标量（例如他们的数据不止一个元素）并且要求梯度，函数要额外指出grad_tensors。它应是一个匹配长度的序列，包含可微函数关于相应张量的梯度（None是一个对所有张量可接受的值，不需要梯度张量）。 此函数在叶节点累积梯度 - 你可能需要在调用前把它初始化为0. 参数： tensors (Tensor序列) – 计算导数的张量。 grad_tensors ([_Tensor](tensors.html#torch.Tensor \"torch.Tensor\") 或 None序列_) – 关于相应张量每个元素的梯度。标量张量或不需要梯度的可用None指定。如果None对所有grad_tensors可接受，则此参数可选。 retain_graph (bool, 可选) – 如果False，用于计算梯度的图将被释放。请注意，在几乎所有情况下，不需要将此选项设置为真，而且通常可以更有效地解决问题。默认为create_graph值。 create_graph (bool, 可选) – 如果True，则构造导数图，以便计算更高阶导数，默认False。 torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False) 计算和返回输出关于输入的梯度和。 grad_outputs 应是长度匹配输出的序列，包含关于输出每个元素的预计算梯度。如果一个输出不要求梯度，则梯度是None。 如果only_inputs是True，此函数将仅返回关于指定输入的梯度list。如果此参数是False，则关于其余全部叶子的梯度仍被计算，并且将累加到.grad属性中。 参数: outputs (Tensor序列) – 可微函数输出 inputs (Tensor序列) – 关于将返回梯度的输入(不累加到.grad)。 grad_outputs (Tensor序列) – 关于每个输入的梯度。标量张量或不需要梯度的可用None指定。如果None对所有grad_tensors可接受，则此参数可选。默认：None。 retain_graph (bool, 可选) – 如果False，用于计算梯度的图将被释放。请注意，在几乎所有情况下，不需要将此选项设置为真，而且通常可以更有效地解决问题。默认为create_graph值。 create_graph (bool, 可选) – 如果True，则构造导数图，以便计算更高阶导数，默认False。 allow_unused (bool, 可选) – 如果False, 当计算输出出错时指明不使用的输入 (因此它们的梯度一直是0)。 默认False。 局部禁用梯度计算 class torch.autograd.no_grad 禁用梯度计算的上下文管理器。 当你确认不会调用 Tensor.backward()，对于推断禁用梯度计算是有用的。它将减少计算的内存消耗，否则会有requires_grad=True。在这个模式中，每个计算结果将导致requires_grad=False, 即便输入有requires_grad=True。 函数还可作为装饰器。 示例： >>> x = torch.tensor([1], requires_grad=True) >>> with torch.no_grad(): ... y = x * 2 >>> y.requires_grad False >>> @torch.no_grad() ... def doubler(x): ... return x * 2 >>> z = doubler(x) >>> z.requires_grad False class torch.autograd.enable_grad 使能梯度计算的上下文管理器。 在一个no_grad上下文中使能梯度计算。在no_grad外部此上下文管理器无影响 函数还可作为装饰器。 示例： >>> x = torch.tensor([1], requires_grad=True) >>> with torch.no_grad(): ... with torch.enable_grad(): ... y = x * 2 >>> y.requires_grad True >>> y.backward() >>> x.grad >>> @torch.enable_grad() ... def doubler(x): ... return x * 2 >>> with torch.no_grad(): ... z = doubler(x) >>> z.requires_grad True class torch.autograd.set_grad_enabled(mode) 设置梯度计算打开或关闭的上下文管理器。 set_grad_enabled将基于它的参数mode使用或禁用梯度。它也能作为一个上下文管理器或函数使用。 参数: mode (bool) – 标记是否使能梯度（True），或使能（False）。这能被用在有条件的使能梯度。 示例： >>> x = torch.tensor([1], requires_grad=True) >>> is_train = False >>> with torch.set_grad_enabled(is_train): ... y = x * 2 >>> y.requires_grad False >>> torch.set_grad_enabled(True) >>> y = x * 2 >>> y.requires_grad True >>> torch.set_grad_enabled(False) >>> y = x * 2 >>> y.requires_grad False 关于Tensors的原位操作 在autograd中支持原位操作是一件很难的事，并且我们在大多数情况下不鼓励使用它们。Autograd积极的缓冲区释放和重用使其非常高效，实际上原位操作会大幅降低内存使用量的情况非常少。你可能永远不会使用它们，除非正在很大的内存压力下操作。 就地正确性检查 全部的Tensor保持追踪应用到它们身上的原位操作，并且如果实现检测到在任何一个函数中，一个tensor为反向传播保存，但是随后被原位修改，一旦反向传播开始将抛出一个错误。此设计确保如果你正在使用原位操作函数并且没有看到任何错误，你可以确保计算的梯度是正确的。 Variable (弃用) 警告 Variable API已经被弃用。对张量使用自动求导不再需要Variable。Autograd自动支持requires_grad参数设置成True的张量。以下是有关更改内容的快速指南： Variable(tensor) 和Variable(tensor, requires_grad)仍然和预期一样工作，但它们返回Tensors代替Variables。 var.data 和 tensor.data是一回事。 方法如var.backward(), var.detach(), var.register_hook()现在在tensors上使用相同的名字起作用。 此外，现在可以使用诸如torch.randn(), torch.zeros(), torch.ones()等工厂方法创建requires_grad=True的张量，如下所示： autograd_tensor = torch.randn((2, 3, 4), requires_grad=True) 张量自动求导函数 class torch.Tensor backward(gradient=None, retain_graph=None, create_graph=False) 计算当前张量关于图叶节点的梯度。 图使用链式反则微分。如果张量是非标量并且要求梯度，函数额外要求指梯度。它应是一个匹配类型和位置的张量，含有可微函数关于它本身的梯度。 此函数在叶节点累加梯度-你可能需要在调用前将它初始化为0。 参数： gradient (Tensor 或 None) – 关于张量的梯度。如果它是一个张量，它将被自动转换成不要求梯度的张量，除非create_graph是True。标量张量或不需要梯度的可用None指定。如果None对所有grad_tensors可接受，则此参数可选。 retain_graph (bool, 可选) – 如果False，用于计算梯度的图将被释放。请注意，在几乎所有情况下，不需要将此选项设置为真，而且通常可以更有效地解决问题。默认为create_graph值。 create_graph (bool, 可选) – 如果True，则构造导数图，以便计算更高阶导数，默认False。 detach() 返回一个新的Tensor，从当前图中分离出来。 结果不要求梯度。 注意 返回的张量与原始张量使用相同的数据。关于它们中任一个原位修改将被看见，并且可能在正确性检查中触发错误。 detach_() 从创建它的图中分离张量，使其成为叶。不能就地分离视图。 grad 此属性默认None，并且调用backward()计算自身梯度时第一时间成为一个Tensor。此属性将含计算的梯度，以后调用backward()将累加提到到自身。 is_leaf 按惯例，所有requires_grad=False的张量将是叶节点张量 如果张量是由用户创建，requires_grad的张量也是叶节点张量。这意味着它们不是一个操作的结果，并且grad_fn是None。 仅叶节点张量在调用backward()时填充它们的grad。为得到从非叶节点张量填充的梯度，你可以使用retain_grad(). 示例： >>> a = torch.rand(10, requires_grad=True) >>> a.is_leaf True >>> b = torch.rand(10, requires_grad=True).cuda() >>> b.is_leaf False # b 是由cpu Tensor投入cuda Tensor的操作创建的 >>> c = torch.rand(10, requires_grad=True) + 2 >>> c.is_leaf False # c 是由加操作创建的 >>> d = torch.rand(10).cuda() >>> d.is_leaf True # d 不要求梯度，所以没有创建它的操作 (被自动求导引擎追踪) >>> e = torch.rand(10).cuda().requires_grad_() >>> e.is_leaf True # e 要求梯度并且没有创建它的操作 >>> f = torch.rand(10, requires_grad=True, device=\"cuda\") >>> f.is_leaf True # f 要求梯度并且没有创建它的操作 register_hook(hook) 注册一个反向钩子 此钩子每次在对应张量梯度被计算时调用。此钩子应有下面鲜明特征： hook(grad) -> Tensor or None 此钩子不应该修改它的参数，但它能可选地返回一个新的用于替代 grad的梯度。 此函数返回一个句柄，其句柄方法为handle.remove()，用于从模块中删除钩子。 示例： >>> v = torch.tensor([0., 0., 0.], requires_grad=True) >>> h = v.register_hook(lambda grad: grad * 2) # double the gradient >>> v.backward(torch.tensor([1., 2., 3.])) >>> v.grad 2 4 6 [torch.FloatTensor of size (3,)] >>> h.remove() # removes the hook requires_grad 如果梯度需要为此张量计算则是True，否则为False 注意 事实是梯度需要为此张量计算不意味着grad属性将被填充，更多细节见is_leaf。 retain_grad() 为非叶节点张量使能.grad属性 Function class torch.autograd.Function 记录操作历史，定义可微操作公式。 在Tensor上执行的每个操作都会创建一个新的函数对象，执行计算，记录它发生的。历史记录以函数的DAG形式保留，DAG的边表示数据的依赖性（input &lt;- output）。然后，当backward被调用，图按拓扑顺序被处理，通过调用每个Function对象的backward()方法，并且传递梯度给下一个Function。 通常，用户与函数交互的唯一方式是通过创建子类和定义新操作。这是一种被推荐的扩展torch.autograd的方式。 每个函数对象只能使用一次（在正向传递中）。 示例： >>> class Exp(Function): >>> >>> @staticmethod >>> def forward(ctx, i): >>> result = i.exp() >>> ctx.save_for_backward(result) >>> return result >>> >>> @staticmethod >>> def backward(ctx, grad_output): >>> result, = ctx.saved_tensors >>> return grad_output * result static backward(ctx, *grad_outputs) 定义一个公式计算操作导数。 此函数被所有子类重载。 它必须接受一个上下文ctx作为第一个参数，随后是forward()返回的大量输出，并且它应返回尽可能多的张量，作为forward()函数输入。每个参数是关于被给输出的梯度，并且每个返回值是关于相应输入的梯度。 ctx上下文可用于恢复保存在前向传播过程的梯度。它有一个ctx.needs_input_grad属性，作为一个代表每个输入是否需要梯度的布尔元组。 static forward(ctx, *args, **kwargs) 执行操作。 此函数被所有子类重载。 它必须接受一个上下文ctx作为第一个参数，随后是任意数量的参数（tensor或其它类型）。 此上下文可被用来存储张量，随后可在反向传播过程取出。 数值梯度检查 torch.autograd.gradcheck(func, inputs, eps=1e-06, atol=1e-05, rtol=0.001, raise_exception=True) 通过小的有限差分与关于浮点类型且requires_grad=True的输入张量来检查计算的梯度。 在数组梯度和分析梯度之间检查使用allclose()。 注意 默认值为双精度输入设计。如果输入欠精度此检查有可能失败，例如，FloatTensor。 警告 如果在输入中任何被检查的张量有重叠的内存，换句话说，指向相同内存地址的不同切片（例如，从torch.expand()），此检查将有可能失败，因为在这个索引通过点扰动计算的数值梯度将改变在全部其它索引处共享内存地址的值。 参数 func (function) – 一个Python函数，输入是张量，返回一个张量或张量元组 inputs (张量元组 or Tensor) – func函数输入 eps (float, 可选) – 有限差分的扰动 atol (float, 可选) – 绝对容差 rtol (float, 可选) – 相对容差 raise_exception (bool, 可选) – 指示如果检查失败是否抛出一个异常。此异常给出关于失败的确切性质的更多信息。这在梯度检查调试时是有用的。 返回: 如果所有差都满足全部闭合条件，则为True torch.autograd.gradgradcheck(func, inputs, grad_outputs=None, eps=1e-06, atol=1e-05, rtol=0.001, gen_non_contig_grad_outputs=False, raise_exception=True) 通过小的有限差分与关于在输入中张量的分析梯度，检查已计算梯度的梯度，并且在requires_grad=True 情况下，grad_outputs是浮点类型。 此函数检查通过计算到给定grad_outputs的梯度的反向传播是否正确。 在数值梯度和分析梯度之间使用allclose()检查。 注意 默认值为双精度输入设计。如果输入欠精度此检查有可能失败，例如，FloatTensor。 警告 如果在输入中任何被检查的张量有重叠的内存，换句话说，指向相同内存地址的不同切片（例如，从torch.expand()），此检查将有可能失败，因为在这个索引通过点扰动计算的数值梯度将改变在全部其它索引处共享内存地址的值。 参数： func (function) – 一个Python函数，输入是张量，返回一个张量或张量元组 inputs (张量元组 or Tensor) – func函数输入 grad_outputs (tuple of Tensor or Tensor, 可选) – The gradients with respect to the function’s outputs. eps (float, 可选) – 有限差分的扰动 atol (float, 可选) – 绝对容差 rtol (float, 可选) – 相对容差 gen_non_contig_grad_outputs (bool, 可选) – 如果 grad_outputs 是 None 并且 gen_non_contig_grad_outputs 是 True，随机生成的梯度输出是不连续的 raise_exception (bool, 可选) – 指示如果检查失败是否抛出一个异常。此异常给出关于失败的确切性质的更多信息。这在梯度检查调试时是有用的。 返回: 如果所有差都满足全部闭合条件，则为True Profiler Autograd 包含一个事件探查器，让你洞察在你的模型中不同操作的代价-CPU和GPU中都有。现在有两种模式实现-CPU-仅使用profile，和使用emit_nvtx的nvprof（注册CPU和GPU活动） class torch.autograd.profiler.profile(enabled=True, use_cuda=False) 上下文管理器管理autograd事件探查器状态和保持一份汇总结果。 参数: enabled (bool, 可选) – 设置成False 让此上下文管理一个 no-op. 默认：True。 use_cuda (bool, 可选) – 使用cudaEvent API也可以启用CUDA事件的计时。 每个张量操作增加大约4us的开销。默认: False 示例： >>> x = torch.randn((1, 1), requires_grad=True) >>> with torch.autograd.profiler.profile() as prof: ... y = x ** 2 ... y.backward() >>> # 注意：为简洁起见，删除了一些列 ... print(prof) ------------------------------------- --------------- --------------- Name CPU time CUDA time ------------------------------------- --------------- --------------- PowConstant 142.036us 0.000us N5torch8autograd9GraphRootE 63.524us 0.000us PowConstantBackward 184.228us 0.000us MulConstant 50.288us 0.000us PowConstant 28.439us 0.000us Mul 20.154us 0.000us N5torch8autograd14AccumulateGradE 13.790us 0.000us N5torch8autograd5CloneE 4.088us 0.000us export_chrome_trace(path) 将EventList导出为Chrome跟踪工具文件。 检查点随后被加载和检查在chrome://tracing URL。 参数: path (str) – 将写入跟踪的路径。 key_averages() 平均键上的所有函数事件. 返回: 一个包含FunctionEventAvg对象的EventList。 table(sort_by=None) 将EventList打印为格式良好的表。 参数: sort_by (str, optional) – 用来排序事件的属性。默认以它们被注册时顺序打印。 合法的关键字包括：cpu_time, cuda_time, cpu_time_total, cuda_time_total, count。 返回: 一个包含表格的字符串。 --- --- total_average() 平均化全部事件。 返回: 一个FunctionEventAvg事件。 class torch.autograd.profiler.emit_nvtx(enabled=True) 让每个自动求导操作发出在一个NVTX范围内的上下文管理器。 当在nvprof下运行程序是有用的： nvprof --profile-from-start off -o trace_name.prof -- 不幸地，没有办法强制nvprof将它收集的数据输出到磁盘，所以对于CUDA分析，必须使用此上下文管理器来声明nvprof跟踪并等待进程在检查之前退出。然后，可使用NVIDIA可视化Profiler(nvvp)来显示时间线，或torch.autograd.profiler.load_nvprof()可加载结果以供检查，例如：在Python REPL中。 参数: enabled (bool, 可选) – 设置成False 让此上下文管理一个 no-op. 默认：True。 示例： >>> with torch.cuda.profiler.profile(): ... model(x) # Warmup CUDA memory allocator and profiler ... with torch.autograd.profiler.emit_nvtx(): ... model(x) Forward-backward correlation 在Nvidia Visual Profiler中查看使用emit_nvtx创建的配置文件时，将每个反向传递操作与相应的前向传递操作相关联可能很困难。 为了简化此任务，emit_nvtx将序列号信息附加到它生成的范围。 在前向传递过程，每个函数范围都用seq=&lt;N&gt;进行修饰。 seq是一个运行计数器，每次创建一个新的反向Function对象时会递增，并对前向不可见。 因此，与每个前向函数范围相关联的seq=&lt;N&gt;注释告诉你，如果此前向函数创建了反向的Function对象，则反向对象将收到序列号N。在反向传递过程，顶层范围包装每个C++反向函数的apply()调用都用不可见的stashed seq=&lt;M&gt;进行修饰。 M是创建反向对象的序列号。 通过比较在反向不可见的序列号和在前向的序列号，你可以跟踪哪个前向操作创建了每个反向函数。 在反向传递期间执行的任何函数也用seq=&lt;N&gt;进行修饰。 在默认反向（使用create_graph=False）时，此信息无关紧要，事实上，对于所有此类函数，N可能只是0。 作为将这些Function对象与早期的向前传递相关联的方法，只有与反向Function对象的apply()方法关联的顶级范围才有用。 Double-backward 另一方面，如果正在进行create_graph=True的反向传递（换句话说，如果你设置为double-backward），则在反向期间执行每个被给一个非零，有用的seq=&lt;N&gt;的函数。 这些函数本身可以稍后在double-backward期间创建Function对象来执行，就像在前向传递中原始函数所做的一样。 反向和double-backward的关系在概念上与前向和反向的关系相同：函数仍然发出当前序列号标记的范围，它们创建的Function对象仍然存储那些序列号，并且在最终的double-backward期间 向后，Function对象的apply()范围仍然用 stashed seq数字标记，可以与反向传递中的seq数字进行比较。 torch.autograd.profiler.load_nvprof(path) 打开一个nvprof跟踪文件并且解析autograd注释。 参数: path (str) – nvprof跟踪路径 异常检测 class torch.autograd.detect_anomaly 上下文管理器，为自动求导引擎使能异常检测。 这做了两件事：- 在启用检测的情况下运行前向传递将允许反向传递打印创建失败的反向函数的前向操作跟踪。 - 任何生成“nan”值的反向计算都会引发错误。 示例 >>> import torch >>> from torch import autograd >>> class MyFunc(autograd.Function): ... @staticmethod ... def forward(ctx, inp): ... return inp.clone() ... @staticmethod ... def backward(ctx, gO): ... # Error during the backward pass ... raise RuntimeError(\"Some error in backward\") ... return gO.clone() >>> def run_fn(a): ... out = MyFunc.apply(a) ... return out.sum() >>> inp = torch.rand(10, 10, requires_grad=True) >>> out = run_fn(inp) >>> out.backward() Traceback (most recent call last): File \"\", line 1, in File \"/your/pytorch/install/torch/tensor.py\", line 93, in backward torch.autograd.backward(self, gradient, retain_graph, create_graph) File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward allow_unreachable=True) # allow_unreachable flag File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply return self._forward_cls.backward(self, *args) File \"\", line 8, in backward RuntimeError: Some error in backward >>> with autograd.detect_anomaly(): ... inp = torch.rand(10, 10, requires_grad=True) ... out = run_fn(inp) ... out.backward() Traceback of forward call that caused the error: File \"tmp.py\", line 53, in out = run_fn(inp) File \"tmp.py\", line 44, in run_fn out = MyFunc.apply(a) Traceback (most recent call last): File \"\", line 4, in File \"/your/pytorch/install/torch/tensor.py\", line 93, in backward torch.autograd.backward(self, gradient, retain_graph, create_graph) File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward allow_unreachable=True) # allow_unreachable flag File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply return self._forward_cls.backward(self, *args) File \"\", line 8, in backward RuntimeError: Some error in backward class torch.autograd.set_detect_anomaly(mode) 上下文管理器，为自动求导引擎设置异常检测开或关。 set_detect_anomaly将基于它的参数mode使能或禁用自动求导异常检测。它也能作为一个上下文管理器或函数使用。 异常检测行为细节见上面detect_anomaly。 参数: mode (bool) – 标记是否使能异常检测（True），或禁用（False）。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed.html":{"url":"distributed.html","title":"Distributed communication package - torch.distributed","keywords":"","body":"分布式通信包 - torch.distributed 译者：univeryinli 后端 torch.distributed 支持三个后端，每个后端具有不同的功能。下表显示哪些功能可用于CPU/CUDA张量。仅当用于构建PyTorch的实现支持时，MPI才支持CUDA。 后端 gloo mpi nccl 设备 CPU GPU CPU GPU CPU GPU --- --- --- --- --- --- --- 发送 ✓ ✘ ✓ ? ✘ ✘ 接收 ✓ ✘ ✓ ? ✘ ✘ 广播 ✓ ✓ ✓ ? ✘ ✓ all_reduce ✓ ✓ ✓ ? ✘ ✓ reduce ✓ ✘ ✓ ? ✘ ✓ all_gather ✓ ✘ ✓ ? ✘ ✓ 收集 ✓ ✘ ✓ ? ✘ ✘ 分散 ✓ ✘ ✓ ? ✘ ✘ 屏障 ✓ ✘ ✓ ? ✘ ✓ PyTorch附带的后端 目前PyTorch分发版仅支持Linux。默认情况下，Gloo和NCCL后端构建并包含在PyTorch的分布之中（仅在使用CUDA构建时为NCCL）。MPI是一个可选的后端，只有从源代码构建PyTorch时才能包含它。（例如，在安装了MPI的主机上构建PyTorch） 哪个后端使用？ 在过去，我们经常被问到：“我应该使用哪个后端？”。 经验法则 使用NCCL后端进行分布式 GPU 训练。 使用Gloo后端进行分布式 CPU 训练。 具有InfiniBand互连的GPU主机 使用NCCL，因为它是目前唯一支持InfiniBand和GPUDirect的后端。 GPU主机与以太网互连 使用NCCL，因为它目前提供最佳的分布式GPU训练性能，特别是对于多进程单节点或多节点分布式训练。如果您遇到NCCL的任何问题，请使用Gloo作为后备选项。（请注意，Gloo目前运行速度比GPU的NCCL慢。） 具有InfiniBand互连的CPU主机 如果您的InfiniBand在IB上已启用IP，请使用Gloo，否则请使用MPI。我们计划在即将发布的版本中为Gloo添加InfiniBand支持。 具有以太网互连的CPU主机 除非您有特殊原因要使用MPI，否则请使用Gloo。 常见的环境变量 选择要使用的网络接口 默认情况下，NCCL和Gloo后端都会尝试查找用于通信的网络接口。但是，从我们的经验来看，并不总能保证这一点。因此，如果您在后端遇到任何问题而无法找到正确的网络接口。您可以尝试设置以下环境变量（每个变量适用于其各自的后端）： NCCL_SOCKET_IFNAME, 比如 export NCCL_SOCKET_IFNAME=eth0 GLOO_SOCKET_IFNAME, 比如 export GLOO_SOCKET_IFNAME=eth0 其他NCCL环境变量 NCCL还提供了许多用于微调目的的环境变量 常用的包括以下用于调试目的： export NCCL_DEBUG=INFO export NCCL_DEBUG_SUBSYS=ALL 有关NCCL环境变量的完整列表，请参阅NVIDIA NCCL的官方文档 基本 torch.distributed包为在一台或多台机器上运行的多个计算节点上的多进程并行性提供PyTorch支持和通信原语。类 torch.nn.parallel.DistributedDataParallel()基于此功能构建，以提供同步分布式训练作为包装器任何PyTorch模型。这与 Multiprocessing package - torch.multiprocessing 和 torch.nn.DataParallel() 因为它支持多个联网的机器，并且用户必须为每个进程显式启动主训练脚本的单独副本。 在单机同步的情况下，torch.distributed 或者 torch.nn.parallel.DistributedDataParallel() 与其他数据并行方法相比，包装器仍然具有优势，包含 torch.nn.DataParallel(): 每个进程都维护自己的优化器，并在每次迭代时执行完整的优化步骤。虽然这可能看起来是多余的，但由于梯度已经聚集在一起并且在整个过程中平均，因此对于每个过程都是相同的，这意味着不需要参数广播步骤，减少了在节点之间传输张量所花费的时间。 每个进程都包含一个独立的Python解释器，消除了额外的解释器开销和来自单个Python进程驱动多个执行线程，模型副本或GPU的“GIL-thrashing”。这对于大量使用Python运行时的模型尤其重要，包括具有循环层或许多小组件的模型。 初始化 这个包在调用其他的方法之前，需要使用 torch.distributed.init_process_group() 函数进行初始化。这将阻止所有进程加入。 torch.distributed.init_process_group(backend, init_method='env://', timeout=datetime.timedelta(seconds=1800), **kwargs) 初始化默认的分布式进程组，这也将初始化分布式程序包 参数: backend (str or Backend) – 后端使用。根据构建时配置，有效值包括 mpi，gloo和nccl。该字段应该以小写字符串形式给出(例如\"gloo\")，也可以通过Backend访问属性(例如Backend.GLOO)。 init_method (str, optional) – 指定如何初始化进程组的URL。 world_size (int, optional) – 参与作业的进程数。 rank (int, optional) – 当前流程的排名。 timeout (timedelta__, optional) – 针对进程组执行的操作超时，默认值等于30分钟，这仅适用于gloo后端。 group_name (str, optional__, deprecated) – 团队名字。 要启用backend == Backend.MPI，PyTorch需要在支持MPI的系统上从源构建，这同样适用于NCCL。 class torch.distributed.Backend 类似枚举的可用后端类：GLOO，NCCL和MPI。 这个类的值是小写字符串，例如“gloo”。它们可以作为属性访问，例如Backend.NCCL。 可以直接调用此类来解析字符串，例如，Backend（backend_str）将检查backend_str是否有效，如果是，则返回解析的小写字符串。它也接受大写字符串，例如`Backend（“GLOO”）return“gloo”。 注意 条目Backend.UNDEFINED存在但仅用作某些字段的初始值。用户既不应直接使用也不应假设存在。 torch.distributed.get_backend(group=) 返回给定进程组的后端 参数: group (ProcessGroup__, optional) – 要处理的进程组。默认值是常规主进程组。如果指定了另一个特定组，则调用进程必须是group的一部分。 返回: 给定进程组的后端作为小写字符串 --- --- torch.distributed.get_rank(group=) 返回当前进程组的排名 Rank是分配给分布式进程组中每个进程的唯一标识符。它们总是从0到world_size的连续整数。 参数: group (ProcessGroup__, optional) – 要处理的进程组 返回: 进程组-1的等级，如果不是该组的一部分 --- --- torch.distributed.get_world_size(group=) 返回当前进程组中的进程数 参数: group (ProcessGroup__, optional) – 要处理的进程组 返回: 进程组-1的世界大小，如果不是该组的一部分 --- --- torch.distributed.is_initialized() 检查是否已初始化默认进程组 torch.distributed.is_mpi_available() 检查MPI是否可用 torch.distributed.is_nccl_available() 检查NCCL是否可用 目前支持三种初始化方法： TCP初始化 有两种方法可以使用TCP进行初始化，这两种方法都需要从所有进程可以访问的网络地址和所需的world_size。第一种方法需要指定属于rank 0进程的地址。此初始化方法要求所有进程都具有手动指定的排名。 请注意，最新的分布式软件包中不再支持多播地址。group_name也被弃用了。 import torch.distributed as dist # 使用其中一台机器的地址 dist.init_process_group(backend, init_method='tcp://10.1.1.20:23456', rank=args.rank, world_size=4) 共享文件系统初始化 另一种初始化方法使用一个文件系统，该文件系统与组中的所有机器共享和可见，以及所需的world_size。URL应以file：//开头，并包含共享文件系统上不存在的文件（在现有目录中）的路径。如果文件不存在，文件系统初始化将自动创建该文件，但不会删除该文件。因此，下一步初始化 init_process_group() 在相同的文件路径发生之前您有责任确保清理文件。 请注意，在最新的分布式软件包中不再支持自动排名分配，并且也不推荐使用group_name。 警告 此方法假定文件系统支持使用fcntl进行锁定 - 大多数本地系统和NFS都支持它。 警告 此方法将始终创建该文件，并尽力在程序结束时清理并删除该文件。换句话说，每次进行初始化都需要创建一个全新的空文件，以便初始化成功。如果再次使用先前初始化使用的相同文件（不会被清除），则这是意外行为，并且经常会导致死锁和故障。因此，即使此方法将尽力清理文件，如果自动删除不成功，您有责任确保在训练结束时删除该文件以防止同一文件被删除 下次再次使用。如果你打算在相同的文件系统路径下多次调用 init_process_group() 的时候，就显得尤为重要了。换一种说法，如果那个文件没有被移除并且你再次调用 init_process_group()，那么失败是可想而知的。这里的经验法则是，每当调用init_process_group()的时候，确保文件不存在或为空。 import torch.distributed as dist # 应始终指定等级 dist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile', world_size=4, rank=args.rank) 环境变量初始化 此方法将从环境变量中读取配置，从而可以完全自定义信息的获取方式。要设置的变量是： MASTER_PORT - 需要; 必须是机器上的自由端口，等级为0。 MASTER_ADDR - 要求（0级除外）; 等级0节点的地址。 WORLD_SIZE - 需要; 可以在这里设置，也可以在调用init函数时设置。 RANK - 需要; 可以在这里设置，也可以在调用init函数时设置。 等级为0的机器将用于设置所有连接。 这是默认方法，意味着不必指定init_method（或者可以是env：//）。 组 默认情况下，集合体在默认组（也称为世界）上运行，并要求所有进程都进入分布式函数调用。但是，一些工作负载可以从更细粒度的通信中受益。这是分布式群体发挥作用的地方。new_group() 函数可用于创建新组，具有所有进程的任意子集。它返回一个不透明的组句柄，可以作为所有集合体的“group”参数给出（集合体是分布式函数，用于在某些众所周知的编程模式中交换信息）。 目前torch.distributed不支持创建具有不同后端的组。换一种说法，每一个正在被创建的组都会用相同的后端，只要你在 init_process_group() 里面声明清楚。 torch.distributed.new_group(ranks=None, timeout=datetime.timedelta(seconds=1800)) 创建一个新的分布式组 此功能要求主组中的所有进程（即属于分布式作业的所有进程）都进入此功能，即使它们不是该组的成员也是如此。此外，应在所有进程中以相同的顺序创建组。 参数: ranks (list[int]) – 小组成员的等级列表。 timeout (timedelta__, optional) – 针对进程组执行的操作超时，默认值等于30分钟，这仅适用于gloo后端。 返回: 分布式组的句柄，可以给予集体调用 点对点通信 torch.distributed.send(tensor, dst, group=, tag=0) 同步发送张量 参数: tensor (Tensor) – 准备发送的张量。 dst (int) – 目的地排名。 group (ProcessGroup__, optional) – 要处理的进程组。 tag (int, optional) – 标记以匹配发送与远程接收。 torch.distributed.recv(tensor, src=None, group=, tag=0) 同步接收张量 参数： tensor (Tensor) – 张量填充接收的数据。 src (int, optional) – 来源排名。如果未指定，将从任何流程收到。 group (ProcessGroup__, optional) – 要处理的进程组。 tag (int, optional) – 标记以匹配接收与远程发送。 返回: 发件人排名-1，如果不是该组的一部分 isend() 和 irecv() 使用时返回分布式请求对象。通常，此对象的类型未指定，因为它们永远不应手动创建，但它们保证支持两种方法： is_completed() - 如果操作已完成，则返回True。 wait() - 将阻止该过程，直到操作完成，is_completed（）保证一旦返回就返回True。 torch.distributed.isend(tensor, dst, group=, tag=0) 异步发送张量 参数: tensor (Tensor) – 准本发送的张量。 dst (int) – 目的地排名。 group (ProcessGroup__, optional) – 要处理的进程组。 tag (int, optional) – 标记以匹配发送与远程接收。 返回: 分布式请求对象。没有，如果不是该组的一部分 torch.distributed.irecv(tensor, src, group=, tag=0) 异步接收张量 参数: tensor (Tensor) – 张量填充接收的数据。 src (int) – 来源排名。 group (ProcessGroup__, optional) – 要处理的进程组。 tag (int, optional) – 标记以匹配接收与远程发送。 返回: 分布式请求对象。没有，如果不是该组的一部分 同步和异步集合操作 每个集合操作函数都支持以下两种操作： 同步操作 - 默认模式，当async_op设置为False时。当函数返回时，保证执行集合操作（如果它是CUDA操作，则不一定完成，因为所有CUDA操作都是异步的），并且可以调用任何进一步的函数调用，这取决于集合操作的数据。在同步模式下，集合函数不返回任何内容。 asynchronous operation - 当async_op设置为True时。集合操作函数返回分布式请求对象。通常，您不需要手动创建它，并且保证支持两种方法： is_completed() - 如果操作已完成，则返回True。 wait() - 将阻止该过程，直到操作完成。 集体职能 torch.distributed.broadcast(tensor, src, group=, async_op=False) 将张量广播到整个群体 tensor必须在参与集合体的所有进程中具有相同数量的元素。 参数: tensor (Tensor) – 如果src是当前进程的等级，则发送的数据，否则用于保存接收数据的张量。 src (int) – 来源排名。 group (ProcessGroup__, optional) – 要处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作。 返回: 异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分 torch.distributed.all_reduce(tensor, op=ReduceOp.SUM, group=, async_op=False) 减少所有机器上的张量数据，以便获得最终结果 调用tensor之后在所有进程中将按位相同。 参数: tensor (Tensor) – 集体的输入和输出。该功能就地运行。 op (optional) – 来自torch.distributed.ReduceOp枚举的值之一。指定用于逐元素减少的操作。 group (ProcessGroup__, optional) – 要处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作。 返回: 异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分 torch.distributed.reduce(tensor, dst, op=ReduceOp.SUM, group=, async_op=False) 减少所有机器的张量数据 只有排名为“dst”的进程才会收到最终结果。 参数: tensor (Tensor) – 集体的输入和输出。该功能就地运行。 dst (int) – 目的地排名。 op (optional) – 来自torch.distributed.ReduceOp枚举的值之一。指定用于逐元素减少的操作。 group (ProcessGroup__, optional) – 要处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作。 返回: 异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分 torch.distributed.all_gather(tensor_list, tensor, group=, async_op=False) 从列表中收集整个组的张量 参数： tensor_list (list[Tensor]) – 输出列表。它应包含正确大小的张量，用于集合的输出。 tensor (Tensor) – 从当前进程广播的张量。 group (ProcessGroup__, optional) – 要处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作。 返回: 异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分 torch.distributed.gather(tensor, gather_list, dst, group=, async_op=False) 在一个过程中收集张量列表 参数： tensor (Tensor) – 输入张量。 gather_list (list[Tensor]) – 用于接收数据的适当大小的张量列表。仅在接收过程中需要。 dst (int) – 目的地排名。除接收数据的进程外，在所有进程中都是必需的。 group (ProcessGroup__, optional) – 要处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作。 返回: 异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分 torch.distributed.scatter(tensor, scatter_list, src, group=, async_op=False) 将张量列表分散到组中的所有进程 每个进程只接收一个张量并将其数据存储在tensor参数中。 参数： tensor (Tensor) – 输出张量。 scatter_list (list[Tensor]) – 要分散的张量列表。仅在发送数据的过程中需要。 src (int) – 来源排名。除发送数据的进程外，在所有进程中都是必需的。 group (ProcessGroup__, optional) – 要处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作。 返回: 异步工作句柄，如果async_op设置为True。如果不是async_op或不是组的一部分，无 torch.distributed.barrier(group=, async_op=False) 同步所有进程 如果async_op为False，或者在wait（）上调用异步工作句柄，则此集合会阻止进程直到整个组进入此函数。 参数： group (ProcessGroup__, optional) – 要处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作。 返回: 异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分 class torch.distributed.ReduceOp 类似枚举的可用减少操作类：SUM，PRODUCT，MIN和MAX。 该类的值可以作为属性访问，例如，ReduceOp.SUM。它们用于指定减少集群的战略，例如 reduce(), all_reduce_multigpu()。 成员： SUM PRODUCT MIN MAX class torch.distributed.reduce_op 用于还原操作的不再使用的枚举类：SUM，PRODUCT，MIN和MAX。 建议使用ReduceOp 代替。 多GPU集群功能 如果每个节点上有多个GPU，则在使用NCCL和Gloo后端时，broadcast_multigpu() all_reduce_multigpu() reduce_multigpu() 和 all_gather_multigpu() 支持每个节点内多个GPU之间的分布式集合操作。这些功能可以潜在地提高整体分布式训练性能，并通过传递张量列表轻松使用。传递的张量列表中的每个张量需要位于调用该函数的主机的单独GPU设备上。请注意，张量列表的长度在所有分布式进程中需要相同。另请注意，目前只有NCCL后端支持多GPU集合功能。 例如，如果我们用于分布式训练的系统有2个节点，每个节点有8个GPU。在16个GPU中的每一个上，都有一个我们希望减少的张量，以下代码可以作为参考： 代码在节点0上运行 import torch import torch.distributed as dist dist.init_process_group(backend=\"nccl\", init_method=\"file:///distributed_test\", world_size=2, rank=0) tensor_list = [] for dev_idx in range(torch.cuda.device_count()): tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx)) dist.all_reduce_multigpu(tensor_list) 代码在节点1上运行 import torch import torch.distributed as dist dist.init_process_group(backend=\"nccl\", init_method=\"file:///distributed_test\", world_size=2, rank=1) tensor_list = [] for dev_idx in range(torch.cuda.device_count()): tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx)) dist.all_reduce_multigpu(tensor_list) 调用结束后，两个节点上的所有16个张量都将具有16的全减值。 torch.distributed.broadcast_multigpu(tensor_list, src, group=, async_op=False, src_tensor=0) 使用每个节点多个GPU张量将张量广播到整个组 tensor必须在参与集合体的所有进程的所有GPU中具有相同数量的元素。列表中的每个张量必须位于不同的GPU上。 目前仅支持nccl和gloo后端张量应该只是GPU张量 参数： tensor_list (List__[Tensor]) – 参与集群操作行动的张量。如果src是排名，那么tensor_list`（`tensor_list [src_tensor]`）的`src_tensor元素将被广播到src进程中的所有其他张量（在不同的GPU上）以及tensor_list中的所有张量其他非src进程。您还需要确保调用此函数的所有分布式进程的len（tensor_list）是相同的。 src (int) – 源排行。 group (ProcessGroup__, optional) – 要被处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作。 src_tensor (int, optional) – 源张量等级在tensor_list内。 返回: 异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分 torch.distributed.all_reduce_multigpu(tensor_list, op=ReduceOp.SUM, group=, async_op=False) 减少所有机器上的张量数据，以便获得最终结果。此功能可减少每个节点上的多个张量，而每个张量位于不同的GPU上。因此，张量列表中的输入张量需要是GPU张量。此外，张量列表中的每个张量都需要驻留在不同的GPU上。 在调用之后，tensor_list中的所有tensor在所有进程中都是按位相同的。 目前仅支持nccl和gloo后端，张量应仅为GPU张量。 参数： list (tensor) – 集体的输入和输出张量列表。该功能就地运行，并要求每个张量在不同的GPU上为GPU张量。您还需要确保调用此函数的所有分布式进程的len（tensor_list）是相同的。 op (optional) – 来自torch.distributed.ReduceOp枚举的值之一，并且指定一个逐元素减少的操作。 group (ProcessGroup__, optional) – 要处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作。 返回: 异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分 torch.distributed.reduce_multigpu(tensor_list, dst, op=ReduceOp.SUM, group=, async_op=False, dst_tensor=0) 减少所有计算机上多个GPU的张量数据。tensor_list中的每个张量应位于单独的GPU上。 只有级别为'dst的进程中的'tensor_list [dst_tensor]的GPU才会收到最终结果。 目前仅支持nccl后端张量应该只是GPU张量。 参数： tensor_list (List__[Tensor]) – 输入和输出集体的GPU张量。该功能就地运行，您还需要确保调用此函数的所有分布式进程的len（tensor_list）是相同的。 dst (int) – 目的地排名。 op (optional) – 来自torch.distributed.ReduceOp枚举的值之一。指定一个逐元素减少的操作。 group (ProcessGroup__, optional) – The process group to work on async_op (bool, optional) – 这个操作是否应该是异步操作。 dst_tensor (int, optional) – 目标张量在tensor_list中排名。 返回: 异步工作句柄，如果async_op设置为True。没有，否则 torch.distributed.all_gather_multigpu(output_tensor_lists, input_tensor_list, group=, async_op=False) 从列表中收集整个组的张量。tensor_list中的每个张量应位于单独的GPU上。 目前仅支持nccl后端张量应该只是GPU张量。 参数： output_tensor_lists (List[List__[Tensor]__]) – 输出列表。它应该在每个GPU上包含正确大小的张量，以用于集合的输出。例如 output_tensor_lists [i]包含驻留在input_tensor_list [i]的GPU上的all_gather结果。请注意，output_tensor_lists [i]的每个元素都具有world_size * len（input_tensor_list）的大小，因为该函数全部收集组中每个GPU的结果。要解释output_tensor_list [i]的每个元素，请注意等级k的input_tensor_list [j]将出现在output_tensor_list [i] [rank * world_size + j]中。还要注意len（output_tensor_lists），并且output_tensor_lists中的每个元素的大小（每个元素都是一个列表，因此len（output_tensor_lists [i]）`）对于调用此函数的所有分布式进程都需要相同。 input_tensor_list (List__[Tensor]) – 从当前进程广播的张量（在不同的GPU上）的列表。请注意，调用此函数的所有分布式进程的len（input_tensor_list）必须相同。 group (ProcessGroup__, optional) – 要处理的进程组。 async_op (bool, optional) – 这个操作是否应该是异步操作 返回: 异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分 启动实用程序 torch.distributed包还在torch.distributed.launch中提供了一个启动实用程序。此帮助实用程序可用于为每个节点启动多个进程以进行分布式训练。该实用程序还支持python2和python3。 torch.distributed.launch是一个模块，它在每个训练节点上产生多个分布式训练过程。 该实用程序可用于单节点分布式训练，其中将生成每个节点的一个或多个进程。该实用程序可用于CPU训练或GPU训练。如果该实用程序用于GPU训练，则每个分布式进程将在单个GPU上运行。这可以实现良好改进的单节点训练性能。它还可以用于多节点分布式训练，通过在每个节点上产生多个进程来获得良好改进的多节点分布式训练性能。这对于具有多个具有直接GPU支持的Infiniband接口的系统尤其有利，因为所有这些接口都可用于聚合通信带宽。 在单节点分布式训练或多节点分布式训练的两种情况下，该实用程序将为每个节点启动给定数量的进程（--nproc_per_node）。如果用于GPU训练，此数字需要小于或等于当前系统上的GPU数量（'nprocper_node`），并且每个进程将在单个GPU上运行，从_GPU 0到GPU（nproc_per_node - 1）。 如何使用这个模块： 单节点多进程分布式训练 >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of your training script) 多节点多进程分布式训练:(例如两个节点） 节点1：（IP：192.168.1.1，并且有一个空闲端口：1234） >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\" --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of your training script) 节点2： >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\" --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of your training script) 1.查找此模块提供的可选参数： >>> python -m torch.distributed.launch --help 重要告示： 1. 这种实用和多进程分布式（单节点或多节点）GPU训练目前仅使用NCCL分布式后端实现最佳性能。因此，NCCL后端是用于GPU训练的推荐后端。 2. 在您的训练程序中，您必须解析命令行参数：--local_rank = LOCAL_PROCESS_RANK，这将由此模块提供。如果您的训练计划使用GPU，则应确保您的代码仅在LOCAL_PROCESS_RANK的GPU设备上运行。这可以通过以下方式完成： 解析local_rank参数 >>> import argparse >>> parser = argparse.ArgumentParser() >>> parser.add_argument(\"--local_rank\", type=int) >>> args = parser.parse_args() 使用其中一个将您的设备设置为本地排名 >>> torch.cuda.set_device(arg.local_rank) # before your code runs 或者 >>> with torch.cuda.device(arg.local_rank): >>> # your code to run 3. 在您的训练计划中，您应该在开始时调用以下函数来启动分布式后端。您需要确保init_method使用env：//，这是该模块唯一支持的init_method。 torch.distributed.init_process_group(backend='YOUR BACKEND', init_method='env://') 4. 在您的训练计划中，您可以使用常规分布式功能或使用 torch.nn.parallel.DistributedDataParallel() 模块。如果您的训练计划使用GPU进行训练，并且您希望使用 torch.nn.parallel.DistributedDataParallel() 模块。这里是如何配置它。 model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[arg.local_rank], output_device=arg.local_rank) 请确保将device_ids参数设置为您的代码将在其上运行的唯一GPU设备ID。这通常是流程的本地排名。换句话说，device_ids需要是[args.local_rank]，output_device需要是'args.local_rank`才能使用这个实用程序。 警告 local_rank不是全局唯一的：它只对机器上的每个进程唯一。因此，不要使用它来决定是否应该，例如，写入网络文件系统，参考 https://github.com/pytorch/pytorch/issues/12042 例如，如果您没有正确执行此操作，事情可能会出错。 Spawn实用程序 在 torch.multiprocessing.spawn() 里面，torch.multiprocessing包还提供了一个spawn函数. 此辅助函数可用于生成多个进程。它通过传递您要运行的函数并生成N个进程来运行它。这也可以用于多进程分布式训练。 有关如何使用它的参考，请参阅 PyToch example - ImageNet implementation 请注意，此函数需要Python 3.4或更高版本。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributions.html":{"url":"distributions.html","title":"Probability distributions - torch.distributions","keywords":"","body":"概率分布 - torch.distributions 译者：hijkzzz distributions 包含可参数化的概率分布和采样函数. 这允许构造用于优化的随机计算图和随机梯度估计器. 这个包一般遵循 TensorFlow Distributions 包的设计. 通常, 不可能直接通过随机样本反向传播. 但是, 有两种主要方法可创建可以反向传播的代理函数. 即得分函数估计器/似然比估计器/REINFORCE和pathwise derivative估计器. REINFORCE通常被视为强化学习中策略梯度方法的基础, 并且pathwise derivative估计器常见于变分自动编码器中的重新参数化技巧. 得分函数仅需要样本的值 , pathwise derivative 需要导数 . 接下来的部分将在一个强化学习示例中讨论这两个问题. 有关详细信息, 请参阅 Gradient Estimation Using Stochastic Computation Graphs . 得分函数 当概率密度函数相对于其参数可微分时, 我们只需要sample()和log_prob()来实现REINFORCE: 是参数, 是学习速率, 是奖励 并且 是在状态 以及给定策略 执行动作 的概率. 在实践中, 我们将从网络输出中采样一个动作, 将这个动作应用于一个环境中, 然后使用log_prob构造一个等效的损失函数. 请注意, 我们使用负数是因为优化器使用梯度下降, 而上面的规则假设梯度上升. 有了确定的策略, REINFORCE的实现代码如下: probs = policy_network(state) # Note that this is equivalent to what used to be called multinomial m = Categorical(probs) action = m.sample() next_state, reward = env.step(action) loss = -m.log_prob(action) * reward loss.backward() Pathwise derivative 实现这些随机/策略梯度的另一种方法是使用来自rsample()方法的重新参数化技巧, 其中参数化随机变量可以通过无参数随机变量的参数确定性函数构造. 因此, 重新参数化的样本变得可微分. 实现Pathwise derivative的代码如下: params = policy_network(state) m = Normal(*params) # Any distribution with .has_rsample == True could work based on the application action = m.rsample() next_state, reward = env.step(action) # Assuming that reward is differentiable loss = -reward loss.backward() 分布 class torch.distributions.distribution.Distribution(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None) 基类: object Distribution是概率分布的抽象基类. arg_constraints 从参数名称返回字典到 Constraint 对象（应该满足这个分布的每个参数）.不是张量的arg不需要出现在这个字典中. batch_shape 返回批量参数的形状. cdf(value) 返回value处的累积密度/质量函数估计. | 参数: | value (Tensor) – | entropy() 返回分布的熵, 批量的形状为 batch_shape. | 返回值: | Tensor 形状为 batch_shape. | enumerate_support(expand=True) 返回包含离散分布支持的所有值的张量. 结果将在维度0上枚举, 所以结果的形状将是 (cardinality,) + batch_shape + event_shape (对于单变量分布 event_shape = ()). 注意, 这在lock-step中枚举了所有批处理张量[[0, 0], [1, 1], …]. 当 expand=False, 枚举沿着维度 0进行, 但是剩下的批处理维度是单维度, [[0], [1], ... 遍历整个笛卡尔积的使用 itertools.product(m.enumerate_support()). | 参数: | expand (bool) – 是否扩展对批处理dim的支持以匹配分布的 batch_shape. | | 返回值: | 张量在维上0迭代. | event_shape 返回单个样本的形状 (非批量). expand(batch_shape, _instance=None) 返回一个新的分布实例(或填充派生类提供的现有实例), 其批处理维度扩展为 batch_shape. 这个方法调用 expand 在分布的参数上. 因此, 这不会为扩展的分布实例分配新的内存. 此外, 第一次创建实例时, 这不会在中重复任何参数检查或参数广播在 __init__.py. 参数: batch_shape (torch.Size) – 所需的扩展尺寸. _instance – 由需要重写.expand的子类提供的新实例. | 返回值: | 批处理维度扩展为batch_size的新分布实例. | icdf(value) 返回按value计算的反向累积密度/质量函数. | 参数: | value (Tensor) – | log_prob(value) 返回按value计算的概率密度/质量函数的对数. | 参数: | value (Tensor) – | mean 返回分布的平均值. perplexity() 返回分布的困惑度, 批量的关于 batch_shape. | 返回值: | 形状为 batch_shape 的张量. | rsample(sample_shape=torch.Size([])) 如果分布的参数是批量的, 则生成sample_shape形状的重新参数化样本或sample_shape形状的批量重新参数化样本. sample(sample_shape=torch.Size([])) 如果分布的参数是批量的, 则生成sample_shape形状的样本或sample_shape形状的批量样本. sample_n(n) 如果分布参数是分批的, 则生成n个样本或n批样本. stddev 返回分布的标准差. support 返回Constraint 对象表示该分布的支持. variance 返回分布的方差. ExponentialFamily class torch.distributions.exp_family.ExponentialFamily(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None) 基类: torch.distributions.distribution.Distribution 指数族是指数族概率分布的抽象基类, 其概率质量/密度函数的形式定义如下 表示自然参数, 表示充分统计量, 是给定族的对数归一化函数 是carrier measure. 注意 该类是Distribution类与指数族分布之间的中介, 主要用于检验.entropy()和解析KL散度方法的正确性. 我们使用这个类来计算熵和KL散度使用AD框架和Bregman散度 (出自: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families). entropy() 利用对数归一化器的Bregman散度计算熵的方法. Bernoulli class torch.distributions.bernoulli.Bernoulli(probs=None, logits=None, validate_args=None) 基类: torch.distributions.exp_family.ExponentialFamily 创建参数化的伯努利分布, 根据 probs 或者 logits (但不是同时都有). 样本是二值的 (0 或者 1). 取值 1 伴随概率 p , 或者 0 伴随概率 1 - p. 例子: >>> m = Bernoulli(torch.tensor([0.3])) >>> m.sample() # 30% chance 1; 70% chance 0 tensor([ 0.]) 参数: probs (Number__, Tensor) – the probabilty of sampling 1 logits (Number__, Tensor) – the log-odds of sampling 1 arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} entropy() enumerate_support(expand=True) expand(batch_shape, _instance=None) has_enumerate_support = True log_prob(value) logits mean param_shape probs sample(sample_shape=torch.Size([])) support = Boolean() variance Beta class torch.distributions.beta.Beta(concentration1, concentration0, validate_args=None) 基类: torch.distributions.exp_family.ExponentialFamily Beta 分布, 参数为 concentration1 和 concentration0. 例子: >>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5])) >>> m.sample() # Beta distributed with concentration concentration1 and concentration0 tensor([ 0.1046]) 参数: concentration1 (float or Tensor) – 分布的第一个浓度参数（通常称为alpha） concentration0 (float or Tensor) – 分布的第二个浓度参数(通常称为beta) arg_constraints = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)} concentration0 concentration1 entropy() expand(batch_shape, _instance=None) has_rsample = True log_prob(value) mean rsample(sample_shape=()) support = Interval(lower_bound=0.0, upper_bound=1.0) variance Binomial class torch.distributions.binomial.Binomial(total_count=1, probs=None, logits=None, validate_args=None) 基类: torch.distributions.distribution.Distribution 创建一个Binomial 分布, 参数为 total_count 和 probs 或者 logits (但不是同时都有使用). total_count 必须和 [probs] 之间可广播(#torch.distributions.binomial.Binomial.probs \"torch.distributions.binomial.Binomial.probs\")/logits. 例子: >>> m = Binomial(100, torch.tensor([0 , .2, .8, 1])) >>> x = m.sample() tensor([ 0., 22., 71., 100.]) >>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8])) >>> x = m.sample() tensor([[ 4., 5.], [ 7., 6.]]) 参数: total_count (int or Tensor) – 伯努利试验次数 probs (Tensor) – 事件概率 logits (Tensor) – 事件 log-odds arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0), 'total_count': IntegerGreaterThan(lower_bound=0)} enumerate_support(expand=True) expand(batch_shape, _instance=None) has_enumerate_support = True log_prob(value) logits mean param_shape probs sample(sample_shape=torch.Size([])) support variance Categorical class torch.distributions.categorical.Categorical(probs=None, logits=None, validate_args=None) 基类: torch.distributions.distribution.Distribution 创建一个 categorical 分布, 参数为 probs 或者 logits (但不是同时都有). 注意 它等价于从 torch.multinomial() 的采样. 样本是整数来自 K 是 probs.size(-1). 如果 probs 是 1D 的, 长度为K, 每个元素是在该索引处对类进行抽样的相对概率. 如果 probs 是 2D 的, 它被视为一组相对概率向量. 注意 probs 必须是非负的、有限的并且具有非零和, 并且它将被归一化为和为1. 请参阅: torch.multinomial() 例子: >>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ])) >>> m.sample() # equal probability of 0, 1, 2, 3 tensor(3) 参数: probs (Tensor) – event probabilities logits (Tensor) – event log probabilities arg_constraints = {'logits': Real(), 'probs': Simplex()} entropy() enumerate_support(expand=True) expand(batch_shape, _instance=None) has_enumerate_support = True log_prob(value) logits mean param_shape probs sample(sample_shape=torch.Size([])) support variance Cauchy class torch.distributions.cauchy.Cauchy(loc, scale, validate_args=None) 基类: torch.distributions.distribution.Distribution 样本来自柯西(洛伦兹)分布. 均值为0的独立正态分布随机变量之比服从柯西分布. 例子: >>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0])) >>> m.sample() # sample from a Cauchy distribution with loc=0 and scale=1 tensor([ 2.3214]) 参数: loc (float or Tensor) – 分布的模态或中值. scale (float or Tensor) – half width at half maximum. arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} cdf(value) entropy() expand(batch_shape, _instance=None) has_rsample = True icdf(value) log_prob(value) mean rsample(sample_shape=torch.Size([])) support = Real() variance Chi2 class torch.distributions.chi2.Chi2(df, validate_args=None) 基类: torch.distributions.gamma.Gamma 创建由形状参数df参数化的Chi2分布. 这完全等同于 Gamma(alpha=0.5*df, beta=0.5) 例子: >>> m = Chi2(torch.tensor([1.0])) >>> m.sample() # Chi2 distributed with shape df=1 tensor([ 0.1046]) | 参数: | df (float or Tensor) – 分布的形状参数 | arg_constraints = {'df': GreaterThan(lower_bound=0.0)} df expand(batch_shape, _instance=None) Dirichlet class torch.distributions.dirichlet.Dirichlet(concentration, validate_args=None) 基类: torch.distributions.exp_family.ExponentialFamily 创建一个 Dirichlet 分布, 参数为concentration. 例子: >>> m = Dirichlet(torch.tensor([0.5, 0.5])) >>> m.sample() # Dirichlet distributed with concentrarion concentration tensor([ 0.1046, 0.8954]) | 参数: | concentration (Tensor) – 分布的浓度参数（通常称为alpha） | arg_constraints = {'concentration': GreaterThan(lower_bound=0.0)} entropy() expand(batch_shape, _instance=None) has_rsample = True log_prob(value) mean rsample(sample_shape=()) support = Simplex() variance Exponential class torch.distributions.exponential.Exponential(rate, validate_args=None) 基类: torch.distributions.exp_family.ExponentialFamily 创建由rate参数化的指数分布. 例子: >>> m = Exponential(torch.tensor([1.0])) >>> m.sample() # Exponential distributed with rate=1 tensor([ 0.1046]) | 参数: | rate (float or Tensor) – rate = 1 / 分布的scale | arg_constraints = {'rate': GreaterThan(lower_bound=0.0)} cdf(value) entropy() expand(batch_shape, _instance=None) has_rsample = True icdf(value) log_prob(value) mean rsample(sample_shape=torch.Size([])) stddev support = GreaterThan(lower_bound=0.0) variance FisherSnedecor class torch.distributions.fishersnedecor.FisherSnedecor(df1, df2, validate_args=None) 基类: torch.distributions.distribution.Distribution 创建由df1和df2参数化的Fisher-Snedecor分布 例子: >>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0])) >>> m.sample() # Fisher-Snedecor-distributed with df1=1 and df2=2 tensor([ 0.2453]) 参数: df1 (float or Tensor) – 自由度参数1 df2 (float or Tensor) – 自由度参数2 arg_constraints = {'df1': GreaterThan(lower_bound=0.0), 'df2': GreaterThan(lower_bound=0.0)} expand(batch_shape, _instance=None) has_rsample = True log_prob(value) mean rsample(sample_shape=torch.Size([])) support = GreaterThan(lower_bound=0.0) variance Gamma class torch.distributions.gamma.Gamma(concentration, rate, validate_args=None) 基类: torch.distributions.exp_family.ExponentialFamily 创建由concentration和rate参数化的伽马分布. . 例子: >>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0])) >>> m.sample() # Gamma distributed with concentration=1 and rate=1 tensor([ 0.1046]) 参数: concentration (float or Tensor) – 分布的形状参数（通常称为alpha） rate (float or Tensor) – rate = 1 / 分布scale (通常称为beta ) arg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'rate': GreaterThan(lower_bound=0.0)} entropy() expand(batch_shape, _instance=None) has_rsample = True log_prob(value) mean rsample(sample_shape=torch.Size([])) support = GreaterThan(lower_bound=0.0) variance Geometric class torch.distributions.geometric.Geometric(probs=None, logits=None, validate_args=None) 基类: torch.distributions.distribution.Distribution 创建由probs参数化的几何分布, 其中probs是伯努利试验成功的概率. 它表示概率在 次伯努利试验中, 前 试验失败, 然后成功. 样本是非负整数 [0, ). 例子: >>> m = Geometric(torch.tensor([0.3])) >>> m.sample() # underlying Bernoulli has 30% chance 1; 70% chance 0 tensor([ 2.]) 参数: probs (Number__, Tensor) – 抽样1的概率 . 必须是在范围 (0, 1] logits (Number__, Tensor) – 抽样 1的log-odds. arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} entropy() expand(batch_shape, _instance=None) log_prob(value) logits mean probs sample(sample_shape=torch.Size([])) support = IntegerGreaterThan(lower_bound=0) variance Gumbel class torch.distributions.gumbel.Gumbel(loc, scale, validate_args=None) 基类: torch.distributions.transformed_distribution.TransformedDistribution 来自Gumbel分布的样本. Examples: >>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0])) >>> m.sample() # sample from Gumbel distribution with loc=1, scale=2 tensor([ 1.0124]) 参数: loc (float or Tensor) – 分布的位置参数 scale (float or Tensor) – 分布的scale 参数 arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} entropy() expand(batch_shape, _instance=None) mean stddev support = Real() variance HalfCauchy class torch.distributions.half_cauchy.HalfCauchy(scale, validate_args=None) 基类: torch.distributions.transformed_distribution.TransformedDistribution 创建scale参数化的半正态分布: X ~ Cauchy(0, scale) Y = |X| ~ HalfCauchy(scale) 例子: >>> m = HalfCauchy(torch.tensor([1.0])) >>> m.sample() # half-cauchy distributed with scale=1 tensor([ 2.3214]) | 参数: | scale (float or Tensor) – 完全柯西分布的scale | arg_constraints = {'scale': GreaterThan(lower_bound=0.0)} cdf(value) entropy() expand(batch_shape, _instance=None) has_rsample = True icdf(prob) log_prob(value) mean scale support = GreaterThan(lower_bound=0.0) variance HalfNormal class torch.distributions.half_normal.HalfNormal(scale, validate_args=None) 基类: torch.distributions.transformed_distribution.TransformedDistribution 创建按scale参数化的半正态分布: X ~ Normal(0, scale) Y = |X| ~ HalfNormal(scale) 例子: >>> m = HalfNormal(torch.tensor([1.0])) >>> m.sample() # half-normal distributed with scale=1 tensor([ 0.1046]) | 参数: | scale (float or Tensor) – 完全正态分布的scale | arg_constraints = {'scale': GreaterThan(lower_bound=0.0)} cdf(value) entropy() expand(batch_shape, _instance=None) has_rsample = True icdf(prob) log_prob(value) mean scale support = GreaterThan(lower_bound=0.0) variance Independent class torch.distributions.independent.Independent(base_distribution, reinterpreted_batch_ndims, validate_args=None) 基类: torch.distributions.distribution.Distribution 重新解释一些分布的批量 dims 作为 event dims. 这主要用于改变log_prob()结果的形状.例如, 要创建与多元正态分布形状相同的对角正态分布(因此它们是可互换的), 您可以这样做: >>> loc = torch.zeros(3) >>> scale = torch.ones(3) >>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale)) >>> [mvn.batch_shape, mvn.event_shape] [torch.Size(()), torch.Size((3,))] >>> normal = Normal(loc, scale) >>> [normal.batch_shape, normal.event_shape] [torch.Size((3,)), torch.Size(())] >>> diagn = Independent(normal, 1) >>> [diagn.batch_shape, diagn.event_shape] [torch.Size(()), torch.Size((3,))] 参数: base_distribution (torch.distributions.distribution.Distribution) – 基础分布 reinterpreted_batch_ndims (int) –要重解释的批量dims的数量 arg_constraints = {} entropy() enumerate_support(expand=True) expand(batch_shape, _instance=None) has_enumerate_support has_rsample log_prob(value) mean rsample(sample_shape=torch.Size([])) sample(sample_shape=torch.Size([])) support variance Laplace class torch.distributions.laplace.Laplace(loc, scale, validate_args=None) 基类: torch.distributions.distribution.Distribution 创建参数化的拉普拉斯分布, 参数是 loc 和 :attr:’scale’. 例子: >>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0])) >>> m.sample() # Laplace distributed with loc=0, scale=1 tensor([ 0.1046]) 参数: loc (float or Tensor) – 分布均值 scale (float or Tensor) – 分布scale arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} cdf(value) entropy() expand(batch_shape, _instance=None) has_rsample = True icdf(value) log_prob(value) mean rsample(sample_shape=torch.Size([])) stddev support = Real() variance LogNormal class torch.distributions.log_normal.LogNormal(loc, scale, validate_args=None) 基类: torch.distributions.transformed_distribution.TransformedDistribution 创建参数化的对数正态分布, 参数为 loc 和 scale: X ~ Normal(loc, scale) Y = exp(X) ~ LogNormal(loc, scale) 例子: >>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0])) >>> m.sample() # log-normal distributed with mean=0 and stddev=1 tensor([ 0.1046]) 参数: loc (float or Tensor) – 分布对数平均值 scale (float or Tensor) – 分布对数的标准差 arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} entropy() expand(batch_shape, _instance=None) has_rsample = True loc mean scale support = GreaterThan(lower_bound=0.0) variance LowRankMultivariateNormal class torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(loc, cov_factor, cov_diag, validate_args=None) 基类: torch.distributions.distribution.Distribution 使用由cov_factor和cov_diag参数化的低秩形式的协方差矩阵创建多元正态分布: covariance_matrix = cov_factor @ cov_factor.T + cov_diag Example >>> m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([1, 0]), torch.tensor([1, 1])) >>> m.sample() # normally distributed with mean=`[0,0]`, cov_factor=`[1,0]`, cov_diag=`[1,1]` tensor([-0.2102, -0.5429]) 参数: loc (Tensor) – 分布的均值, 形状为 batch_shape + event_shape cov_factor (Tensor) – 协方差矩阵低秩形式的因子部分, 形状为 batch_shape + event_shape + (rank,) cov_diag (Tensor) – 协方差矩阵的低秩形式的对角部分, 形状为 batch_shape + event_shape 注意 避免了协方差矩阵的行列式和逆的计算, 当 cov_factor.shape[1] 由于 Woodbury matrix identity 和 matrix determinant lemma. 由于这些公式, 我们只需要计算小尺寸“capacitance”矩阵的行列式和逆: capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor arg_constraints = {'cov_diag': GreaterThan(lower_bound=0.0), 'cov_factor': Real(), 'loc': Real()} covariance_matrix entropy() expand(batch_shape, _instance=None) has_rsample = True log_prob(value) mean precision_matrix rsample(sample_shape=torch.Size([])) scale_tril support = Real() variance Multinomial class torch.distributions.multinomial.Multinomial(total_count=1, probs=None, logits=None, validate_args=None) 基类: torch.distributions.distribution.Distribution 创建由total_count和probs或logits（但不是两者）参数化的多项式分布. probs的最内层维度是对类别的索引. 所有其他维度索引批次. 注意 total_count 不需要指定, 当只有 log_prob() 被调用 注意 probs 必须是非负的、有限的并且具有非零和, 并且它将被归一化为和为1. sample() 所有参数和样本都需要一个共享的total_count. log_prob() 允许每个参数和样本使用不同的total_count. 例子: >>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.])) >>> x = m.sample() # equal probability of 0, 1, 2, 3 tensor([ 21., 24., 30., 25.]) >>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x) tensor([-4.1338]) 参数: total_count (int) – 试验次数 probs (Tensor) – 事件概率 logits (Tensor) – 事件对数概率 arg_constraints = {'logits': Real(), 'probs': Simplex()} expand(batch_shape, _instance=None) log_prob(value) logits mean param_shape probs sample(sample_shape=torch.Size([])) support variance MultivariateNormal class torch.distributions.multivariate_normal.MultivariateNormal(loc, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=None) 基类: torch.distributions.distribution.Distribution 创建由均值向量和协方差矩阵参数化的多元正态(也称为高斯)分布. 多元正态分布可以用正定协方差矩阵来参数化或者一个正定的精度矩阵 或者是一个正对角项的下三角矩阵 , 例如 . 这个三角矩阵可以通过协方差的Cholesky分解得到. 例子 >>> m = MultivariateNormal(torch.zeros(2), torch.eye(2)) >>> m.sample() # normally distributed with mean=`[0,0]` and covariance_matrix=`I` tensor([-0.2102, -0.5429]) 参数: loc (Tensor) – 分布的均值 covariance_matrix (Tensor) – 正定协方差矩阵 precision_matrix (Tensor) – 正定精度矩阵 scale_tril (Tensor) – 具有正值对角线的下三角协方差因子 注意 仅仅一个 covariance_matrix 或者 precision_matrix 或者 scale_tril 可被指定. 使用 scale_tril 会更有效率: 内部的所有计算都基于 scale_tril. 如果 covariance_matrix 或者 precision_matrix 已经被传入, 它仅用于使用Cholesky分解计算相应的下三角矩阵. arg_constraints = {'covariance_matrix': PositiveDefinite(), 'loc': RealVector(), 'precision_matrix': PositiveDefinite(), 'scale_tril': LowerCholesky()} covariance_matrix entropy() expand(batch_shape, _instance=None) has_rsample = True log_prob(value) mean precision_matrix rsample(sample_shape=torch.Size([])) scale_tril support = Real() variance NegativeBinomial class torch.distributions.negative_binomial.NegativeBinomial(total_count, probs=None, logits=None, validate_args=None) 基类: torch.distributions.distribution.Distribution 创建一个负二项分布, 即在达到total_count失败之前所需的独立相同伯努利试验的数量的分布. 每次伯努利试验成功的概率都是probs. 参数: total_count (float or Tensor) – 非负数伯努利试验停止的次数, 虽然分布仍然对实数有效 probs (Tensor) – 事件概率, 区间为 [0, 1) logits (Tensor) – 事件对数几率 - 成功概率的几率 arg_constraints = {'logits': Real(), 'probs': HalfOpenInterval(lower_bound=0.0, upper_bound=1.0), 'total_count': GreaterThanEq(lower_bound=0)} expand(batch_shape, _instance=None) log_prob(value) logits mean param_shape probs sample(sample_shape=torch.Size([])) support = IntegerGreaterThan(lower_bound=0) variance Normal class torch.distributions.normal.Normal(loc, scale, validate_args=None) 基类: torch.distributions.exp_family.ExponentialFamily 创建由loc和scale参数化的正态（也称为高斯）分布 例子: >>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0])) >>> m.sample() # normally distributed with loc=0 and scale=1 tensor([ 0.1046]) 参数: loc (float or Tensor) – 均值 (也被称为 mu) scale (float or Tensor) – 标准差(也被称为) sigma) arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} cdf(value) entropy() expand(batch_shape, _instance=None) has_rsample = True icdf(value) log_prob(value) mean rsample(sample_shape=torch.Size([])) sample(sample_shape=torch.Size([])) stddev support = Real() variance OneHotCategorical class torch.distributions.one_hot_categorical.OneHotCategorical(probs=None, logits=None, validate_args=None) 基类: torch.distributions.distribution.Distribution 创建一个由probs或logits参数化的One Hot Categorical 分布 样本是大小为 probs.size(-1)热编码向量. 注意 probs必须是非负的, 有限的并且具有非零和, 并且它将被归一化为总和为1. 请参见: torch.distributions.Categorical() 对于指定 probs 和 logits. 例子: >>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ])) >>> m.sample() # equal probability of 0, 1, 2, 3 tensor([ 0., 0., 0., 1.]) 参数: probs (Tensor) – event probabilities logits (Tensor) – event log probabilities arg_constraints = {'logits': Real(), 'probs': Simplex()} entropy() enumerate_support(expand=True) expand(batch_shape, _instance=None) has_enumerate_support = True log_prob(value) logits mean param_shape probs sample(sample_shape=torch.Size([])) support = Simplex() variance Pareto class torch.distributions.pareto.Pareto(scale, alpha, validate_args=None) 基类: torch.distributions.transformed_distribution.TransformedDistribution 来自Pareto Type 1分布的样本. 例子: >>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0])) >>> m.sample() # sample from a Pareto distribution with scale=1 and alpha=1 tensor([ 1.5623]) 参数: scale (float or Tensor) – 分布的Scale alpha (float or Tensor) – 分布的Shape arg_constraints = {'alpha': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)} entropy() expand(batch_shape, _instance=None) mean support variance Poisson class torch.distributions.poisson.Poisson(rate, validate_args=None) 基类: torch.distributions.exp_family.ExponentialFamily 创建按rate参数化的泊松分布 样本是非负整数, pmf是 例子: >>> m = Poisson(torch.tensor([4])) >>> m.sample() tensor([ 3.]) | 参数: | rate (Number__, Tensor) – rate 参数 | arg_constraints = {'rate': GreaterThan(lower_bound=0.0)} expand(batch_shape, _instance=None) log_prob(value) mean sample(sample_shape=torch.Size([])) support = IntegerGreaterThan(lower_bound=0) variance RelaxedBernoulli class torch.distributions.relaxed_bernoulli.RelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None) 基类: torch.distributions.transformed_distribution.TransformedDistribution 创建一个RelaxedBernoulli分布, 通过temperature参数化, 以及probs或logits（但不是两者）. 这是伯努利分布的松弛版本, 因此值在（0,1）中, 并且具有可重参数化的样本. 例子: >>> m = RelaxedBernoulli(torch.tensor([2.2]), torch.tensor([0.1, 0.2, 0.3, 0.99])) >>> m.sample() tensor([ 0.2951, 0.3442, 0.8918, 0.9021]) 参数: temperature (Tensor) – 松弛 temperature probs (Number__, Tensor) –采样 1 的概率 logits (Number__, Tensor) – 采样 1 的对数概率 arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} expand(batch_shape, _instance=None) has_rsample = True logits probs support = Interval(lower_bound=0.0, upper_bound=1.0) temperature RelaxedOneHotCategorical class torch.distributions.relaxed_categorical.RelaxedOneHotCategorical(temperature, probs=None, logits=None, validate_args=None) 基类: torch.distributions.transformed_distribution.TransformedDistribution 创建一个由温度参数化的RelaxedOneHotCategorical分布, 以及probs或logits. 这是OneHotCategorical分布的松弛版本, 因此它的样本是单一的, 并且可以重参数化. 例子: >>> m = RelaxedOneHotCategorical(torch.tensor([2.2]), torch.tensor([0.1, 0.2, 0.3, 0.4])) >>> m.sample() tensor([ 0.1294, 0.2324, 0.3859, 0.2523]) 参数: temperature (Tensor) – 松弛 temperature probs (Tensor) – 事件概率 logits (Tensor) –对数事件概率. arg_constraints = {'logits': Real(), 'probs': Simplex()} expand(batch_shape, _instance=None) has_rsample = True logits probs support = Simplex() temperature StudentT class torch.distributions.studentT.StudentT(df, loc=0.0, scale=1.0, validate_args=None) 基类: torch.distributions.distribution.Distribution 根据自由度df, 平均loc和scale创建学生t分布. 例子: >>> m = StudentT(torch.tensor([2.0])) >>> m.sample() # Student's t-distributed with degrees of freedom=2 tensor([ 0.1046]) 参数: df (float or Tensor) – 自由度 loc (float or Tensor) – 均值 scale (float or Tensor) – 分布的scale arg_constraints = {'df': GreaterThan(lower_bound=0.0), 'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} entropy() expand(batch_shape, _instance=None) has_rsample = True log_prob(value) mean rsample(sample_shape=torch.Size([])) support = Real() variance TransformedDistribution class torch.distributions.transformed_distribution.TransformedDistribution(base_distribution, transforms, validate_args=None) 基类: torch.distributions.distribution.Distribution Distribution类的扩展, 它将一系列变换应用于基本分布. 假设f是所应用变换的组成: X ~ BaseDistribution Y = f(X) ~ TransformedDistribution(BaseDistribution, f) log p(Y) = log p(X) + log |det (dX/dY)| 注意 .event_shape of a TransformedDistribution 是其基本分布及其变换的最大形状, 因为变换可以引入事件之间的相关性. 一个使用例子 TransformedDistribution: # Building a Logistic Distribution # X ~ Uniform(0, 1) # f = a + b * logit(X) # Y ~ f(X) ~ Logistic(a, b) base_distribution = Uniform(0, 1) transforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)] logistic = TransformedDistribution(base_distribution, transforms) 有关更多示例, 请查看有关实现 Gumbel, HalfCauchy, HalfNormal, LogNormal, Pareto, Weibull, RelaxedBernoulli 和 RelaxedOneHotCategorical arg_constraints = {} cdf(value) 通过逆变换和计算基分布的分数来计算累积分布函数. expand(batch_shape, _instance=None) has_rsample icdf(value) 使用transform(s)计算逆累积分布函数, 并计算基分布的分数. log_prob(value) 通过反转变换并使用基本分布的分数和日志abs det jacobian计算分数来对样本进行评分 rsample(sample_shape=torch.Size([])) 如果分布参数是批处理的, 则生成sample_shape形状的重新参数化样本或sample_shape形状的重新参数化样本批次. 首先从基本分布中采样, 并对列表中的每个变换应用transform() sample(sample_shape=torch.Size([])) 如果分布参数是批处理的, 则生成sample_shape形样本或sample_shape形样本批处理. 首先从基本分布中采样, 并对列表中的每个变换应用transform(). support Uniform class torch.distributions.uniform.Uniform(low, high, validate_args=None) 基类: torch.distributions.distribution.Distribution 从半开区间[low, high)生成均匀分布的随机样本 例子: >>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0])) >>> m.sample() # uniformly distributed in the range [0.0, 5.0) tensor([ 2.3418]) 参数: low (float or Tensor) – 下限（含）. high (float or Tensor) – 上限(排除). arg_constraints = {'high': Dependent(), 'low': Dependent()} cdf(value) entropy() expand(batch_shape, _instance=None) has_rsample = True icdf(value) log_prob(value) mean rsample(sample_shape=torch.Size([])) stddev support variance Weibull class torch.distributions.weibull.Weibull(scale, concentration, validate_args=None) 基类: torch.distributions.transformed_distribution.TransformedDistribution 来自双参数Weibull分布的样本. Example >>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0])) >>> m.sample() # sample from a Weibull distribution with scale=1, concentration=1 tensor([ 0.4784]) 参数: scale (float or Tensor) – Scale (lambda). concentration (float or Tensor) – Concentration (k/shape). arg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)} entropy() expand(batch_shape, _instance=None) mean support = GreaterThan(lower_bound=0.0) variance KL Divergence torch.distributions.kl.kl_divergence(p, q) 计算Kullback-Leibler散度 对于两个分布. 参数: p (Distribution) – Distribution 对象. q (Distribution) – Distribution 对象. | 返回值: | 批量的 KL 散度, 形状为 batch_shape. | | 返回类型： | Tensor | | 异常: | NotImplementedError – 如果分布类型尚未通过注册 register_kl(). | torch.distributions.kl.register_kl(type_p, type_q) 装饰器注册kl_divergence()的成对函数 @register_kl(Normal, Normal) def kl_normal_normal(p, q): # insert implementation here Lookup返回由子类排序的最具体(type,type)匹配. 如果匹配不明确, 则会引发RuntimeWarning. 例如, 解决模棱两可的情况 @register_kl(BaseP, DerivedQ) def kl_version1(p, q): ... @register_kl(DerivedP, BaseQ) def kl_version2(p, q): ... 你应该注册第三个最具体的实现, 例如: register_kl(DerivedP, DerivedQ)(kl_version1) # Break the tie. 参数: type_p (type) – 子类 Distribution. type_q (type) – 子类 Distribution. Transforms class torch.distributions.transforms.Transform(cache_size=0) 有可计算的log det jacobians进行可逆变换的抽象类. 它们主要用于 torch.distributions.TransformedDistribution. 缓存对于其反转昂贵或数值不稳定的变换很有用. 请注意, 必须注意记忆值, 因为可以颠倒自动记录图. 例如, 以下操作有或没有缓存: y = t(x) t.log_abs_det_jacobian(x, y).backward() # x will receive gradients. 但是, 由于依赖性反转, 缓存时会出现以下错误: y = t(x) z = t.inv(y) grad(z.sum(), [y]) # error because z is x 派生类应该实现_call()或_inverse()中的一个或两个. 设置bijective=True的派生类也应该实现log_abs_det_jacobian() | 参数: | cache_size (int) – 缓存大小. 如果为零, 则不进行缓存. 如果是, 则缓存最新的单个值. 仅支持0和1 | | Variables: | domain (Constraint) – 表示该变换有效输入的约束. codomain (Constraint) – 表示此转换的有效输出的约束, 这些输出是逆变换的输入. bijective (bool) – 这个变换是否是双射的. 变换 t 是双射的 如果 t.inv(t(x)) == x 并且 t(t.inv(y)) == y 对于每一个 x 和 y. 不是双射的变形应该至少保持较弱的伪逆属性 t(t.inv(t(x)) == t(x) and t.inv(t(t.inv(y))) == t.inv(y). sign (int or Tensor) – 对于双射单变量变换, 它应该是+1或-1, 这取决于变换是单调递增还是递减. event_dim (int) – 变换event_shape中相关的维数. 这对于逐点变换应该是0, 对于在矢量上共同作用的变换是1, 对于在矩阵上共同作用的变换是2, 等等. inv 返回逆Transform. 满足 t.inv.inv is t. sign 如果适用, 返回雅可比行列式的符号. 一般来说, 这只适用于双射变换. log_abs_det_jacobian(x, y) 计算 log det jacobian log |dy/dx| 给定输入和输出. class torch.distributions.transforms.ComposeTransform(parts) 在一个链中组合多个转换. 正在组合的转换负责缓存. | 参数: | parts (list of Transform) – 列表 transforms. | class torch.distributions.transforms.ExpTransform(cache_size=0) 转换通过映射 . class torch.distributions.transforms.PowerTransform(exponent, cache_size=0) 转换通过映射 . class torch.distributions.transforms.SigmoidTransform(cache_size=0) 转换通过映射 and . class torch.distributions.transforms.AbsTransform(cache_size=0) 转换通过映射 . class torch.distributions.transforms.AffineTransform(loc, scale, event_dim=0, cache_size=0) 通过逐点仿射映射进行转换 . 参数: loc (Tensor or float) – Location. scale (Tensor or float) – Scale. event_dim (int) – 可选的 event_shape 大小. T对于单变量随机变量, 该值应为零, 对于矢量分布, 1应为零, 对于矩阵的分布, 应为2. class torch.distributions.transforms.SoftmaxTransform(cache_size=0) 从无约束空间到单纯形的转换, 通过 然后归一化. 这不是双射的, 不能用于HMC. 然而, 这主要是协调的（除了最终的归一化）, 因此适合于坐标方式的优化算法. class torch.distributions.transforms.StickBreakingTransform(cache_size=0) 将无约束空间通过 stick-breaking 过程转化为一个额外维度的单纯形. 这种变换是Dirichlet分布的破棒构造中的迭代sigmoid变换:第一个逻辑通过sigmoid变换成第一个概率和所有其他概率, 然后这个过程重复出现. 这是双射的, 适合在HMC中使用; 然而, 它将坐标混合在一起, 不太适合优化. class torch.distributions.transforms.LowerCholeskyTransform(cache_size=0) 将无约束矩阵转换为具有非负对角项的下三角矩阵. 这对于根据Cholesky分解来参数化正定矩阵是有用的. Constraints The following constraints are implemented: constraints.boolean constraints.dependent constraints.greater_than(lower_bound) constraints.integer_interval(lower_bound, upper_bound) constraints.interval(lower_bound, upper_bound) constraints.lower_cholesky constraints.lower_triangular constraints.nonnegative_integer constraints.positive constraints.positive_definite constraints.positive_integer constraints.real constraints.real_vector constraints.simplex constraints.unit_interval class torch.distributions.constraints.Constraint constraints 的抽象基类. constraint对象表示变量有效的区域, 例如, 其中可以优化变量 check(value) 返回一个字节张量 sample_shape + batch_shape 指示值中的每个事件是否满足此约束. torch.distributions.constraints.dependent_property alias of torch.distributions.constraints._DependentProperty torch.distributions.constraints.integer_interval alias of torch.distributions.constraints._IntegerInterval torch.distributions.constraints.greater_than alias of torch.distributions.constraints._GreaterThan torch.distributions.constraints.greater_than_eq alias of torch.distributions.constraints._GreaterThanEq torch.distributions.constraints.less_than alias of torch.distributions.constraints._LessThan torch.distributions.constraints.interval alias of torch.distributions.constraints._Interval torch.distributions.constraints.half_open_interval alias of torch.distributions.constraints._HalfOpenInterval Constraint Registry PyTorch 提供两个全局 ConstraintRegistry 对象 , 链接 Constraint 对象到 Transform 对象. 这些对象既有输入约束, 也有返回变换, 但是它们对双射性有不同的保证. biject_to(constraint) 查找一个双射的 Transform 从 constraints.real 到给定的 constraint. 返回的转换保证具有 .bijective = True 并且应该实现了 .log_abs_det_jacobian(). transform_to(constraint) 查找一个不一定是双射的 Transform 从 constraints.real 到给定的 constraint. 返回的转换不保证实现 .log_abs_det_jacobian(). transform_to()注册表对于对概率分布的约束参数执行无约束优化非常有用, 这些参数由每个分布的.arg_constraints指示. 这些变换通常会过度参数化空间以避免旋转; 因此, 它们更适合像Adam那样的坐标优化算法 loc = torch.zeros(100, requires_grad=True) unconstrained = torch.zeros(100, requires_grad=True) scale = transform_to(Normal.arg_constraints['scale'])(unconstrained) loss = -Normal(loc, scale).log_prob(data).sum() biject_to() 注册表对于Hamiltonian Monte Carlo非常有用, 其中来自具有约束. .support的概率分布的样本在无约束空间中传播, 并且算法通常是旋转不变的 dist = Exponential(rate) unconstrained = torch.zeros(100, requires_grad=True) sample = biject_to(dist.support)(unconstrained) potential_energy = -dist.log_prob(sample).sum() 注意 一个 transform_to 和 biject_to 不同的例子是 constraints.simplex: transform_to(constraints.simplex) 返回一个 SoftmaxTransform 简单地对其输入进行指数化和归一化; 这是一种廉价且主要是坐标的操作, 适用于像SVI这样的算法. 相反, biject_to(constraints.simplex) 返回一个 StickBreakingTransform 将其输入生成一个较小维度的空间; 这是一种更昂贵的数值更少的数值稳定的变换, 但对于像HM​​C这样的算法是必需的. biject_to 和 transform_to 对象可以通过用户定义的约束进行扩展, 并使用.register()方法进行转换, 作为单例约束的函数 transform_to.register(my_constraint, my_transform) 或作为参数化约束的装饰器: @transform_to.register(MyConstraintClass) def my_factory(constraint): assert isinstance(constraint, MyConstraintClass) return MyTransform(constraint.param1, constraint.param2) 您可以通过创建新的ConstraintRegistry创建自己的注册表. class torch.distributions.constraint_registry.ConstraintRegistry 注册表, 将约束链接到转换. register(constraint, factory=None) 在此注册表注册一个 Constraint 子类. 用法: @my_registry.register(MyConstraintClass) def construct_transform(constraint): assert isinstance(constraint, MyConstraint) return MyTransform(constraint.arg_constraints) 参数: constraint (subclass of Constraint) – [Constraint]的子类(#torch.distributions.constraints.Constraint \"torch.distributions.constraints.Constraint\"), 或者派生类的对象. factory (callable) – 可调用对象, 输入 constraint 对象返回 Transform 对象. 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"jit.html":{"url":"jit.html","title":"Torch Script","keywords":"","body":"Torch 脚本 译者：keyianpai 创建 Torch 脚本代码 将追踪和脚本化结合起来 Torch 脚本语言参考 类型 表达式 语句 变量解析 python值的使用 调试 内置函数 Torch脚本是一种从PyTorch代码创建可序列化和可优化模型的方法。用Torch脚本编写的代码可以从Python进程中保存，并在没有Python依赖的进程中加载。 我们提供了一些工具帮助我们将模型从纯Python程序逐步转换为可以独立于Python运行的Torch脚本程序。Torch脚本程序可以在其他语言的程序中运行（例如，在独立的C ++程序中）。这使得我们可以使用熟悉的工具在PyTorch中训练模型，而将模型导出到出于性能和多线程原因不能将模型作为Python程序运行的生产环境中去。 class torch.jit.ScriptModule(optimize=True) ScriptModule与其内部的Torch脚本函数可以通过两种方式创建： 追踪： 使用torch.jit.trace。torch.jit.trace以现有模块或python函数和样例输入作为参数，它会运行该python函数，记录函数在所有张量上执行的操作，并将记录转换为Torch脚本方法以作为ScriptModule的forward方法。创建的模块包含原始模块的所有参数。 例： import torch def foo(x, y): return 2*x + y traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3))) 注意 追踪一个 函数 将生成一个ScriptModule，该ScriptModule中包含一个实现被追踪函数的forward方法，但不包含任何参数。 例： import torch import torchvision traced_net = torch.jit.trace(torchvision.models.resnet18(), torch.rand(1, 3, 224, 224)) 注意 追踪仅记录在给定张量上运行给定函数时执行的操作。因此，返回的ScriptModule在任何输入上将运行相同的追踪图。当你的模块需要根据输入和/或模块状态运行不同的操作集时，这会产生一些重要的影响。例如， >* 追踪不会记录if语句或循环之类的控制流。当这个控制流在你的模块中保持不变时，这很好，它通常只是内联配置决策。但有时控制流实际上是模型本身的一部分。例如，序列到序列转换中的beam搜索是对（可变）输入序列长度的循环。 >*在返回的ScriptModule中，在training和eval模式中具有不同行为的操作将始终表现为处于追踪期间的模式。 在上述情况下，脚本化是一个比追踪更好的选择。 脚本化 你可以使用Python语法直接编写Torch脚本代码。你可以使用torch.jit.script注释（对于函数）或torch.jit.script_method注释（对于ScriptModule子类的方法）来编写Torch脚本代码。通过注释，被注释函数的主体将直接转换为Torch脚本。 Torch脚本本身只是Python语言的一个子集，因此不是python中的所有特性都可以使用，但我们提供了足够的功能来计算张量并执行与控制相关的操作。 例: import torch @torch.jit.script def foo(x, y): if x.max() &gt; y.max(): r = x else: r = y return r 注意 脚本 函数 注释将构造带有一个forward方法的ScriptModule，该forward方法实现被注释函数，并且不包含任何参数。 例： import torch class MyModule(torch.jit.ScriptModule): def __init__(self, N, M): super(MyModule, self).__init__() self.weight = torch.nn.Parameter(torch.rand(N, M)) @torch.jit.script_method def forward(self, input): return self.weight.mv(input) 例： import torch import torch.nn as nn import torch.nn.functional as F from torch.jit import ScriptModule, script_method, trace class MyScriptModule(ScriptModule): def __init__(self): super(MyScriptModule, self).__init__() # 通过追踪产生ScriptModule的 conv1和conv2 self.conv1 = trace(nn.Conv2d(1, 20, 5), torch.rand(1, 1, 16, 16)) self.conv2 = trace(nn.Conv2d(20, 20, 5), torch.rand(1, 20, 16, 16)) @script_method def forward(self, input): input = F.relu(self.conv1(input)) input = F.relu(self.conv2(input)) return input save(filename) 保存离线版本的模块，以便将来在其他的进程中使用。保存的模块会序列化当前模块的所有方法和参数。保存的模块可以使用torch :: jit :: load（filename）加载到C ++ API中，也可以使用torch.jit.load（filename）加载到Python API中。 为了能够保存模块，当前模块不能调用原生python函数。也就是说要保存模块的所有子模块也必须是ScriptModules的子类。 危险 所有模块，不论其设备，在加载过程中始终都会加载到CPU中。这与torch.load()的语义不同，将来可能会发生变化。 torch.jit.load(f, map_location=None) 使用load加载之前用save保存的ScriptModule。 所有先前保存的模块，不论其设备，首先加载到CPU上，然后移动到之前保存它们的设备上。如果此操作失败（例如，运行时系统没有某些设备），则会引发异常。此时可以使用map_location参数将存储重新映射到另一组设备。与torch.load()相比，此函数中的map_location被简化为只接受字符串（例如'cpu'，'cuda：0'）或torch.device（例如，torch.device（'cpu'）） 参数： f – 文件类对象（必须实现read，readline，tell和seek），或为文件名的字符串 map_location – 可以是一个字符串（例如，'cpu'，'cuda：0'），一个设备（例如，torch.device（'cpu'）） 返回值: ScriptModule 对象. 例： >>> torch.jit.load('scriptmodule.pt') # 从io.BytesIO对象加载ScriptModule >>> with open('scriptmodule.pt', 'rb') as f: buffer = io.BytesIO(f.read()) # 将所有张量加载到原来的设备上 >>> torch.jit.load(buffer) # 用设备将所有张量加载到CPU上 >>> torch.jit.load(buffer, map_location=torch.device('cpu')) # 用字符串将所有张量加载到CPU上 >>> torch.jit.load(buffer, map_location='cpu') torch.jit.trace(func, example_inputs, optimize=True, check_trace=True, check_inputs=None, check_tolerance=1e-05, _force_outplace=False) 追踪一个函数并返回一个使用即时编译优化过的可执行追踪。 警告 追踪仅正确记录不依赖于数据的函数和模块（例如，对张量中的数据进行条件判断），并且没有任何未追踪的外部依赖性（例如，执行输入/输出或访问全局变量）。如果你追踪此类模型，则可能会在随后的模型调用中静默获取不正确的结果。当执行可能生成错误追踪的内容时，追踪器将尝试发出警告。 参数： func (callable or torch.nn.Module) – 将使用example_inputs作为输入运行的python函数或torch.nn.Module。参数和返回值必须是Tensor或（嵌套的）包含张量的元组。 example_inputs (tuple) – 在追踪时将传递给函数的示例输入元组。假设被追踪操作支持这些类型和形状的情况下，生成的追踪可以在不同类型和形状的输入下运行。 example_inputs也可以是单个Tensor，这种情况下，它会自动包装到元组中。 关键字参数： optimize (bool, optional) – 是否应用优化。默认值：True。 check_trace (bool, optional) – 检查被追踪代码在相同输入下输出是否相同。默认值：True。你可以在某些情况下禁用此功能。例如，你的网络包含非确定性操作，或者你确定网络正确。 check_inputs (list of tuples__, optional) – 应该用于根据预期检查追踪的输入参数元组列表。每个元组相当于一个将在args中指定的输入参数集合。为获得最佳结果，请传递一组检查输入表示你期望网络接受的形状和输入类型范围。如果未指定，则用原来的args检查。 check_tolerance (float, optional) – 在检查过程中使用的浮点比较容差。用于放松检查严格性。 返回值： 含有forward()方法的ScriptModule对象，该方法包含被追踪代码。当func是torch.nn.Module时，返回的ScriptModule具有与原始模块相同的子模块和参数集。 例： >>> def f(x): ... return x * 2 >>> traced_f = torch.jit.trace(f, torch.rand(1)) 在许多情况下，追踪或脚本是转换模型的更简单方法。我们允许你将追踪和脚本组合使用以满足模型特定部分的特定要求。 脚本函数可以调用被追踪函数。当你需要使用控制流控制简单的前馈模型时，这尤其有用。例如，序列到序列模型的beam搜索通常将以脚本编写，但可以调用使用追踪生成的编码器模块。 例： import torch def foo(x, y): return 2 * x + y traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3))) @torch.jit.script def bar(x): return traced_foo(x, x) 被追踪函数也可以调用脚本函数。当模型大体是一个前馈网络，只有模型的一小部分需要一些控制流时，这也很有用。由追踪函数调用的脚本函数内部的控制流会被正确地保留。 例： import torch @torch.jit.script def foo(x, y): if x.max() > y.max(): r = x else: r = y return r def bar(x, y, z): return foo(x, y) + z traced_bar = torch.jit.trace(bar, (torch.rand(3), torch.rand(3), torch.rand(3)) 组合也适用于模块，例如可以从脚本模块的方法调用追踪来生成子模块： 例： import torch import torchvision class MyScriptModule(torch.jit.ScriptModule): def __init__(self): super(MyScriptModule, self).__init__() self.means = torch.nn.Parameter(torch.tensor([103.939, 116.779, 123.68]) .resize_(1, 3, 1, 1)) self.resnet = torch.jit.trace(torchvision.models.resnet18(), torch.rand(1, 3, 224, 224)) @torch.jit.script_method def forward(self, input): return self.resnet(input - self.means) Torch脚本是Python的一个子集，可以直接编写（使用@script注释），也可以通过追踪从Python代码自动生成。使用追踪时，代码会自动转换为Python的这个子集，方法是仅记录和执行张量上的实际运算符，并丢弃其他Python代码。 使用@script注释直接编写Torch脚本时，程序员必须只使用Torch脚本支持的Python子集。本节以语言参考的形式介绍Torch脚本支持的功能。本参考中未提及的Python的其他功能都不是Torch脚本的一部分。 作为Python的一个子集，任何有效的Torch脚本函数也是一个有效的Python函数。因此你可以删除@script注释后使用标准Python工具（如pdb）调试函数。反之则不然：有许多有效的python程序不是有效的Torch脚本程序。Torch脚本专注于在Torch中表示神经网络模型所需的Python特性。 PYTORCH_JIT=1 设置环境变量PYTORCH_JIT = 0将禁用所有脚本和追踪注释。如果在ScriptModule中遇到难以调试的错误，则可以使用此标志强制使用原生Python运行所有内容。此时可使用pdb之类的工具调试代码。 Torch脚本与完整Python语言之间的最大区别在于Torch脚本仅支持表达神经网络模型所需的一些类型。特别地，Torch脚本支持： Tensor 具有任何dtype，维度或backend的PyTorch张量。 Tuple[T0, T1, ...] 包含子类型T0，T1等的元组（例如Tuple [Tensor，Tensor]）。 int 标量整数 float 标量浮点数 List[T] 所有成员都是T类型的列表T 与Python不同，Torch脚本函数中的每个变量都必须具有一个静态类型。这样以便于优化Torch脚本功能。 例： @torch.jit.script def an_error(x): if x: r = torch.rand(1) else: r = 4 return r # 类型不匹配：在条件为真时r为Tensor类型 # 而为假时却是int类型 默认情况下，Torch脚本函数的所有参数都为Tensor类型，因为这是模块中最常用的类型。要将Torch脚本函数的参数指定为另一种类型，可以通过MyPy风格的注释使用上面列出的类型： 例： @torch.jit.script def foo(x, tup): # type: (int, Tuple[Tensor, Tensor]) -> Tensor t0, t1 = tup return t0 + t1 + x print(foo(3, (torch.rand(3), torch.rand(3)))) 注意 也可以使用Python 3类型注释来注释类型。在示例中，我们使用基于注释的注释来确保对Python 2的兼容性。 Torch脚本支持以下Python表达式 字面常量 True, False, None, 'string literals', \"string literals\", 字面值3（解释为int）3.4（解释为float） 变量 a 注意 请参阅变量解析，了解变量的解析方式。 元组构造 (3, 4), (3,) 列表构造 [3, 4], [], [torch.rand(3), torch.rand(4)] 注意 空列表具有类型List[Tensor] 。其他列表字面常量的类型由成员的类型推出。 算术运算符 a + b a - b a * b a / b a ^ b a @ b 比较运算符 a == b a != b a a > b a a >= b 逻辑运算符 a and b a or b not b 索引 t[0] t[-1] t[0:2] t[1:] t[:1] t[:] t[0, 1] t[0, 1:2] t[0, :1] t[-1, 1:, 0] t[1:, -1, 0] t[i:j, i] 注意 Torch脚本目前不支持原地修改张量，因此对张量索引只能出现在表达式的右侧。 函数调用 调用内置函数：torch.rand(3, dtype=torch.int) 调用其他脚本函数： import torch @torch.jit.script def foo(x): return x + 1 @torch.jit.script def bar(x): return foo(x) 方法调用 调用内置类型的方法，如tensor: x.mm(y) 在ScriptModule中定义Script方法时，使用@script_method批注。Script方法可以调用模块内其他方法或子模块的方法。 直接调用子模块（例如self.resnet（input））等同于调用其forward方法（例如self.resnet.forward（input）） import torch class MyScriptModule(torch.jit.ScriptModule): def __init__(self): super(MyScriptModule, self).__init__() self.means = torch.nn.Parameter(torch.tensor([103.939, 116.779, 123.68]) .resize_(1, 3, 1, 1)) self.resnet = torch.jit.trace(torchvision.models.resnet18(), torch.rand(1, 3, 224, 224)) @torch.jit.script_method def helper(self, input): return self.resnet(input - self.means) @torch.jit.script_method def forward(self, input): return self.helper(input) If 表达式 x if x > y else y 类型转换 float(ten), int(3.5), bool(ten) 访问模块参数 self.my_parameter self.my_submodule.my_parameter Torch脚本支持以下类型的语句： 简单赋值 a = b a += b # short-hand for a = a + b, does not operate in-place on a a -= b 模式匹配赋值 a, b = tuple_or_list a, b, *c = a_tuple Print 语句 print(\"the result of an add:\", a + b) If 语句 if a &lt; 4: r = -a elif a &lt; 3: r = a + a else: r = 3 * a While 循环 a = 0 while a &lt; 4: print(a) a += 1 带 range 的for循环 x = 0 for i in range(10): x *= i 注意 脚本目前不支持对一般可迭代对象（如列表或张量）进行迭代，也不支持range起始与增量参数，这些将在未来版本中添加。 对元组的for循环： tup = (3, torch.rand(4)) for x in tup: print(x) 注意 对于元组循环将展开循环，为元组的每个成员生成一个循环体。循环体内必须确保每个成员类型正确。 对常量 torch.nn.ModuleList 的for循环 class SubModule(torch.jit.ScriptModule): def __init__(self): super(Sub, self).__init__() self.weight = nn.Parameter(torch.randn(2)) @torch.jit.script_method def forward(self, input): return self.weight + input class MyModule(torch.jit.ScriptModule): __constants__ = ['mods'] def __init__(self): super(MyModule, self).__init__() self.mods = torch.nn.ModuleList([SubModule() for i in range(10)]) @torch.jit.script_method def forward(self, v): for module in self.mods: v = m(v) return v 注意 要在@script_method中使用模块列表，必须通过将属性的名称添加到类型的__constants__列表来将其标记为常量。ModuleList上的for循环在编译时使用常量模块列表的每个成员展开循环体。 Return 语句 return a, b 注意 return语句必须作为函数的最后一个成员，而不能出现在函数的其他位置。此限制将在以后删除。 Torch脚本支持Python变量解析（即作用域）规则的子集。局部变量的行为与Python中的相同，但变量必须在函数的所有路径中具有相同类型。如果变量在if语句的不同侧具有不同的类型，则在if语句结束后使用它会抱错。 类似地，如果仅在函数的某些执行路径上定义变量也会出错。 例： @torch.jit.script def foo(x): if x 定义函数的非局部变量在编译时解析为Python值。然后，用Python值的使用中的规则将这些值转换为Torch脚本值。 为了使编写Torch脚本更方便，我们允许脚本代码引用周围的Python值。例如，当我们引用torch时，Torch脚本编译器实际上在声明函数时将其解析为Python的torch模块。这些Python值不是Torch脚本的一部分，它们在编译时被转换成Torch脚本支持的原始类型。本节介绍在Torch脚本中访问Python值时使用的规则。它们依赖于引用的python值的动态类型。 函数 Torch脚本可以调用python函数。此功能在将模型逐步转换为脚本时非常有用。可以将模型中的函数逐个转成脚本，保留对其余Python函数的调用。这样，在逐步转换的过程中你可以随时检查模型的正确性。 例： def foo(x): print(\"I am called with {}\".format(x)) import pdb; pdb.set_trace() return x @torch.jit.script def bar(x) return foo(x + 1) 注意 不能在包含Python函数调用的ScriptModule上调用save。该功能仅用于调试，应在保存之前删除调用或将其转换为脚本函数。 Python模块的属性查找 Torch脚本可以在模块上查找属性。像torch.add这样的内置函数就以这种方式访问。这允许Torch脚本调用其他模块中定义的函数。 Python 中定义的常量 Torch脚本还提供了使用Python常量的方法。这可用于将超参数硬编码到函数中，或用于定义通用常量。有两种方法可以指定Python值为常量。 查找的值为模块的属性,例如：math.pi 可以将ScriptModule的属性标记为常量，方法是将其列为类的__constants__属性成员： 例： class Foo(torch.jit.ScriptModule): __constants__ = ['a'] def __init__(self): super(Foo, self).__init__(False) self.a = 1 + 4 @torch.jit.ScriptModule def forward(self, input): return self.a + input 支持的Python常量值有 int bool torch.device torch.layout torch.dtype 包含支持类型的元组 torch.nn.ModuleList ，可以将其用在Torch 脚本for循环中 禁用JIT以方便调试 可以通过将PYTORCH_JIT环境变量值设置为0禁用所有JIT模式（追踪和脚本化）以便在原始Python中调试程序。下面是一个示例脚本： @torch.jit.script def scripted_fn(x : torch.Tensor): for i in range(12): x = x + x return x def fn(x): x = torch.neg(x) import pdb; pdb.set_trace() return scripted_fn(x) traced_fn = torch.jit.trace(fn, (torch.rand(4, 5),)) traced_fn(torch.rand(3, 4)) 为了使用PDB调试此脚本。我们可以全局禁用JIT，这样我们就可以将@script函数作为普通的python函数调用而不会编译它。如果上面的脚本名为disable_jit_example.py，我们这样调用它： $ PYTORCH_JIT=0 python disable_jit_example.py 这样,我们就能够作为普通的Python函数步入@script函数。 解释图 TorchScript使用静态单一指派（SSA）中间表示（IR）来表示计算。这种格式的指令包括ATen（PyTorch的C ++后端）运算符和其他原始运算符，包括循环和条件的控制流运算符。举个例子： @torch.jit.script def foo(len): # type: (int) -> torch.Tensor rv = torch.zeros(3, 4) for i in range(len): if i 具有单个forward方法的ScriptModule具有graph属性，你可以使用该图来检查表示计算的IR。如果ScriptModule有多个方法，则需要访问方法本身的.graph属性。例如我们可以通过访问.bar.graph来检查ScriptModule上名为bar的方法的图。 上面的示例脚本生成图形： graph(%len : int) { %13 : float = prim::Constant[value=1]() %10 : int = prim::Constant[value=10]() %2 : int = prim::Constant[value=4]() %1 : int = prim::Constant[value=3]() %3 : int[] = prim::ListConstruct(%1, %2) %4 : int = prim::Constant[value=6]() %5 : int = prim::Constant[value=0]() %6 : int[] = prim::Constant[value=[0, -1]]() %rv.1 : Dynamic = aten::zeros(%3, %4, %5, %6) %8 : int = prim::Constant[value=1]() %rv : Dynamic = prim::Loop(%len, %8, %rv.1) block0(%i : int, %12 : Dynamic) { %11 : int = aten::lt(%i, %10) %rv.4 : Dynamic = prim::If(%11) block0() { %14 : int = prim::Constant[value=1]() %rv.2 : Dynamic = aten::sub(%12, %13, %14) -> (%rv.2) } block1() { %16 : int = prim::Constant[value=1]() %rv.3 : Dynamic = aten::add(%12, %13, %16) -> (%rv.3) } %19 : int = prim::Constant[value=1]() -> (%19, %rv.4) } return (%rv); } 以指令％rv.1：Dynamic = aten :: zeros（％3，％4，％5，％6）为例。％rv.1：Dynamic将输出分配给名为rv.1的（唯一）值，该值是动态类型，即我们不知道它的具体形状。aten :: zeros是运算符（相当于torch.zeros），它的输入列表（％3，％4，％5，％6）指定范围中的哪些值应作为输入传递。内置函数（如aten :: zeros）的模式可以在内置函数中找到。 注意，运算符也可以有关联的block，如prim :: Loop和prim :: If运算符。在图形打印输出中，这些运算符被格式化以反映与其等价的源代码形式，以便于调试。 可以检查图以确认ScriptModule描述的计算是正确的，方法如下所述。 追踪的边缘情况 在一些边缘情况下一些Python函数/模块的追踪不能代表底层代码。这些情况可以包括： 追踪依赖于输入的控制流（例如张量形状） 追踪张量视图的就地操作（例如，在分配的左侧进行索引） 请注意，这些情况在将来版本中可能是可追踪的。 自动追踪检查 通过在torch.jit.trace()API上使用check_inputs，是自动捕获追踪中错误的一种方法。 check_inputs是用于重新追踪计算并验证结果的输入元组列表。例如： def loop_in_traced_fn(x): result = x[0] for i in range(x.size(0)): result = result * x[i] return result inputs = (torch.rand(3, 4, 5),) check_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)] traced = torch.jit.trace(loop_in_traced_fn, inputs, check_inputs=check_inputs) 上面代码会为我们提供以下诊断信息： ERROR: Graphs differed across invocations! Graph diff: graph(%0 : Dynamic) { %1 : int = prim::Constant[value=0]() %2 : int = prim::Constant[value=0]() %3 : Dynamic = aten::select(%0, %1, %2) %4 : int = prim::Constant[value=0]() %5 : int = prim::Constant[value=0]() %6 : Dynamic = aten::select(%0, %4, %5) %7 : Dynamic = aten::mul(%3, %6) %8 : int = prim::Constant[value=0]() %9 : int = prim::Constant[value=1]() %10 : Dynamic = aten::select(%0, %8, %9) %11 : Dynamic = aten::mul(%7, %10) %12 : int = prim::Constant[value=0]() %13 : int = prim::Constant[value=2]() %14 : Dynamic = aten::select(%0, %12, %13) %15 : Dynamic = aten::mul(%11, %14) + %16 : int = prim::Constant[value=0]() + %17 : int = prim::Constant[value=3]() + %18 : Dynamic = aten::select(%0, %16, %17) + %19 : Dynamic = aten::mul(%15, %18) - return (%15); ? ^ + return (%19); ? ^ } 此消息表明，我们第一次追踪函数和使用check_inputs追踪函数时的计算存在差异。事实上，loop_in_traced_fn体内的循环取决于输入x的形状，因此当我们输入不同形状的x时，轨迹会有所不同。 在这种情况下，可以使用脚本捕获此类数据相关控制流： def fn(x): result = x[0] for i in range(x.size(0)): result = result * x[i] return result inputs = (torch.rand(3, 4, 5),) check_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)] scripted_fn = torch.jit.script(fn) print(scripted_fn.graph) for input_tuple in [inputs] + check_inputs: torch.testing.assert_allclose(fn(*input_tuple), scripted_fn(*input_tuple)) 上面代码会为我们提供以下信息： graph(%x : Dynamic) { %1 : int = prim::Constant[value=0]() %2 : int = prim::Constant[value=0]() %result.1 : Dynamic = aten::select(%x, %2, %1) %4 : int = aten::size(%x, %1) %5 : int = prim::Constant[value=1]() %result : Dynamic = prim::Loop(%4, %5, %result.1) block0(%i : int, %7 : Dynamic) { %9 : int = prim::Constant[value=0]() %10 : Dynamic = aten::select(%x, %9, %i) %result.2 : Dynamic = aten::mul(%7, %10) %12 : int = prim::Constant[value=1]() -> (%12, %result.2) } return (%result); } 追踪器警告 追踪器会在追踪计算中对有问题的模式生成警告。例如，追踪包含在Tensor的切片（视图）上就地赋值操作的函数： def fill_row_zero(x): x[0] = torch.rand(*x.shape[1:2]) return x traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),)) print(traced.graph) 这会出现如下警告和一个简单返回输入的图： fill_row_zero.py:4: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe. x[0] = torch.rand(*x.shape[1:2]) fill_row_zero.py:6: TracerWarning: Output nr 1\\. of the traced function does not match the corresponding output of the Python function. Detailed error: Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 1] (0.09115803241729736 vs. 0.6782537698745728) and 3 other locations (33.00%) traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),)) graph(%0 : Float(3, 4)) { return (%0); } 我们可以通过使用torch.cat返回结果张量避免就地更新问题： def fill_row_zero(x): x = torch.cat((torch.rand(1, *x.shape[1:2]), x[1:2]), dim=0) return x traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),)) print(traced.graph) Torch脚本支持部分PyTorch内置张量和神经网络函数。 Tensor上的大多数方法以及torch命名空间中的函数都可用。 torch.nn.functional中的许多函数也可用。 我们目前不提供像 Linear 或 Conv 模块之类内置ScriptModule,此功能将在未来开发。目前我们建议使用torch.jit.trace将标准的torch.nn模块转换为ScriptModule。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"multiprocessing.html":{"url":"multiprocessing.html","title":"多进程包 - torch.multiprocessing","keywords":"","body":"多进程包 - torch.multiprocessing 译者：hijkzzz torch.multiprocessing 是一个本地 multiprocessing 模块的包装. 它注册了自定义的reducers, 并使用共享内存为不同的进程在同一份数据上提供共享的视图. 一旦 tensor/storage 被移动到共享内存 (见 share_memory_()), 将其发送到任何进程不会造成拷贝开销. 此 API 100% 兼容原生模块 - 所以足以将 import multiprocessing 改成 import torch.multiprocessing 使得所有的 tensors 通过队列发送或者使用其它共享机制, 移动到共享内存. 因为 APIs 的相似性, 我们没有为此包提供足够的文档, 所以推荐参考非常优秀的原生进程模块文档. 警告 如果主进程意外退出 (比如 因为一个信号的到来), Python’s multiprocessing 有时候会无法请理它的子进程. 这是一个众所周知的警告, 因此，如果你在中断解释器后发现任何资源泄漏，这可能意味着你刚刚发生了这种情况. 策略管理 torch.multiprocessing.get_all_sharing_strategies() 返回当前系统支持的共享策略的集合. torch.multiprocessing.get_sharing_strategy() 返回当前的 CPU tensors 共享策略. torch.multiprocessing.set_sharing_strategy(new_strategy) 设置一个新的 CPU tensors 共享策略. 参数: new_strategy (str) – 选定策略的名字. 必须是 get_all_sharing_strategies() 的返回值中的一个. 共享 CUDA tensors 在进程间共享 CUDA tensors 仅仅在 Python 3 中被支持, 使用 spawn 或者 forkserver 启动方法. multiprocessing 在 Python 2 中只能使用 fork 创建新进程, 然而 CUDA 运行时不支持它. 警告 CUDA API要求导出到其他进程的分配只要被其他进程使用就保持有效. 您应该小心，并确保共享的CUDA tensor在必要时不会超出范围. 共享模型参数不应该是一个问题，但是传递其他类型的数据应该小心。注意，此限制不适用于共享CPU内存. 共享策略 本节简要概述不同的共享策略是如何工作的。注意，它只适用于CPU tensor——CUDA tensor总是使用CUDA API，因为这是它们可以共享的唯一方式。 文件描述符 - file_descriptor 注意 这是默认策略(macOS和OS X因为不支持除外) 该策略将使用文件描述符作为共享内存句柄。每当一个存储被移动到共享内存时，从shm open获得的文件描述符就会被对象缓存，当它被发送到其他进程时，文件描述符就会被传输(例如通过UNIX套接字)到它。接收者还将缓存文件描述符并mmap它，以获得存储数据上的共享视图。 请注意，如果共享了很多tensor，那么这种策略将在大多数情况下打开大量的文件描述符。如果您的系统对打开的文件描述符的数量限制很低，并且您不能提高它们的数量，那么您应该使用file_system策略。 文件系统 - file_system 该策略将使用指定给shm open的文件名来标识共享内存区域。这样做的好处是不需要实现缓存从中获得的文件描述符，但同时容易导致共享内存泄漏。文件不能在创建之后立即删除，因为其他进程需要访问它来打开它们的视图。如果进程致命地崩溃或被杀死，并且不调用存储析构函数，那么文件将保留在系统中。这是非常严重的，因为它们会一直使用内存，直到系统重新启动，或者重新手动释放。 为了解决共享内存文件泄漏的问题，torch.multiprocessing将生成一个名为torch_shm_manager的守护进程，它将自己与当前进程组隔离，并跟踪所有共享内存分配。连接到它的所有进程退出后，它将等待一段时间以确保没有新的连接，并将遍历组分配的所有共享内存文件。如果它发现其中任何一个仍然存在，就会解除它们的分配。我们对这种方法进行了测试，证明它对各种故障都具有鲁棒性。 不过，如果您的系统有足够高的限制，并且file_descriptor是受支持的策略，我们不建议切换到这个策略。 Spawning 子线程 注意 仅支持 Python >= 3.4. 依赖于 spawn 启动方法(在 Python 的 multiprocessing 包中)。 通过创建进程实例并调用join来等待它们完成，可以生成大量子进程来执行某些功能。这种方法在处理单个子进程时工作得很好，但在处理多个进程时可能会出现问题。 也就是说，顺序连接进程意味着它们将顺序终止。如果没有，并且第一个进程没有终止，那么进程终止将不被注意。 此外，没有用于错误传播的本地工具. 下面的spawn函数解决了这些问题，并负责错误传播、无序终止，并在检测到其中一个错误时主动终止进程. torch.multiprocessing.spawn(fn, args=(), nprocs=1, join=True, daemon=False) Spawns nprocs 进程运行 fn 使用参数 args. 如果其中一个进程以非零退出状态退出，则会杀死其余进程，并引发异常，导致终止。在子进程中捕获异常的情况下，将转发该异常，并将其跟踪包含在父进程中引发的异常中。 参数: fn (function) – 函数被称为派生进程的入口点。必须在模块的顶层定义此函数，以便对其进行pickle和派生。这是多进程强加的要求。 该函数称为fn(i， *args)，其中i是进程索引，args是传递的参数元组。 args (tuple) – 传递给 fn 的参数. nprocs (int) – 派生的进程数. join (bool) – 执行一个阻塞的join对于所有进程. daemon (bool) – 派生进程守护进程标志。如果设置为True，将创建守护进程. 返回值: None 如果 join 是 True, SpawnContext 如果 join 是 False class torch.multiprocessing.SpawnContext 由 spawn() 返回, 当 join=False. join(timeout=None) 尝试连接此派生上下文中的一个或多个进程。如果其中一个进程以非零退出状态退出，则此函数将杀死其余进程，并引发异常，导致第一个进程退出。 返回 True如果所有进程正常退出, False 如果有更多的进程需要 join. Parameters: timeout (float) – 放弃等待的最长时间. 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"bottleneck.html":{"url":"bottleneck.html","title":"torch.utils.bottleneck","keywords":"","body":"torch.utils.bottleneck 译者: belonHan torch.utils.bottleneck是 调试瓶颈bottleneck时首先用到的工具.它总结了python分析工具与PyTorch自动梯度分析工具在脚本运行中情况. 在命令行运行如下命令 python -m torch.utils.bottleneck /path/to/source/script.py [args] 其中 [args] 是script.py脚本的参数(任意个数).运行python -m torch.utils.bottleneck -h命令获取更多帮助说明. 警告 请确保脚本在分析时能够在有限时间内退出. 警告 当运行CUDA代码时，由于CUDA内核的异步特性, cProfile的输出 和cpu模式的autograd分析工具可能无法显示正确的计时: 报告的CPU时间 是用于启动内核的时间,不包括在GPU上执行的时间。 在常规cpu模式分析器下，同步操作是非常昂贵的。在这种无法准确计时的情况下，可以使用cuda模式的autograd分析工具。 注意 选择查看哪个分析工具的输出结果(CPU模式还是CUDA模式) ,首先应确定脚本是不是CPU密集型CPU-bound(“CPU总时间远大于CUDA总时间”)。如果是cpu密集型，选择查看cpu模式的结果。相反，如果大部分时间都运行在GPU上，再查看CUDA分析结果中相应的CUDA操作。 当然，实际情况取决于您的模型，可能会更复杂，不属于上面两种极端情况。除了分析结果之外,可以尝试使用nvprof命令查看torch.autograd.profiler.emit_nvtx()的结果.然而需要注意NVTX的开销是非常高的,时间线经常会有严重的偏差。 警告 如果您在分析CUDA代码, bottleneck运行的第一个分析工具 (cProfile),它的时间中会包含CUDA的启动(CUDA缓存分配)时间。当然，如果CUDA启动时间远小于代码的中瓶颈,这就被可以忽略。 更多更复杂关于分析工具的使用方法(比如多GPU),请点击https://docs.python.org/3/library/profile.html 或者 torch.autograd.profiler.profile(). 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"checkpoint.html":{"url":"checkpoint.html","title":"torch.utils.checkpoint","keywords":"","body":"torch.utils.checkpoint 译者: belonHan 注意 checkpointing的实现方法是在向后传播期间重新运行已被checkpint的前向传播段。 所以会导致像RNG这类(模型)的持久化的状态比实际更超前。默认情况下，checkpoint包含了使用RNG状态的逻辑(例如通过dropout)，与non-checkpointed传递相比,checkpointed具有更确定的输出。RNG状态的存储逻辑可能会导致一定的性能损失。如果不需要确定的输出，设置全局标志(global flag) torch.utils.checkpoint.preserve_rng_state=False 忽略RNG状态在checkpoint时的存取。 torch.utils.checkpoint.checkpoint(function, *args) checkpoint模型或模型的一部分 checkpoint通过计算换内存空间来工作。与向后传播中存储整个计算图的所有中间激活不同的是，checkpoint不会保存中间激活部分，而是在反向传递中重新计算它们。它被应用于模型的任何部分。 具体来说，在正向传播中，function将以torch.no_grad()方式运行 ，即不存储中间激活,但保存输入元组和 function的参数。在向后传播中，保存的输入变量以及 function会被取回，并且function在正向传播中被重新计算.现在跟踪中间激活，然后使用这些激活值来计算梯度。 Warning 警告 Checkpointing 在 torch.autograd.grad()中不起作用, 仅作用于 torch.autograd.backward(). 警告 如果function在向后执行和前向执行不同，例如,由于某个全局变量，checkpoint版本将会不同，并且无法被检测到。 参数: function - 描述在模型的正向传递或模型的一部分中运行的内容。它也应该知道如何处理作为元组传递的输入。例如，在LSTM中，如果用户通过 ，应正确使用第一个输入作为第二个输入(activation, hidden)functionactivationhidden args – 包含输入的元组function Returns: 输出 torch.utils.checkpoint.checkpoint_sequential(functions, segments, *inputs) 用于checkpoint sequential模型的辅助函数 Sequential模型按顺序执行模块/函数。因此，我们可以将这样的模型划分为不同的段(segment)，并对每个段进行checkpoint。除最后一段外的所有段都将以torch.no_grad()方式运行，即，不存储中间活动。将保存每个checkpoint段的输入，以便在向后传递中重新运行该段。 checkpointing工作方式: checkpoint(). 警告 Checkpointing无法作用于torch.autograd.grad(), 只作用于torch.autograd.backward(). 参数: functions – 按顺序执行的模型， 一个 torch.nn.Sequential对象,或者一个由modules或functions组成的list。 segments – 段的数量 inputs – 输入,Tensor组成的元组 Returns: 按顺序返回每个*inputs的结果 例子 >>> model = nn.Sequential(...) >>> input_var = checkpoint_sequential(model, chunks, input_var) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"docs_cpp_extension.html":{"url":"docs_cpp_extension.html","title":"torch.utils.cpp_extension","keywords":"","body":"torch.utils.cpp_extension 译者: belonHan torch.utils.cpp_extension.CppExtension(name, sources, *args, **kwargs) 创建一个C++的setuptools.Extension。 便捷地创建一个setuptools.Extension具有最小（但通常是足够）的参数来构建C++扩展的方法。 所有参数都被转发给setuptools.Extension构造函数。 例子 >>> from setuptools import setup >>> from torch.utils.cpp_extension import BuildExtension, CppExtension >>> setup( name='extension', ext_modules=[ CppExtension( name='extension', sources=['extension.cpp'], extra_compile_args=['-g'])), ], cmdclass={ 'build_ext': BuildExtension }) torch.utils.cpp_extension.CUDAExtension(name, sources, *args, **kwargs) 为CUDA/C++创建一个setuptools.Extension。 创建一个setuptools.Extension用于构建CUDA/C ++扩展的最少参数（但通常是足够的）的便捷方法。这里包括CUDA路径，库路径和运行库。 所有参数都被转发给setuptools.Extension构造函数。 所有参数都被转发给setuptools.Extension构造函数。 例子 >>> from setuptools import setup >>> from torch.utils.cpp_extension import BuildExtension, CUDAExtension >>> setup( name='cuda_extension', ext_modules=[ CUDAExtension( name='cuda_extension', sources=['extension.cpp', 'extension_kernel.cu'], extra_compile_args={'cxx': ['-g'], 'nvcc': ['-O2']}) ], cmdclass={ 'build_ext': BuildExtension }) torch.utils.cpp_extension.BuildExtension(*args, **kwargs) 自定义setuptools构建扩展。 setuptools.build_ext子类负责传递所需的最小编译器参数（例如-std=c++11）以及混合的C ++/CUDA编译（以及一般对CUDA文件的支持）。 当使用BuildExtension时，它将提供一个用于extra_compile_args（不是普通列表）的词典，通过语言（cxx或cuda）映射到参数列表提供给编译器。这样可以在混合编译期间为C ++和CUDA编译器提供不同的参数。 torch.utils.cpp_extension.load(name, sources, extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda=None, is_python_module=True) 即时加载(JIT)PyTorch C ++扩展。 为了加载扩展，会创建一个Ninja构建文件，该文件用于将指定的源编译为动态库。随后将该库作为模块加载到当前Python进程中，并从该函数返回，以备使用。 默认情况下，构建文件创建的目录以及编译结果库是&lt;tmp&gt;/torch_extensions/&lt;name&gt;，其中&lt;tmp&gt;是当前平台上的临时文件夹以及&lt;name&gt;为扩展名。这个位置可以通过两种方式被覆盖。首先，如果TORCH_EXTENSIONS_DIR设置了环境变量，它将替换&lt;tmp&gt;/torch_extensions并将所有扩展编译到此目录的子文件夹中。其次，如果build_directory函数设置了参数，它也将覆盖整个路径，即,库将直接编译到该文件夹中。 要编译源文件，使用默认的系统编译器（c++），可以通过设置CXX环境变量来覆盖它。将其他参数传递给编译过程，extra_cflags或者extra_ldflags可以提供。例如，要通过优化来编译您的扩展，你可以传递extra_cflags=['-O3']，也可以使用 extra_cflags传递进一步包含目录。 提供了混合编译的CUDA支持。只需将CUDA源文件（.cu或.cuh）与其他源一起传递即可。这些文件将被检测，并且使用nvcc而不是C ++编译器进行编译。包括将CUDA lib64目录作为库目录传递并进行cudart链接。您可以将其他参数传递给nvcc extra_cuda_cflags，就像使用C ++的extra_cflags一样。使用了各种原始方法来查找CUDA安装目录，通常情况下可以正常运行。如果不可以，最好设置CUDA_HOME环境变量。 参数: name - 要构建的扩展名。这个必须和pybind11模块的名字一样！ sources - C++源文件的相对或绝对路径列表。 extra_cflags - 编译器参数的可选列表，用于转发到构建。 extra_cuda_cflags - 编译器标记的可选列表，在构建CUDA源时转发给nvcc。 extra_ldflags - 链接器参数的可选列表，用于转发到构建。 extra_include_paths - 转发到构建的包含目录的可选列表。 build_directory - 可选路径作为构建区域。 verbose - 如果为True，打开加载步骤的详细记录。 with_cuda – 确定构建是是否包含CUDA头/库. 默认值 None, 自动通过sources目录是否存在 .cu 或 .cuh文件确定. True强制包含. is_python_module – 默认值 True: python模块方式导入. False: 普通动态库方式加载到程序. 返回: is_python_module == True, 加载PyTorch扩展作为Python模块。If is_python_module == False 无返回 (副作用是共享库被加载到进程). 例子 >>> from torch.utils.cpp_extension import load >>> module = load( name='extension', sources=['extension.cpp', 'extension_kernel.cu'], extra_cflags=['-O2'], verbose=True) torch.utils.cpp_extension.load_inline(name, cpp_sources, cuda_sources=None, functions=None, extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda=None, is_python_module=True) 在运行时编译加载PyTorch C++ 扩展 这个函数很像load()，但是它的源文件是字符串而不是文件名。在把这些字符串保存到构建目录后，load_inline() 等价于 load(). 例子： the tests 源代码可能会省略非内联c++扩展的两个必要部分:必要的头文件,以及(pybind11)绑定代码。更准确地说，传递给cpp_sources的字符串首先连接成一个单独的.cpp文件。然后在这个文件前面加上#include & lt;torch/extension.h&gt; 此外，如果提供了functions的参数，指定的函数将自动生成绑定。functions可以是函数名列表，也可以是{函数名:文档字符串}的字典。如果给定了一个列表，则每个函数的名称用作其文档字符串。 cuda_sources中的代码按顺序连接到单独的.cu文件,追加torch/types.h, cuda.h and cuda_runtime.h头文件..cpp 和 .cu 文件分开编译, 最终连接到一个库中. 注意cuda_sources中的函数本身没有绑定,为了绑定CUDA核函数,必须新建一个C++函数来调用它,或者在cpp_sources 中声明或定义(并且在functions中包含它). load()查看下面忽略的参数. 参数: cpp_sources – 字符串, or 字符串列表, 包含C++源代码 cuda_sources – 字符串, or 字符串列表, 包含CUDA源代码 functions – 函数名列表 用于生成函数绑定. 如果是字典,key=函数名,value=文档描述. with_cuda – 确定是否添加CUDA头/库. 默认值 None (default), 取决于参数cuda_sources . True强制包含CUDA头/库. 例子 >>> from torch.utils.cpp_extension import load_inline >>> source = ''' at::Tensor sin_add(at::Tensor x, at::Tensor y) { return x.sin() + y.sin(); } ''' >>> module = load_inline(name='inline_extension', cpp_sources=[source], functions=['sin_add']) torch.utils.cpp_extension.include_paths(cuda=False) 获取构建C++或CUDA扩展所需的路径。 参数： cuda - 如果为True，则包含CUDA特定的包含路径。 返回： 包含路径字符串的列表。 torch.utils.cpp_extension.check_compiler_abi_compatibility(compiler) 验证给定的编译器是否与PyTorch ABI兼容。 参数：compiler(str) - 要检查可执行的编译器文件名(例如g++),必须在shell进程中可执行。 返回：如果编译器（可能）与PyTorchABI不兼容，则为False，否则返回True。 torch.utils.cpp_extension.verify_ninja_availability() 如果可以在ninja上运行则返回True。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"data.html":{"url":"data.html","title":"torch.utils.data","keywords":"","body":"torch.utils.data 译者：BXuan694 class torch.utils.data.Dataset 表示数据集的抽象类。 所有用到的数据集都必须是其子类。这些子类都必须重写以下方法：__len__：定义了数据集的规模；__getitem__：支持0到len(self)范围内的整数索引。 class torch.utils.data.TensorDataset(*tensors) 用于张量封装的Dataset类。 张量可以沿第一个维度划分为样例之后进行检索。 参数： *tensors (Tensor) – 第一个维度相同的张量。 class torch.utils.data.ConcatDataset(datasets) 用于融合不同数据集的Dataset类。目的：组合不同的现有数据集，鉴于融合操作是同时执行的，数据集规模可以很大。 参数： datasets（序列）– 要融合的数据集列表。 class torch.utils.data.Subset(dataset, indices) 用索引指定的数据集子集。 参数： dataset（Dataset）– 原数据集。 indices（序列）– 全集中选择作为子集的索引。 class torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None) 数据加载器。组合数据集和采样器，并在数据集上提供单进程或多进程迭代器。 参数： dataset（Dataset) – 要加载数据的数据集。 batch_size（int, 可选) – 每一批要加载多少数据（默认：1）。 shuffle（bool, 可选) – 如果每一个epoch内要打乱数据，就设置为True（默认：False）。 sampler（Sampler, 可选）– 定义了从数据集采数据的策略。如果这一选项指定了，shuffle必须是False。 batch_sampler（Sampler, 可选）– 类似于sampler，但是每次返回一批索引。和batch_size，shuffle，sampler，drop_last互相冲突。 num_workers（int, 可选) – 加载数据的子进程数量。0表示主进程加载数据（默认：0）。 collate_fn（可调用 , 可选）– 归并样例列表来组成小批。 pin_memory（bool, 可选）– 如果设置为True，数据加载器会在返回前将张量拷贝到CUDA锁页内存。 drop_last（bool, 可选）– 如果数据集的大小不能不能被批大小整除，该选项设为True后不会把最后的残缺批作为输入；如果设置为False，最后一个批将会稍微小一点。（默认：False） timeout（数值 , 可选） – 如果是正数，即为收集一个批数据的时间限制。必须非负。（默认：0） worker_init_fn（可调用 , 可选）– 如果不是None，每个worker子进程都会使用worker id（在[0, num_workers - 1]内的整数）进行调用作为输入，这一过程发生在设置种子之后、加载数据之前。（默认：None） 注意： 默认地，每个worker都会有各自的PyTorch种子，设置方法是base_seed + worker_id，其中base_seed是主进程通过随机数生成器生成的long型数。而其它库（如NumPy）的种子可能由初始worker复制得到, 使得每一个worker返回相同的种子。（见FAQ中的My data loader workers return identical random numbers部分。）你可以用torch.initial_seed()查看worker_init_fn中每个worker的PyTorch种子，也可以在加载数据之前设置其他种子。 警告： 如果使用了spawn方法，那么worker_init_fn不能是不可序列化对象，如lambda函数。 torch.utils.data.random_split(dataset, lengths) 以给定的长度将数据集随机划分为不重叠的子数据集。 参数： dataset (Dataset) – 要划分的数据集。 lengths（序列）– 要划分的长度。 class torch.utils.data.Sampler(data_source) 所有采样器的基类。 每个Sampler子类必须提供iter方法，以便基于索引迭代数据集元素，同时len方法可以返回数据集大小。 class torch.utils.data.SequentialSampler(data_source) 以相同的顺序依次采样。 参数： data_source (Dataset) – 要从中采样的数据集。 class torch.utils.data.RandomSampler(data_source, replacement=False, num_samples=None) 随机采样元素。如果replacement不设置，则从打乱之后的数据集采样。如果replacement设置了，那么用户可以指定num_samples来采样。 参数： data_source (Dataset) – 要从中采样的数据集。 num_samples (int) – 采样的样本数，默认为len(dataset)。 replacement (bool) – 如果设置为True，替换采样。默认False。 class torch.utils.data.SubsetRandomSampler(indices) 从给定的索引列表中采样，不替换。 参数： indices（序列）– 索引序列 class torch.utils.data.WeightedRandomSampler(weights, num_samples, replacement=True) 样本元素来自[0,..,len(weights)-1]，，给定概率(权重)。 参数： weights（序列) – 权重序列，不需要和为1。 num_samples (int) – 采样数。 replacement (bool) – 如果是True，替换采样。否则不替换，即：如果某个样本索引已经采过了，那么不会继续被采。 class torch.utils.data.BatchSampler(sampler, batch_size, drop_last) 打包采样器来获得小批。 参数： sampler（Sampler）– 基采样器。 batch_size（int）– 小批的规模。 drop_last（bool）– 如果设置为True，采样器会丢弃最后一个不够batch_size的小批（如果存在的话）。 示例 >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False)) [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True)) [[0, 1, 2], [3, 4, 5], [6, 7, 8]] class torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=None, rank=None) 将数据加载限制到数据集子集的采样器。 和torch.nn.parallel.DistributedDataParallel同时使用时尤其有效。在这中情况下，每个进程会传递一个DistributedSampler实例作为DataLoader采样器，并加载独占的原始数据集的子集。 注意： 假设数据集的大小不变。 参数： dataset – 采样的数据集。 num_replicas（可选）– 参与分布式训练的进程数。 rank（可选）– num_replicas中当前进程的等级。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"dlpack.html":{"url":"dlpack.html","title":"torch.utils.dlpack","keywords":"","body":"torch.utils.dlpack 译者：kunwuz torch.utils.dlpack.from_dlpack(dlpack) → Tensor 将DLPack解码成Tensor张量。 参数: dlpack – 一个有着dltensor张量的PyCapsule对象 这个张量会与dlpack对象共享存储空间。注意每个dlpack对象只能使用一次。 torch.utils.dlpack.to_dlpack(tensor) → PyCapsule 返回一个表示张量的DLPack。 参数: tensor –一个用来输出的tensor张量 这个张量会与dlpack对象共享存储空间。注意每个dlpack对象只能使用一次。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"hub.html":{"url":"hub.html","title":"torch.hub","keywords":"","body":"torch.hub 译者：kunwuz torch.hub.load(github, model, force_reload=False, *args, **kwargs) 从github上加载一个带有预训练权重的模型。 参数: github – 必需，一个字符串对象，格式为“repo_owner/repo_name[:tag_name]”，可选 tag/branch。如果未做指定，默认的 branch 是 master 。比方说: ‘pytorch/vision[:hub]’ model – 必须，一个字符串对象，名字在hubconf.py中定义。 force_reload – 可选， 是否丢弃现有缓存并强制重新下载。默认是：False。 *args – 可选， 可调用的model的相关args参数。 **kwargs – 可选， 可调用的model的相关kwargs参数。 返回: 一个有相关预训练权重的单一模型。 torch.hub.set_dir(d) 也可以将hub_dir设置为本地目录来保存中间模型和检查点文件。 如果未设置此参数,环境变量TORCH_HUB_DIR 会被首先搜寻，~/.torch/hub 将被创建并用作后备。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"model_zoo.html":{"url":"model_zoo.html","title":"torch.utils.model_zoo","keywords":"","body":"torch.utils.model_zoo 译者：BXuan694 torch.utils.model_zoo.load_url(url, model_dir=None, map_location=None, progress=True) 由给定URL加载Torch序列化对象。 如果该对象已经存在于model_dir中，将被反序列化并返回。URL的文件名部分应该遵循约定filename-.ext，其中是文件内容的SHA256哈希的前八位或更多位数。（哈希用于确保唯一的名称并验证文件的内容） model_dir默认为$TORCH_HOME/models，其中$TORCH_HOME默认是~/.torch。如果不需要默认目录，可以通过环境变量$TORCH_MODEL_ZOO指定其它的目录。 参数： url（string）– 要下载的对象的URL链接 model_dir（string , 可选）– 保存下载对象的目录 map_location（可选）– 函数或字典，指定如何重新映射存储位置（见torch.load） progress（bool, 可选）– 是否向标准输出展示进度条 示例 >>> state_dict = torch.utils.model_zoo.load_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth') 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"onnx.html":{"url":"onnx.html","title":"torch.onnx","keywords":"","body":"torch.onnx 译者：guobaoyo 示例:从Pytorch到Caffe2的端对端AlexNet模型 这里是一个简单的脚本程序,它将一个在 torchvision 中已经定义的预训练 AlexNet 模型导出到 ONNX 格式. 它会运行一次,然后把模型保存至 alexnet.onnx: import torch import torchvision dummy_input = torch.randn(10, 3, 224, 224, device='cuda') model = torchvision.models.alexnet(pretrained=True).cuda() # 可以根据模块图形的数值设置输入输出的显示名称。这些设置不会改变此图形的语义。只是会变得更加可读了。 #该网络的输入包含了输入的扁平表(flat list)。也就是说传入forward()里面的值，其后是扁平表的参数。你可以指定一部分名字，例如指定一个比该模块输入数量更少的表，随后我们会从一开始就设定名字。 input_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ] output_names = [ \"output1\" ] torch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, input_names=input_names, output_names=output_names) 得到的 alexnet.onnx 是一个 protobuf 二值文件, 它包含所导出模型 ( 这里是 AlexNet )中网络架构和网络参数. 关键参数 verbose=True 会使导出过程中打印出的网络更可读: #这些是网络的输入和参数，包含了我们之前设定的名称。 graph(%actual_input_1 : Float(10, 3, 224, 224) %learned_0 : Float(64, 3, 11, 11) %learned_1 : Float(64) %learned_2 : Float(192, 64, 5, 5) %learned_3 : Float(192) # ---- 为了简介可以省略 ---- %learned_14 : Float(1000, 4096) %learned_15 : Float(1000)) { # 每个声明都包含了一些输出张量以及他们的类型，以及即将运行的操作符（并且包含它的属性，例如核部分，步长等等）它的输入张量（%actual_input_1, %learned_0, %learned_1） %17 : Float(10, 64, 55, 55) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%actual_input_1, %learned_0, %learned_1), scope: AlexNet/Sequential[features]/Conv2d[0] %18 : Float(10, 64, 55, 55) = onnx::Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1] %19 : Float(10, 64, 27, 27) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2] # ---- 为了简洁可以省略 ---- %29 : Float(10, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12] #动态意味着它的形状是未知的。这可能是因为我们的执行操作或者其形状大小是否确实为动态的而受到了限制。（这一点我们想在将来的版本中修复） %30 : Dynamic = onnx::Shape(%29), scope: AlexNet %31 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%30), scope: AlexNet %32 : Long() = onnx::Squeeze[axes=[0]](%31), scope: AlexNet %33 : Long() = onnx::Constant[value={9216}](), scope: AlexNet # ---- 为了简洁可以省略 ---- %output1 : Float(10, 1000) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%45, %learned_14, %learned_15), scope: AlexNet/Sequential[classifier]/Linear[6] return (%output1); } 你可以使用 onnx 库验证 protobuf, 并且用 conda 安装 onnx conda install -c conda-forge onnx 然后运行: import onnx # 载入onnx模块 model = onnx.load(\"alexnet.onnx\") #检查IR是否良好 onnx.checker.check_model(model) #输出一个图形的可读表示方式 onnx.helper.printable_graph(model.graph) 为了能够使用 caffe2 运行脚本，你需要安装 Caffe2. 如果你之前没有安装,请参照 安装指南。 一旦这些安装完成,你就可以在后台使用 Caffe2 : # ...接着上面的继续 import onnx_caffe2.backend as backend import numpy as np rep = backend.prepare(model, device=\"CUDA:0\") #或者 \"CPU\" #后台运行Caffe2： #rep.predict_net是该网络的Caffe2 protobuf #rep.workspace是该网络的Caffe2 workspace #（详见类“onnx_caffe2.backend.Workspace”） outputs = rep.run(np.random.randn(10, 3, 224, 224).astype(np.float32)) #为了多输入地运行该网络，应该传递元组而不是一个单元格。 print(outputs[0]) 之后,我们还会提供其它框架的后端支持. 局限 ONNX 导出器是一个基于轨迹的导出器，这意味着它执行时需要运行一次模型，然后导出实际参与运算的运算符。这也意味着，如果你的模型是动态的，例如，改变一些依赖于输入数据的操作，这时的导出结果是不准确的。同样，一个轨迹可能只对一个具体的输入尺寸有效 (这就是我们在轨迹中需要有明确的输入的原因之一。) 我们建议检查模型的轨迹，确保被追踪的运算符是合理的。 Pytorch和Caffe2中的一些运算符经常有着数值上的差异.根据模型的结构,这些差异可能是微小的,但它们会在表现上产生很大的差别 (尤其是对于未训练的模型。)之后，为了帮助你在准确度要求很高的情况中，能够轻松地避免这些差异带来的影响，我们计划让Caffe2能够直接调用Torch的运算符. 支持的运算符 以下是已经被支持的运算符: add (不支持非零α) sub (不支持非零α) mul div cat mm addmm neg sqrt tanh sigmoid mean sum prod t expand (只有在扩展onnx操作符之前可以使用，例如add) transpose view split squeeze prelu (不支持输入通道之间的单重共享) threshold (不支持非零值阈值/非零值) leaky_relu glu softmax (只支持dim=-1) avg_pool2d (不支持ceil_mode) log_softmax unfold (为ATen-Caffe2集成作实验支撑) elu concat abs index_select pow clamp max min eq gt lt ge le exp sin cos tan asin acos atan permute Conv BatchNorm MaxPool1d (不支持ceil_mode) MaxPool2d (不支持ceil_mode) MaxPool3d (不支持ceil_mode) Embedding (不支持可选参数) RNN ConstantPadNd Dropout FeatureDropout (不支持训练模式) Index (支持常量整数和元组索引) 上面的运算符足够导出下面的模型: AlexNet DCGAN DenseNet Inception (注意:该模型对操作符十分敏感) ResNet SuperResolution VGG word_language_model 为操作符增加导出支持是一种 提前的用法。为了实现这一点，开发者需要掌握PyTorch的源代码。请按照这个网址链接 去下载PyTorch。如果您想要的运算符已经在ONNX标准化了，那么支持对导出此类运算符的操作（为运算符添加符号函数）就很容易了。为了确认运算符是否已经被标准化，请检查ONNX 操作符列表.如果这个操作符是ATen操作符，这就意味着你可以在 torch/csrc/autograd/generated/VariableType.h找到它的定义。(在PyTorch安装文件列表的合成码中可见)，你应该在 torch/onnx/symbolic.py里面加上符号并且遵循下面的指令： 在 torch/onnx/symbolic.py里面定义符号。确保该功能与在ATen操作符在VariableType.h的功能相同。 第一个参数总是ONNX图形参数，参数的名字必须与 VariableType.h里的匹配，因为调度是依赖于关键字参数完成的。 参数排序不需要严格与VariableType.h匹配，首先的张量一定是输入的张量，然后是非张量参数。 在符号功能里，如果操作符已经在ONNX标准化了，我们只需要创建一个代码去表示在图形里面的ONNX操作符。 如果输入参数是一个张量，但是ONNX需要的是一个标量形式的输入，我们需要做个转化。_scalar可以帮助我们将一个张量转化为一个python标量，并且_if_scalar_type_as函数可以将python标量转化为PyTorch张量。 如果操作符是一个非ATen操作符，那么符号功能需要加在相应的PyTorch函数类中。请阅读下面的指示： 在相应的函数类中创建一个符号函数命名为symbolic。 第一个参数总是导出ONNX图形参数。 参数的名字除了第一个必须与前面的形式严格匹配。 输出元组大小必须与前面的形式严格匹配。 在符号功能中，如果操作符已经在ONNX标准化了，我们只需要创建一个代码去表示在图形里面的ONNX操作符。 符号功能应该在Python里面配置好。所有的这些与Python方法相关的功能都通过C++-Python绑定配置好，且上者提供的界面直观地显示如下： def operator/symbolic(g, *inputs): \"\"\" 修改图像(例如使用 \"op\")，加上代表这个PyTorch功能的ONNX操作符，并且返回一个指定的ONNX输出值或者元组值，这些值与最开始PyTorch返回的自动求导功能相关(或者如果ONNX不支持输出，则返回none。 ). 参数： g (图形)：写入图形的ONNX表示方法。 inputs (值...)：该值的列表表示包含这个功能的输入的可变因素。 \"\"\" class Value(object): \"\"\"代表一个在ONNX里计算的中间张量。\"\"\" def type(self): \"\"\"返回值的类型\"\"\" class Type(object): def sizes(self): \"\"\"返回代表这个张量大小形状的整数元组\"\"\" class Graph(object): def op(self, opname, *inputs, **attrs): \"\"\" 闯将一个ONNX操作符'opname'，将'args'作为输入和属性'kwargs'并且将它作为当前图形的节点，返回代表这个操作符的单一输出值(详见`outputs`多关键参数返回节点)。 操作符的设置和他们输入属性详情请见 https://github.com/onnx/onnx/blob/master/docs/Operators.md 参数： opname (字符串)：ONNX操作符的名字，例如`Abs`或者`Add`。 args (值...)：该操作符的输入经常被作为`symbolic`定义参数输入。 kwargs：该ONNX操作符的属性键名根据以下约定：`alpha_f` 代表着`alpha`具有`f`的属性。有效的类型说明符是 `f`（float），`i`（int），`s`（string）或`t`（Tensor）。使用float类型指定的属性接受单个float或float列表（例如，对于带有整数列表的`dims`属性，你可以称其为'dims_i`）。 outputs (证书，可选)：这个运算符返回的输出参数的数量，默认情况下，假定运算符返回单个输出。 如果`输出`不止一个，这个功能将会返回一个输出值的元组，代表着每个ONNX操作符的输出的位置。 \"\"\" ONNX的图形C++定义详情请见torch/csrc/jit/ir.h。 这是一个处理elu操作符缺少符号函数的例子。我们尝试导出模型并查看错误消息，如下所示： UserWarning: ONNX export failed on elu because torch.onnx.symbolic.elu does not exist RuntimeError: ONNX export failed: Couldn't export operator elu 导出失败，因为PyTorch不支持导出elu操作符。 我们发现virtual Tensor elu（const Tensor＆input，Scalar alpha，bool inplace）const override;```VariableType.h。 这意味着elu是一个ATen操作符。 我们可以参考ONNX操作运算符列表，并且确认 Elu 在ONNX中已经被标准化。我们将以下行添加到symbolic.py： def elu(g, input, alpha, inplace=False): return g.op(\"Elu\", input, alpha_f=_scalar(alpha)) 现在PyTorch能够导出elu操作符： 在下面的链接中有更多的例子： symbolic.py, tensor.py, padding.py. 用于指定运算符定义的接口是实验性的; 喜欢尝试的用户应该注意，API可能会在未来的界面中发生变化。 功能函数 torch.onnx.export(*args, **kwargs) 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed_deprecated.html":{"url":"distributed_deprecated.html","title":"Distributed communication package (deprecated) - torch.distributed.deprecated","keywords":"","body":"分布式通信包（已弃用）-torch.distributed.deprecated 警告 torch.distributed.deprecated 是 torch.distributed 的早期版本，当前已被弃用，并且很快将被删除。请参照使用 torch.distributed 的文档，这是PyTorch最新的分布式通信包。 torch.distributed提供类似MPI的接口，用于跨多机网络交换张量数据。它提供一些不同的后台并且支持不同的初始化方法。 当前的torch.distributed.deprecated支持四个后台，每个都有不同的功能。这个表展示了对于CPU/GPU张量来说，哪些函数是可以使用的。只有在用于构建PyTorch的实现支持时，MPI才支持cuda。 基础 torch.distributed.deprecated为在一台或多台机器上运行的多个计算节点上的多进程并行性提供PyTorch支持和通信原语。torch.nn.parallel.deprecated.DistributedDataParallel()类以此功能为基础，提供同步分布式培训，作为任何PyTorch模型的包装器。这与Multiprocessing包提供的那种并行性不同，torch.multiprocessing和torch.nn.DataParallel()支持多个联网的计算机，并且用户必须为每个进程显式地启动主要训练脚本的独立副本。 在单机同步的情况下，torch.distributed.deprecated或者torch.nn.parallel.deprecated.DistributedDataParallel()包装器仍比其他数据并行方法有优势，包括torch.nn.DataParallel(): 每个进程都维护自己的优化器，并在每次迭代时执行完整的优化步骤。虽然这可能看似多余，但由于梯度已经聚集在一起并且在整个过程中进行平均，因此对于每个过程都是相同的，这意味着不需要参数广播步骤，从而减少了在节点之间传输张量所花费的时间 每个进程都包含一个独立的Python解释器，消除了额外的解释器开销以及来自单个Python进程驱动多个执行单元、模型副本或者GPUs的“GIL-thrashing” 初始化 在调用任何其他方法之前，需要使用torch.distributed.deprecated.init_process_group（）函数初始化包。这将阻止所有进程加入。 torch.distributed.deprecated.init_process_group(backend, init_method='env://', **kwargs) 初始化分布式包 参数： backend(str)-待使用后台的名字。取决于构建时配置有效值，包括：tco,mpi,gloo以及nccl。 init_method(str,optional)-指定如何初始化包的URL world_size(int,optional)-参与的进程数量 rank(int,optional)-当前进程的等级 group_name(str,optional)-组名。可以参考初始化方法的描述。 设置backend == mpi，需要在支持MPI的系统上用源码构建。如果您想使用支持CUDA的Open MPI，请使用Open MPI主要版本2及更高版本。 注意 此方法初始化CUDA上下文。 因此，如果多个进程在单个计算机上运行但使用不同的GPU，请确保在此方法之前使用torch.cuda.set_device（）以避免在第一个可见设备上不必要地创建上下文。 torch.distributed.deprecated.get_rank() 返回当前进程的等级。Rank是分配给分布式组中每个进程的唯一标识符。 它们总是连续的整数，范围从0到world_size - 1（包括）。 torch.distributed.deprecated.get_world_size() 返回分布式组中进程的数量。 当前支持三种初始化方法： TCP初始化 有两种使用TCP初始化的方法，两种方法都需要从所有进程可以访问的网络地址和所需的world_size。 第一种方法需要指定属于rank 0进程的地址。 此初始化方法要求所有进程都具有手动指定的等级。 或者，地址必须是有效的IP多播地址，在这种情况下，可以自动分配等级。 多播初始化还支持group_name参数，该参数允许您为多个作业使用相同的地址，只要它们使用不同的组名称即可。 import torch.distributed.deprecated as dist #Use address of one of the machines dist.init_process_group(backend, init_method='tcp://10.1.1.20:23456', rank=args.rank, world_size=4) #or a multicast address - rank will be assigned automatically if unspecified dist.init_process_group(backend,init_method='tcp://[ff15:1e18:5d4c:4cf0:d02d:b659:53ba:b0a7]:23456',world_size=4) 共享文件系统初始化 另一种初始化方法使用从组中的所有机器共享和可见的文件系统，以及期望的world_size。 URL应以file：//开头，并包含共享文件系统上不存在的文件（在现有目录中）的路径。 此初始化方法还支持group_name参数，该参数允许您为多个作业使用相同的共享文件路径，只要它们使用不同的组名称即可。 警告 此方法假定文件系统支持使用fcntl进行锁定 - 大多数本地系统和NFS都支持它 import torch.distributed.deprecated as dist #Rank will be assigned automatically if unspecified dist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile', world_size=4, group_name=args.group) 环境变量初始化 此方法将从环境变量中读取配置，从而可以完全自定义信息的获取方式。 要设置的变量是： MASTER_PORT-必要；必须是机器上的自由端口且等级为0 MASTER_ADDR-必要（除非等级为0）；等级为0的节点的地址 WORLD_SIZE-必要；可以在这里设置，也可以在调用初始化函数中 RANK-必要；可以在这里设置，也可以在调用初始化函数中 等级为0的机器将用于设置所有连接。 这是默认方法，这意味着不必指定init_method（或者可以是env：//）。 组 默认情况下，集合体在默认组（也称为世界）上运行，并要求所有进程都进入分布式函数调用。但是，一些工作负载可以从更细粒度的通信中受益。 这是分布式群体发挥作用的地方。new_group()函数可以用来创建具有所有进程的任意子集的新组。它返回一个不透明的组句柄，可以作为所有集合体的组参数给出（集合体是分布式函数，用于在某些众所周知的编程模式中交换信息）。 torch.distributed.deprecated.new_group(ranks=None) 创建一个新的分布式组。 此功能要求主组中的所有进程（即，作为分布式作业一部分的所有进程）都进入此功能，即使它们不是该组的成员也是如此。 此外，应在所有进程中以相同的顺序创建组。 点到点通讯 torch.distributed.deprecated.send(tensor, dst) 同步发送张量。 参数： tensor(Tensor)-接受数据的张量 dst(int)-目的等级 orch.distributed.deprecated.recv(tensor, src=None) 同步接收张量。 参数： tensor(Tensor)-接收数据的张量 src(int,optional)-源等级，如果未指定，将会接受任意进程的数据 isend（）和irecv（）在使用时返回分布式请求对象。 通常，此对象的类型未指定，因为它们永远不应手动创建，但它们保证支持两种方法： is_completed()-操作完成返回真 wait()-将阻止该过程，直到操作完成。 is_completed（）保证一旦返回就返回True。 当使用MPI后台时，isend()和irecv()支持非插队特性，这样可以保证信息的顺序。关于更多细节，请访问 http://mpi-forum.org/docs/mpi-2.2/mpi22-report/node54.htm#Node54 torch.distributed.deprecated.isend(tensor, dst) 异步发送张量。 参数： tensor(Tensor)-发送的张量 dst(int)-目的等级 torch.distributed.deprecated.irecv(tensor, src) 异步接收张量 参数： tensor(Tensor)-接收数据的张量 src(int)-源等级 集体函数 torch.distributed.deprecated.broadcast(tensor, src, group=) 将张量广播到整个组。 tensor必须在参与集合体的所有过程中具有相同数量的元素。 参数： tensor(Tensor)-如果src是当前进程的等级，则发送数据，否则张量则用于保存接收的数据。 src(int)-源等级 group(optional)-整体的组 torch.distributed.deprecated.all_reduce(tensor, op=, group=) 减少所有机器上的张量数据，以便获得最终结果。 在所有进程中调用张量将按位相同。 参数： tensor(Tensor)-集体的输入和输出，该函数原地运行 op(optional)-一个来自torch.distributed.deprecated.reduce_op枚举的值。指定一个操作用于逐元素减少 group(optional)-整体的组 torch.distributed.deprecated.reduce(tensor, dst, op=, group=) 减少所有机器的张量数据。 只有等级为dst的进程将接收最终结果。 参数： tensor(tensor)-集体的输入和输出，该函数原地运行 dst(int)-目的等级 op(optional)-一个来自torch.distributed.deprecated.reduce_op枚举的值。指定一个操作用于逐元素减少 group(optional)-整体的组 torch.distributed.deprecated.all_gather(tensor_list, tensor, group=) 从列表中收集整个组的张量。 参数： tensor_list(list[Tensor])-输出列表。它包括用来作为整体输出的正确尺寸的张量。 tensor(tensor)-在当前进程进行广播的张量 group(optional)-整体的组 torch.distributed.deprecated.gather(tensor, **kwargs) 从单个进程中收集张量列表。 参数： tensor(Tensor)-输入的张量 dst(int)-目的等级。除了接收数据的进程外，其余进程都需要这个值 gather_list(list[Tensor])-用于接收数据的适当大小的张量列表。 仅在接收进程中需要。 group(optional)-整体的组 torch.distributed.deprecated.scatter(tensor, **kwargs) 将张量列表分散到组中的所有进程。 每个进程将只接收一个张量并将其数据存储在tensor参数中。 参数： tensor（Tensor） - 输出张量。 src（int） - 源排名。除发送数据的进程外，在所有进程中都是必需的。 scatter_list（list [ Tensor ]） - 要分散的张量列表。仅在发送数据的过程中需要。 group(optional)-整体的组 torch.distributed.deprecated.barrier（group = ） 同步所有进程。 此集合会阻止进程，直到整个组进入此函数。 多GPU整体函数 如果每个节点上有多个GPU，则在使用NCCL后端时，支持每个节点内多个GPU之间的分布式集合操作。这些函数可以潜在地提高整体分布式训练性能，并通过传递张量列表轻松使用。传递的张量列表中的每个Tensor需要位于调用该函数的主机的单独GPU设备上。请注意，张量列表的长度在所有分布式进程中需要相同。另请注意，目前只有NCCL后端支持多GPU整体函数。broadcast_multigpu()all_reduce_multigpu()reduce_multigpu()all_gather_multigpu() 例如，如果我们用于分布式培训的系统有2个节点，每个节点有8个GPU。在16个GPU中的每一个上，都有一个我们希望减少的张量。以下代码可以作为参考： 代码在节点0上运行： import torch import torch.distributed.deprecated as dist dist.init_process_group(backend=\"nccl\", init_method=\"file:///distributed_test\", world_size=2, rank=0) tensor_list = [] for dev_idx in range(torch.cuda.device_count()): tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx)) dist.all_reduce_multigpu(tensor_list) 代码在节点1上运行： import torch import torch.distributed.deprecated as dist dist.init_process_group(backend=\"nccl\", init_method=\"file:///distributed_test\", world_size=2, rank=1) tensor_list = [] for dev_idx in range(torch.cuda.device_count()): tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx)) dist.all_reduce_multigpu(tensor_list) 调用结束后，两个节点上的16个张量均具有16的全减值。 torch.distributed.deprecated.broadcast_multigpu(tensor_list, src, group=) 使用每个节点多个GPU张量将张量广播到整个组。 tensor必须在参与集合体的所有进程的所有GPU中具有相同数量的元素。列表中的每个张量必须位于不同的GPU上。 注意 目前仅支持NCCL后端。tensor_list应该只包含GPU张量。 参数： tensorlist（List [ Tensor ] _) - 整体的输入和输出张量列表。该函数原地运行，并要求每个张量在不同的GPU上为GPU张量。您还需要确保len(tensor_list)调用此函数的所有分布式进程都是相同的。 op（optional） - torch.distributed.deprecated.reduce_op枚举中的一个值。指定用于逐元素减少的操作。 group(optional)-整体的组 torch.distributed.deprecated.reduce_multigpu（tensor_list，dst，op = ，group = ） 减少所有计算机上多个GPU的张量数据。每个张量：attr tensor_list应该驻留在一个单独的GPU上。 只有tensor_list[0]的GPU上的等级为dst的进程可以接收最终值。 注意 目前仅支持NCCL后端。tensor_list应该只包含GPU张量。 参数： tensorlist（List [ Tensor ] _) -整体的输入和输出张量列表。该函数原地运行，并要求每个张量在不同的GPU上为GPU张量。您还需要确保len(tensor_list)调用此函数的所有分布式进程都是相同的。 dst(int)-目的等级 op（optional） - torch.distributed.deprecated.reduce_op枚举中的一个值。指定用于逐元素减少的操作。 group(optional)-整体的组 torch.distributed.deprecated.all_gather_multigpu（output_tensor_lists，input_tensor_list，group = ） 从列表中收集整个组的张量。每个张量input_tensor_list应位于单独的GPU上。 注意 目前仅支持NCCL后端。output_tensor_lists和input_tensor_list应该只包含GPU张量。 参数： outputtensor_lists（List [ List [ Tensor ] _] ） - 输出列表。它应该在每个GPU上包含正确大小的张量，以用于整体的输出。例如，output_tensor_lists[i]包含驻留在GPU上的all_gather结果input_tensor_list[i]。请注意，每个元素output_tensor_lists[i]的大小都是world_size len(input_tensor_list)，因为函数都会从组中的每个GPU中收集结果。为了解释每个元素output_tensor_list[i]，请注意input_tensor_list[j]等级k将出现在output_tensor_list[i][rank world_size + j]另外注意len(output_tensor_lists)，并且因此，每个元素的大小output_tensor_lists（每个元素是一个列表len(output_tensor_lists[i])）对于调用此函数的所有分布式进程都需要相同。 inputtensor_list（List [ Tensor ] _） - 要从当前进程广播的张量列表（在不同的GPU上）。请注意，len(input_tensor_list)调用此函数的所有分布式进程需要相同。 group(optional)-整体的组 启动实用程序 该torch.distributed.deprecated软件包还提供了一个启动实用程序torch.distributed.deprecated.launch。 torch.distributed.launch 是一个模块，它在每个训练节点上产生多个分布式训练过程。 该实用程序可用于单节点分布式训练，其中将生成每个节点的一个或多个进程。该应用程序可以用CPU或者GPU训练。这可以实现良好改进的单节点训练性能。它还可以用于多节点分布式训练，通过在每个节点上产生多个进程来获得良好改进的多节点分布式训练性能。这对于具有多个具有直接GPU支持的Infiniband接口的系统尤其有利，因为所有这些接口都可用于聚合通信带宽。 在单节点分布式训练或多节点分布式训练的两种情况下，该实用程序将为每个节点启动给定数量的进程（--nproc_per_node）。如果用于GPU训练，则此数字需要小于或等于当前系统上的GPU数量（nproc_per_node），并且每个进程将在GPU 0到GPU（nproc_per_node - 1）的单个GPU上运行。 如何使用这个模块： 单节点多进程分布式训练 >>>python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE YOUR_TRAINING_SCRIPT.py(--arg1 --arg2 --arg3 and all other arguments of your training script) 多节点多进程分布式训练:(比如，两个节点) 节点1：(IP:192.168.1.1,一个自由接口：1234) >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\" --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of your training script) 节点2： >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\" --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of your training script) 要查找此模块提供的可选参数： >>> python -m torch.distributed.launch --help 重要提示 此实用和多进程分布式（单节点或多节点）GPU训练目前仅使用NCCL分布式后端实现最佳性能。因此，NCCL后端是用于GPU训练的推荐后端。 在训练程序中，必须解析命令行参数：--local_rank=LOCAL_PROCESS_RANK，该参数将由此模块提供。如果您的训练计划使用GPU，则应确保您的代码仅在LOCAL_PROCESS_RANK的GPU设备上运行。这可以通过以下方式完成： 解析local_rank参数 >>> import argparse >>> parser = argparse.ArgumentParser() >>> parser.add_argument(\"--local_rank\", type=int) >>> args = parser.parse_args() 使用其中一个将您的设备设置为本地等级 >>> torch.cuda.set_device(arg.local_rank) # before your code runs 或 >>> with torch.cuda.device(arg.local_rank): >>> # your code to run 在训练程序中，您应该在开始时调用以下函数来启动分布式后端。您需要确保init_method使用env://，这是init_method此模块唯一支持的。 torch.distributed.init_process_group(backend='YOUR BACKEND', init_method='env://') 在你的训练程序中，你可以选择常规分布式函数或使用torch.nn.parallel.DistributedDataParallel()。如果计划使用GPU训练，并且您希望使用torch.nn.parallel.DistributedDataParallel()模块，以下是如何配置它。 model = torch.nn.parallel.DistributedDataParallel（model， device_ids = [arg.local_rank]， output_device = arg.local_rank） 请确保将device_ids参数设置为您的代码将在其上运行的唯一GPU设备ID。这通常是程序的本地排名。换句话说，为了使用这个实用程序，device_ids需要是[args.local_rank]，并且output_device需要是args.local_rank。 警告 local_rank并非全局唯一：它在计算机上的每个进程唯一。因此，不要使用它来决定是否应该写入网络文件系统。有关如何正确执行此操作可能出错的示例，请参阅https://github.com/pytorch/pytorch/issues/12042。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"docs_torchvision_ref.html":{"url":"docs_torchvision_ref.html","title":"torchvision 参考","keywords":"","body":"torchvision 参考 译者：BXuan694 torchvision 包收录了若干重要的公开数据集、网络模型和计算机视觉中的常用图像变换 包参考 torchvision.datasets MNIST Fashion-MNIST EMNIST COCO LSUN ImageFolder DatasetFolder Imagenet-12 CIFAR STL10 SVHN PhotoTour SBU Flickr VOC torchvision.models Alexnet VGG ResNet SqueezeNet DenseNet Inception v3 torchvision.transforms Transforms on PIL Image Transforms on torch.*Tensor Conversion Transforms Generic Transforms Functional Transforms torchvision.utils torchvision.get_image_backend() 查看载入图片的包的名称 torchvision.set_image_backend(backend) 指定用于载入图片的包 参数: backend (string) – 图片处理后端的名称，须为{‘PIL’, ‘accimage’}中的一个。accimage包使用了英特尔IPP库。这个库通常比PIL快，但是支持的操作比PIL要少。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"torchvision_datasets.html":{"url":"torchvision_datasets.html","title":"torchvision.datasets","keywords":"","body":"torchvision.datasets 译者：BXuan694 所有的数据集都是torch.utils.data.Dataset的子类， 即：它们实现了__getitem__和__len__方法。因此，它们都可以传递给torch.utils.data.DataLoader，进而通过torch.multiprocessing实现批数据的并行化加载。例如： imagenet_data = torchvision.datasets.ImageFolder('path/to/imagenet_root/') data_loader = torch.utils.data.DataLoader(imagenet_data, batch_size=4, shuffle=True, num_workers=args.nThreads) 目前为止，收录的数据集包括： 数据集 MNIST Fashion-MNIST EMNIST COCO Captions Detection LSUN ImageFolder DatasetFolder Imagenet-12 CIFAR STL10 SVHN PhotoTour SBU Flickr VOC 以上数据集的接口基本上很相近。它们至少包括两个公共的参数transform和target_transform，以便分别对输入和和目标做变换。 class torchvision.datasets.MNIST(root, train=True, transform=None, target_transform=None, download=False) MNIST数据集。 参数： root（string）– 数据集的根目录，其中存放processed/training.pt和processed/test.pt文件。 train（bool, 可选）– 如果设置为True，从training.pt创建数据集，否则从test.pt创建。 download（bool, 可选）– 如果设置为True, 从互联网下载数据并放到root文件夹下。如果root目录下已经存在数据，不会再次下载。 transform（可被调用 , 可选）– 一种函数或变换，输入PIL图片，返回变换之后的数据。如：transforms.RandomCrop。 target_transform （可被调用 , 可选）– 一种函数或变换，输入目标，进行变换。 class torchvision.datasets.FashionMNIST(root, train=True, transform=None, target_transform=None, download=False) Fashion-MNIST数据集。 参数： root（string）– 数据集的根目录，其中存放processed/training.pt和processed/test.pt文件。 train（bool, 可选）– 如果设置为True，从training.pt创建数据集，否则从test.pt创建。 download（bool, 可选）– 如果设置为True，从互联网下载数据并放到root文件夹下。如果root目录下已经存在数据，不会再次下载。 transform（可被调用 , 可选）– 一种函数或变换，输入PIL图片，返回变换之后的数据。如：transforms.RandomCrop。 target_transform（可被调用 , 可选）– 一种函数或变换，输入目标，进行变换。 class torchvision.datasets.EMNIST(root, split, **kwargs) EMNIST数据集。 参数: root（string）– 数据集的根目录，其中存放processed/training.pt和processed/test.pt文件。 split（string）– 该数据集分成6种：byclass，bymerge，balanced，letters，digits和mnist。这个参数指定了选择其中的哪一种。 train（bool, 可选）– 如果设置为True，从training.pt创建数据集，否则从test.pt创建。 download（bool, 可选）– 如果设置为True, 从互联网下载数据并放到root文件夹下。如果root目录下已经存在数据，不会再次下载。 transform（可被调用 , 可选）– 一种函数或变换，输入PIL图片，返回变换之后的数据。如：transforms.RandomCrop。 target_transform（可被调用 , 可选) – 一种函数或变换，输入目标，进行变换。 注意： 以下要求预先安装COCO API。 class torchvision.datasets.CocoCaptions(root, annFile, transform=None, target_transform=None) MS Coco Captions数据集。 参数： root（string）– 下载数据的目标目录。 annFile（string）– json标注文件的路径。 transform（可被调用 , 可选）– 一种函数或变换，输入PIL图片，返回变换之后的数据。如：transforms.ToTensor。 target_transform（可被调用 , 可选）– 一种函数或变换，输入目标，进行变换。 示例 import torchvision.datasets as dset import torchvision.transforms as transforms cap = dset.CocoCaptions(root = 'dir where images are', annFile = 'json annotation file', transform=transforms.ToTensor()) print('Number of samples: ', len(cap)) img, target = cap[3] # load 4th sample print(\"Image Size: \", img.size()) print(target) 输出： Number of samples: 82783 Image Size: (3L, 427L, 640L) [u'A plane emitting smoke stream flying over a mountain.', u'A plane darts across a bright blue sky behind a mountain covered in snow', u'A plane leaves a contrail above the snowy mountain top.', u'A mountain that has a plane flying overheard in the distance.', u'A mountain view with a plume of smoke in the background'] __getitem__(index) 参数： index (int) – 索引 返回： 元组(image, target)，其中target是列表类型，包含了对图片image的描述。 --- --- 返回类型： tuple --- --- class torchvision.datasets.CocoDetection(root, annFile, transform=None, target_transform=None) MS Coco Detection数据集。 参数： root（string）– 下载数据的目标目录。 annFile（string）– json标注文件的路径。 transform（可被调用 , 可选）– 一种函数或变换，输入PIL图片，返回变换之后的数据。如：transforms.ToTensor。 target_transform（可被调用 , 可选）– 一种函数或变换，输入目标，进行变换。 __getitem__(index) 参数: index (int) – 索引 返回： 元组(image, target)，其中target是coco.loadAnns返回的对象。 --- --- 返回类型： tuple --- --- class torchvision.datasets.LSUN(root, classes='train', transform=None, target_transform=None) LSUN数据集。 参数： root（string）– 存放数据文件的根目录。 classes（string 或 list）– {‘train’, ‘val’, ‘test’}之一，或要加载类别的列表，如[‘bedroom_train’, ‘church_train’]。 transform（可被调用 , 可选) – 一种函数或变换，输入PIL图片，返回变换之后的数据。如：transforms.RandomCrop。 target_transform（可被调用 , 可选）– 一种函数或变换，输入目标，进行变换。 __getitem__(index) 参数： index (int) – 索引 返回： 元组(image, target)，其中target是目标类别的索引。 --- --- Return type: tuple --- --- class torchvision.datasets.ImageFolder(root, transform=None, target_transform=None, loader=) 一种通用数据加载器，其图片应该按照如下的形式保存： root/dog/xxx.png root/dog/xxy.png root/dog/xxz.png root/cat/123.png root/cat/nsdf3.png root/cat/asd932_.png 参数： root（string）– 根目录路径。 transform（可被调用 , 可选）– 一种函数或变换，输入PIL图片，返回变换之后的数据。如：transforms.RandomCrop。 target_transform（可被调用 , 可选）– 一种函数或变换，输入目标，进行变换。 loader – 一种函数，可以由给定的路径加载图片。 __getitem__(index) 参数： index (int) – 索引 返回： (sample, target)，其中target是目标类的类索引。 --- --- 返回类型： tuple --- --- class torchvision.datasets.DatasetFolder(root, loader, extensions, transform=None, target_transform=None) 一种通用数据加载器，其数据应该按照如下的形式保存： root/class_x/xxx.ext root/class_x/xxy.ext root/class_x/xxz.ext root/class_y/123.ext root/class_y/nsdf3.ext root/class_y/asd932_.ext 参数: root（string）– 根目录路径。 loader（可被调用）– 一种函数，可以由给定的路径加载数据。 extensions（list[string]）– 列表，包含允许的扩展。 transform（可被调用 , 可选）– 一种函数或变换，输入数据，返回变换之后的数据。如：对于图片有transforms.RandomCrop。 target_transform – 一种函数或变换，输入目标，进行变换。 __getitem__(index) 参数： index (int) – 索引 返回： (sample, target)，其中target是目标类的类索引. --- --- 返回类型： tuple --- --- 这个类可以很容易地实现ImageFolder数据集。数据预处理见此处。 示例。 class torchvision.datasets.CIFAR10(root, train=True, transform=None, target_transform=None, download=False) CIFAR10数据集。 参数： root（string）– 数据集根目录，要么其中应存在cifar-10-batches-py文件夹，要么当download设置为True时cifar-10-batches-py文件夹保存在此处。 train（bool, 可选）– 如果设置为True, 从训练集中创建，否则从测试集中创建。 transform（可被调用 , 可选）– 一种函数或变换，输入PIL图片，返回变换之后的数据。如：transforms.RandomCrop。 target_transform（可被调用 , 可选）– 一种函数或变换，输入目标，进行变换。 download（bool, 可选）– 如果设置为True，从互联网下载数据并放到root文件夹下。如果root目录下已经存在数据，不会再次下载。 __getitem__(index) 参数： index (int) – 索引 返回： (image, target)，其中target是目标类的类索引。 --- --- 返回类型： tuple --- --- class torchvision.datasets.CIFAR100(root, train=True, transform=None, target_transform=None, download=False) CIFAR100数据集。 这是CIFAR10数据集的一个子集。 class torchvision.datasets.STL10(root, split='train', transform=None, target_transform=None, download=False) STL10数据集。 参数： root（string）– 数据集根目录，应该包含stl10_binary文件夹。 split（string）– {‘train’, ‘test’, ‘unlabeled’, ‘train+unlabeled’}之一，选择相应的数据集。 transform（可被调用 , 可选）– 一种函数或变换，输入PIL图片，返回变换之后的数据。如：transforms.RandomCrop。 target_transform（可被调用 , 可选）– 一种函数或变换，输入目标，进行变换。 download（bool, optional）– 如果设置为True，从互联网下载数据并放到root文件夹下。如果root目录下已经存在数据，不会再次下载。 __getitem__(index) 参数： index (int) – 索引 返回： (image, target)，其中target应是目标类的类索引。 --- --- 返回类型： tuple --- --- class torchvision.datasets.SVHN(root, split='train', transform=None, target_transform=None, download=False) SVHN数据集。注意：SVHN数据集将10指定为数字0的标签。然而，这里我们将0指定为数字0的标签以兼容PyTorch的损失函数，因为损失函数要求类标签在[0, C-1]的范围内。 参数： root（string）– 数据集根目录，应包含SVHN文件夹。 split（string）– {‘train’, ‘test’, ‘extra’}之一，相应的数据集会被选择。‘extra’是extra训练集。 transform（可被调用 , 可选）– 一种函数或变换，输入PIL图片，返回变换之后的数据。如：transforms.RandomCrop。 target_transform（可被调用 , 可选）– 一种函数或变换，输入目标，进行变换。 download（bool, 可选）– 如果设置为True，从互联网下载数据并放到root文件夹下。如果root目录下已经存在数据，不会再次下载。 __getitem__(index) 参数： index (int) – 索引 返回： (image, target)，其中target是目标类的类索引。 --- --- 返回类型： tuple --- --- class torchvision.datasets.PhotoTour(root, name, train=True, transform=None, download=False) Learning Local Image Descriptors Data数据集。 参数： root（string）– 保存图片的根目录。 name（string）– 要加载的数据集。 transform（可被调用 , 可选）– 一种函数或变换，输入PIL图片，返回变换之后的数据。 download (bool, optional) – 如果设置为True，从互联网下载数据并放到root文件夹下。如果root目录下已经存在数据，不会再次下载。 __getitem__(index) 参数： index (int) – 索引 返回： (data1, data2, matches) --- --- 返回类型： tuple --- --- 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"torchvision_models.html":{"url":"torchvision_models.html","title":"torchvision.models","keywords":"","body":"torchvision.models 译者：BXuan694 models子包定义了以下模型架构： AlexNet VGG ResNet SqueezeNet DenseNet Inception v3 你可以通过调用以下构造函数构造随机权重的模型： import torchvision.models as models resnet18 = models.resnet18() alexnet = models.alexnet() vgg16 = models.vgg16() squeezenet = models.squeezenet1_0() densenet = models.densenet161() inception = models.inception_v3() 我们在torch.utils.model_zoo中提供了预训练模型。预训练模型可以通过传递参数pretrained=True构造： import torchvision.models as models resnet18 = models.resnet18(pretrained=True) alexnet = models.alexnet(pretrained=True) squeezenet = models.squeezenet1_0(pretrained=True) vgg16 = models.vgg16(pretrained=True) densenet = models.densenet161(pretrained=True) inception = models.inception_v3(pretrained=True) 定义预训练模型时会把权值下载到一个缓存文件夹中，这个缓存文件可以通过环境变量TORCH_MODEL_ZOO来指定。更多细节见torch.utils.model_zoo.load_url()。 有些模型在训练和测试阶段用到了不同的模块，例如批标准化（batch normalization）。使用model.train()或model.eval()可以切换到相应的模式。更多细节见train()或eval()。 所有的预训练模型都要求输入图片以相同的方式进行标准化，即：小批（mini-batch）三通道RGB格式（3 x H x W），其中H和W不得小于224。图片加载时像素值的范围应在[0, 1]内，然后通过指定mean = [0.485, 0.456, 0.406]和std = [0.229, 0.224, 0.225]进行标准化，例如： normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) 在imagenet的示例中可以看到标准化的一个应用。 下表是ImageNet单次224x224中心裁剪的错误率。 网络 Top-1错误率（%） Top-5错误率（%） AlexNet 43.45 20.91 VGG-11 30.98 11.37 VGG-13 30.07 10.75 VGG-16 28.41 9.62 VGG-19 27.62 9.12 带有批标准化的VGG-11 29.62 10.19 带有批标准化的VGG-13 28.45 9.63 带有批标准化的VGG-16 26.63 8.50 带有批标准化的VGG-19 25.76 8.15 ResNet-18 30.24 10.92 ResNet-34 26.70 8.58 ResNet-50 23.85 7.13 ResNet-101 22.63 6.44 ResNet-152 21.69 5.94 SqueezeNet 1.0 41.90 19.58 SqueezeNet 1.1 41.81 19.38 Densenet-121 25.35 7.83 Densenet-169 24.00 7.00 Densenet-201 22.80 6.43 Densenet-161 22.35 6.20 Inception v3 22.55 6.44 Alexnet torchvision.models.alexnet(pretrained=False, **kwargs) AlexNet模型，参见论文《One weird trick…》 。 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 VGG torchvision.models.vgg11(pretrained=False, **kwargs) VGG11模型。（论文中的“A”模型） 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 torchvision.models.vgg11_bn(pretrained=False, **kwargs) VGG11模型，带有批标准化。（论文中的“A”模型） 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 torchvision.models.vgg13(pretrained=False, **kwargs) VGG13模型。（论文中的“B”模型） 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 torchvision.models.vgg13_bn(pretrained=False, **kwargs) VGG13模型，带有批标准化。（论文中的“B”模型） 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 torchvision.models.vgg16(pretrained=False, **kwargs) VGG16模型。（论文中的“D”模型） 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 torchvision.models.vgg16_bn(pretrained=False, **kwargs) VGG16模型，带有批标准化。（论文中的“D”模型） 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 torchvision.models.vgg19(pretrained=False, **kwargs) VGG19模型。（论文中的“E”模型） 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 torchvision.models.vgg19_bn(pretrained=False, **kwargs) VGG19模型，带有批标准化。（论文中的“E”模型） 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 ResNet torchvision.models.resnet18(pretrained=False, **kwargs) 构造ResNet-18模型。 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 torchvision.models.resnet34(pretrained=False, **kwargs) 构造ResNet-34模型。 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 torchvision.models.resnet50(pretrained=False, **kwargs) 构造ResNet-50模型。 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 torchvision.models.resnet101(pretrained=False, **kwargs) 构造ResNet-101模型。 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 torchvision.models.resnet152(pretrained=False, **kwargs) 构造ResNet-152模型。 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 SqueezeNet torchvision.models.squeezenet1_0(pretrained=False, **kwargs) SqueezeNet模型，参见论文《SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and 。 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 torchvision.models.squeezenet1_1(pretrained=False, **kwargs) SqueezeNet 1.1模型，参见SqueezeNet官方仓库。SqueezeNet 1.1比SqueezeNet 1.0节约2.4倍的计算量，参数也略少，然而精度未做牺牲。 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 DenseNet torchvision.models.densenet121(pretrained=False, **kwargs) Densenet-121模型，参见《Densely Connected Convolutional Networks》。 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 torchvision.models.densenet169(pretrained=False, **kwargs) Densenet-169模型，参见《Densely Connected Convolutional Networks》。 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 torchvision.models.densenet161(pretrained=False, **kwargs) Densenet-161模型，参见《Densely Connected Convolutional Networks》。 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 torchvision.models.densenet201(pretrained=False, **kwargs) Densenet-201模型，参见《Densely Connected Convolutional Networks》。 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 Inception v3 torchvision.models.inception_v3(pretrained=False, **kwargs) Inception v3模型，参见《Rethinking the Inception Architecture for Computer Vision》。 参数： pretrained (bool) – 如果设置为True，返回ImageNet预训练模型 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"torchvision_transforms.html":{"url":"torchvision_transforms.html","title":"torchvision.transforms","keywords":"","body":"torchvision.transforms 译者：BXuan694 transforms包含了一些常用的图像变换，这些变换能够用Compose串联组合起来。另外，torchvision提供了torchvision.transforms.functional模块。functional可以提供了一些更加精细的变换，用于搭建复杂的变换流水线（例如分割任务）。 class torchvision.transforms.Compose(transforms) 用于把一系列变换组合到一起。 参数： transforms（list或Transform对象）- 一系列需要进行组合的变换。 示例： >>> transforms.Compose([ >>> transforms.CenterCrop(10), >>> transforms.ToTensor(), >>> ]) 对PIL图片的变换 class torchvision.transforms.CenterCrop(size) 在中心处裁剪PIL图片。 参数： size（序列 或 int）– 需要裁剪出的形状。如果size是int，将会裁剪成正方形；如果是形如(h, w)的序列，将会裁剪成矩形。 class torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0) 随机改变图片的亮度、对比度和饱和度。 参数： brightness（float或 float类型元组(min, max)）– 亮度的扰动幅度。brightness_factor从[max(0, 1 - brightness), 1 + brightness]中随机采样产生。应当是非负数。 contrast（float或 float类型元组(min, max)）– 对比度扰动幅度。contrast_factor从[max(0, 1 - contrast), 1 + contrast]中随机采样产生。应当是非负数。 saturation（float或 float类型元组(min, max)）– 饱和度扰动幅度。saturation_factor从[max(0, 1 - saturation), 1 + saturation]中随机采样产生。应当是非负数。 hue（float或 float类型元组(min, max)）– 色相扰动幅度。hue_factor从[-hue, hue]中随机采样产生，其值应当满足0 class torchvision.transforms.FiveCrop(size) 从四角和中心裁剪PIL图片。 注意： 该变换返回图像元组，可能会导致图片在网络中传导后和数据集给出的标签等信息不能匹配。处理方法见下面的示例。 参数： size（序列 或 int）– 需要裁剪出的形状。如果size是int，将会裁剪成正方形；如果是序列，如(h, w)，将会裁剪成矩形。 示例： >>> transform = Compose([ >>> FiveCrop(size), # 这里产生了一个PIL图像列表 >>> Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) # 返回4维张量 >>> ]) >>> # 在测试阶段你可以这样做： >>> input, target = batch # input是5维张量，target是2维张量。 >>> bs, ncrops, c, h, w = input.size() >>> result = model(input.view(-1, c, h, w)) # 把batch size和ncrops融合在一起 >>> result_avg = result.view(bs, ncrops, -1).mean(1) # crops上的平均值 class torchvision.transforms.Grayscale(num_output_channels=1) 把图片转换为灰阶。 参数： num_output_channels（int，1或3）– 希望得到的图片通道数。 返回： 输入图片的灰阶版本。 - 如果num_output_channels == 1：返回单通道图像；- 如果num_output_channels == 3：返回3通道图像，其中r == g == b。 返回类型： PIL图像。 class torchvision.transforms.Pad(padding, fill=0, padding_mode='constant') 对PIL图像的各条边缘进行扩展。 参数： padding（int 或 tuple）– 在每条边上展开的宽度。如果传入的是单个int，就在所有边展开。如果传入长为2的元组，则指定左右和上下的展开宽度。如果传入长为4的元组，则依次指定为左、上、右、下的展开宽度。 fill（int 或 tuple） – 像素填充值。默认是0。如果指定长度为3的元组，表示分别填充R, G, B通道。这个参数仅在padding_mode是‘constant’时指定有效。 padding_mode（str）– 展开类型。应当是‘constant’，‘edge’，‘reflect’或‘symmetric’之一。默认为‘constant’。 constant：用常数扩展，这个值由fill参数指定。 edge：用图像边缘上的指填充。 reflect：以边缘为对称轴进行轴对称填充（边缘值不重复）。 > 例如，在[1, 2, 3, 4]的两边填充2个元素会得到[3, 2, 1, 2, 3, 4, 3, 2]。 symmetric：用图像边缘的反转进行填充（图像的边缘值需要重复）。 > 例如，在[1, 2, 3, 4]的两边填充2个元素会得到[2, 1, 1, 2, 3, 4, 4, 3]。 class torchvision.transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0) 保持像素的分布中心不变，对图像做随机仿射变换。 参数： degrees（序列 或 float 或 int）– 旋转角度的筛选范围。如果是序列（min, max），从中随机均匀采样；如果是数字，则从（-degrees, +degrees）中采样。如果不需要旋转，那么设置为0。 translate（tuple, 可选）– 元组，元素值是水平和垂直平移变换的最大绝对值。例如translate=(a, b)时，水平位移值从 -img_width a a中随机采样得到，垂直位移值从-img_height b b中随机采样得到。默认不做平移变换。 scale（tuple, 可选）– 尺度放缩因子的内区间，如[a, b]，放缩因子scale的随机采样区间为：a shear（序列 或 float 或 int, 可选）– 扭曲角度的筛选范围。如果是序列（min, max），从中随机均匀采样；如果是数字，则从（-degrees, +degrees）中采样。默认不会进行扭曲操作。 resample（{PIL.Image.NEAREST , PIL.Image.BILINEAR , PIL.Image.BICUBIC} , 可选）– 可选的重采样滤波器，见filters。如果没有该选项，或者图片模式是“1”或“P”，设置为PIL.Image.NEAREST。 fillcolor（int）– 在输出图片的变换外区域可选地填充颜色。（Pillow>=5.0.0）。 class torchvision.transforms.RandomApply(transforms, p=0.5) 对transforms中的各变换以指定的概率决定是否选择。 参数： transforms（list or tuple）– 变换的集合。 p（float）– 概率。 class torchvision.transforms.RandomChoice(transforms) 从列表中随机选择一种变换。 class torchvision.transforms.RandomCrop(size, padding=0, pad_if_needed=False) 对给出的PIL图片在随机位置处进行裁剪。 参数： size（序列 或 int）– 想要裁剪出的图片的形状。如果size是int，按照正方形（size, size）裁剪； 如果size是序列（h, w），裁剪为矩形。 padding（int 或 序列 , 可选）– 在图像的边缘进行填充，默认0，即不做填充。如果指定长为4的序列，则分别指定左、上、右、下的填充宽度。 pad_if_needed（boolean）– 如果设置为True，若图片小于目标形状，将进行填充以避免报异常。 fill（int 或 tuple） – 像素填充值。默认是0。如果指定长度为3的元组，表示分别填充R, G, B通道。这个参数仅在padding_mode是‘constant’时指定有效。 padding_mode（str）– 展开类型。应当是‘constant’，‘edge’，‘reflect’或‘symmetric’之一。默认为‘constant’。 constant：用常数扩展，这个值由fill参数指定。 edge：用图像边缘上的指填充。 reflect：以边缘为对称轴进行轴对称填充（边缘值不重复）。 > 例如，在[1, 2, 3, 4]的两边填充2个元素会得到[3, 2, 1, 2, 3, 4, 3, 2]。 symmetric：用图像边缘的反转进行填充（图像的边缘值需要重复）。 > 例如，在[1, 2, 3, 4]的两边填充2个元素会得到[2, 1, 1, 2, 3, 4, 4, 3]。 class torchvision.transforms.RandomGrayscale(p=0.1) 以概率p（默认0.1）将图片随机转化为灰阶图片。 参数： p（float）–图像转化为灰阶的概率。 返回： 以概率p转换为灰阶，以概率（1-p）不做变换。如果输入图像为1通道，则灰阶版本也是1通道。如果输入图像为3通道，则灰阶版本是3通道，r == g == b。 返回类型： PIL图像。 class torchvision.transforms.RandomHorizontalFlip(p=0.5) 以给定的概率随机水平翻折PIL图片。 参数： p（float）– 翻折图片的概率。默认0.5。 class torchvision.transforms.RandomOrder(transforms) 以随机的顺序对图片做变换。 class torchvision.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2) 以随机的形状和长宽比裁剪图片。 以随机的形状（默认从原始图片的0.08到1.0) 和随机长宽比（默认从3/4到4/3）裁剪图片。然后调整到指定形状。这一变换通常用于训练Inception网络。 参数： size – 每条边的期望输出形状。 scale – 裁剪原始图片出的形状范围。 ratio – 原始长宽比裁剪出的目标范围。 interpolation – 默认：PIL.Image.BILINEAR。 class torchvision.transforms.RandomRotation(degrees, resample=False, expand=False, center=None) 以指定的角度选装图片。 参数： degrees（序列 或 float or int）– 旋转角度的随机选取范围。如果degrees是序列（min, max），则从中随机选取；如果是数字，则选择范围是（-degrees, +degrees）。 resample（{PIL.Image.NEAREST , PIL.Image.BILINEAR , PIL.Image.BICUBIC} , 可选) – 可选的重采样滤波器，见filters。如果该选项忽略，或图片模式是“1”或者“P”则设置为PIL.Image.NEAREST。 expand（bool, 可选）– 可选的扩展标志。如果设置为True, 将输出扩展到足够大从而能容纳全图。如果设置为False或不设置，输出图片将和输入同样大。注意expand标志要求 flag assumes rotation around the center and no translation。 center（2-tuple , 可选）– 可选的旋转中心坐标。以左上角为原点计算。默认是图像中心。 class torchvision.transforms.RandomSizedCrop(*args, **kwargs) 注意：该变换已被弃用，可用RandomResizedCrop代替。 class torchvision.transforms.RandomVerticalFlip(p=0.5) 以给定的概率随机垂直翻折PIL图片。 参数： p (float) – 翻折图片的概率。默认0.5。 class torchvision.transforms.Resize(size, interpolation=2) 将输入PIL图片调整大小到给定形状。 参数： size（序列 或 int）– 期望输出形状。如果size形如（h, w），输出就以该形状。 如果size是int更短的边将调整为int，即如果高>宽，那么图片将调整为（size * 高 / 宽，size）。 interpolation（int, 可选）– 插值方式。默认采用PIL.Image.BILINEAR。 class torchvision.transforms.Scale(*args, **kwargs) 注意：该变换已被弃用，可用Resize代替。 class torchvision.transforms.TenCrop(size, vertical_flip=False) 将PIL图片以四角和中心裁剪，同时加入翻折版本。（默认以水平的方式翻折） 注意： 该变换返回图像元组，可能会导致图片在网络中传导后和数据集给出的标签等信息不能匹配。处理方法见下面的示例。 参数： size（序列 或 int）– 期望裁剪输出的形状。需要裁剪出的形状。如果size是int，将会裁剪成正方形；如果是序列，如(h, w)，将会裁剪成矩形。 vertical_flip（bool）– 是否用垂直翻折。 示例： >>> transform = Compose([ >>> TenCrop(size), # 这里产生了一个PIL图像列表 >>> Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) 返回4维张量 >>> ]) >>> # 在测试阶段你可以这样做： >>> input, target = batch # input是5维张量, target是2维张量 >>> bs, ncrops, c, h, w = input.size() >>> result = model(input.view(-1, c, h, w)) # 把batch size和ncrops融合在一起 >>> result_avg = result.view(bs, ncrops, -1).mean(1) # crops上的平均值 torch.*Tensor上的变换 class torchvision.transforms.LinearTransformation(transformation_matrix) 用一个预先准备好的变换方阵对图片张量做变换。 torch.*Tensor会被transformation_matrix拉平，和变换矩阵做点积后调整到原始张量的形状。 应用： 白化：将数据的分布中心处理到0，计算数据的协方差矩阵。 用np.dot(X.T, X)可以处理到[D x D]的形状，对此做奇异值分解然后传给transformation_matrix即可。 参数： transformation_matrix（Tensor）– [D x D]的张量，D = C x H x W。 class torchvision.transforms.Normalize(mean, std) 用平均值和标准差标准化输入图片。给定n个通道的平均值(M1,...,Mn)和标准差(S1,..,Sn)，这一变换会在torch.*Tensor的每一个通道上进行标准化，即input[channel] = (input[channel] - mean[channel]) / std[channel]。 参数： mean（序列）– 序列，包含各通道的平均值。 std（序列）– 序列，包含各通道的标准差。 __call__(tensor) 参数： tensor (Tensor) – 需要标准化的图像Tensor，形状须为(C, H, W)。 返回： 标准化之后的图片Tensor 返回类型： Tensor。 格式变换 class torchvision.transforms.ToPILImage(mode=None) 把张量或ndarray转化为PIL图像。 把形状为C x H x W的torch.*Tensor或者形状为H x W x C的numpy矩阵ndarray转化为PIL图像，保留值的上下界。 参数 ： mode (PIL.Image mode) – 输入数据的颜色空间或者像素深度（可选）。 如果mode设置为None（默认），按照下面的规则进行处理： 如果输入3通道，mode会设置为RGB。 如果输入4通道，mode会设置为RGBA。 如果输入1通道，mode由数据类型决定(即int，float，short)。 __call__(pic) 参数： pic (Tensor 或 numpy.ndarray）– 要转化成PIL图像的图片。 返回： 转化后的PIL图像。 返回类型： PIL图像。 class torchvision.transforms.ToTensor 将PIL Image或numpy.ndarray转化成张量。 把PIL图像或[0, 255]范围内的numpy.ndarray（形状(H x W x C)）转化成torch.FloatTensor，张量形状(C x H x W)，范围在[0.0, 1.0]中。输入应是是PIL图像且是模式（L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1）中的一种，或输入是numpy.ndarray且类型为np.uint8。 __call__(pic) 参数： pic (PIL图像 或 numpy.ndarray) – 要转化成张量的图片。 返回： 转化后的图像。 返回类型： Tensor 。 通用变换 class torchvision.transforms.Lambda(lambd) 将用户定义的函数用作变换。 参数： lambd (函数) – 用于变换的Lambda函数或函数名。 Functional变换 functional可以提供了一些更加精细的变换，用于搭建复杂的变换流水线。和前面的变换相反，函数变换的参数不包含随机数种子生成器。这意味着你必须指定所有参数的值，但是你可以自己引入随机数。比如，对一组图片使用如下的functional变换： import torchvision.transforms.functional as TF import random def my_segmentation_transforms(image, segmentation): if random.random() > 5: angle = random.randint(-30, 30) image = TF.rotate(image, angle) segmentation = TF.rotate(segmentation, angle) # 更多变换 ... return image, segmentation torchvision.transforms.functional.adjust_brightness(img, brightness_factor) 调整图像亮度。 参数： img（PIL图像）– 要调整的PIL图像。 brightness_factor（float）– 亮度的调整值，可以是任意非负整数。0表示黑色图像，1表示原始图像，2表示增加到2倍亮度。 返回： 调整亮度后的图像。 返回类型： PIL图像。 torchvision.transforms.functional.adjust_contrast(img, contrast_factor) 调整图像对比度。 参数： img（PIL图像）– 要调整的PIL图像。 contrast_factor（float）– 对比度的调整幅度，可以是任意非负数。0表示灰阶图片，1表示原始图片，2表示对比度增加到2倍。 返回： 调整对比度之后的图像。 返回类型： PIL图像。 torchvision.transforms.functional.adjust_gamma(img, gamma, gain=1) 对图像进行伽马矫正。 又称幂率变换。RGB模式下的强度按照下面的等式进行调整： I_{out} = 255 \\times gain \\times (\\dfrac{I_{in}}{255})^\\gamma 更多细节见伽马矫正。 参数： img（PIL图像）– PIL要调整的PIL图像。 gamma（float）– 非负实数，公式中的 \\gamma。gamma大于1时令暗区更暗，gamma小于1时使得暗区更亮。 gain（float）– 常数乘数。 torchvision.transforms.functional.adjust_hue(img, hue_factor) 调整图像色相。 调整时，先把图像转换到HSV空间，然后沿着色相轴（H轴）循环移动。最后切换回图像原始模式。 hue_factor是H通道的偏移量，必须在[-0.5, 0.5]的范围内。 更多细节见色相。 参数： img（PIL图像）– 要调整的PIL图像。 hue_factor（float）– H通道的偏移量应该在[-0.5, 0.5]的范围内。0.5和-0.5分别表示在HSV空间的H轴上沿正、负方向进行移动，0表示不偏移。因此，-0.5和0.5都能表示补色，0表示原图。 返回： 色相调整后的图像。 返回类型： PIL图像。 torchvision.transforms.functional.adjust_saturation(img, saturation_factor) 调整图像的颜色饱和度。 参数： img（PIL图像）– 要调整的PIL图像。 saturation_factor（float）– 饱和度调整值。0表示纯黑白图像，1表示原始图像，2表示增加到原来的2倍。 返回： 调整饱和度之后的图像。 返回类型： PIL图像。 torchvision.transforms.functional.affine(img, angle, translate, scale, shear, resample=0, fillcolor=None) 保持图片像素分布中心不变，进行仿射变换。 参数： img（PIL图像）– 要旋转的PIL图像。 angle（{python:float 或 int}）– 旋转角度，应在时钟方向的-180到180度之间。 translate（list 或 整形数元组）– 水平和垂直变换（旋转之后） scale（float）– 尺度变换。 shear（float）– 扭曲角度，应在时钟方向的-180到180度之间。 resample（PIL.Image.NEAREST 或 PIL.Image.BILINEAR 或 PIL.Image.BICUBIC , 可选）– 可选的重采样滤波器，见滤波器。如果不设置该选项，或者图像模式是“1”或“P”，设置为PIL.Image.NEAREST。 fillcolor（int）– 可选在输出图片的变换外区域可选地填充颜色。（Pillow>=5.0.0） torchvision.transforms.functional.crop(img, i, j, h, w) 裁剪指定PIL图像。 参数： img（PIL图像）– 要裁剪的图像。 i – 最上侧像素的坐标。 j – 最左侧像素的坐标。 h – 要裁剪出的高度。 w – 要裁剪出的宽度。 返回： 裁剪出的图像。 返回类型： PIL图像。 torchvision.transforms.functional.five_crop(img, size) 在四角和中心处裁剪图片。 注意： 该变换返回图像元组，可能会导致图片在网络中传导后和你的Dataset给出的标签等信息不能匹配。 参数： size（序列 或 int）– 希望得到的裁剪输出。如果size是序列(h, w)，输出矩形；如果是int ，输出形状为(size, size)的正方形。 返回： 元组(tl, tr, bl, br, center)，分别表示左上、右上、左下、右下。 返回类型： tuple torchvision.transforms.functional.hflip(img) 将指定图像水平翻折。 参数： img（PIL图像）– 要翻折的图像。 返回： 水平翻折后的图像。 返回类型： PIL图像。 torchvision.transforms.functional.normalize(tensor, mean, std) 用均值和方差将图像标准化。 更多细节见Normalize。 参数： tensor（Tensor）– 需要标准化的图像Tensor，形状应是(C, H, W)。 mean（序列）– 各通道的均值。 std（序列）– 各通道的标准差。 返回： 标准化之后的图像Tensor。 返回类型： Tensor。 torchvision.transforms.functional.pad(img, padding, fill=0, padding_mode='constant') 用指定的填充模式和填充值填充PIL图像。 参数： img（PIL图像）– 要填充的图像。 padding（int 或 tuple）– 各边的填充可宽度。如果指定为int，表示所有边都按照此宽度填充。如果指定为长为2的元组，表示左右和上下边的填充宽度。如果指定为长为4的元组，分别表示左、上、右、下的填充宽度。 fill – 要填充的像素值，默认是0。如果指定为长为3的元组，表示RGB三通道的填充值。这个选项仅在padding_mode是constant时有用。 padding_mode – 填充类型，应当为：constant，edge，reflect或symmetric。默认是constant。 constant：用常数填充，该常数值由fill指定。 edge：用边上的值填充。 reflect： 以边为对称轴进行填充。（不重复边上的值） 在reflect模式中，在两边分别用2个元素填充[1, 2, 3, 4]将会得到[3, 2, 1, 2, 3, 4, 3, 2]。 symmetric：以边为对称轴进行填充。（重复边上的值） 在symmetric模式中，在两边分别用2个元素填充[1, 2, 3, 4]将会得到[2, 1, 1, 2, 3, 4, 4, 3]。 返回： 填充后的图像。 返回类型： PIL图像 torchvision.transforms.functional.resize(img, size, interpolation=2) 将原是PIL图像重新调整到指定形状。 参数： img（PIL图像）– 要调整形状的图像。 size（序列 或 int）– 输出图像的形状。如果size指定为序列(h, w)，输出矩形。如果size指定为int图片的短边将调整为这个数，长边按照相同的长宽比进行调整。即，如果高度>宽度，则图片形状将调整为 (size\\times\\frac{高度}{宽度}, size) interpolation（int, 可选）– 插值方式，默认是PIL.Image.BILINEAR。 返回： 调整大小之后的图片。 返回类型： PIL图像。 torchvision.transforms.functional.resized_crop(img, i, j, h, w, size, interpolation=2) 裁剪PIL并调整到目标形状。 注意：在RandomResizedCrop被调用。 参数： img（PIL图像）– 要裁剪的图像。 i – 最上侧的像素坐标。 j – 最左侧的像素坐标。 h – 裁剪出的图像高度。 w – 裁剪出的图像宽度。 size（序列 或 int）– 要输出的图像形状，同scale。 interpolation（int, 可选）– 插值方式，默认是 PIL.Image.BILINEAR。 返回： 裁剪后的图片。 返回类型： PIL图像。 torchvision.transforms.functional.rotate(img, angle, resample=False, expand=False, center=None) 旋转图片。 参数： img（PIL图像）– 要旋转的PIL图像。 angle（float 或 int}）– 顺时针旋转角度。 resample（PIL.Image.NEAREST 或 PIL.Image.BILINEAR 或 PIL.Image.BICUBIC , 可选） – 可选的重采样滤波器，见滤波器。如果该选项不设置，或者图像模式是“1”或“P”，将被设置为PIL.Image.NEAREST。 expand（bool, 可选）– 可选的扩展选项。如果设置为True，使输出足够大，从而包含了所有像素。如果设置为False或不设置，则输出应和输入形状相同。注意expand选项假定旋转中心是center且不做平移。 center（2-tuple , 可选）– 可选的旋转中心。原点在左上角。默认以图片中心为旋转中心。 torchvision.transforms.functional.ten_crop(img, size, vertical_flip=False) 将图片在四角和中心处裁剪，同时返回它们翻折后的图片。（默认水平翻折） 注意： 该变换返回图像元组，可能会导致图片在网络中传导后和你的Dataset给出的标签等信息不能匹配。 参数： size（序列 或 int）- 裁剪后输出的形状。如果size是int，输出(size, size)的正方形；如果size是序列，输出矩形。 vertical_flip（bool）- 使用垂直翻折。 返回： 元组（tl, tr, bl, br, center, tl_flip, tr_flip, bl_flip, br_flip, center_flip） - 对应的左上、右上、左下、右下、中心裁剪图片和水平翻折后的图片。 返回类型： 元组 torchvision.transforms.functional.to_grayscale(img, num_output_channels=1) 将图像输出成灰阶版本。 参数： img（PIL图像）– 要转化成灰阶图像的图片。 返回： 灰阶版本的图像。如果num_output_channels == 1：返回单通道图像；如果num_output_channels == 3：返回三通道图像，其中r == g == b。 返回类型： PIL图像。 torchvision.transforms.functional.to_pil_image(pic, mode=None) 将张量或ndarray转化为PIL图像。 更多细节见ToPIlImage。 参数： pic（Tensor 或 numpy.ndarray）– 要转化成PIL的图片。 mode（PIL.Image mode）– 输入数据的色彩空间和像素深度。（可选） 返回： 要转化成PIL图像的数据。 返回类型： PIL图像。 torchvision.transforms.functional.to_tensor(pic) 将PIL Image或numpy.ndarray转化成张量。 更多细节见ToTensor。 参数： pic (PIL图像 或 numpy.ndarray) – 要转化成张量的图片。 返回： 转化后的图片。 返回类型： Tensor。 torchvision.transforms.functional.vflip(img) 垂直翻折PIL图像。 参数： img（PIL图像）– 要翻折的图像。 返回： 垂直翻折后的图像。 返回类型： PIL图像。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"torchvision_utils.html":{"url":"torchvision_utils.html","title":"torchvision.utils","keywords":"","body":"torchvision.utils 译者：BXuan694 torchvision.utils.make_grid(tensor, nrow=8, padding=2, normalize=False, range=None, scale_each=False, pad_value=0) 把图片排列成网格形状。 参数： tensor（Tensor 或 list）– 四维批（batch）Tensor或列表。如果是Tensor，其形状应是（B x C x H x W）；如果是列表，元素应为相同大小的图片。 nrow（int, 可选）– 最终展示的图片网格中每行摆放的图片数量。网格的长宽应该是（B / nrow, nrow）。默认是8。 padding（int, 可选）– 扩展填充的像素宽度。默认是2。 normalize（bool, 可选）– 如果设置为True，通过减去最小像素值然后除以最大像素值，把图片移到（0，1）的范围内。 range（tuple, 可选）– 元组（min, max），min和max用于对图片进行标准化处理。默认的，min和max由输入的张量计算得到。 scale_each（bool, 可选）– 如果设置为True，将批中的每张图片按照各自的最值分别缩放，否则使用当前批中所有图片的最值(min, max)进行统一缩放。 pad_value（float, 可选）– 扩展填充的像素值。 示例： 请看 这里 torchvision.utils.save_image(tensor, filename, nrow=8, padding=2, normalize=False, range=None, scale_each=False, pad_value=0) 用于把指定的Tensor保存成图片文件。 参数： tensor（Tensor 或 list）– 需要保存成图片的Tensor。如果Tensor以批的形式给出，则会调用make_grid将这些图片保存成网格的形式。 **kwargs – 其他参数同make_grid。 我们一直在努力 apachecn/pytorch-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-10'); const gitalk = new Gitalk({ clientID: '2e62dee5b9896e2eede6', clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53', repo: 'pytorch-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2019-12-27 08:05:23 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"}}