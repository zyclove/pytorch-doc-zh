
<!DOCTYPE HTML>
<html lang="zh-hans" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>torch.nn · Pytorch 中文文档</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        <meta name="author" content="ApacheCN">
        
        
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-insert-logo/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-code/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-pageview-count/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-emphasize/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-alerts/style.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-auto-scroll-table/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-page-toc-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-tbfed-pagefooter/footer.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-advanced-emoji/emoji-website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search-pro/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="nn_functional.html" />
    
    
    <link rel="prev" href="storage.html" />
    

    <style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
    <script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"}]};
    </script>

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="输入并搜索" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" >
            
                <span>
            
                    
                    中文教程
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="tut_getting_started.html">
            
                <a href="tut_getting_started.html">
            
                    
                    起步
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1.1" data-path="deep_learning_60min_blitz.html">
            
                <a href="deep_learning_60min_blitz.html">
            
                    
                    PyTorch 深度学习: 60 分钟极速入门
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1.1.1" data-path="blitz_tensor_tutorial.html">
            
                <a href="blitz_tensor_tutorial.html">
            
                    
                    什么是 PyTorch？
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.1.2" data-path="blitz_autograd_tutorial.html">
            
                <a href="blitz_autograd_tutorial.html">
            
                    
                    Autograd：自动求导
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.1.3" data-path="blitz_neural_networks_tutorial.html">
            
                <a href="blitz_neural_networks_tutorial.html">
            
                    
                    神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.1.4" data-path="blitz_cifar10_tutorial.html">
            
                <a href="blitz_cifar10_tutorial.html">
            
                    
                    训练分类器
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.1.5" data-path="blitz_data_parallel_tutorial.html">
            
                <a href="blitz_data_parallel_tutorial.html">
            
                    
                    可选：数据并行处理
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.1.2" data-path="data_loading_tutorial.html">
            
                <a href="data_loading_tutorial.html">
            
                    
                    数据加载和处理教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.3" data-path="pytorch_with_examples.html">
            
                <a href="pytorch_with_examples.html">
            
                    
                    用例子学习 PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.4" data-path="transfer_learning_tutorial.html">
            
                <a href="transfer_learning_tutorial.html">
            
                    
                    迁移学习教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.5" data-path="deploy_seq2seq_hybrid_frontend_tutorial.html">
            
                <a href="deploy_seq2seq_hybrid_frontend_tutorial.html">
            
                    
                    混合前端的 seq2seq 模型部署
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.6" data-path="saving_loading_models.html">
            
                <a href="saving_loading_models.html">
            
                    
                    Saving and Loading Models
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.7" data-path="nn_tutorial.html">
            
                <a href="nn_tutorial.html">
            
                    
                    What is torch.nn really?
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="tut_image.html">
            
                <a href="tut_image.html">
            
                    
                    图像
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.2.1" data-path="finetuning_torchvision_models_tutorial.html">
            
                <a href="finetuning_torchvision_models_tutorial.html">
            
                    
                    Torchvision 模型微调
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.2" data-path="spatial_transformer_tutorial.html">
            
                <a href="spatial_transformer_tutorial.html">
            
                    
                    空间变换器网络教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.3" data-path="neural_style_tutorial.html">
            
                <a href="neural_style_tutorial.html">
            
                    
                    使用 PyTorch 进行图像风格转换
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.4" data-path="fgsm_tutorial.html">
            
                <a href="fgsm_tutorial.html">
            
                    
                    对抗性示例生成
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.5" data-path="super_resolution_with_caffe2.html">
            
                <a href="super_resolution_with_caffe2.html">
            
                    
                    使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="tut_text.html">
            
                <a href="tut_text.html">
            
                    
                    文本
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.3.1" data-path="chatbot_tutorial.html">
            
                <a href="chatbot_tutorial.html">
            
                    
                    聊天机器人教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.2" data-path="char_rnn_generation_tutorial.html">
            
                <a href="char_rnn_generation_tutorial.html">
            
                    
                    使用字符级别特征的 RNN 网络生成姓氏
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.3" data-path="char_rnn_classification_tutorial.html">
            
                <a href="char_rnn_classification_tutorial.html">
            
                    
                    使用字符级别特征的 RNN 网络进行姓氏分类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4" data-path="deep_learning_nlp_tutorial.html">
            
                <a href="deep_learning_nlp_tutorial.html">
            
                    
                    Deep Learning for NLP with Pytorch
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.3.4.1" data-path="nlp_pytorch_tutorial.html">
            
                <a href="nlp_pytorch_tutorial.html">
            
                    
                    PyTorch 介绍
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4.2" data-path="nlp_deep_learning_tutorial.html">
            
                <a href="nlp_deep_learning_tutorial.html">
            
                    
                    使用 PyTorch 进行深度学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4.3" data-path="nlp_word_embeddings_tutorial.html">
            
                <a href="nlp_word_embeddings_tutorial.html">
            
                    
                    Word Embeddings: Encoding Lexical Semantics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4.4" data-path="nlp_sequence_models_tutorial.html">
            
                <a href="nlp_sequence_models_tutorial.html">
            
                    
                    序列模型和 LSTM 网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4.5" data-path="nlp_advanced_tutorial.html">
            
                <a href="nlp_advanced_tutorial.html">
            
                    
                    Advanced: Making Dynamic Decisions and the Bi-LSTM CRF
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.3.5" data-path="seq2seq_translation_tutorial.html">
            
                <a href="seq2seq_translation_tutorial.html">
            
                    
                    基于注意力机制的 seq2seq 神经网络翻译
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="tut_generative.html">
            
                <a href="tut_generative.html">
            
                    
                    生成
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.4.1" data-path="dcgan_faces_tutorial.html">
            
                <a href="dcgan_faces_tutorial.html">
            
                    
                    DCGAN Tutorial
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="tut_reinforcement_learning.html">
            
                <a href="tut_reinforcement_learning.html">
            
                    
                    强化学习
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.5.1" data-path="reinforcement_q_learning.html">
            
                <a href="reinforcement_q_learning.html">
            
                    
                    Reinforcement Learning (DQN) Tutorial
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.6" data-path="tut_extending_pytorch.html">
            
                <a href="tut_extending_pytorch.html">
            
                    
                    扩展 PyTorch
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.6.1" data-path="numpy_extensions_tutorial.html">
            
                <a href="numpy_extensions_tutorial.html">
            
                    
                    用 numpy 和 scipy 创建扩展
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.2" data-path="cpp_extension.html">
            
                <a href="cpp_extension.html">
            
                    
                    Custom C++   and CUDA Extensions
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.3" data-path="torch_script_custom_ops.html">
            
                <a href="torch_script_custom_ops.html">
            
                    
                    Extending TorchScript with Custom C++   Operators
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.7" data-path="tut_production_usage.html">
            
                <a href="tut_production_usage.html">
            
                    
                    生产性使用
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.7.1" data-path="dist_tuto.html">
            
                <a href="dist_tuto.html">
            
                    
                    Writing Distributed Applications with PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.2" data-path="aws_distributed_training_tutorial.html">
            
                <a href="aws_distributed_training_tutorial.html">
            
                    
                    使用 Amazon AWS 进行分布式训练
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.3" data-path="ONNXLive.html">
            
                <a href="ONNXLive.html">
            
                    
                    ONNX 现场演示教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.4" data-path="cpp_export.html">
            
                <a href="cpp_export.html">
            
                    
                    在 C++ 中加载 PYTORCH 模型
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.8" data-path="tut_other_language.html">
            
                <a href="tut_other_language.html">
            
                    
                    其它语言中的 PyTorch
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.8.1" data-path="cpp_frontend.html">
            
                <a href="cpp_frontend.html">
            
                    
                    使用 PyTorch C++ 前端
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" >
            
                <span>
            
                    
                    中文文档
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="docs_notes.html">
            
                <a href="docs_notes.html">
            
                    
                    注解
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1.1" data-path="notes_autograd.html">
            
                <a href="notes_autograd.html">
            
                    
                    自动求导机制
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.2" data-path="notes_broadcasting.html">
            
                <a href="notes_broadcasting.html">
            
                    
                    广播语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.3" data-path="notes_cuda.html">
            
                <a href="notes_cuda.html">
            
                    
                    CUDA 语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.4" data-path="notes_extending.html">
            
                <a href="notes_extending.html">
            
                    
                    Extending PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.5" data-path="notes_faq.html">
            
                <a href="notes_faq.html">
            
                    
                    Frequently Asked Questions
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.6" data-path="notes_multiprocessing.html">
            
                <a href="notes_multiprocessing.html">
            
                    
                    Multiprocessing best practices
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.7" data-path="notes_randomness.html">
            
                <a href="notes_randomness.html">
            
                    
                    Reproducibility
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.8" data-path="notes_serialization.html">
            
                <a href="notes_serialization.html">
            
                    
                    Serialization semantics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.9" data-path="notes_windows.html">
            
                <a href="notes_windows.html">
            
                    
                    Windows FAQ
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="docs_package_ref.html">
            
                <a href="docs_package_ref.html">
            
                    
                    包参考
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.2.1" data-path="torch.html">
            
                <a href="torch.html">
            
                    
                    torch
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.2.1.1" data-path="torch_tensors.html">
            
                <a href="torch_tensors.html">
            
                    
                    Tensors
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.1.2" data-path="torch_random_sampling.html">
            
                <a href="torch_random_sampling.html">
            
                    
                    Random sampling
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.1.3" data-path="torch_serialization_parallelism_utilities.html">
            
                <a href="torch_serialization_parallelism_utilities.html">
            
                    
                    Serialization, Parallelism, Utilities
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.1.4" data-path="torch_math_operations.html">
            
                <a href="torch_math_operations.html">
            
                    
                    Math operations
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.2.1.4.1" data-path="torch_math_operations_pointwise_ops.html">
            
                <a href="torch_math_operations_pointwise_ops.html">
            
                    
                    Pointwise Ops
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.1.4.2" data-path="torch_math_operations_reduction_ops.html">
            
                <a href="torch_math_operations_reduction_ops.html">
            
                    
                    Reduction Ops
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.1.4.3" data-path="torch_math_operations_comparison_ops.html">
            
                <a href="torch_math_operations_comparison_ops.html">
            
                    
                    Comparison Ops
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.1.4.4" data-path="torch_math_operations_spectral_ops.html">
            
                <a href="torch_math_operations_spectral_ops.html">
            
                    
                    Spectral Ops
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.1.4.5" data-path="torch_math_operations_other_ops.html">
            
                <a href="torch_math_operations_other_ops.html">
            
                    
                    Other Operations
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.1.4.6" data-path="torch_math_operations_blas_lapack_ops.html">
            
                <a href="torch_math_operations_blas_lapack_ops.html">
            
                    
                    BLAS and LAPACK Operations
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.2.2" data-path="tensors.html">
            
                <a href="tensors.html">
            
                    
                    torch.Tensor
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.3" data-path="tensor_attributes.html">
            
                <a href="tensor_attributes.html">
            
                    
                    Tensor Attributes
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.4" data-path="type_info.html">
            
                <a href="type_info.html">
            
                    
                    数据类型信息
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.5" data-path="sparse.html">
            
                <a href="sparse.html">
            
                    
                    torch.sparse
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.6" data-path="cuda.html">
            
                <a href="cuda.html">
            
                    
                    torch.cuda
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.7" data-path="storage.html">
            
                <a href="storage.html">
            
                    
                    torch.Storage
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.3.2.8" data-path="nn.html">
            
                <a href="nn.html">
            
                    
                    torch.nn
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.9" data-path="nn_functional.html">
            
                <a href="nn_functional.html">
            
                    
                    torch.nn.functional
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.10" data-path="nn_init.html">
            
                <a href="nn_init.html">
            
                    
                    torch.nn.init
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.11" data-path="optim.html">
            
                <a href="optim.html">
            
                    
                    torch.optim
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.12" data-path="autograd.html">
            
                <a href="autograd.html">
            
                    
                    Automatic differentiation package - torch.autograd
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.13" data-path="distributed.html">
            
                <a href="distributed.html">
            
                    
                    Distributed communication package - torch.distributed
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.14" data-path="distributions.html">
            
                <a href="distributions.html">
            
                    
                    Probability distributions - torch.distributions
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.15" data-path="jit.html">
            
                <a href="jit.html">
            
                    
                    Torch Script
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.16" data-path="multiprocessing.html">
            
                <a href="multiprocessing.html">
            
                    
                    多进程包 - torch.multiprocessing
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.17" data-path="bottleneck.html">
            
                <a href="bottleneck.html">
            
                    
                    torch.utils.bottleneck
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.18" data-path="checkpoint.html">
            
                <a href="checkpoint.html">
            
                    
                    torch.utils.checkpoint
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.19" data-path="docs_cpp_extension.html">
            
                <a href="docs_cpp_extension.html">
            
                    
                    torch.utils.cpp_extension
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.20" data-path="data.html">
            
                <a href="data.html">
            
                    
                    torch.utils.data
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.21" data-path="dlpack.html">
            
                <a href="dlpack.html">
            
                    
                    torch.utils.dlpack
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.22" data-path="hub.html">
            
                <a href="hub.html">
            
                    
                    torch.hub
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.23" data-path="model_zoo.html">
            
                <a href="model_zoo.html">
            
                    
                    torch.utils.model_zoo
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.24" data-path="onnx.html">
            
                <a href="onnx.html">
            
                    
                    torch.onnx
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.25" data-path="distributed_deprecated.html">
            
                <a href="distributed_deprecated.html">
            
                    
                    Distributed communication package (deprecated) - torch.distributed.deprecated
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="docs_torchvision_ref.html">
            
                <a href="docs_torchvision_ref.html">
            
                    
                    torchvision 参考
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.3.1" data-path="torchvision_datasets.html">
            
                <a href="torchvision_datasets.html">
            
                    
                    torchvision.datasets
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.2" data-path="torchvision_models.html">
            
                <a href="torchvision_models.html">
            
                    
                    torchvision.models
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.3" data-path="torchvision_transforms.html">
            
                <a href="torchvision_transforms.html">
            
                    
                    torchvision.transforms
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.4" data-path="torchvision_utils.html">
            
                <a href="torchvision_utils.html">
            
                    
                    torchvision.utils
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本书使用 GitBook 发布
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >torch.nn</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="torchnn">torch.nn</h1>
<h2 id="parameters&#xFF08;&#x53C2;&#x6570;&#xFF09;">Parameters&#xFF08;&#x53C2;&#x6570;&#xFF09;</h2>
<pre><code class="lang-py">class torch.nn.Parameter
</code></pre>
<p>Parameters&#x5BF9;&#x8C61;&#x662F;&#x4E00;&#x79CD;&#x4F1A;&#x88AB;&#x89C6;&#x4E3A;&#x6A21;&#x5757;&#x53C2;&#x6570;&#xFF08;module parameter&#xFF09;&#x7684;Tensor&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters&#x7C7B;&#x662F;<a href="tensors.html#torch.Tensor" title="torch.Tensor"><code>Tensor</code></a> &#x7684;&#x5B50;&#x7C7B;, &#x4E0D;&#x8FC7;&#x76F8;&#x5BF9;&#x4E8E;&#x5B83;&#x7684;&#x7236;&#x7C7B;&#xFF0C;Parameters&#x7C7B;&#x6709;&#x4E00;&#x4E2A;&#x5F88;&#x91CD;&#x8981;&#x7684;&#x7279;&#x6027;&#x5C31;&#x662F;&#x5F53;&#x5176;&#x5728; <a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a>&#x7C7B;&#x4E2D;&#x88AB;&#x4F7F;&#x7528;&#x5E76;&#x88AB;&#x5F53;&#x505A;&#x8FD9;&#x4E2A;<a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a>&#x7C7B;&#x7684;&#x6A21;&#x5757;&#x5C5E;&#x6027;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x90A3;&#x4E48;&#x8FD9;&#x4E2A;Parameters&#x5BF9;&#x8C61;&#x4F1A;&#x88AB;&#x81EA;&#x52A8;&#x5730;&#x6DFB;&#x52A0;&#x5230;&#x8FD9;&#x4E2A;<a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a>&#x7C7B;&#x7684;&#x53C2;&#x6570;&#x5217;&#x8868;(list of parameters)&#x4E4B;&#x4E2D;&#xFF0C;&#x540C;&#x65F6;&#x4E5F;&#x5C31;&#x4F1A;&#x88AB;&#x6DFB;&#x52A0;&#x5165;&#x6B64;<a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a>&#x7C7B;&#x7684; <a href="#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><code>parameters()</code></a>&#x65B9;&#x6CD5;&#x6240;&#x8FD4;&#x56DE;&#x7684;&#x53C2;&#x6570;&#x8FED;&#x4EE3;&#x5668;&#x4E2D;&#x3002;&#x800C;Parameters&#x7C7B;&#x7684;&#x7236;&#x7C7B;Tensor&#x7C7B;&#x4E5F;&#x53EF;&#x4EE5;&#x88AB;&#x7528;&#x4E3A;&#x6784;&#x5EFA;&#x6A21;&#x5757;&#x7684;&#x5C5E;&#x6027;&#xFF0C;&#x4F46;&#x4E0D;&#x4F1A;&#x88AB;&#x52A0;&#x5165;&#x53C2;&#x6570;&#x5217;&#x8868;&#x3002;&#x8FD9;&#x6837;&#x4E3B;&#x8981;&#x662F;&#x56E0;&#x4E3A;&#xFF0C;&#x6709;&#x65F6;&#x53EF;&#x80FD;&#x9700;&#x8981;&#x5728;&#x6A21;&#x578B;&#x4E2D;&#x5B58;&#x50A8;&#x4E00;&#x4E9B;&#x975E;&#x6A21;&#x578B;&#x53C2;&#x6570;&#x7684;&#x4E34;&#x65F6;&#x72B6;&#x6001;&#xFF0C;&#x6BD4;&#x5982;RNN&#x4E2D;&#x7684;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x9690;&#x72B6;&#x6001;&#x3002;&#x800C;&#x901A;&#x8FC7;&#x4F7F;&#x7528;&#x975E;<a href="#torch.nn.Parameter" title="torch.nn.Parameter"><code>Parameter</code></a>&#x7684;Tensor&#x7C7B;&#xFF0C;&#x53EF;&#x4EE5;&#x5C06;&#x8FD9;&#x4E9B;&#x4E34;&#x65F6;&#x53D8;&#x91CF;&#x6CE8;&#x518C;(register)&#x4E3A;&#x6A21;&#x578B;&#x7684;&#x5C5E;&#x6027;&#x7684;&#x540C;&#x65F6;&#x4F7F;&#x5176;&#x4E0D;&#x88AB;&#x52A0;&#x5165;&#x53C2;&#x6570;&#x5217;&#x8868;&#x3002;</p>
<p>Parameters: </p>
<ul>
<li><strong>data</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; &#x53C2;&#x6570;&#x5F20;&#x91CF;(parameter tensor).</li>
<li><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; &#x53C2;&#x6570;&#x662F;&#x5426;&#x9700;&#x8981;&#x68AF;&#x5EA6;&#xFF0C; &#x9ED8;&#x8BA4;&#x4E3A; <code>True</code>&#x3002;&#x66F4;&#x591A;&#x7EC6;&#x8282;&#x8BF7;&#x770B; <a href="notes/autograd.html#excluding-subgraphs">&#x5982;&#x4F55;&#x5C06;&#x5B50;&#x56FE;&#x8E22;&#x51FA;&#x53CD;&#x5411;&#x4F20;&#x64AD;&#x8FC7;&#x7A0B;</a>&#x3002; </li>
</ul>
<h2 id="containers&#xFF08;&#x5BB9;&#x5668;&#xFF09;">Containers&#xFF08;&#x5BB9;&#x5668;&#xFF09;</h2>
<h3 id="module&#xFF08;&#x6A21;&#x5757;&#xFF09;">Module&#xFF08;&#x6A21;&#x5757;&#xFF09;</h3>
<pre><code class="lang-py">class torch.nn.Module
</code></pre>
<p>&#x6A21;&#x5757;&#xFF08;Module&#xFF09;&#x662F;&#x6240;&#x6709;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x6A21;&#x578B;&#x7684;&#x57FA;&#x7C7B;&#x3002;</p>
<p>&#x4F60;&#x521B;&#x5EFA;&#x6A21;&#x578B;&#x7684;&#x65F6;&#x5019;&#x4E5F;&#x5E94;&#x8BE5;&#x7EE7;&#x627F;&#x8FD9;&#x4E2A;&#x7C7B;&#x54E6;&#x3002;</p>
<p>&#x6A21;&#x5757;(Module)&#x4E2D;&#x8FD8;&#x53EF;&#x4EE5;&#x5305;&#x542B;&#x5176;&#x4ED6;&#x7684;&#x6A21;&#x5757;&#xFF0C;&#x4F60;&#x53EF;&#x4EE5;&#x5C06;&#x4E00;&#x4E2A;&#x6A21;&#x5757;&#x8D4B;&#x503C;&#x6210;&#x4E3A;&#x53E6;&#x4E00;&#x4E2A;&#x6A21;&#x5757;&#x7684;&#x5C5E;&#x6027;&#xFF0C;&#x4ECE;&#x800C;&#x6210;&#x4E3A;&#x8FD9;&#x4E2A;&#x6A21;&#x5757;&#x7684;&#x4E00;&#x4E2A;&#x5B50;&#x6A21;&#x5757;&#x3002;&#x800C;&#x901A;&#x8FC7;&#x4E0D;&#x65AD;&#x7684;&#x8D4B;&#x503C;&#xFF0C;&#x4F60;&#x53EF;&#x4EE5;&#x5C06;&#x4E0D;&#x540C;&#x7684;&#x6A21;&#x5757;&#x7EC4;&#x7EC7;&#x6210;&#x4E00;&#x4E2A;&#x6811;&#x7ED3;&#x6784;:</p>
<pre><code class="lang-py"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Model</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>) <span class="hljs-comment"># &#x5F53;&#x524D;&#x7684;nn.Conv2d&#x6A21;&#x5757;&#x5C31;&#x88AB;&#x8D4B;&#x503C;&#x6210;&#x4E3A;Model&#x6A21;&#x5757;&#x7684;&#x4E00;&#x4E2A;&#x5B50;&#x6A21;&#x5757;&#xFF0C;&#x6210;&#x4E3A;&#x201C;&#x6811;&#x7ED3;&#x6784;&#x201D;&#x7684;&#x53F6;&#x5B50;</span>
        self.conv2 = nn.Conv2d(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
       x = F.relu(self.conv1(x))
       <span class="hljs-keyword">return</span> F.relu(self.conv2(x))
</code></pre>
<p>&#x901A;&#x8FC7;&#x8D4B;&#x503C;&#x8FD9;&#x79CD;&#x65B9;&#x5F0F;&#x6DFB;&#x52A0;&#x7684;&#x5B50;&#x6A21;&#x5757;&#x5C06;&#x4F1A;&#x88AB;&#x6A21;&#x578B;&#x6CE8;&#x518C;(register)&#xFF0C;&#x800C;&#x540E;&#x5F53;&#x8C03;&#x7528;&#x6A21;&#x5757;&#x7684;&#x4E00;&#x4E9B;&#x53C2;&#x6570;&#x8F6C;&#x6362;&#x51FD;&#x6570;&#xFF08;<a href="#torch.nn.Module.to" title="torch.nn.Module.to"><code>to()</code></a>&#xFF09;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x5B50;&#x6A21;&#x5757;&#x7684;&#x53C2;&#x6570;&#x4E5F;&#x4F1A;&#x4E00;&#x5E76;&#x8F6C;&#x6362;&#x3002;</p>
<pre><code class="lang-py">add_module(name, module)
</code></pre>
<p>&#x5411;&#x5F53;&#x524D;&#x6A21;&#x5757;&#x6DFB;&#x52A0;&#x4E00;&#x4E2A;&#x5B50;&#x6A21;&#x5757;&#x3002;
&#x6B64;&#x5B50;&#x6A21;&#x5757;&#x53EF;&#x4EE5;&#x4F5C;&#x4E3A;&#x5F53;&#x524D;&#x6A21;&#x5757;&#x7684;&#x5C5E;&#x6027;&#x88AB;&#x8BBF;&#x95EE;&#x5230;&#xFF0C;&#x800C;&#x5C5E;&#x6027;&#x540D;&#x5C31;&#x662F;add_module()&#x51FD;&#x6570;&#x4E2D;&#x7684;name&#x53C2;&#x6570;&#x3002;</p>
<p>add_module()&#x51FD;&#x6570;&#x53C2;&#x6570;: </p>
<ul>
<li><strong>name</strong> (<em>string</em>) &#x2013; &#x5B50;&#x6A21;&#x5757;&#x7684;&#x540D;&#x5B57;. &#x51FD;&#x6570;&#x8C03;&#x7528;&#x5B8C;&#x6210;&#x540E;&#xFF0C;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x8BBF;&#x95EE;&#x5F53;&#x524D;&#x6A21;&#x5757;&#x7684;&#x6B64;&#x5B57;&#x6BB5;&#x6765;&#x8BBF;&#x95EE;&#x8BE5;&#x5B50;&#x6A21;&#x5757;&#x3002;</li>
<li><strong>parameter</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) &#x2013; &#x8981;&#x6DFB;&#x52A0;&#x5230;&#x5F53;&#x524D;&#x6A21;&#x5757;&#x7684;&#x5B50;&#x6A21;&#x5757;&#x3002;</li>
</ul>
<pre><code class="lang-py">apply(fn)
</code></pre>
<p>apply()&#x51FD;&#x6570;&#x7684;&#x4E3B;&#x8981;&#x4F5C;&#x7528;&#x662F;&#x5C06; <code>fn</code> &#x9012;&#x5F52;&#x5730;&#x5E94;&#x7528;&#x4E8E;&#x6A21;&#x5757;&#x7684;&#x6240;&#x6709;&#x5B50;&#x6A21;&#x5757;&#xFF08;<code>.children()</code>&#x51FD;&#x6570;&#x7684;&#x8FD4;&#x56DE;&#x503C;&#xFF09;&#x4EE5;&#x53CA;&#x6A21;&#x5757;&#x81EA;&#x8EAB;&#x3002;&#x6B64;&#x51FD;&#x6570;&#x7684;&#x4E00;&#x4E2A;&#x7ECF;&#x5178;&#x5E94;&#x7528;&#x5C31;&#x662F;&#x521D;&#x59CB;&#x5316;&#x6A21;&#x578B;&#x7684;&#x6240;&#x6709;&#x53C2;&#x6570;&#x8FD9;&#x4E00;&#x8FC7;&#x7A0B;(&#x540C;&#x6837;&#x53C2;&#x89C1;&#x4E8E; torch-nn-init)&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>fn</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> -&gt; None) &#x2013; &#x8981;&#x5E94;&#x7528;&#x4E8E;&#x6240;&#x6709;&#x5B50;&#x6A21;&#x578B;&#x7684;&#x51FD;&#x6570;</th>
</tr>
</thead>
<tbody>
<tr>
<td>Returns:</td>
<td>self</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
<tr>
<td>Return type:</td>
<td><a href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init_weights</span><span class="hljs-params">(m)</span>:</span>
 print(m)
 <span class="hljs-keyword">if</span> type(m) == nn.Linear:
 m.weight.data.fill_(<span class="hljs-number">1.0</span>)
 print(m.weight)

<span class="hljs-meta">&gt;&gt;&gt; </span>net = nn.Sequential(nn.Linear(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), nn.Linear(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>net.apply(init_weights) <span class="hljs-comment"># &#x5C06;init_weights()&#x51FD;&#x6570;&#x5E94;&#x7528;&#x4E8E;&#x6A21;&#x5757;&#x7684;&#x6240;&#x6709;&#x5B50;&#x6A21;&#x5757;</span>
Linear(in_features=<span class="hljs-number">2</span>, out_features=<span class="hljs-number">2</span>, bias=<span class="hljs-keyword">True</span>)
Parameter containing:
tensor([[ <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>],
 [ <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>]])
Linear(in_features=<span class="hljs-number">2</span>, out_features=<span class="hljs-number">2</span>, bias=<span class="hljs-keyword">True</span>)
Parameter containing:
tensor([[ <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>],
 [ <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>]])
Sequential(
 (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">2</span>, out_features=<span class="hljs-number">2</span>, bias=<span class="hljs-keyword">True</span>)
 (<span class="hljs-number">1</span>): Linear(in_features=<span class="hljs-number">2</span>, out_features=<span class="hljs-number">2</span>, bias=<span class="hljs-keyword">True</span>)
)
Sequential(
 (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">2</span>, out_features=<span class="hljs-number">2</span>, bias=<span class="hljs-keyword">True</span>)
 (<span class="hljs-number">1</span>): Linear(in_features=<span class="hljs-number">2</span>, out_features=<span class="hljs-number">2</span>, bias=<span class="hljs-keyword">True</span>)
)
</code></pre>
<pre><code class="lang-py">buffers(recurse=<span class="hljs-keyword">True</span>)
</code></pre>
<p>&#x8FD4;&#x56DE;&#x6A21;&#x5757;&#x7684;&#x7F13;&#x51B2;&#x533A;&#x7684;&#x8FED;&#x4EE3;&#x5668;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>recurse</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a>) &#x2013; &#x5982;&#x679C;&#x8BBE;&#x7F6E;&#x4E3A;True&#xFF0C;&#x4EA7;&#x751F;&#x7684;&#x7F13;&#x51B2;&#x533A;&#x8FED;&#x4EE3;&#x5668;&#x4F1A;&#x904D;&#x5386;&#x6A21;&#x5757;&#x81EA;&#x5DF1;&#x4E0E;&#x6240;&#x6709;&#x5B50;&#x6A21;&#x5757;&#xFF0C;&#x5426;&#x5219;&#x53EA;&#x4F1A;&#x904D;&#x5386;&#x6A21;&#x5757;&#x7684;&#x76F4;&#x8FDE;&#x7684;&#x6210;&#x5458;&#x3002;</th>
</tr>
</thead>
<tbody>
<tr>
<td>Yields:</td>
<td><em>torch.Tensor</em> &#x2013; &#x6A21;&#x578B;&#x7F13;&#x51B2;&#x533A;</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<p>&#x4E3E;&#x4F8B;:</p>
<pre><code class="lang-py">&gt;&gt;&gt; for buf in model.buffers():
&gt;&gt;&gt;     print(type(buf.data), buf.size())
&lt;class &apos;torch.FloatTensor&apos;&gt; (20L,)
&lt;class &apos;torch.FloatTensor&apos;&gt; (20L, 1L, 5L, 5L)
</code></pre>
<pre><code class="lang-py">children()
</code></pre>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5F53;&#x524D;&#x6240;&#x6709;&#x5B50;&#x6A21;&#x5757;&#x7684;&#x8FED;&#x4EE3;&#x5668;
Returns an iterator over immediate children modules.</p>
<table>
<thead>
<tr>
<th>Yields:</th>
<th><em>Module</em> &#x2013; &#x5B50;&#x6A21;&#x5757;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">cpu()
</code></pre>
<p>&#x5C06;&#x6A21;&#x578B;&#x7684;&#x6240;&#x6709;&#x53C2;&#x6570;(parameter)&#x548C;&#x7F13;&#x51B2;&#x533A;(buffer)&#x90FD;&#x8F6C;&#x79FB;&#x5230;CPU&#x5185;&#x5B58;&#x4E2D;&#x3002;</p>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>self</th>
</tr>
</thead>
<tbody>
<tr>
<td>Return type:</td>
<td><a href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">cuda(device=<span class="hljs-keyword">None</span>)
</code></pre>
<p>&#x5C06;&#x6A21;&#x578B;&#x7684;&#x6240;&#x6709;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x533A;&#x90FD;&#x8F6C;&#x79FB;&#x5230;CUDA&#x8BBE;&#x5907;&#x5185;&#x5B58;&#x4E2D;&#x3002;</p>
<p>&#x56E0;&#x4E3A;cuda()&#x51FD;&#x6570;&#x540C;&#x65F6;&#x4F1A;&#x5C06;&#x5904;&#x7406;&#x6A21;&#x5757;&#x4E2D;&#x7684;&#x6240;&#x6709;&#x53C2;&#x6570;&#x5E76;&#x7F13;&#x5B58;&#x8FD9;&#x4E9B;&#x53C2;&#x6570;&#x7684;&#x5BF9;&#x8C61;&#x3002;&#x6240;&#x4EE5;&#x5982;&#x679C;&#x60F3;&#x8BA9;&#x6A21;&#x5757;&#x5728;GPU&#x4E0A;&#x8FDB;&#x884C;&#x4F18;&#x5316;&#x64CD;&#x4F5C;&#xFF0C;&#x4E00;&#x5B9A;&#x8981;&#x5728;&#x6784;&#x5EFA;&#x4F18;&#x5316;&#x5668;&#x4E4B;&#x524D;&#x8C03;&#x7528;&#x6A21;&#x5757;&#x7684;cuda()&#x51FD;&#x6570;&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>device</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5982;&#x679C;&#x8BBE;&#x5907;&#x7F16;&#x53F7;&#x88AB;&#x6307;&#x5B9A;&#xFF0C;&#x6240;&#x6709;&#x7684;&#x53C2;&#x6570;&#x90FD;&#x4F1A;&#x88AB;&#x62F7;&#x8D1D;&#x5230;&#x7F16;&#x53F7;&#x6307;&#x5B9A;&#x8BBE;&#x5907;&#x4E0A;</th>
</tr>
</thead>
<tbody>
<tr>
<td>Returns:</td>
<td>self</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
<tr>
<td>Return type:</td>
<td><a href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">double()
</code></pre>
<p>&#x5C06;&#x6240;&#x6709;&#x7684;&#x6D6E;&#x70B9;&#x6570;&#x7C7B;&#x578B;&#x7684;&#x53C2;&#x6570;(parameters)&#x548C;&#x7F13;&#x51B2;&#x533A;(buffers)&#x8F6C;&#x6362;&#x4E3A;<code>double</code>&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;</p>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>self</th>
</tr>
</thead>
<tbody>
<tr>
<td>Return type:</td>
<td><a href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">dump_patches = <span class="hljs-keyword">False</span>
</code></pre>
<p>&#x8FD9;&#x4E2A;&#x5B57;&#x6BB5;&#x53EF;&#x4EE5;&#x4E3A;<a href="#torch.nn.Module.load_state_dict" title="torch.nn.Module.load_state_dict"><code>load_state_dict()</code></a>&#x63D0;&#x4F9B; BC &#x652F;&#x6301;&#xFF08;BC support&#x5B9E;&#x5728;&#x4E0D;&#x61C2;&#x662F;&#x4EC0;&#x4E48;&#x610F;&#x601D;-.-&#xFF09;&#x3002; &#x5728; <a href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict()</code></a>&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x7684;&#x72B6;&#x6001;&#x5B57;&#x5178;&#xFF08;state dict&#xFF09;&#x4E2D;&#xFF0C; &#x6709;&#x4E00;&#x4E2A;&#x540D;&#x4E3A;<code>_metadata</code>&#x7684;&#x5C5E;&#x6027;&#x4E2D;&#x5B58;&#x50A8;&#x4E86;&#x8FD9;&#x4E2A;state_dict&#x7684;&#x7248;&#x672C;&#x53F7;&#x3002;<code>_metadata</code>&#x662F;&#x4E00;&#x4E2A;&#x9075;&#x4ECE;&#x4E86;&#x72B6;&#x6001;&#x5B57;&#x5178;&#xFF08;state dict&#xFF09;&#x7684;&#x547D;&#x540D;&#x89C4;&#x8303;&#x7684;&#x5173;&#x952E;&#x5B57;&#x5B57;&#x5178;&#xFF0C; &#x8981;&#x60F3;&#x4E86;&#x89E3;&#x8FD9;&#x4E2A;<code>_metadata</code>&#x5728;&#x52A0;&#x8F7D;&#x72B6;&#x6001;&#xFF08;loading state dict&#xFF09;&#x7684;&#x65F6;&#x5019;&#x662F;&#x600E;&#x4E48;&#x7528;&#x7684;&#xFF0C;&#x53EF;&#x4EE5;&#x770B;&#x4E00;&#x4E0B; <code>_load_from_state_dict</code>&#x90E8;&#x5206;&#x7684;&#x6587;&#x6863;&#x3002;</p>
<p>&#x5982;&#x679C;&#x65B0;&#x7684;&#x53C2;&#x6570;/&#x7F13;&#x51B2;&#x533A;&#x88AB;&#x6DFB;&#x52A0;&#x4E8E;/&#x79FB;&#x9664;&#x81EA;&#x8FD9;&#x4E2A;&#x6A21;&#x5757;&#x4E4B;&#x4E2D;&#x65F6;&#xFF0C;&#x8FD9;&#x4E2A;&#x7248;&#x672C;&#x53F7;&#x6570;&#x5B57;&#x4F1A;&#x968F;&#x4E4B;&#x53D1;&#x751F;&#x53D8;&#x5316;&#x3002;&#x540C;&#x65F6;&#x6A21;&#x5757;&#x7684;<code>_load_from_state_dict</code>&#x65B9;&#x6CD5;&#x4F1A;&#x6BD4;&#x8F83;&#x7248;&#x672C;&#x53F7;&#x7684;&#x4FE1;&#x606F;&#x5E76;&#x4F9D;&#x636E;&#x6B64;&#x72B6;&#x6001;&#x8BCD;&#x5178;&#xFF08;state dict&#xFF09;&#x7684;&#x53D8;&#x5316;&#x505A;&#x51FA;&#x4E00;&#x4E9B;&#x9002;&#x5F53;&#x7684;&#x8C03;&#x6574;&#x3002;</p>
<pre><code class="lang-py">eval()
</code></pre>
<p>&#x5C06;&#x6A21;&#x5757;&#x8F6C;&#x6362;&#x4E3A;&#x6D4B;&#x8BD5;&#x6A21;&#x5F0F;&#x3002;</p>
<p>&#x8FD9;&#x4E2A;&#x51FD;&#x6570;&#x53EA;&#x5BF9;&#x7279;&#x5B9A;&#x7684;&#x6A21;&#x5757;&#x7C7B;&#x578B;&#x6709;&#x6548;&#xFF0C;&#x5982; <a href="#torch.nn.Dropout" title="torch.nn.Dropout"><code>Dropout</code></a>&#x548C;<code>BatchNorm</code>&#x7B49;&#x7B49;&#x3002;&#x5982;&#x679C;&#x60F3;&#x4E86;&#x89E3;&#x8FD9;&#x4E9B;&#x7279;&#x5B9A;&#x6A21;&#x5757;&#x5728;&#x8BAD;&#x7EC3;/&#x6D4B;&#x8BD5;&#x6A21;&#x5F0F;&#x4E0B;&#x5404;&#x81EA;&#x7684;&#x8FD0;&#x4F5C;&#x7EC6;&#x8282;&#xFF0C;&#x53EF;&#x4EE5;&#x770B;&#x4E00;&#x4E0B;&#x8FD9;&#x4E9B;&#x7279;&#x6B8A;&#x6A21;&#x5757;&#x7684;&#x6587;&#x6863;&#x90E8;&#x5206;&#x3002;</p>
<pre><code class="lang-py">extra_repr()
</code></pre>
<p>&#x4E3A;&#x6A21;&#x5757;&#x8BBE;&#x7F6E;&#x989D;&#x5916;&#x7684;&#x5C55;&#x793A;&#x4FE1;&#x606F;(extra representation)&#x3002;</p>
<p>&#x5982;&#x679C;&#x60F3;&#x8981;&#x6253;&#x5370;&#x5C55;&#x793A;(print)&#x4F60;&#x7684;&#x6A21;&#x5757;&#x7684;&#x4E00;&#x4E9B;&#x5B9A;&#x5236;&#x7684;&#x989D;&#x5916;&#x4FE1;&#x606F;&#xFF0C;&#x90A3;&#x4F60;&#x5E94;&#x8BE5;&#x5728;&#x4F60;&#x7684;&#x6A21;&#x5757;&#x4E2D;&#x590D;&#x73B0;&#x8FD9;&#x4E2A;&#x51FD;&#x6570;&#x3002;&#x5355;&#x884C;&#x548C;&#x591A;&#x884C;&#x7684;&#x5B57;&#x7B26;&#x4E32;&#x90FD;&#x53EF;&#x4EE5;&#x88AB;&#x63A5;&#x53D7;&#x3002;</p>
<pre><code class="lang-py">float()
</code></pre>
<p>&#x5C06;&#x6240;&#x6709;&#x6D6E;&#x70B9;&#x6570;&#x7C7B;&#x578B;&#x7684;&#x53C2;&#x6570;(parameters)&#x548C;&#x7F13;&#x51B2;&#x533A;(buffers)&#x8F6C;&#x6362;&#x4E3A;<code>float</code>&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;</p>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>self</th>
</tr>
</thead>
<tbody>
<tr>
<td>Return type:</td>
<td><a href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">forward(*input)
</code></pre>
<p>&#x5B9A;&#x4E49;&#x4E86;&#x6BCF;&#x6B21;&#x6A21;&#x5757;&#x88AB;&#x8C03;&#x7528;&#x4E4B;&#x540E;&#x6240;&#x8FDB;&#x884C;&#x7684;&#x8BA1;&#x7B97;&#x8FC7;&#x7A0B;&#x3002;</p>
<p>&#x5E94;&#x8BE5;&#x88AB;Module&#x7C7B;&#x7684;&#x6240;&#x6709;&#x5B50;&#x7C7B;&#x91CD;&#x5199;&#x3002;</p>
<p>Note</p>
<p>&#x5C3D;&#x7BA1;&#x6A21;&#x5757;&#x7684;&#x524D;&#x5411;&#x64CD;&#x4F5C;&#x90FD;&#x88AB;&#x5B9A;&#x4E49;&#x5728;&#x8FD9;&#x4E2A;&#x51FD;&#x6570;&#x91CC;&#x9762;&#xFF0C;&#x4F46;&#x662F;&#x5F53;&#x4F60;&#x8981;&#x8FDB;&#x884C;&#x6A21;&#x5757;&#x7684;&#x524D;&#x5411;&#x64CD;&#x4F5C;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x8FD8;&#x662F;&#x8981;&#x76F4;&#x63A5;&#x8C03;&#x7528;&#x6A21;&#x5757;<a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> &#x7684;&#x5B9E;&#x4F8B;&#x51FD;&#x6570;&#xFF0C;&#x800C;&#x4E0D;&#x662F;&#x76F4;&#x63A5;&#x8C03;&#x7528;&#x8FD9;&#x4E2A;forward()&#x51FD;&#x6570;&#x3002;&#x8FD9;&#x4E3B;&#x8981;&#x662F;&#x56E0;&#x4E3A;&#x524D;&#x8005;&#x4F1A;&#x7167;&#x987E;&#x5230;&#x6CE8;&#x518C;&#x5728;&#x6B64;&#x6A21;&#x5757;&#x4E4B;&#x4E0A;&#x7684;&#x94A9;&#x5B50;&#x51FD;&#x6570;&#xFF08;the registered hooks&#xFF09;&#x7684;&#x8FD0;&#x884C;&#xFF0C;&#x800C;&#x540E;&#x8005;&#x5219;&#x4E0D;&#x4F1A;&#x3002;</p>
<pre><code class="lang-py">half()
</code></pre>
<p>&#x5C06;&#x6240;&#x6709;&#x7684;&#x6D6E;&#x70B9;&#x6570;&#x7C7B;&#x578B;&#x7684;&#x53C2;&#x6570;(parameters)&#x548C;&#x7F13;&#x51B2;&#x533A;(buffers)&#x8F6C;&#x6362;&#x4E3A;<code>half</code>&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;</p>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>self</th>
</tr>
</thead>
<tbody>
<tr>
<td>Return type:</td>
<td><a href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">load_state_dict(state_dict, strict=<span class="hljs-keyword">True</span>)
</code></pre>
<p>&#x5C06;<a href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict</code></a>&#x4E2D;&#x7684;&#x53C2;&#x6570;&#xFF08;parameters&#xFF09;&#x548C;&#x7F13;&#x51B2;&#x533A;&#xFF08;buffers&#xFF09;&#x62F7;&#x8D1D;&#x5230;&#x6A21;&#x5757;&#x548C;&#x5176;&#x5B50;&#x6A21;&#x5757;&#x4E4B;&#x4E2D;&#x3002;&#x5982;&#x679C;<code>strict</code>&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>&#xFF0C;&#x90A3;&#x4E48;<a href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict</code></a>&#x4E2D;&#x7684;&#x952E;&#x503C;&#xFF08;keys&#xFF09;&#x5FC5;&#x987B;&#x4E0E;&#x6A21;&#x578B;&#x7684;[<code>state_dict()</code>]&#x51FD;&#x6570;&#x6240;&#x8FD4;&#x56DE;&#x7684;&#x952E;&#x503C;&#xFF08;keys&#xFF09;&#x4FE1;&#x606F;&#x4FDD;&#x6301;&#x5B8C;&#x5168;&#x7684;&#x4E00;&#x81F4;&#x3002;</p>
<p>load_state_dict()&#x51FD;&#x6570;&#x53C2;&#x6570;&#xFF1A; </p>
<ul>
<li><strong>state_dict</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)" target="_blank"><em>dict</em></a>) &#x2013; &#x4E00;&#x4E2A;&#x5305;&#x542B;&#x4E86;&#x53C2;&#x6570;&#x548C;&#x6301;&#x4E45;&#x7F13;&#x51B2;&#x533A;&#x7684;&#x5B57;&#x5178;&#x3002;</li>
<li><strong>strict</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; &#x662F;&#x5426;&#x4E25;&#x683C;&#x8981;&#x6C42; <a href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict</code></a> &#x4E2D;&#x7684;&#x952E;&#x503C;&#xFF08;keys&#xFF09;&#x4E0E;&#x6A21;&#x578B; <a href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict()</code></a> &#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x7684;&#x952E;&#x503C;&#xFF08;keys&#xFF09;&#x4FE1;&#x606F;&#x4FDD;&#x6301;&#x5B8C;&#x5168;&#x4E00;&#x81F4;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; <code>True</code></li>
</ul>
<pre><code class="lang-py">modules()
</code></pre>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5F53;&#x524D;&#x6A21;&#x5757;&#x5185;&#x6240;&#x6709;&#x6A21;&#x5757;&#xFF08;&#x5305;&#x62EC;&#x81EA;&#x8EAB;&#xFF09;&#x7684;&#x8FED;&#x4EE3;&#x5668;&#x3002;</p>
<table>
<thead>
<tr>
<th>Yields:</th>
<th><em>Module</em> &#x2013; a module in the network</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>Note</p>
<p>&#x6CE8;&#x610F;&#x91CD;&#x590D;&#x7684;&#x6A21;&#x5757;&#x53EA;&#x4F1A;&#x88AB;&#x8FD4;&#x56DE;&#x4E00;&#x6B21;&#x3002;&#x6BD4;&#x5728;&#x4E0B;&#x9762;&#x8FD9;&#x4E2A;&#x4F8B;&#x5B50;&#x4E2D;&#xFF0C;<code>l</code>&#x5C31;&#x53EA;&#x4F1A;&#x88AB;&#x8FD4;&#x56DE;&#x4E00;&#x6B21;&#x3002;</p>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py">&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.modules()):
 print(idx, &apos;-&gt;&apos;, m)

0 -&gt; Sequential (
 (0): Linear (2 -&gt; 2)
 (1): Linear (2 -&gt; 2)
)
1 -&gt; Linear (2 -&gt; 2)
</code></pre>
<pre><code class="lang-py">named_buffers(prefix=<span class="hljs-string">&apos;&apos;</span>, recurse=<span class="hljs-keyword">True</span>)
</code></pre>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x6A21;&#x5757;&#x7F13;&#x51B2;&#x533A;&#x7684;&#x8FED;&#x4EE3;&#x5668;&#xFF0C;&#x6BCF;&#x6B21;&#x8FD4;&#x56DE;&#x7684;&#x5143;&#x7D20;&#x662F;&#x7531;&#x7F13;&#x51B2;&#x533A;&#x7684;&#x540D;&#x5B57;&#x548C;&#x7F13;&#x51B2;&#x533A;&#x81EA;&#x8EAB;&#x7EC4;&#x6210;&#x7684;&#x5143;&#x7EC4;&#x3002;</p>
<p>named_buffers()&#x51FD;&#x6570;&#x7684;&#x53C2;&#x6570;: </p>
<ul>
<li><strong>prefix</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)" target="_blank"><em>str</em></a>) &#x2013; &#x8981;&#x6DFB;&#x52A0;&#x5728;&#x6240;&#x6709;&#x7F13;&#x51B2;&#x533A;&#x540D;&#x5B57;&#x4E4B;&#x524D;&#x7684;&#x524D;&#x7F00;&#x3002;</li>
<li><strong>recurse</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a>) &#x2013; &#x5982;&#x679C;&#x8BBE;&#x7F6E;&#x4E3A;True&#xFF0C;&#x90A3;&#x6837;&#x8FED;&#x4EE3;&#x5668;&#x4E2D;&#x4E0D;&#x5149;&#x4F1A;&#x8FD4;&#x56DE;&#x8FD9;&#x4E2A;&#x6A21;&#x5757;&#x81EA;&#x8EAB;&#x76F4;&#x8FDE;&#x6210;&#x5458;&#x7684;&#x7F13;&#x51B2;&#x533A;&#xFF0C;&#x540C;&#x65F6;&#x4E5F;&#x4F1A;&#x9012;&#x5F52;&#x8FD4;&#x56DE;&#x5176;&#x5B50;&#x6A21;&#x5757;&#x7684;&#x7F13;&#x51B2;&#x533A;&#x3002;&#x5426;&#x5219;&#xFF0C;&#x53EA;&#x8FD4;&#x56DE;&#x8FD9;&#x4E2A;&#x6A21;&#x5757;&#x76F4;&#x8FDE;&#x6210;&#x5458;&#x7684;&#x7F13;&#x51B2;&#x533A;&#x3002;</li>
</ul>
<table>
<thead>
<tr>
<th>Yields:</th>
<th><em>(string, torch.Tensor)</em> &#x2013; &#x5305;&#x542B;&#x4E86;&#x7F13;&#x51B2;&#x533A;&#x7684;&#x540D;&#x5B57;&#x548C;&#x7F13;&#x51B2;&#x533A;&#x81EA;&#x8EAB;&#x7684;&#x5143;&#x7EC4;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> name, buf <span class="hljs-keyword">in</span> self.named_buffers():
<span class="hljs-meta">&gt;&gt;&gt; </span>   <span class="hljs-keyword">if</span> name <span class="hljs-keyword">in</span> [<span class="hljs-string">&apos;running_var&apos;</span>]:
<span class="hljs-meta">&gt;&gt;&gt; </span>       print(buf.size())
</code></pre>
<pre><code class="lang-py">named_children()
</code></pre>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5F53;&#x524D;&#x6A21;&#x578B;&#x76F4;&#x8FDE;&#x7684;&#x5B50;&#x6A21;&#x5757;&#x7684;&#x8FED;&#x4EE3;&#x5668;&#xFF0C;&#x6BCF;&#x6B21;&#x8FD4;&#x56DE;&#x7684;&#x5143;&#x7D20;&#x662F;&#x7531;&#x5B50;&#x6A21;&#x5757;&#x7684;&#x540D;&#x5B57;&#x548C;&#x5B50;&#x6A21;&#x5757;&#x81EA;&#x8EAB;&#x7EC4;&#x6210;&#x7684;&#x5143;&#x7EC4;&#x3002;</p>
<table>
<thead>
<tr>
<th>Yields:</th>
<th><em>(string, Module)</em> &#x2013; &#x5305;&#x542B;&#x4E86;&#x5B50;&#x6A21;&#x5757;&#x7684;&#x540D;&#x5B57;&#x548C;&#x5B50;&#x6A21;&#x5757;&#x81EA;&#x8EAB;&#x7684;&#x5143;&#x7EC4;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> name, module <span class="hljs-keyword">in</span> model.named_children():
<span class="hljs-meta">&gt;&gt;&gt; </span>    <span class="hljs-keyword">if</span> name <span class="hljs-keyword">in</span> [<span class="hljs-string">&apos;conv4&apos;</span>, <span class="hljs-string">&apos;conv5&apos;</span>]:
<span class="hljs-meta">&gt;&gt;&gt; </span>        print(module)
</code></pre>
<pre><code class="lang-py">named_modules(memo=<span class="hljs-keyword">None</span>, prefix=<span class="hljs-string">&apos;&apos;</span>)
</code></pre>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5F53;&#x524D;&#x6A21;&#x5757;&#x5185;&#x6240;&#x6709;&#x6A21;&#x5757;&#xFF08;&#x5305;&#x62EC;&#x81EA;&#x8EAB;&#xFF09;&#x7684;&#x8FED;&#x4EE3;&#x5668;&#xFF0C;&#x6BCF;&#x6B21;&#x8FD4;&#x56DE;&#x7684;&#x5143;&#x7D20;&#x662F;&#x7531;&#x6A21;&#x5757;&#x7684;&#x540D;&#x5B57;&#x548C;&#x6A21;&#x5757;&#x81EA;&#x8EAB;&#x7EC4;&#x6210;&#x7684;&#x5143;&#x7EC4;&#x3002;</p>
<table>
<thead>
<tr>
<th>Yields:</th>
<th><em>(string, Module)</em> &#x2013; &#x6A21;&#x5757;&#x540D;&#x5B57;&#x548C;&#x6A21;&#x5757;&#x81EA;&#x8EAB;&#x7EC4;&#x6210;&#x7684;&#x5143;&#x7EC4;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>Note</p>
<p>&#x91CD;&#x590D;&#x7684;&#x6A21;&#x5757;&#x53EA;&#x4F1A;&#x88AB;&#x8FD4;&#x56DE;&#x4E00;&#x6B21;&#x3002;&#x5728;&#x4E0B;&#x9762;&#x7684;&#x4F8B;&#x5B50;&#x4E2D;&#xFF0C;<code>l</code>&#x53EA;&#x88AB;&#x8FD4;&#x56DE;&#x4E86;&#x4E00;&#x6B21;&#x3002;</p>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-py">&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):
 print(idx, &apos;-&gt;&apos;, m)

0 -&gt; (&apos;&apos;, Sequential (
 (0): Linear (2 -&gt; 2)
 (1): Linear (2 -&gt; 2)
))
1 -&gt; (&apos;0&apos;, Linear (2 -&gt; 2))
</code></pre>
<pre><code class="lang-py">named_parameters(prefix=<span class="hljs-string">&apos;&apos;</span>, recurse=<span class="hljs-keyword">True</span>)
</code></pre>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5F53;&#x524D;&#x6A21;&#x5757;&#x5185;&#x6240;&#x6709;&#x53C2;&#x6570;&#x7684;&#x8FED;&#x4EE3;&#x5668;&#xFF0C;&#x6BCF;&#x6B21;&#x8FD4;&#x56DE;&#x7684;&#x5143;&#x7D20;&#x662F;&#x7531;&#x53C2;&#x6570;&#x7684;&#x540D;&#x5B57;&#x548C;&#x53C2;&#x6570;&#x81EA;&#x8EAB;&#x7EC4;&#x6210;&#x7684;&#x5143;&#x7EC4;&#x3002;</p>
<p>named_parameters()&#x51FD;&#x6570;&#x53C2;&#x6570;&#xFF1A;</p>
<ul>
<li><strong>prefix</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)" target="_blank"><em>str</em></a>) &#x2013; &#x8981;&#x5728;&#x6240;&#x6709;&#x53C2;&#x6570;&#x540D;&#x5B57;&#x524D;&#x9762;&#x6DFB;&#x52A0;&#x7684;&#x524D;&#x7F00;&#x3002;</li>
<li><strong>recurse</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a>) &#x2013; &#x5982;&#x679C;&#x8BBE;&#x7F6E;&#x4E3A;True&#xFF0C;&#x90A3;&#x6837;&#x8FED;&#x4EE3;&#x5668;&#x4E2D;&#x4E0D;&#x5149;&#x4F1A;&#x8FD4;&#x56DE;&#x8FD9;&#x4E2A;&#x6A21;&#x5757;&#x81EA;&#x8EAB;&#x76F4;&#x8FDE;&#x6210;&#x5458;&#x7684;&#x53C2;&#x6570;&#xFF0C;&#x540C;&#x65F6;&#x4E5F;&#x4F1A;&#x8FD4;&#x56DE;&#x5176;&#x5B50;&#x6A21;&#x5757;&#x7684;&#x53C2;&#x6570;&#x3002;&#x5426;&#x5219;&#xFF0C;&#x53EA;&#x8FD4;&#x56DE;&#x8FD9;&#x4E2A;&#x6A21;&#x5757;&#x76F4;&#x8FDE;&#x6210;&#x5458;&#x7684;&#x53C2;&#x6570;&#x3002;</li>
</ul>
<table>
<thead>
<tr>
<th>Yields:</th>
<th><em>(string, Parameter)</em> &#x2013; &#x53C2;&#x6570;&#x540D;&#x5B57;&#x548C;&#x53C2;&#x6570;&#x81EA;&#x8EAB;&#x7EC4;&#x6210;&#x7684;&#x5143;&#x7EC4;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> self.named_parameters():
<span class="hljs-meta">&gt;&gt;&gt; </span>   <span class="hljs-keyword">if</span> name <span class="hljs-keyword">in</span> [<span class="hljs-string">&apos;bias&apos;</span>]:
<span class="hljs-meta">&gt;&gt;&gt; </span>       print(param.size())
</code></pre>
<pre><code class="lang-py">parameters(recurse=<span class="hljs-keyword">True</span>)
</code></pre>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x904D;&#x5386;&#x6A21;&#x5757;&#x6240;&#x6709;&#x53C2;&#x6570;&#x7684;&#x8FED;&#x4EE3;&#x5668;&#x3002;
parameters()&#x51FD;&#x6570;&#x4E00;&#x4E2A;&#x7ECF;&#x5178;&#x7684;&#x5E94;&#x7528;&#x5C31;&#x662F;&#x5B9E;&#x8DF5;&#x4E2D;&#x7ECF;&#x5E38;&#x5C06;&#x6B64;&#x51FD;&#x6570;&#x7684;&#x8FD4;&#x56DE;&#x503C;&#x4F20;&#x5165;&#x4F18;&#x5316;&#x5668;&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>recurse</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a>) &#x2013;  &#x5982;&#x679C;&#x8BBE;&#x7F6E;&#x4E3A;True&#xFF0C;&#x90A3;&#x6837;&#x8FED;&#x4EE3;&#x5668;&#x4E2D;&#x4E0D;&#x5149;&#x4F1A;&#x8FD4;&#x56DE;&#x8FD9;&#x4E2A;&#x6A21;&#x5757;&#x81EA;&#x8EAB;&#x76F4;&#x8FDE;&#x6210;&#x5458;&#x7684;&#x53C2;&#x6570;&#xFF0C;&#x540C;&#x65F6;&#x4E5F;&#x4F1A;&#x9012;&#x5F52;&#x8FD4;&#x56DE;&#x5176;&#x5B50;&#x6A21;&#x5757;&#x7684;&#x53C2;&#x6570;&#x3002;&#x5426;&#x5219;&#xFF0C;&#x53EA;&#x8FD4;&#x56DE;&#x8FD9;&#x4E2A;&#x6A21;&#x5757;&#x76F4;&#x8FDE;&#x6210;&#x5458;&#x7684;&#x53C2;&#x6570;&#x3002;</th>
</tr>
</thead>
<tbody>
<tr>
<td>Yields:</td>
<td><em>Parameter</em> &#x2013; &#x6A21;&#x5757;&#x53C2;&#x6570;</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py">&gt;&gt;&gt; for param in model.parameters():
&gt;&gt;&gt;     print(type(param.data), param.size())
&lt;class &apos;torch.FloatTensor&apos;&gt; (20L,)
&lt;class &apos;torch.FloatTensor&apos;&gt; (20L, 1L, 5L, 5L)
</code></pre>
<pre><code class="lang-py">register_backward_hook(hook)
</code></pre>
<p>&#x5728;&#x6A21;&#x5757;&#x4E0A;&#x6CE8;&#x518C;&#x4E00;&#x4E2A;&#x6302;&#x8F7D;&#x5728;&#x53CD;&#x5411;&#x64CD;&#x4F5C;&#x4E4B;&#x540E;&#x7684;&#x94A9;&#x5B50;&#x51FD;&#x6570;&#x3002;&#xFF08;&#x6302;&#x8F7D;&#x5728;backward&#x4E4B;&#x540E;&#x8FD9;&#x4E2A;&#x70B9;&#x4E0A;&#x7684;&#x94A9;&#x5B50;&#x51FD;&#x6570;&#xFF09;</p>
<p>&#x5BF9;&#x4E8E;&#x6BCF;&#x6B21;&#x8F93;&#x5165;&#xFF0C;&#x5F53;&#x6A21;&#x5757;&#x5173;&#x4E8E;&#x6B64;&#x6B21;&#x8F93;&#x5165;&#x7684;&#x53CD;&#x5411;&#x68AF;&#x5EA6;&#x7684;&#x8BA1;&#x7B97;&#x8FC7;&#x7A0B;&#x5B8C;&#x6210;&#xFF0C;&#x8BE5;&#x94A9;&#x5B50;&#x51FD;&#x6570;&#x90FD;&#x4F1A;&#x88AB;&#x8C03;&#x7528;&#x4E00;&#x6B21;&#x3002;&#x6B64;&#x94A9;&#x5B50;&#x51FD;&#x6570;&#x9700;&#x8981;&#x9075;&#x4ECE;&#x4EE5;&#x4E0B;&#x51FD;&#x6570;&#x7B7E;&#x540D;&#xFF1A;</p>
<pre><code class="lang-py">hook(module, grad_input, grad_output) -&gt; Tensor or None
</code></pre>
<p>&#x5982;&#x679C;&#x6A21;&#x5757;&#x7684;&#x8F93;&#x5165;&#x6216;&#x8F93;&#x51FA;&#x662F;&#x591A;&#x91CD;&#x7684;&#xFF08;multiple inputs or outputs&#xFF09;&#xFF0C;&#x90A3; <code>grad_input</code> &#x548C; <code>grad_output</code> &#x5E94;&#x5F53;&#x662F;&#x5143;&#x7EC4;&#x6570;&#x636E;&#x3002; &#x94A9;&#x5B50;&#x51FD;&#x6570;&#x4E0D;&#x80FD;&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x53C2;&#x6570;<code>grad_input</code> &#x548C; <code>grad_output</code>&#x8FDB;&#x884C;&#x4EFB;&#x4F55;&#x66F4;&#x6539;&#xFF0C;&#x4F46;&#x662F;&#x53EF;&#x4EE5;&#x9009;&#x62E9;&#x6027;&#x5730;&#x6839;&#x636E;&#x8F93;&#x5165;&#x7684;&#x53C2;&#x6570;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x68AF;&#x5EA6;&#x56DE;&#x53BB;&#xFF0C;&#x800C;&#x8FD9;&#x4E2A;&#x65B0;&#x7684;&#x68AF;&#x5EA6;&#x5728;&#x540E;&#x7EED;&#x7684;&#x8BA1;&#x7B97;&#x4E2D;&#x4F1A;&#x66FF;&#x6362;&#x6389;<code>grad_input</code>&#x3002;</p>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>&#x4E00;&#x4E2A;&#x53E5;&#x67C4;&#xFF08;handle&#xFF09;&#xFF0C;&#x8FD9;&#x4E2A;handle&#x7684;&#x7279;&#x70B9;&#x5C31;&#x662F;&#x901A;&#x8FC7;&#x8C03;&#x7528;<code>handle.remove()</code>&#x51FD;&#x6570;&#x5C31;&#x53EF;&#x4EE5;&#x5C06;&#x8FD9;&#x4E2A;&#x6DFB;&#x52A0;&#x4E8E;&#x6A21;&#x5757;&#x4E4B;&#x4E0A;&#x7684;&#x94A9;&#x5B50;&#x79FB;&#x9664;&#x6389;&#x3002;</th>
</tr>
</thead>
<tbody>
<tr>
<td>Return type:</td>
<td><code>torch.utils.hooks.RemovableHandle</code></td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<p>Warning</p>
<p>&#x5BF9;&#x4E8E;&#x4E00;&#x4E9B;&#x5177;&#x6709;&#x5F88;&#x591A;&#x590D;&#x6742;&#x64CD;&#x4F5C;&#x7684;<a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a>&#xFF0C;&#x5F53;&#x524D;&#x7684;hook&#x5B9E;&#x73B0;&#x7248;&#x672C;&#x8FD8;&#x4E0D;&#x80FD;&#x8FBE;&#x5230;&#x5B8C;&#x5168;&#x7406;&#x60F3;&#x7684;&#x6548;&#x679C;&#x3002;&#x4E3E;&#x4E2A;&#x4F8B;&#x5B50;&#xFF0C;&#x6709;&#x4E9B;&#x9519;&#x8BEF;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x51FD;&#x6570;&#x7684;&#x8F93;&#x5165;&#x53C2;&#x6570;<code>grad_input</code> &#x548C; <code>grad_output</code>&#x4E2D;&#x53EF;&#x80FD;&#x53EA;&#x662F;&#x771F;&#x6B63;&#x7684;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x53D8;&#x91CF;&#x7684;&#x4E00;&#x4E2A;&#x5B50;&#x96C6;&#x3002;&#x5BF9;&#x4E8E;&#x6B64;&#x7C7B;&#x7684;<a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a>&#xFF0C;&#x4F60;&#x5E94;&#x8BE5;&#x4F7F;&#x7528;[<code>torch.Tensor.register_hook()</code>]&#x76F4;&#x63A5;&#x5C06;&#x94A9;&#x5B50;&#x6302;&#x8F7D;&#x5230;&#x67D0;&#x4E2A;&#x7279;&#x5B9A;&#x7684;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x7684;&#x53D8;&#x91CF;&#x4E0A;&#xFF0C;&#x800C;&#x4E0D;&#x662F;&#x5F53;&#x524D;&#x7684;&#x6A21;&#x5757;&#x3002;</p>
<pre><code class="lang-py">register_buffer(name, tensor)
</code></pre>
<p>&#x5F80;&#x6A21;&#x5757;&#x4E0A;&#x6DFB;&#x52A0;&#x4E00;&#x4E2A;&#x6301;&#x4E45;&#x7F13;&#x51B2;&#x533A;&#x3002;</p>
<p>&#x8FD9;&#x4E2A;&#x51FD;&#x6570;&#x7684;&#x7ECF;&#x5E38;&#x4F1A;&#x88AB;&#x7528;&#x4E8E;&#x5411;&#x6A21;&#x5757;&#x6DFB;&#x52A0;&#x4E0D;&#x4F1A;&#x88AB;&#x8BA4;&#x4E3A;&#x662F;&#x6A21;&#x5757;&#x53C2;&#x6570;&#xFF08;model parameter&#xFF09;&#x7684;&#x7F13;&#x51B2;&#x533A;&#x3002;&#x4E3E;&#x4E2A;&#x6817;&#x5B50;&#xFF0C;BatchNorm&#x7684;<code>running_mean</code>&#x5C31;&#x4E0D;&#x662F;&#x4E00;&#x4E2A;&#x53C2;&#x6570;&#xFF0C;&#x4F46;&#x5374;&#x5C5E;&#x4E8E;&#x6301;&#x4E45;&#x72B6;&#x6001;&#x3002;</p>
<p>&#x6240;&#x6DFB;&#x52A0;&#x7684;&#x7F13;&#x51B2;&#x533A;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x7ED9;&#x5B9A;&#x7684;&#x540D;&#x5B57;(name&#x53C2;&#x6570;)&#x4EE5;&#x8BBF;&#x95EE;&#x6A21;&#x5757;&#x7684;&#x5C5E;&#x6027;&#x7684;&#x65B9;&#x5F0F;&#x8FDB;&#x884C;&#x8BBF;&#x95EE;&#x3002;</p>
<p>register_buffer()&#x51FD;&#x6570;&#x7684;&#x53C2;&#x6570;: </p>
<ul>
<li><strong>name</strong> (<em>string</em>) &#x2013; &#x8981;&#x6DFB;&#x52A0;&#x7684;&#x7F13;&#x51B2;&#x533A;&#x7684;&#x540D;&#x5B57;&#x3002;&#x6240;&#x6DFB;&#x52A0;&#x7684;&#x7F13;&#x51B2;&#x533A;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x6B64;&#x540D;&#x5B57;&#x4EE5;&#x8BBF;&#x95EE;&#x6A21;&#x5757;&#x7684;&#x5C5E;&#x6027;&#x7684;&#x65B9;&#x5F0F;&#x8FDB;&#x884C;&#x8BBF;&#x95EE;&#x3002;</li>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; &#x9700;&#x8981;&#x6CE8;&#x518C;&#x5230;&#x6A21;&#x5757;&#x4E0A;&#x7684;&#x7F13;&#x51B2;&#x533A;&#x3002;</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>self.register_buffer(<span class="hljs-string">&apos;running_mean&apos;</span>, torch.zeros(num_features))
</code></pre>
<pre><code class="lang-py">register_forward_hook(hook)
</code></pre>
<p>&#x5728;&#x6A21;&#x5757;&#x4E0A;&#x6CE8;&#x518C;&#x4E00;&#x4E2A;&#x6302;&#x8F7D;&#x5728;&#x524D;&#x5411;&#x64CD;&#x4F5C;&#x4E4B;&#x540E;&#x7684;&#x94A9;&#x5B50;&#x51FD;&#x6570;&#x3002;&#xFF08;&#x6302;&#x8F7D;&#x5728;forward&#x64CD;&#x4F5C;&#x7ED3;&#x675F;&#x4E4B;&#x540E;&#x8FD9;&#x4E2A;&#x70B9;&#xFF09;</p>
<p>&#x6B64;&#x94A9;&#x5B50;&#x51FD;&#x6570;&#x5728;&#x6BCF;&#x6B21;&#x6A21;&#x5757;&#x7684; <a href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code>forward()</code></a>&#x51FD;&#x6570;&#x8FD0;&#x884C;&#x7ED3;&#x675F;&#x4EA7;&#x751F;output&#x4E4B;&#x540E;&#x5C31;&#x4F1A;&#x88AB;&#x89E6;&#x53D1;&#x3002;&#x6B64;&#x94A9;&#x5B50;&#x51FD;&#x6570;&#x9700;&#x8981;&#x9075;&#x4ECE;&#x4EE5;&#x4E0B;&#x51FD;&#x6570;&#x7B7E;&#x540D;&#xFF1A;</p>
<pre><code class="lang-py">hook(module, input, output) -&gt; None
</code></pre>
<p>&#x6B64;&#x94A9;&#x5B50;&#x51FD;&#x6570;&#x4E0D;&#x80FD;&#x8FDB;&#x884C;&#x4F1A;&#x4FEE;&#x6539; input &#x548C; output &#x8FD9;&#x4E24;&#x4E2A;&#x53C2;&#x6570;&#x7684;&#x64CD;&#x4F5C;&#x3002;</p>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>&#x4E00;&#x4E2A;&#x53E5;&#x67C4;&#xFF08;handle&#xFF09;&#xFF0C;&#x8FD9;&#x4E2A;handle&#x7684;&#x7279;&#x70B9;&#x5C31;&#x662F;&#x901A;&#x8FC7;&#x8C03;&#x7528;<code>handle.remove()</code>&#x51FD;&#x6570;&#x5C31;&#x53EF;&#x4EE5;&#x5C06;&#x8FD9;&#x4E2A;&#x6DFB;&#x52A0;&#x4E8E;&#x6A21;&#x5757;&#x4E4B;&#x4E0A;&#x7684;&#x94A9;&#x5B50;&#x79FB;&#x9664;&#x6389;&#x3002;</th>
</tr>
</thead>
<tbody>
<tr>
<td>Return type:</td>
<td><code>torch.utils.hooks.RemovableHandle</code></td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">register_forward_pre_hook(hook)
</code></pre>
<p>&#x5728;&#x6A21;&#x5757;&#x4E0A;&#x6CE8;&#x518C;&#x4E00;&#x4E2A;&#x6302;&#x8F7D;&#x5728;&#x524D;&#x5411;&#x64CD;&#x4F5C;&#x4E4B;&#x524D;&#x7684;&#x94A9;&#x5B50;&#x51FD;&#x6570;&#x3002;&#xFF08;&#x6302;&#x8F7D;&#x5728;forward&#x64CD;&#x4F5C;&#x5F00;&#x59CB;&#x4E4B;&#x524D;&#x8FD9;&#x4E2A;&#x70B9;&#xFF09;</p>
<p>&#x6B64;&#x94A9;&#x5B50;&#x51FD;&#x6570;&#x5728;&#x6BCF;&#x6B21;&#x6A21;&#x5757;&#x7684; <a href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code>forward()</code></a>&#x51FD;&#x6570;&#x8FD0;&#x884C;&#x5F00;&#x59CB;&#x4E4B;&#x524D;&#x4F1A;&#x88AB;&#x89E6;&#x53D1;&#x3002;&#x6B64;&#x94A9;&#x5B50;&#x51FD;&#x6570;&#x9700;&#x8981;&#x9075;&#x4ECE;&#x4EE5;&#x4E0B;&#x51FD;&#x6570;&#x7B7E;&#x540D;&#xFF1A;
The hook will be called every time before <a href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code>forward()</code></a> is invoked. It should have the following signature:</p>
<pre><code class="lang-py">hook(module, input) -&gt; None
</code></pre>
<p>&#x6B64;&#x94A9;&#x5B50;&#x51FD;&#x6570;&#x4E0D;&#x80FD;&#x8FDB;&#x884C;&#x4F1A;&#x4FEE;&#x6539; input &#x8FD9;&#x4E2A;&#x53C2;&#x6570;&#x7684;&#x64CD;&#x4F5C;&#x3002;</p>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>&#x4E00;&#x4E2A;&#x53E5;&#x67C4;&#xFF08;handle&#xFF09;&#xFF0C;&#x8FD9;&#x4E2A;handle&#x7684;&#x7279;&#x70B9;&#x5C31;&#x662F;&#x901A;&#x8FC7;&#x8C03;&#x7528;<code>handle.remove()</code>&#x51FD;&#x6570;&#x5C31;&#x53EF;&#x4EE5;&#x5C06;&#x8FD9;&#x4E2A;&#x6DFB;&#x52A0;&#x4E8E;&#x6A21;&#x5757;&#x4E4B;&#x4E0A;&#x7684;&#x94A9;&#x5B50;&#x79FB;&#x9664;&#x6389;&#x3002;</th>
</tr>
</thead>
<tbody>
<tr>
<td>Return type:</td>
<td><code>torch.utils.hooks.RemovableHandle</code></td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">register_parameter(name, param)
</code></pre>
<p>&#x5411;&#x6A21;&#x5757;&#x6DFB;&#x52A0;&#x4E00;&#x4E2A;&#x53C2;&#x6570;&#xFF08;parameter&#xFF09;&#x3002;</p>
<p>&#x6240;&#x6DFB;&#x52A0;&#x7684;&#x53C2;&#x6570;&#xFF08;parameter&#xFF09;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x7ED9;&#x5B9A;&#x7684;&#x540D;&#x5B57;(name&#x53C2;&#x6570;)&#x4EE5;&#x8BBF;&#x95EE;&#x6A21;&#x5757;&#x7684;&#x5C5E;&#x6027;&#x7684;&#x65B9;&#x5F0F;&#x8FDB;&#x884C;&#x8BBF;&#x95EE;&#x3002;</p>
<p>register_parameter()&#x51FD;&#x6570;&#x7684;&#x53C2;&#x6570;&#xFF1A; </p>
<ul>
<li><strong>name</strong> (<em>string</em>) &#x2013; &#x6240;&#x6DFB;&#x52A0;&#x7684;&#x53C2;&#x6570;&#x7684;&#x540D;&#x5B57;. &#x6240;&#x6DFB;&#x52A0;&#x7684;&#x53C2;&#x6570;&#xFF08;parameter&#xFF09;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x6B64;&#x540D;&#x5B57;&#x4EE5;&#x8BBF;&#x95EE;&#x6A21;&#x5757;&#x7684;&#x5C5E;&#x6027;&#x7684;&#x65B9;&#x5F0F;&#x8FDB;&#x884C;&#x8BBF;&#x95EE;</li>
<li><strong>parameter</strong> (<a href="#torch.nn.Parameter" title="torch.nn.Parameter"><em>Parameter</em></a>) &#x2013; &#x8981;&#x6DFB;&#x52A0;&#x5230;&#x6A21;&#x5757;&#x4E4B;&#x4E0A;&#x7684;&#x53C2;&#x6570;&#x3002;</li>
</ul>
<pre><code class="lang-py">state_dict(destination=<span class="hljs-keyword">None</span>, prefix=<span class="hljs-string">&apos;&apos;</span>, keep_vars=<span class="hljs-keyword">False</span>)
</code></pre>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5305;&#x542B;&#x4E86;&#x6A21;&#x5757;&#x5F53;&#x524D;&#x6240;&#x6709;&#x72B6;&#x6001;(state)&#x7684;&#x5B57;&#x5178;(dictionary)&#x3002;</p>
<p>&#x6240;&#x6709;&#x7684;&#x53C2;&#x6570;&#x548C;&#x6301;&#x4E45;&#x7F13;&#x51B2;&#x533A;&#x90FD;&#x88AB;&#x56CA;&#x62EC;&#x5728;&#x5176;&#x4E2D;&#x3002;&#x5B57;&#x5178;&#x7684;&#x952E;&#x503C;&#x5C31;&#x662F;&#x54CD;&#x5E94;&#x7684;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x533A;&#x7684;&#x540D;&#x5B57;(name)&#x3002;</p>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>&#x4E00;&#x4E2A;&#x5305;&#x542B;&#x4E86;&#x6A21;&#x5757;&#x5F53;&#x524D;&#x6240;&#x6709;&#x72B6;&#x6001;&#x7684;&#x5B57;&#x5178;</th>
</tr>
</thead>
<tbody>
<tr>
<td>Return type:</td>
<td><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)" target="_blank">dict</a></td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>module.state_dict().keys()
[<span class="hljs-string">&apos;bias&apos;</span>, <span class="hljs-string">&apos;weight&apos;</span>]
</code></pre>
<pre><code class="lang-py">to(*args, **kwargs)
</code></pre>
<p>&#x79FB;&#x52A8; &#x5E76;&#x4E14;/&#x6216;&#x8005;&#xFF08;and/or&#xFF09;&#x8F6C;&#x6362;&#x6240;&#x6709;&#x7684;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x533A;&#x3002;</p>
<p>&#x8FD9;&#x4E2A;&#x51FD;&#x6570;&#x53EF;&#x4EE5;&#x8FD9;&#x6837;&#x8C03;&#x7528;&#xFF1A;</p>
<pre><code class="lang-py">to(device=<span class="hljs-keyword">None</span>, dtype=<span class="hljs-keyword">None</span>, non_blocking=<span class="hljs-keyword">False</span>)
</code></pre>
<pre><code class="lang-py">to(dtype, non_blocking=<span class="hljs-keyword">False</span>)
</code></pre>
<pre><code class="lang-py">to(tensor, non_blocking=<span class="hljs-keyword">False</span>)
</code></pre>
<p>&#x6B64;&#x51FD;&#x6570;&#x7684;&#x51FD;&#x6570;&#x7B7E;&#x540D;&#x8DDF;<a href="tensors.html#torch.Tensor.to" title="torch.Tensor.to"><code>torch.Tensor.to()</code></a>&#x51FD;&#x6570;&#x7684;&#x51FD;&#x6570;&#x7B7E;&#x540D;&#x5F88;&#x76F8;&#x4F3C;&#xFF0C;&#x53EA;&#x4E0D;&#x8FC7;&#x8FD9;&#x4E2A;&#x51FD;&#x6570;<code>dtype</code>&#x53C2;&#x6570;&#x53EA;&#x63A5;&#x53D7;&#x6D6E;&#x70B9;&#x6570;&#x7C7B;&#x578B;&#x7684;dtype&#xFF0C;&#x5982;float&#xFF0C; double&#xFF0C; half&#xFF08; floating point desired <code>dtype</code> s&#xFF09;&#x3002;&#x540C;&#x65F6;&#xFF0C;&#x8FD9;&#x4E2A;&#x65B9;&#x6CD5;&#x53EA;&#x4F1A;&#x5C06;&#x6D6E;&#x70B9;&#x6570;&#x7C7B;&#x578B;&#x7684;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x533A;&#xFF08;the floating point parameters and buffers&#xFF09;&#x8F6C;&#x5316;&#x4E3A;<code>dtype</code>&#xFF08;&#x5982;&#x679C;&#x8F93;&#x5165;&#x53C2;&#x6570;&#x4E2D;&#x7ED9;&#x5B9A;&#x7684;&#x8BDD;&#xFF09;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;&#x800C;&#x5BF9;&#x4E8E;&#x6574;&#x6570;&#x7C7B;&#x578B;&#x7684;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x533A;&#xFF08;the integral parameters and buffers&#xFF09;&#xFF0C;&#x5373;&#x4FBF;&#x8F93;&#x5165;&#x53C2;&#x6570;&#x4E2D;&#x7ED9;&#x5B9A;&#x4E86;<code>dtype</code>&#xFF0C;&#x4E5F;&#x4E0D;&#x4F1A;&#x8FDB;&#x884C;&#x8F6C;&#x6362;&#x64CD;&#x4F5C;&#xFF0C;&#x800C;&#x5982;&#x679C;&#x7ED9;&#x5B9A;&#x4E86; <code>device</code>&#x53C2;&#x6570;&#xFF0C;&#x79FB;&#x52A8;&#x64CD;&#x4F5C;&#x5219;&#x4F1A;&#x6B63;&#x5E38;&#x8FDB;&#x884C;&#x3002;&#x5F53;<code>non_blocking</code>&#x53C2;&#x6570;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;True&#x4E4B;&#x540E;&#xFF0C;&#x6B64;&#x51FD;&#x6570;&#x4F1A;&#x5C3D;&#x53EF;&#x80FD;&#x5730;&#x76F8;&#x5BF9;&#x4E8E; host &#x8FDB;&#x884C;&#x5F02;&#x6B65;&#x7684; &#x8F6C;&#x6362;/&#x79FB;&#x52A8; &#x64CD;&#x4F5C;&#xFF0C;&#x6BD4;&#x5982;&#xFF0C;&#x5C06;&#x5B58;&#x50A8;&#x5728;&#x56FA;&#x5B9A;&#x5185;&#x5B58;&#xFF08;pinned memory&#xFF09;&#x4E0A;&#x7684;CPU Tensors&#x79FB;&#x52A8;&#x5230;CUDA&#x8BBE;&#x5907;&#x4E0A;&#x8FD9;&#x4E00;&#x8FC7;&#x7A0B;&#x65E2;&#x662F;&#x5982;&#x6B64;&#x3002;</p>
<p>&#x4F8B;&#x5B50;&#x5728;&#x4E0B;&#x9762;&#x3002;</p>
<p>Note</p>
<p>&#x8FD9;&#x4E2A;&#x65B9;&#x6CD5;&#x5BF9;&#x6A21;&#x5757;&#x7684;&#x4FEE;&#x6539;&#x90FD;&#x662F;in-place&#x64CD;&#x4F5C;&#x3002;</p>
<p>to()&#x51FD;&#x6570;&#x7684;&#x53C2;&#x6570;: </p>
<ul>
<li><strong>device</strong> (<code>torch.device</code>) &#x2013; &#x60F3;&#x8981;&#x5C06;&#x8FD9;&#x4E2A;&#x6A21;&#x5757;&#x4E2D;&#x7684;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x533A;&#x8F6C;&#x79FB;&#x5230;&#x7684;&#x8BBE;&#x5907;&#x3002;</li>
<li><strong>dtype</strong> (<code>torch.dtype</code>) &#x2013; &#x60F3;&#x8981;&#x5C06;&#x8FD9;&#x4E2A;&#x6A21;&#x5757;&#x4E2D;&#x6D6E;&#x70B9;&#x6570;&#x7684;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x533A;&#x8F6C;&#x5316;&#x4E3A;&#x7684;&#x6D6E;&#x70B9;&#x6570;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;</li>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a>) &#x2013; &#x4E00;&#x4E2A;Tensor&#xFF0C;&#x5982;&#x679C;&#x88AB;&#x6307;&#x5B9A;&#xFF0C;&#x5176;dtype&#x548C;device&#x4FE1;&#x606F;&#xFF0C;&#x5C06;&#x5206;&#x522B;&#x8D77;&#x5230;&#x4E0A;&#x9762;&#x4E24;&#x4E2A;&#x53C2;&#x6570;&#x7684;&#x4F5C;&#x7528;&#xFF0C;&#x4E5F;&#x5C31;&#x662F;&#x8BF4;&#xFF0C;&#x8FD9;&#x4E2A;&#x6A21;&#x5757;&#x7684;&#x6D6E;&#x70B9;&#x6570;&#x7684;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x533A;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x5C06;&#x4F1A;&#x88AB;&#x8F6C;&#x5316;&#x4E3A;&#x8FD9;&#x4E2A;Tensor&#x7684;dtype&#x7C7B;&#x578B;&#xFF0C;&#x540C;&#x65F6;&#x88AB;&#x8F6C;&#x79FB;&#x5230;&#x6B64;Tensor&#x6240;&#x5904;&#x7684;&#x8BBE;&#x5907;device&#x4E0A;&#x53BB;&#x3002;</li>
</ul>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>self</th>
</tr>
</thead>
<tbody>
<tr>
<td>Return type:</td>
<td><a href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>linear = nn.Linear(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>linear.weight
Parameter containing:
tensor([[ <span class="hljs-number">0.1913</span>, <span class="hljs-number">-0.3420</span>],
 [<span class="hljs-number">-0.5113</span>, <span class="hljs-number">-0.2325</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>linear.to(torch.double)
Linear(in_features=<span class="hljs-number">2</span>, out_features=<span class="hljs-number">2</span>, bias=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>linear.weight
Parameter containing:
tensor([[ <span class="hljs-number">0.1913</span>, <span class="hljs-number">-0.3420</span>],
 [<span class="hljs-number">-0.5113</span>, <span class="hljs-number">-0.2325</span>]], dtype=torch.float64)
<span class="hljs-meta">&gt;&gt;&gt; </span>gpu1 = torch.device(<span class="hljs-string">&quot;cuda:1&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>linear.to(gpu1, dtype=torch.half, non_blocking=<span class="hljs-keyword">True</span>)
Linear(in_features=<span class="hljs-number">2</span>, out_features=<span class="hljs-number">2</span>, bias=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>linear.weight
Parameter containing:
tensor([[ <span class="hljs-number">0.1914</span>, <span class="hljs-number">-0.3420</span>],
 [<span class="hljs-number">-0.5112</span>, <span class="hljs-number">-0.2324</span>]], dtype=torch.float16, device=<span class="hljs-string">&apos;cuda:1&apos;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>cpu = torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>linear.to(cpu)
Linear(in_features=<span class="hljs-number">2</span>, out_features=<span class="hljs-number">2</span>, bias=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>linear.weight
Parameter containing:
tensor([[ <span class="hljs-number">0.1914</span>, <span class="hljs-number">-0.3420</span>],
 [<span class="hljs-number">-0.5112</span>, <span class="hljs-number">-0.2324</span>]], dtype=torch.float16)
</code></pre>
<pre><code class="lang-py">train(mode=<span class="hljs-keyword">True</span>)
</code></pre>
<p>&#x5C06;&#x6A21;&#x5757;&#x8F6C;&#x6362;&#x6210;&#x8BAD;&#x7EC3;&#x6A21;&#x5F0F;&#x3002;</p>
<p>&#x8FD9;&#x4E2A;&#x51FD;&#x6570;&#x53EA;&#x5BF9;&#x7279;&#x5B9A;&#x7684;&#x6A21;&#x5757;&#x7C7B;&#x578B;&#x6709;&#x6548;&#xFF0C;&#x5982; <a href="#torch.nn.Dropout" title="torch.nn.Dropout"><code>Dropout</code></a>&#x548C;<code>BatchNorm</code>&#x7B49;&#x7B49;&#x3002;&#x5982;&#x679C;&#x60F3;&#x4E86;&#x89E3;&#x8FD9;&#x4E9B;&#x7279;&#x5B9A;&#x6A21;&#x5757;&#x5728;&#x8BAD;&#x7EC3;/&#x6D4B;&#x8BD5;&#x6A21;&#x5F0F;&#x4E0B;&#x5404;&#x81EA;&#x7684;&#x8FD0;&#x4F5C;&#x7EC6;&#x8282;&#xFF0C;&#x53EF;&#x4EE5;&#x770B;&#x4E00;&#x4E0B;&#x8FD9;&#x4E9B;&#x7279;&#x6B8A;&#x6A21;&#x5757;&#x7684;&#x6587;&#x6863;&#x90E8;&#x5206;&#x3002;</p>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>self</th>
</tr>
</thead>
<tbody>
<tr>
<td>Return type:</td>
<td><a href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">type(dst_type)
</code></pre>
<p>&#x5C06;&#x6240;&#x6709;&#x7684;&#x53C2;&#x6570;&#x548C;&#x7F13;&#x51B2;&#x533A;&#x8F6C;&#x5316;&#x4E3A; <code>dst_type</code>&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>dst_type</strong> (<a href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.7)" target="_blank"><em>type</em></a> <em>or</em> <em>string</em>) &#x2013; &#x8981;&#x8F6C;&#x5316;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;</th>
</tr>
</thead>
<tbody>
<tr>
<td>Returns:</td>
<td>self</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
<tr>
<td>Return type:</td>
<td><a href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">zero_grad()
</code></pre>
<p>&#x8BB2;&#x6A21;&#x5757;&#x6240;&#x6709;&#x53C2;&#x6570;&#x7684;&#x68AF;&#x5EA6;&#x8BBE;&#x7F6E;&#x4E3A;0&#x3002;</p>
<h3 id="sequential">Sequential</h3>
<pre><code class="lang-py">class torch.nn.Sequential(*args)
</code></pre>
<p>&#x4E00;&#x79CD;&#x987A;&#x5E8F;&#x5BB9;&#x5668;&#x3002;&#x4F20;&#x5165;Sequential&#x6784;&#x9020;&#x5668;&#x4E2D;&#x7684;&#x6A21;&#x5757;&#x4F1A;&#x88AB;&#x6309;&#x7167;&#x4ED6;&#x4EEC;&#x4F20;&#x5165;&#x7684;&#x987A;&#x5E8F;&#x4F9D;&#x6B21;&#x6DFB;&#x52A0;&#x5230;Sequential&#x4E4B;&#x4E0A;&#x3002;&#x76F8;&#x5E94;&#x7684;&#xFF0C;&#x4E00;&#x4E2A;&#x7531;&#x6A21;&#x5757;&#x7EC4;&#x6210;&#x7684;&#x987A;&#x5E8F;&#x8BCD;&#x5178;&#x4E5F;&#x53EF;&#x4EE5;&#x88AB;&#x4F20;&#x5165;&#x5230;Sequential&#x7684;&#x6784;&#x9020;&#x5668;&#x4E2D;&#x3002;</p>
<p>&#x4E3A;&#x4E86;&#x65B9;&#x4FBF;&#x5927;&#x5BB6;&#x7406;&#x89E3;&#xFF0C;&#x4E3E;&#x4E2A;&#x7B80;&#x5355;&#x7684;&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-py"><span class="hljs-comment"># &#x6784;&#x5EFA;Sequential&#x7684;&#x4F8B;&#x5B50;</span>
model = nn.Sequential(
          nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>,<span class="hljs-number">5</span>),
          nn.ReLU(),
          nn.Conv2d(<span class="hljs-number">20</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>),
          nn.ReLU()
        )

<span class="hljs-comment"># &#x5229;&#x7528;OrderedDict&#x6784;&#x5EFA;Sequential&#x7684;&#x4F8B;&#x5B50;</span>
model = nn.Sequential(OrderedDict([
          (<span class="hljs-string">&apos;conv1&apos;</span>, nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>,<span class="hljs-number">5</span>)),
          (<span class="hljs-string">&apos;relu1&apos;</span>, nn.ReLU()),
          (<span class="hljs-string">&apos;conv2&apos;</span>, nn.Conv2d(<span class="hljs-number">20</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>)),
          (<span class="hljs-string">&apos;relu2&apos;</span>, nn.ReLU())
        ]))
</code></pre>
<h3 id="modulelist-&#x6A21;&#x5757;&#x5217;&#x8868;">ModuleList (&#x6A21;&#x5757;&#x5217;&#x8868;)</h3>
<pre><code class="lang-py">class torch.nn.ModuleList(modules=None)
</code></pre>
<p>ModuleList&#x7684;&#x4F5C;&#x7528;&#x662F;&#x5C06;&#x4E00;&#x5806;&#x6A21;&#x5757;&#xFF08;module&#xFF09;&#x5B58;&#x50A8;&#x5728;&#x4E00;&#x4E2A;&#x5217;&#x8868;&#x4E4B;&#x4E2D;&#x3002;</p>
<p>ModuleList &#x53EF;&#x4EE5;&#x6309;&#x4E00;&#x822C;&#x7684;python&#x5217;&#x8868;&#x7684;&#x7D22;&#x5F15;&#x65B9;&#x5F0F;&#x8FDB;&#x884C;&#x7D22;&#x5F15;&#xFF0C;&#x4F46;ModuleList&#x4E2D;&#x7684;&#x6A21;&#x5757;&#x90FD;&#x5DF2;&#x88AB;&#x6B63;&#x786E;&#x6CE8;&#x518C;&#xFF0C;&#x5E76;&#x4E14;&#x5BF9;&#x6240;&#x6709;&#x7684;Module method&#x53EF;&#x89C1;&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>modules</strong> (<em>iterable__,</em> <em>optional</em>) &#x2013; &#x4E00;&#x4E2A;&#x8981;&#x6DFB;&#x52A0;&#x5230;ModuleList&#x4E2D;&#x7684;&#x7531;&#x6A21;&#x5757;&#x7EC4;&#x6210;&#x7684;&#x53EF;&#x8FED;&#x4EE3;&#x7ED3;&#x6784;(an iterable of modules)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModule</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(MyModule, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>)])

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        <span class="hljs-comment"># ModuleList&#x53EF;&#x4EE5;&#x88AB;&#x5F53;&#x4F5C;&#x4E00;&#x4E2A;&#x8FED;&#x4EE3;&#x5668;&#xFF0C;&#x540C;&#x65F6;&#x4E5F;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;index&#x7D22;&#x5F15;</span>
        <span class="hljs-keyword">for</span> i, l <span class="hljs-keyword">in</span> enumerate(self.linears):
            x = self.linears[i // <span class="hljs-number">2</span>](x) + l(x)
        <span class="hljs-keyword">return</span> x
</code></pre>
<pre><code class="lang-py">append(module)
</code></pre>
<p>&#x5C06;&#x4E00;&#x4E2A;&#x6A21;&#x5757;&#x6DFB;&#x52A0;&#x5230;ModuleList&#x7684;&#x672B;&#x5C3E;&#xFF0C;&#x4E0E;python list&#x7684;append()&#x4E00;&#x81F4;&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) &#x2013; &#x8981;&#x6DFB;&#x52A0;&#x7684;&#x6A21;&#x5757;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">extend(modules)
</code></pre>
<p>&#x5C06;&#x4E00;&#x4E2A;&#x7531;&#x6A21;&#x5757;&#x7EC4;&#x6210;&#x7684;&#x53EF;&#x8FED;&#x4EE3;&#x7ED3;&#x6784;&#x6DFB;&#x52A0;&#x5230;ModuleList&#x7684;&#x672B;&#x5C3E;&#xFF0C;&#x4E0E;python list&#x7684;extend()&#x4E00;&#x81F4;&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>modules</strong> (<em>iterable</em>) &#x2013; &#x8981;&#x6DFB;&#x52A0;&#x5230;ModuleList&#x672B;&#x5C3E;&#x7684;&#x7531;&#x6A21;&#x5757;&#x7EC4;&#x6210;&#x7684;&#x53EF;&#x8FED;&#x4EE3;&#x7ED3;&#x6784;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">insert(index, module)
</code></pre>
<p>&#x5C06;&#x7ED9;&#x5B9A;&#x7684;<code>module</code>&#x63D2;&#x5165;&#x5230;ModuleList&#x7684;<code>index</code>&#x4F4D;&#x7F6E;&#x3002;</p>
<p>insert()&#x51FD;&#x6570;&#x7684;&#x53C2;&#x6570;: </p>
<ul>
<li><strong>index</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; &#x8981;&#x63D2;&#x5165;&#x7684;&#x4F4D;&#x7F6E;</li>
<li><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) &#x2013; &#x8981;&#x63D2;&#x5165;&#x7684;&#x6A21;&#x5757;</li>
</ul>
<h3 id="moduledict-&#x6A21;&#x5757;&#x8BCD;&#x5178;">ModuleDict (&#x6A21;&#x5757;&#x8BCD;&#x5178;)</h3>
<pre><code class="lang-py">class torch.nn.ModuleDict(modules=None)
</code></pre>
<p>ModuleDict&#x7684;&#x4F5C;&#x7528;&#x662F;&#x5C06;&#x4E00;&#x5806;&#x6A21;&#x5757;&#xFF08;module&#xFF09;&#x5B58;&#x50A8;&#x5728;&#x4E00;&#x4E2A;&#x8BCD;&#x5178;&#x4E4B;&#x4E2D;&#x3002;</p>
<p>ModuleDict &#x53EF;&#x4EE5;&#x6309;&#x4E00;&#x822C;&#x7684;python&#x8BCD;&#x5178;&#x7684;&#x7D22;&#x5F15;&#x65B9;&#x5F0F;&#x8FDB;&#x884C;&#x7D22;&#x5F15;&#xFF0C;&#x4F46;ModuleDict&#x4E2D;&#x7684;&#x6A21;&#x5757;&#x90FD;&#x5DF2;&#x88AB;&#x6B63;&#x786E;&#x6CE8;&#x518C;&#xFF0C;&#x5E76;&#x4E14;&#x5BF9;&#x6240;&#x6709;&#x7684;Module method&#x53EF;&#x89C1;&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>modules</strong> (<em>iterable__,</em> <em>optional</em>) &#x2013; &#x4E00;&#x4E2A;&#x7531;(string: module)&#x6620;&#x5C04;&#x7EC4;&#x6210;&#x7684;&#x6620;&#x5C04;&#x96C6;&#x5408;&#xFF08;&#x8BCD;&#x5178;&#xFF09;&#x6216;&#x8005; &#x4E00;&#x4E2A;&#x7531;(string, module)&#x952E;/&#x503C;&#x5BF9;&#x7EC4;&#x6210;&#x7684;&#x53EF;&#x8FED;&#x4EE3;&#x7ED3;&#x6784;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModule</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(MyModule, self).__init__()
        self.choices = nn.ModuleDict({
                <span class="hljs-string">&apos;conv&apos;</span>: nn.Conv2d(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">3</span>),
                <span class="hljs-string">&apos;pool&apos;</span>: nn.MaxPool2d(<span class="hljs-number">3</span>)
        })
        self.activations = nn.ModuleDict([
                [<span class="hljs-string">&apos;lrelu&apos;</span>, nn.LeakyReLU()],
                [<span class="hljs-string">&apos;prelu&apos;</span>, nn.PReLU()]
        ])

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x, choice, act)</span>:</span>
        x = self.choices[choice](x)
        x = self.activations[act](x)
        <span class="hljs-keyword">return</span> x
</code></pre>
<pre><code class="lang-py">clear()
</code></pre>
<p>&#x79FB;&#x9664;ModuleDict&#x4E2D;&#x6240;&#x6709;&#x7684;&#x5143;&#x7D20;&#x3002;</p>
<pre><code class="lang-py">items()
</code></pre>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x7531;ModuleDict&#x4E2D;&#x7684;&#x952E;/&#x503C;&#x5BF9;&#x7EC4;&#x6210;&#x7684;&#x53EF;&#x8FED;&#x4EE3;&#x7ED3;&#x6784;&#x3002;</p>
<pre><code class="lang-py">keys()
</code></pre>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x7531;ModuleDict&#x4E2D;&#x7684;&#x952E;&#x7EC4;&#x6210;&#x7684;&#x53EF;&#x8FED;&#x4EE3;&#x7ED3;&#x6784;&#x3002;</p>
<pre><code class="lang-py">pop(key)
</code></pre>
<p>&#x5C06;<code>key</code>&#x8FD9;&#x4E2A;&#x952E;&#x4ECE;ModuleDict&#x4E2D;&#x5220;&#x9664;&#xFF0C;&#x5E76;&#x5C06;&#x5176;&#x5BF9;&#x5E94;&#x7684;&#x6A21;&#x5757;&#x8FD4;&#x56DE;&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>key</strong> (<em>string</em>) &#x2013; &#x8981;&#x4ECE;ModuleDict&#x4E2D;&#x5F39;&#x51FA;&#x7684;&#x952E;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">update(modules)
</code></pre>
<p>&#x901A;&#x8FC7;&#x4F20;&#x5165;&#x7684;&#x6620;&#x5C04;&#x6216;&#x8005;&#x7531;&#x952E;/&#x503C;&#x5BF9;&#x7EC4;&#x6210;&#x7684;&#x53EF;&#x8FED;&#x4EE3;&#x7ED3;&#x6784;&#x5BF9;&#x5F53;&#x524D;&#x7684;ModuleDict&#x8FDB;&#x884C;&#x66F4;&#x65B0;&#xFF0C;&#x5982;&#x679C;&#x4F20;&#x5165;&#x5BF9;&#x8C61;&#x4E0E;&#x5F53;&#x524D;ModuleDict&#x4E2D;&#x5B58;&#x5728;&#x952E;&#x91CD;&#x590D;&#xFF0C;&#x5F53;&#x524D;ModuleDict&#x4E2D;&#x8FD9;&#x4E9B;&#x91CD;&#x590D;&#x7684;&#x952E;&#x6240;&#x5BF9;&#x5E94;&#x7684;&#x503C;&#x5C06;&#x88AB;&#x8986;&#x76D6;&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>modules</strong> (<em>iterable</em>) &#x2013; &#x4E00;&#x4E2A;&#x7531;(string: <code>Module</code>)&#x6620;&#x5C04;&#x7EC4;&#x6210;&#x7684;&#x6620;&#x5C04;&#x96C6;&#x5408;&#xFF08;&#x8BCD;&#x5178;&#xFF09;&#x6216;&#x8005; &#x4E00;&#x4E2A;&#x7531;(string: <code>Module</code>)&#x952E;/&#x503C;&#x5BF9;&#x7EC4;&#x6210;&#x7684;&#x53EF;&#x8FED;&#x4EE3;&#x7ED3;&#x6784;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">values()
</code></pre>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x7531;ModuleDict&#x4E2D;&#x7684;&#x503C;&#x7EC4;&#x6210;&#x7684;&#x53EF;&#x8FED;&#x4EE3;&#x7ED3;&#x6784;&#x3002;</p>
<h3 id="parameterlist-&#x53C2;&#x6570;&#x5217;&#x8868;">ParameterList (&#x53C2;&#x6570;&#x5217;&#x8868;)</h3>
<pre><code class="lang-py">class torch.nn.ParameterList(parameters=None)
</code></pre>
<p>ParameterList&#x7684;&#x4F5C;&#x7528;&#x662F;&#x5C06;&#x4E00;&#x5806;&#x53C2;&#x6570;&#xFF08;parameter&#xFF09;&#x5B58;&#x50A8;&#x5230;&#x4E00;&#x4E2A;&#x5217;&#x8868;&#x4E2D;&#x3002;</p>
<p>ParameterList &#x53EF;&#x4EE5;&#x6309;&#x4E00;&#x822C;&#x7684;python&#x5217;&#x8868;&#x7684;&#x7D22;&#x5F15;&#x65B9;&#x5F0F;&#x8FDB;&#x884C;&#x7D22;&#x5F15;&#xFF0C;&#x4F46;ParameterList&#x4E2D;&#x7684;&#x53C2;&#x6570;&#xFF08;parameter&#xFF09;&#x90FD;&#x5DF2;&#x88AB;&#x6B63;&#x786E;&#x6CE8;&#x518C;&#xFF0C;&#x5E76;&#x4E14;&#x5BF9;&#x6240;&#x6709;&#x7684;Module method&#x53EF;&#x89C1;&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>parameters</strong> (<em>iterable__,</em> <em>optional</em>) &#x2013; &#x8981;&#x6DFB;&#x52A0;&#x5230;ParameterList&#x4E4B;&#x4E0A;&#x7684;&#x7531;parameter&#x7EC4;&#x6210;&#x7684;&#x53EF;&#x8FED;&#x4EE3;&#x7ED3;&#x6784;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModule</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(MyModule, self).__init__()
        self.params = nn.ParameterList([nn.Parameter(torch.randn(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>)])

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        <span class="hljs-comment"># ParameterList&#x53EF;&#x4EE5;&#x88AB;&#x5F53;&#x4F5C;&#x4E00;&#x4E2A;&#x8FED;&#x4EE3;&#x5668;&#xFF0C;&#x540C;&#x65F6;&#x4E5F;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;index&#x7D22;&#x5F15;</span>
        <span class="hljs-keyword">for</span> i, p <span class="hljs-keyword">in</span> enumerate(self.params):
            x = self.params[i // <span class="hljs-number">2</span>].mm(x) + p.mm(x)
        <span class="hljs-keyword">return</span> x
</code></pre>
<pre><code class="lang-py">append(parameter)
</code></pre>
<p>&#x5C06;&#x4E00;&#x4E2A;parameter&#x6DFB;&#x52A0;&#x5230;ParameterList&#x7684;&#x672B;&#x5C3E;&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>parameter</strong> (<a href="#torch.nn.Parameter" title="torch.nn.Parameter"><em>nn.Parameter</em></a>) &#x2013; &#x8981;&#x6DFB;&#x52A0;&#x7684;&#x53C2;&#x6570;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">extend(parameters)
</code></pre>
<p>&#x5C06;&#x4E00;&#x4E2A;&#x7531;parameter&#x7EC4;&#x6210;&#x7684;Python&#x53EF;&#x8FED;&#x4EE3;&#x7ED3;&#x6784;&#x6DFB;&#x52A0;&#x5230;ParameterList&#x7684;&#x672B;&#x5C3E;&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>parameters</strong> (<em>iterable</em>) &#x2013; &#x8981;&#x6DFB;&#x52A0;&#x5230;ParameterList&#x7684;&#x672B;&#x5C3E;&#x7684;&#x7531;parameter&#x7EC4;&#x6210;&#x7684;Python&#x53EF;&#x8FED;&#x4EE3;&#x7ED3;&#x6784;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<h3 id="parameterdict-&#x53C2;&#x6570;&#x8BCD;&#x5178;">ParameterDict (&#x53C2;&#x6570;&#x8BCD;&#x5178;)</h3>
<pre><code class="lang-py">class torch.nn.ParameterDict(parameters=None)
</code></pre>
<p>ParameterDict&#x7684;&#x4F5C;&#x7528;&#x662F;&#x5C06;&#x4E00;&#x5806;&#x53C2;&#x6570;&#xFF08;Parameter&#xFF09;&#x5B58;&#x50A8;&#x5728;&#x4E00;&#x4E2A;&#x8BCD;&#x5178;&#x4E4B;&#x4E2D;&#x3002;</p>
<p>ParameterDict &#x53EF;&#x4EE5;&#x6309;&#x4E00;&#x822C;&#x7684;python&#x8BCD;&#x5178;&#x7684;&#x7D22;&#x5F15;&#x65B9;&#x5F0F;&#x8FDB;&#x884C;&#x7D22;&#x5F15;&#xFF0C;&#x4F46;ParameterDictt&#x4E2D;&#x7684;&#x53C2;&#x6570;&#x90FD;&#x5DF2;&#x88AB;&#x6B63;&#x786E;&#x6CE8;&#x518C;&#xFF0C;&#x5E76;&#x4E14;&#x5BF9;&#x6240;&#x6709;&#x7684;Module method&#x53EF;&#x89C1;&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>parameters</strong> (<em>iterable__,</em> <em>optional</em>) &#x2013; &#x4E00;&#x4E2A;&#x7531;(string:<a href="#torch.nn.Parameter" title="torch.nn.Parameter"><code>Parameter</code></a>)&#x6620;&#x5C04;&#x7EC4;&#x6210;&#x7684;&#x6620;&#x5C04;&#x96C6;&#x5408;&#xFF08;&#x8BCD;&#x5178;&#xFF09;&#x6216;&#x8005; &#x4E00;&#x4E2A;&#x7531;(string, <a href="#torch.nn.Parameter" title="torch.nn.Parameter"><code>Parameter</code></a>)&#x952E;/&#x503C;&#x5BF9;&#x7EC4;&#x6210;&#x7684;&#x53EF;&#x8FED;&#x4EE3;&#x7ED3;&#x6784;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModule</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(MyModule, self).__init__()
        self.params = nn.ParameterDict({
                <span class="hljs-string">&apos;left&apos;</span>: nn.Parameter(torch.randn(<span class="hljs-number">5</span>, <span class="hljs-number">10</span>)),
                <span class="hljs-string">&apos;right&apos;</span>: nn.Parameter(torch.randn(<span class="hljs-number">5</span>, <span class="hljs-number">10</span>))
        })

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x, choice)</span>:</span>
        x = self.params[choice].mm(x)
        <span class="hljs-keyword">return</span> x
</code></pre>
<pre><code class="lang-py">clear()
</code></pre>
<p>&#x79FB;&#x9664;ParameterDict&#x4E2D;&#x6240;&#x6709;&#x7684;&#x5143;&#x7D20;&#x3002;</p>
<pre><code class="lang-py">items()
</code></pre>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x7531;ParameterDict&#x4E2D;&#x7684;&#x952E;/&#x503C;&#x5BF9;&#x7EC4;&#x6210;&#x7684;&#x53EF;&#x8FED;&#x4EE3;&#x7ED3;&#x6784;&#x3002;</p>
<pre><code class="lang-py">keys()
</code></pre>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x7531; ParameterDict&#x4E2D;&#x7684;&#x952E;&#x7EC4;&#x6210;&#x7684;&#x53EF;&#x8FED;&#x4EE3;&#x7ED3;&#x6784;&#x3002;</p>
<pre><code class="lang-py">pop(key)
</code></pre>
<p>&#x5C06;key&#x8FD9;&#x4E2A;&#x952E;&#x4ECE;ParameterDict&#x4E2D;&#x5220;&#x9664;&#xFF0C;&#x5E76;&#x5C06;&#x5176;&#x5BF9;&#x5E94;&#x7684;&#x6A21;&#x5757;&#x8FD4;&#x56DE;&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>key</strong> (<em>string</em>) &#x2013; &#x8981;&#x4ECE;ParameterDict&#x4E2D;&#x5F39;&#x51FA;&#x7684;&#x952E;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">update(parameters)
</code></pre>
<p>&#x901A;&#x8FC7;&#x4F20;&#x5165;&#x7684;&#x6620;&#x5C04;&#x6216;&#x8005;&#x7531;&#x952E;/&#x503C;&#x5BF9;&#x7EC4;&#x6210;&#x7684;&#x53EF;&#x8FED;&#x4EE3;&#x7ED3;&#x6784;&#x5BF9;&#x5F53;&#x524D;&#x7684;ParameterDict&#x8FDB;&#x884C;&#x66F4;&#x65B0;&#xFF0C;&#x5982;&#x679C;&#x4F20;&#x5165;&#x5BF9;&#x8C61;&#x4E0E;&#x5F53;&#x524D;ParameterDict&#x4E2D;&#x5B58;&#x5728;&#x952E;&#x91CD;&#x590D;&#xFF0C;&#x5F53;&#x524D;ParameterDict&#x4E2D;&#x8FD9;&#x4E9B;&#x91CD;&#x590D;&#x7684;&#x952E;&#x6240;&#x5BF9;&#x5E94;&#x7684;&#x503C;&#x5C06;&#x88AB;&#x8986;&#x76D6;&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>parameters</strong> (<em>iterable</em>) &#x2013; modules (iterable) &#x2013; &#x4E00;&#x4E2A;&#x7531;(string: <a href="#torch.nn.Parameter" title="torch.nn.Parameter"><code>Parameter</code></a>)&#x6620;&#x5C04;&#x7EC4;&#x6210;&#x7684;&#x6620;&#x5C04;&#x96C6;&#x5408;&#xFF08;&#x8BCD;&#x5178;&#xFF09;&#x6216;&#x8005; &#x4E00;&#x4E2A;&#x7531;(string: <a href="#torch.nn.Parameter" title="torch.nn.Parameter"><code>Parameter</code></a>)&#x952E;/&#x503C;&#x5BF9;&#x7EC4;&#x6210;&#x7684;&#x53EF;&#x8FED;&#x4EE3;&#x7ED3;&#x6784;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">values()
</code></pre>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x7531;ParameterDict&#x4E2D;&#x7684;&#x503C;&#x7EC4;&#x6210;&#x7684;&#x53EF;&#x8FED;&#x4EE3;&#x7ED3;&#x6784;&#x3002;</p>
<h2 id="convolution-layers-&#x5377;&#x79EF;&#x5C42;">Convolution layers (&#x5377;&#x79EF;&#x5C42;)</h2>
<h3 id="conv1d">Conv1d</h3>
<pre><code class="lang-py">class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
</code></pre>
<p>&#x5229;&#x7528;&#x6307;&#x5B9A;&#x5927;&#x5C0F;&#x7684;&#x4E00;&#x7EF4;&#x5377;&#x79EF;&#x6838;&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4E00;&#x7EF4;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x8FDB;&#x884C;&#x4E00;&#x7EF4;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x7684;&#x5377;&#x79EF;&#x5C42;&#x3002;</p>
<p>&#x5728;&#x6700;&#x7B80;&#x5355;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5BF9;&#x4E8E;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x4E3A;<img src="img/1dad4f3ff614c986028f7100e0205f6d.jpg" alt="">&#xFF0C;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x4E3A;<img src="img/a03de8b18f61a493174a56530fb03f1d.jpg" alt="">&#x7684;&#x4E00;&#x7EF4;&#x5377;&#x79EF;&#x5C42;&#xFF0C;&#x5176;&#x5377;&#x79EF;&#x8BA1;&#x7B97;&#x8FC7;&#x7A0B;&#x53EF;&#x4EE5;&#x5982;&#x4E0B;&#x8868;&#x8FF0;&#xFF1A;</p>
<p><img src="img/806f7530da55bf294a636b8c7ed38bcb.jpg" alt=""></p>
<p>&#x8FD9;&#x91CC;&#x7684;<img src="img/d5d3d32b4a35f91edb54c3c3f87d582e.jpg" alt="">&#x7B26;&#x53F7;&#x5B9E;&#x9645;&#x4E0A;&#x662F;&#x4E00;&#x4E2A;&#x4E92;&#x76F8;&#x5173;&#xFF08;<a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">cross-correlation</a>&#xFF09; &#x64CD;&#x4F5C;&#x7B26;&#xFF08;&#x5927;&#x5BB6;&#x53EF;&#x4EE5;&#x81EA;&#x5DF1;&#x67E5;&#x4E00;&#x4E0B;&#x4E92;&#x76F8;&#x5173;&#x548C;&#x771F;&#x5377;&#x79EF;&#x7684;&#x533A;&#x522B;&#xFF0C;&#x4E92;&#x76F8;&#x5173;&#x56E0;&#x4E3A;&#x5B9E;&#x73B0;&#x8D77;&#x6765;&#x5F88;&#x7B80;&#x5355;&#xFF0C;&#x6240;&#x4EE5;&#x4E00;&#x822C;&#x7684;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x6846;&#x67B6;&#x90FD;&#x662F;&#x7528;&#x4E92;&#x76F8;&#x5173;&#x64CD;&#x4F5C;&#x53D6;&#x4EE3;&#x771F;&#x5377;&#x79EF;&#xFF09;, <img src="img/9341d9048ac485106d2b2ee8de14876f.jpg" alt=""> is a batch size, <img src="img/6c8feca3b2da3d6cf371417edff4be4f.jpg" alt=""> &#x4EE3;&#x8868;&#x901A;&#x9053;&#x7684;&#x6570;&#x91CF;, <img src="img/db4a9fef02111450bf98261889de550c.jpg" alt=""> &#x4EE3;&#x8868;&#x4FE1;&#x53F7;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#x3002;</p>
<ul>
<li><p><code>stride</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x4E92;&#x76F8;&#x5173;&#x64CD;&#x4F5C;&#xFF08;&#x4F2A;&#x5377;&#x79EF;&#xFF09;&#x7684;&#x6B65;&#x957F;&#xFF0C;&#x53C2;&#x6570;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x4E00;&#x822C;&#x662F;&#x5355;&#x4E2A;&#x6570;&#x5B57;&#x6216;&#x8005;&#x4E00;&#x4E2A;&#x53EA;&#x6709;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5143;&#x7EC4;&#x3002;</p>
</li>
<li><p><code>padding</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x8981;&#x5728;&#x4E00;&#x7EF4;&#x5377;&#x79EF;&#x6838;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x3002;</p>
</li>
<li><p><code>dilation</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x5377;&#x79EF;&#x6838;&#x4E2D;&#x5404;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#xFF1B;&#x8FD9;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x591A;&#x5B54;&#x7B97;&#x6CD5;(&#xE0; trous algorithm)&#x3002;&#x8FD9;&#x4E2A;&#x6982;&#x5FF5;&#x6709;&#x70B9;&#x96BE;&#x89E3;&#x91CA;&#xFF0C;&#x8FD9;&#x4E2A;&#x94FE;&#x63A5;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">link</a>&#x7528;&#x53EF;&#x89C6;&#x5316;&#x7684;&#x65B9;&#x6CD5;&#x5F88;&#x597D;&#x5730;&#x89E3;&#x91CA;&#x4E86;<code>dilation</code>&#x7684;&#x4F5C;&#x7528;&#x3002;</p>
</li>
<li><p><code>groups</code> &#x63A7;&#x5236;&#x4E86;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x4E4B;&#x95F4;&#x7684;&#x8FDE;&#x63A5;&#xFF08;connections&#xFF09;&#x7684;&#x6570;&#x91CF;&#x3002;<code>in_channels</code> &#x548C; <code>out_channels</code> &#x5FC5;&#x987B;&#x80FD;&#x88AB; <code>groups</code> &#x6574;&#x9664;&#x3002;&#x4E3E;&#x4E2A;&#x6817;&#x5B50;&#xFF0C; </p>
<p>&gt; *   &#x5F53; groups=1, &#x6B64;Conv1d&#x5C42;&#x4F1A;&#x4F7F;&#x7528;&#x4E00;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x8FDB;&#x884C;&#x6240;&#x6709;&#x8F93;&#x5165;&#x5230;&#x8F93;&#x51FA;&#x7684;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&gt; *   &#x5F53; groups=2, &#x6B64;&#x65F6;Conv1d&#x5C42;&#x4F1A;&#x4EA7;&#x751F;&#x4E24;&#x4E2A;&#x5E76;&#x5217;&#x7684;&#x5377;&#x79EF;&#x5C42;&#x3002;&#x540C;&#x65F6;&#xFF0C;&#x8F93;&#x5165;&#x901A;&#x9053;&#x88AB;&#x5206;&#x4E3A;&#x4E24;&#x534A;&#xFF0C;&#x4E24;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x5206;&#x522B;&#x5904;&#x7406;&#x4E00;&#x534A;&#x7684;&#x8F93;&#x5165;&#x901A;&#x9053;&#xFF0C;&#x540C;&#x65F6;&#x5404;&#x81EA;&#x4EA7;&#x751F;&#x4E00;&#x534A;&#x7684;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x3002;&#x6700;&#x540E;&#x8FD9;&#x4E24;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x4F1A;&#x88AB;concatenated&#x4E00;&#x8D77;&#xFF0C;&#x4F5C;&#x4E3A;&#x6B64;Conv1d&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x3002;</p>
<p>&gt; *   &#x5F53; groups= <code>in_channels</code>, &#x6BCF;&#x4E2A;&#x8F93;&#x5165;&#x901A;&#x9053;&#x90FD;&#x4F1A;&#x88AB;&#x5355;&#x72EC;&#x7684;&#x4E00;&#x7EC4;&#x5377;&#x79EF;&#x5C42;&#x5904;&#x7406;&#xFF0C;&#x8FD9;&#x4E2A;&#x7EC4;&#x7684;&#x5927;&#x5C0F;&#x662F;<img src="img/19131f9f53448ae579b613bc7bc90158.jpg" alt=""></p>
</li>
</ul>
<p>Note</p>
<p>&#x53D6;&#x51B3;&#x4E8E;&#x4F60;&#x5377;&#x79EF;&#x6838;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x6709;&#x4E9B;&#x65F6;&#x5019;&#x8F93;&#x5165;&#x6570;&#x636E;&#x4E2D;&#x67D0;&#x4E9B;&#x5217;&#xFF08;&#x6700;&#x540E;&#x51E0;&#x5217;&#xFF09;&#x53EF;&#x80FD;&#x4E0D;&#x4F1A;&#x53C2;&#x4E0E;&#x8BA1;&#x7B97;&#xFF08;&#x6BD4;&#x5982;&#x5217;&#x6570;&#x6574;&#x9664;&#x5377;&#x79EF;&#x6838;&#x5927;&#x5C0F;&#x6709;&#x4F59;&#x6570;&#xFF0C;&#x800C;&#x53C8;&#x6CA1;&#x6709;padding&#xFF0C;&#x90A3;&#x6700;&#x540E;&#x7684;&#x4F59;&#x6570;&#x5217;&#x4E00;&#x822C;&#x4E0D;&#x4F1A;&#x53C2;&#x4E0E;&#x5377;&#x79EF;&#x8BA1;&#x7B97;&#xFF09;&#xFF0C;&#x8FD9;&#x4E3B;&#x8981;&#x662F;&#x56E0;&#x4E3A;pytorch&#x4E2D;&#x7684;&#x4E92;&#x76F8;&#x5173;&#x64CD;&#x4F5C;<a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">cross-correlation</a>&#x662F;&#x4FDD;&#x8BC1;&#x8BA1;&#x7B97;&#x6B63;&#x786E;&#x7684;&#x64CD;&#x4F5C;(valid operation)&#xFF0C; &#x800C;&#x4E0D;&#x662F;&#x6EE1;&#x64CD;&#x4F5C;(full operation)&#x3002;&#x6240;&#x4EE5;&#x5B9E;&#x9645;&#x64CD;&#x4F5C;&#x4E2D;&#xFF0C;&#x8FD8;&#x662F;&#x8981;&#x4EB2;&#x5C3D;&#x91CF;&#x9009;&#x62E9;&#x597D;&#x5408;&#x9002;&#x7684;padding&#x53C2;&#x6570;&#x54E6;&#x3002;</p>
<p>Note</p>
<p>&#x5F53;<code>groups == in_channels</code> &#x5E76;&#x4E14; <code>out_channels == K * in_channels</code>&#xFF08;&#x5176;&#x4E2D;K&#x662F;&#x6B63;&#x6574;&#x6570;&#xFF09;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x8FD9;&#x4E2A;&#x64CD;&#x4F5C;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x6DF1;&#x5EA6;&#x5377;&#x79EF;&#x3002;
&#x4E3E;&#x4E2A;&#x521B;&#x5EFA;&#x6DF1;&#x5EA6;&#x5377;&#x79EF;&#x5C42;&#x7684;&#x4F8B;&#x5B50;&#xFF0C;&#x5BF9;&#x4E8E;&#x4E00;&#x4E2A;&#x5927;&#x5C0F;&#x4E3A; <img src="img/7db3e5e5d600c81e77756d5eee050505.jpg" alt=""> &#x7684;&#x8F93;&#x5165;&#xFF0C;&#x8981;&#x6784;&#x5EFA;&#x4E00;&#x4E2A;&#x6DF1;&#x5EA6;&#x4E58;&#x6570;&#x4E3A;<code>K</code>&#x7684;&#x6DF1;&#x5EA6;&#x5377;&#x79EF;&#x5C42;&#xFF0C;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x4EE5;&#x4E0B;&#x53C2;&#x6570;&#x6765;&#x521B;&#x5EFA;&#xFF1A;<img src="img/eab8f2745761d762e48a59446243af90.jpg" alt="">&#x3002;</p>
<p>Note</p>
<p>&#x5F53;&#x7A0B;&#x5E8F;&#x7684;&#x8FD0;&#x884C;&#x73AF;&#x5883;&#x662F;&#x4F7F;&#x7528;&#x4E86;CuDNN&#x7684;CUDA&#x73AF;&#x5883;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4E00;&#x4E9B;&#x975E;&#x786E;&#x5B9A;&#x6027;&#x7684;&#x7B97;&#x6CD5;&#xFF08;nondeterministic algorithm&#xFF09;&#x53EF;&#x80FD;&#x4F1A;&#x88AB;&#x91C7;&#x7528;&#x4EE5;&#x63D0;&#x9AD8;&#x6574;&#x4E2A;&#x8BA1;&#x7B97;&#x7684;&#x6027;&#x80FD;&#x3002;&#x5982;&#x679C;&#x4E0D;&#x60F3;&#x4F7F;&#x7528;&#x8FD9;&#x4E9B;&#x975E;&#x786E;&#x5B9A;&#x6027;&#x7684;&#x7B97;&#x6CD5;&#xFF0C;&#x4F60;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x8BBE;&#x7F6E;<code>torch.backends.cudnn.deterministic = True</code>&#x6765;&#x8BA9;&#x6574;&#x4E2A;&#x8BA1;&#x7B97;&#x8FC7;&#x7A0B;&#x4FDD;&#x6301;&#x786E;&#x5B9A;&#x6027;&#xFF08;&#x53EF;&#x80FD;&#x4F1A;&#x635F;&#x5931;&#x4E00;&#x5B9A;&#x7684;&#x8BA1;&#x7B97;&#x6027;&#x80FD;&#xFF09;&#x3002;&#x5BF9;&#x4E8E;&#x540E;&#x7AEF;(background)&#xFF0C;&#x4F60;&#x53EF;&#x4EE5;&#x770B;&#x4E00;&#x4E0B;&#x8FD9;&#x4E00;&#x90E8;&#x5206;<a href="notes/randomness.html">Reproducibility</a>&#x4E86;&#x89E3;&#x5176;&#x76F8;&#x5173;&#x4FE1;&#x606F;&#x3002;</p>
<p>Conv1d&#x7684;&#x53C2;&#x6570;: </p>
<ul>
<li><strong>in_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; &#x8F93;&#x5165;&#x901A;&#x9053;&#x4E2A;&#x6570;</li>
<li><strong>out_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; &#x8F93;&#x51FA;&#x901A;&#x9053;&#x4E2A;&#x6570;</li>
<li><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x5377;&#x79EF;&#x6838;&#x5927;&#x5C0F;</li>
<li><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x7684;&#x6B65;&#x957F;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; 1</li>
<li><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x8F93;&#x5165;&#x6570;&#x636E;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; 0</li>
<li><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5377;&#x79EF;&#x6838;&#x5404;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; 1</li>
<li><strong>groups</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; &#x8F93;&#x5165;&#x901A;&#x9053;&#x4E0E;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x4E4B;&#x95F4;&#x76F8;&#x4E92;&#x9694;&#x79BB;&#x7684;&#x8FDE;&#x63A5;&#x7684;&#x4E2A;&#x6570;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A;1</li>
<li><strong>bias</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5982;&#x679C;&#x88AB;&#x7F6E;&#x4E3A; <code>True</code>&#xFF0C;&#x5411;&#x8F93;&#x51FA;&#x589E;&#x52A0;&#x4E00;&#x4E2A;&#x504F;&#x5DEE;&#x91CF;&#xFF0C;&#x6B64;&#x504F;&#x5DEE;&#x662F;&#x53EF;&#x5B66;&#x4E60;&#x53C2;&#x6570;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A;<code>True</code></li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li><p>&#x8F93;&#x5165;: <img src="img/7db3e5e5d600c81e77756d5eee050505.jpg" alt=""></p>
</li>
<li><p>&#x8F93;&#x51FA;: <img src="img/3423094375906aa21d1b2e095e95c230.jpg" alt=""> &#x5176;&#x4E2D;</p>
<p><img src="img/91d48a39a90c6b4ed37ac863c1a8ff7b.jpg" alt=""></p>
</li>
</ul>
<p>| &#x5185;&#x90E8;Variables&#xFF1A; | </p>
<ul>
<li><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; Conv1d&#x6A21;&#x5757;&#x4E2D;&#x7684;&#x4E00;&#x4E2A;&#x5927;&#x5C0F;&#x4E3A;(out_channels, in_channels, kernel_size)&#x7684;&#x6743;&#x91CD;&#x5F20;&#x91CF;&#xFF0C;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x53EF;&#x8BAD;&#x7EC3;&#x5B66;&#x4E60;(learnable)&#x3002;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x521D;&#x59CB;&#x503C;&#x7684;&#x91C7;&#x6837;&#x7A7A;&#x95F4;&#x662F;<img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt="">&#xFF0C; &#x5176;&#x4E2D;<img src="img/69aab1ce658aabc9a2d986ae8281e2ad.jpg" alt="">&#x3002;</li>
<li><strong>bias</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; &#x6A21;&#x5757;&#x7684;&#x504F;&#x5DEE;&#x9879;&#xFF0C;&#x5927;&#x5C0F;&#x4E3A;(out_channels)&#xFF0C;&#x53EF;&#x8BAD;&#x7EC3;&#x5B66;&#x4E60;&#x3002;&#x5982;&#x679C;&#x6784;&#x9020;Conv1d&#x65F6;&#x6784;&#x9020;&#x51FD;&#x6570;&#x4E2D;&#x7684;<code>bias</code> &#x88AB;&#x7F6E;&#x4E3A; <code>True</code>&#xFF0C;&#x90A3;&#x4E48;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x521D;&#x59CB;&#x503C;&#x7684;&#x91C7;&#x6837;&#x7A7A;&#x95F4;&#x662F;<img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt="">&#xFF0C; &#x5176;&#x4E2D; <img src="img/69aab1ce658aabc9a2d986ae8281e2ad.jpg" alt="">&#x3002;</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Conv1d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="conv2d">Conv2d</h3>
<pre><code class="lang-py">class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
</code></pre>
<p>&#x5229;&#x7528;&#x6307;&#x5B9A;&#x5927;&#x5C0F;&#x7684;&#x4E8C;&#x7EF4;&#x5377;&#x79EF;&#x6838;&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4E8C;&#x7EF4;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x8FDB;&#x884C;&#x4E8C;&#x7EF4;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x7684;&#x5377;&#x79EF;&#x5C42;&#x3002;</p>
<p>&#x5728;&#x6700;&#x7B80;&#x5355;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5BF9;&#x4E8E;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x4E3A;<img src="img/a6c3a4e9779c159b39576bee3400a00b.jpg" alt="">&#xFF0C;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x4E3A;<img src="img/4b354af142fb0f01680d390ef552829f.jpg" alt="">&#x7684;&#x4E8C;&#x7EF4;&#x7EF4;&#x5377;&#x79EF;&#x5C42;&#xFF0C;&#x5176;&#x5377;&#x79EF;&#x8BA1;&#x7B97;&#x8FC7;&#x7A0B;&#x53EF;&#x4EE5;&#x5982;&#x4E0B;&#x8868;&#x8FF0;&#xFF1A;</p>
<p><img src="img/a4928651cb959fa7871eaebdb489b083.jpg" alt=""></p>
<p>&#x8FD9;&#x91CC;&#x7684;<img src="img/d5d3d32b4a35f91edb54c3c3f87d582e.jpg" alt="">&#x7B26;&#x53F7;&#x5B9E;&#x9645;&#x4E0A;&#x662F;&#x4E00;&#x4E2A;&#x4E8C;&#x7EF4;&#x4E92;&#x76F8;&#x5173;&#xFF08;<a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">cross-correlation</a>&#xFF09; &#x64CD;&#x4F5C;&#x7B26;&#xFF08;&#x5927;&#x5BB6;&#x53EF;&#x4EE5;&#x81EA;&#x5DF1;&#x67E5;&#x4E00;&#x4E0B;&#x4E92;&#x76F8;&#x5173;&#x548C;&#x771F;&#x5377;&#x79EF;&#x7684;&#x533A;&#x522B;&#xFF0C;&#x4E92;&#x76F8;&#x5173;&#x56E0;&#x4E3A;&#x5B9E;&#x73B0;&#x8D77;&#x6765;&#x5F88;&#x7B80;&#x5355;&#xFF0C;&#x6240;&#x4EE5;&#x4E00;&#x822C;&#x7684;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x6846;&#x67B6;&#x90FD;&#x662F;&#x7528;&#x4E92;&#x76F8;&#x5173;&#x64CD;&#x4F5C;&#x53D6;&#x4EE3;&#x771F;&#x5377;&#x79EF;&#xFF09;, <img src="img/9341d9048ac485106d2b2ee8de14876f.jpg" alt=""> is a batch size, <img src="img/6c8feca3b2da3d6cf371417edff4be4f.jpg" alt=""> &#x4EE3;&#x8868;&#x901A;&#x9053;&#x7684;&#x6570;&#x91CF;, <img src="img/9b7d9beafd65e2cf6493bdca741827a5.jpg" alt=""> &#x662F;&#x8F93;&#x5165;&#x7684;&#x4E8C;&#x7EF4;&#x6570;&#x636E;&#x7684;&#x50CF;&#x7D20;&#x9AD8;&#x5EA6;&#xFF0C;<img src="img/90490a34512e9bd1843ed4da713d0813.jpg" alt=""> &#x662F;&#x8F93;&#x5165;&#x7684;&#x4E8C;&#x7EF4;&#x6570;&#x636E;&#x7684;&#x50CF;&#x7D20;&#x5BBD;&#x5EA6;&#x3002;</p>
<ul>
<li><p><code>stride</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x4E92;&#x76F8;&#x5173;&#x64CD;&#x4F5C;&#xFF08;&#x4F2A;&#x5377;&#x79EF;&#xFF09;&#x7684;&#x6B65;&#x957F;&#xFF0C;&#x53C2;&#x6570;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x4E00;&#x822C;&#x662F;&#x5355;&#x4E2A;&#x6570;&#x5B57;&#x6216;&#x8005;&#x4E00;&#x4E2A;&#x53EA;&#x6709;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5143;&#x7EC4;&#x3002;</p>
</li>
<li><p><code>padding</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x8981;&#x5728;&#x4E8C;&#x7EF4;&#x5377;&#x79EF;&#x6838;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x3002;</p>
</li>
<li><p><code>dilation</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x5377;&#x79EF;&#x6838;&#x4E2D;&#x5404;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#xFF1B;&#x8FD9;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x591A;&#x5B54;&#x7B97;&#x6CD5;(&#xE0; trous algorithm)&#x3002;&#x8FD9;&#x4E2A;&#x6982;&#x5FF5;&#x6709;&#x70B9;&#x96BE;&#x89E3;&#x91CA;&#xFF0C;&#x8FD9;&#x4E2A;&#x94FE;&#x63A5;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">link</a>&#x7528;&#x53EF;&#x89C6;&#x5316;&#x7684;&#x65B9;&#x6CD5;&#x5F88;&#x597D;&#x5730;&#x89E3;&#x91CA;&#x4E86;<code>dilation</code>&#x7684;&#x4F5C;&#x7528;&#x3002;</p>
</li>
<li><p><code>groups</code> &#x63A7;&#x5236;&#x4E86;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x4E4B;&#x95F4;&#x7684;&#x8FDE;&#x63A5;&#xFF08;connections&#xFF09;&#x7684;&#x6570;&#x91CF;&#x3002;<code>in_channels</code> &#x548C; <code>out_channels</code> &#x5FC5;&#x987B;&#x80FD;&#x88AB; <code>groups</code> &#x6574;&#x9664;&#x3002;&#x4E3E;&#x4E2A;&#x6817;&#x5B50;&#xFF0C; </p>
<p>&gt; *   &#x5F53; groups=1, &#x6B64;Conv1d&#x5C42;&#x4F1A;&#x4F7F;&#x7528;&#x4E00;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x8FDB;&#x884C;&#x6240;&#x6709;&#x8F93;&#x5165;&#x5230;&#x8F93;&#x51FA;&#x7684;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&gt; *   &#x5F53; groups=2, &#x6B64;&#x65F6;Conv1d&#x5C42;&#x4F1A;&#x4EA7;&#x751F;&#x4E24;&#x4E2A;&#x5E76;&#x5217;&#x7684;&#x5377;&#x79EF;&#x5C42;&#x3002;&#x540C;&#x65F6;&#xFF0C;&#x8F93;&#x5165;&#x901A;&#x9053;&#x88AB;&#x5206;&#x4E3A;&#x4E24;&#x534A;&#xFF0C;&#x4E24;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x5206;&#x522B;&#x5904;&#x7406;&#x4E00;&#x534A;&#x7684;&#x8F93;&#x5165;&#x901A;&#x9053;&#xFF0C;&#x540C;&#x65F6;&#x5404;&#x81EA;&#x4EA7;&#x751F;&#x4E00;&#x534A;&#x7684;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x3002;&#x6700;&#x540E;&#x8FD9;&#x4E24;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x4F1A;&#x88AB;concatenated&#x4E00;&#x8D77;&#xFF0C;&#x4F5C;&#x4E3A;&#x6B64;Conv1d&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x3002;</p>
<p>&gt; *   &#x5F53; groups= <code>in_channels</code>, &#x6BCF;&#x4E2A;&#x8F93;&#x5165;&#x901A;&#x9053;&#x90FD;&#x4F1A;&#x88AB;&#x5355;&#x72EC;&#x7684;&#x4E00;&#x7EC4;&#x5377;&#x79EF;&#x5C42;&#x5904;&#x7406;&#xFF0C;&#x8FD9;&#x4E2A;&#x7EC4;&#x7684;&#x5927;&#x5C0F;&#x662F;<img src="img/19131f9f53448ae579b613bc7bc90158.jpg" alt=""></p>
</li>
</ul>
<p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code>&#x8FD9;&#x51E0;&#x4E2A;&#x53C2;&#x6570;&#x5747;&#x652F;&#x6301;&#x4E00;&#x4E0B;&#x8F93;&#x5165;&#x5F62;&#x5F0F;&#xFF1A;</p>
<blockquote>
<ul>
<li>&#x4E00;&#x4E2A; <code>int</code> &#x6570;&#x5B57; &#x2013; &#x4E8C;&#x7EF4;&#x6570;&#x636E;&#x7684;&#x9AD8;&#x548C;&#x5BBD;&#x8FD9;&#x4E24;&#x4E2A;&#x7EF4;&#x5EA6;&#x90FD;&#x4F1A;&#x91C7;&#x7528;&#x8FD9;&#x4E00;&#x4E2A;&#x6570;&#x5B57;&#x3002;</li>
<li>&#x4E00;&#x4E2A;&#x7531;&#x4E24;&#x4E2A;int&#x6570;&#x5B57;&#x7EC4;&#x6210;&#x7684;<code>tuple</code>&#x2013; &#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x4E8C;&#x7EF4;&#x6570;&#x636E;&#x7684;&#x9AD8;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x5143;&#x7EC4;&#x4E2D;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#xFF0C;&#x5BBD;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x7B2C;&#x4E8C;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#x3002;</li>
</ul>
</blockquote>
<p>Note</p>
<p>&#x53D6;&#x51B3;&#x4E8E;&#x4F60;&#x5377;&#x79EF;&#x6838;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x6709;&#x4E9B;&#x65F6;&#x5019;&#x8F93;&#x5165;&#x6570;&#x636E;&#x4E2D;&#x67D0;&#x4E9B;&#x5217;&#xFF08;&#x6700;&#x540E;&#x51E0;&#x5217;&#xFF09;&#x53EF;&#x80FD;&#x4E0D;&#x4F1A;&#x53C2;&#x4E0E;&#x8BA1;&#x7B97;&#xFF08;&#x6BD4;&#x5982;&#x5217;&#x6570;&#x6574;&#x9664;&#x5377;&#x79EF;&#x6838;&#x5927;&#x5C0F;&#x6709;&#x4F59;&#x6570;&#xFF0C;&#x800C;&#x53C8;&#x6CA1;&#x6709;padding&#xFF0C;&#x90A3;&#x6700;&#x540E;&#x7684;&#x4F59;&#x6570;&#x5217;&#x4E00;&#x822C;&#x4E0D;&#x4F1A;&#x53C2;&#x4E0E;&#x5377;&#x79EF;&#x8BA1;&#x7B97;&#xFF09;&#xFF0C;&#x8FD9;&#x4E3B;&#x8981;&#x662F;&#x56E0;&#x4E3A;pytorch&#x4E2D;&#x7684;&#x4E92;&#x76F8;&#x5173;&#x64CD;&#x4F5C;<a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">cross-correlation</a>&#x662F;&#x4FDD;&#x8BC1;&#x8BA1;&#x7B97;&#x6B63;&#x786E;&#x7684;&#x64CD;&#x4F5C;(valid operation)&#xFF0C; &#x800C;&#x4E0D;&#x662F;&#x6EE1;&#x64CD;&#x4F5C;(full operation)&#x3002;&#x6240;&#x4EE5;&#x5B9E;&#x9645;&#x64CD;&#x4F5C;&#x4E2D;&#xFF0C;&#x8FD8;&#x662F;&#x8981;&#x4EB2;&#x5C3D;&#x91CF;&#x9009;&#x62E9;&#x597D;&#x5408;&#x9002;&#x7684;padding&#x53C2;&#x6570;&#x54E6;&#x3002;</p>
<p>Note
&#x5F53;<code>groups == in_channels</code> &#x5E76;&#x4E14; <code>out_channels == K * in_channels</code>&#xFF08;&#x5176;&#x4E2D;K&#x662F;&#x6B63;&#x6574;&#x6570;&#xFF09;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x8FD9;&#x4E2A;&#x64CD;&#x4F5C;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x6DF1;&#x5EA6;&#x5377;&#x79EF;&#x3002;</p>
<p>&#x6362;&#x53E5;&#x8BDD;&#x8BF4;&#xFF0C;&#x5BF9;&#x4E8E;&#x4E00;&#x4E2A;&#x5927;&#x5C0F;&#x4E3A;<img src="img/0385ad868fed790d36381b9e8788c18b.jpg" alt="">&#x7684;&#x8F93;&#x5165;&#xFF0C;&#x8981;&#x6784;&#x5EFA;&#x4E00;&#x4E2A;&#x6DF1;&#x5EA6;&#x4E58;&#x6570;&#x4E3A;<code>K</code>&#x7684;&#x6DF1;&#x5EA6;&#x5377;&#x79EF;&#x5C42;&#xFF0C;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x4EE5;&#x4E0B;&#x53C2;&#x6570;&#x6765;&#x521B;&#x5EFA;&#xFF1A;<img src="img/8aee041e54a302b342d50912ce67f44b.jpg" alt="">&#x3002;</p>
<p>Note</p>
<p>&#x5F53;&#x7A0B;&#x5E8F;&#x7684;&#x8FD0;&#x884C;&#x73AF;&#x5883;&#x662F;&#x4F7F;&#x7528;&#x4E86;CuDNN&#x7684;CUDA&#x73AF;&#x5883;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4E00;&#x4E9B;&#x975E;&#x786E;&#x5B9A;&#x6027;&#x7684;&#x7B97;&#x6CD5;&#xFF08;nondeterministic algorithm&#xFF09;&#x53EF;&#x80FD;&#x4F1A;&#x88AB;&#x91C7;&#x7528;&#x4EE5;&#x63D0;&#x9AD8;&#x6574;&#x4E2A;&#x8BA1;&#x7B97;&#x7684;&#x6027;&#x80FD;&#x3002;&#x5982;&#x679C;&#x4E0D;&#x60F3;&#x4F7F;&#x7528;&#x8FD9;&#x4E9B;&#x975E;&#x786E;&#x5B9A;&#x6027;&#x7684;&#x7B97;&#x6CD5;&#xFF0C;&#x4F60;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x8BBE;&#x7F6E;<code>torch.backends.cudnn.deterministic = True</code>&#x6765;&#x8BA9;&#x6574;&#x4E2A;&#x8BA1;&#x7B97;&#x8FC7;&#x7A0B;&#x4FDD;&#x6301;&#x786E;&#x5B9A;&#x6027;&#xFF08;&#x53EF;&#x80FD;&#x4F1A;&#x635F;&#x5931;&#x4E00;&#x5B9A;&#x7684;&#x8BA1;&#x7B97;&#x6027;&#x80FD;&#xFF09;&#x3002;&#x5BF9;&#x4E8E;&#x540E;&#x7AEF;(background)&#xFF0C;&#x4F60;&#x53EF;&#x4EE5;&#x770B;&#x4E00;&#x4E0B;&#x8FD9;&#x4E00;&#x90E8;&#x5206;<a href="notes/randomness.html">Reproducibility</a>&#x4E86;&#x89E3;&#x5176;&#x76F8;&#x5173;&#x4FE1;&#x606F;&#x3002;</p>
<p>Conv2d&#x7684;&#x53C2;&#x6570;: </p>
<ul>
<li><strong>in_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; &#x8F93;&#x5165;&#x901A;&#x9053;&#x4E2A;&#x6570;</li>
<li><strong>out_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; &#x8F93;&#x51FA;&#x901A;&#x9053;&#x4E2A;&#x6570;</li>
<li><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x5377;&#x79EF;&#x6838;&#x5927;&#x5C0F;</li>
<li><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x7684;&#x6B65;&#x957F;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; 1</li>
<li><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x8F93;&#x5165;&#x6570;&#x636E;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; 0</li>
<li><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013;&#x5377;&#x79EF;&#x6838;&#x5404;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; 1</li>
<li><strong>groups</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; &#x8F93;&#x5165;&#x901A;&#x9053;&#x4E0E;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x4E4B;&#x95F4;&#x76F8;&#x4E92;&#x9694;&#x79BB;&#x7684;&#x8FDE;&#x63A5;&#x7684;&#x4E2A;&#x6570;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A;1</li>
<li><strong>bias</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5982;&#x679C;&#x88AB;&#x7F6E;&#x4E3A; <code>True</code>&#xFF0C;&#x5411;&#x8F93;&#x51FA;&#x589E;&#x52A0;&#x4E00;&#x4E2A;&#x504F;&#x5DEE;&#x91CF;&#xFF0C;&#x6B64;&#x504F;&#x5DEE;&#x662F;&#x53EF;&#x5B66;&#x4E60;&#x53C2;&#x6570;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A;<code>True</code></li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li><p>&#x8F93;&#x5165;: <img src="img/0385ad868fed790d36381b9e8788c18b.jpg" alt=""></p>
</li>
<li><p>&#x8F93;&#x51FA;: <img src="img/d3edfe8a9bbdd73ba5c4b566353777f0.jpg" alt=""> &#x5176;&#x4E2D;</p>
<p><img src="img/a89a5326ab89279b92f4720f63b4eaae.jpg" alt=""></p>
<p><img src="img/03f69d6e3dffc3254359e41f8b310667.jpg" alt=""></p>
</li>
</ul>
<p>| &#x5185;&#x90E8;Variables: | </p>
<ul>
<li><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; Conv2d&#x6A21;&#x5757;&#x4E2D;&#x7684;&#x4E00;&#x4E2A;&#x5927;&#x5C0F;&#x4E3A; (out_channels, in_channels, kernel_size[0], kernel_size[1])&#x7684;&#x6743;&#x91CD;&#x5F20;&#x91CF;&#xFF0C;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x53EF;&#x8BAD;&#x7EC3;&#x5B66;&#x4E60;(learnable)&#x3002;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x521D;&#x59CB;&#x503C;&#x7684;&#x91C7;&#x6837;&#x7A7A;&#x95F4;&#x662F; <img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt="">&#xFF0C; &#x5176;&#x4E2D;<img src="img/c12e2153347b696ebb784e5675cc566e.jpg" alt="">&#x3002;</li>
<li><strong>bias</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; &#x5757;&#x7684;&#x504F;&#x5DEE;&#x9879;&#xFF0C;&#x5927;&#x5C0F;&#x4E3A;(out_channels)&#xFF0C;&#x53EF;&#x8BAD;&#x7EC3;&#x5B66;&#x4E60;&#x3002;&#x5982;&#x679C;&#x6784;&#x9020;Conv2d&#x65F6;&#x6784;&#x9020;&#x51FD;&#x6570;&#x4E2D;&#x7684;<code>bias</code> &#x88AB;&#x7F6E;&#x4E3A; <code>True</code>&#xFF0C;&#x90A3;&#x4E48;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x521D;&#x59CB;&#x503C;&#x7684;&#x91C7;&#x6837;&#x7A7A;&#x95F4;&#x662F;<img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt="">&#xFF0C;&#x5176;&#x4E2D;<img src="img/c12e2153347b696ebb784e5675cc566e.jpg" alt="">&#x3002;</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With square kernels and equal stride</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># non-square kernels and unequal stride and with padding</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">5</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">4</span>, <span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># non-square kernels and unequal stride and with padding and dilation</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">5</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">4</span>, <span class="hljs-number">2</span>), dilation=(<span class="hljs-number">3</span>, <span class="hljs-number">1</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>, <span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="conv3d">Conv3d</h3>
<pre><code class="lang-py">class torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
</code></pre>
<p>&#x5229;&#x7528;&#x6307;&#x5B9A;&#x5927;&#x5C0F;&#x7684;&#x4E09;&#x7EF4;&#x5377;&#x79EF;&#x6838;&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4E09;&#x7EF4;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x8FDB;&#x884C;&#x4E09;&#x7EF4;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x7684;&#x5377;&#x79EF;&#x5C42;&#x3002;</p>
<p>&#x6700;&#x7B80;&#x5355;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5BF9;&#x4E8E;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x4E3A;<img src="img/ca863d6b44a0246998de77c7c423ec32.jpg" alt="">&#xFF0C;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x4E3A;<img src="img/f05e8faaf90b4c16b23ca0165e8e09f4.jpg" alt=""> &#x7684;&#x4E09;&#x7EF4;&#x5377;&#x79EF;&#x5C42;&#xFF0C;&#x5176;&#x5377;&#x79EF;&#x8BA1;&#x7B97;&#x8FC7;&#x7A0B;&#x53EF;&#x4EE5;&#x5982;&#x4E0B;&#x8868;&#x8FF0;&#xFF1A;</p>
<p><img src="img/39831867c152a21de6e580bf01c0cb7f.jpg" alt=""></p>
<p>&#x8FD9;&#x91CC;&#x7684; <img src="img/d5d3d32b4a35f91edb54c3c3f87d582e.jpg" alt="">&#x7B26;&#x53F7;&#x5B9E;&#x9645;&#x4E0A;&#x662F;&#x4E00;&#x4E2A;&#x4E09;&#x7EF4;&#x4E92;&#x76F8;&#x5173; <a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">cross-correlation</a> &#x64CD;&#x4F5C;&#x7B26;&#x3002;</p>
<ul>
<li><p><code>stride</code> &#x6570;&#x63A7;&#x5236;&#x4E86;&#x4E92;&#x76F8;&#x5173;&#x64CD;&#x4F5C;&#xFF08;&#x4F2A;&#x5377;&#x79EF;&#xFF09;&#x7684;&#x6B65;&#x957F;&#x3002;</p>
</li>
<li><p><code>padding</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x8981;&#x5728;&#x4E09;&#x7EF4;&#x5377;&#x79EF;&#x6838;&#x7684;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x3002;</p>
</li>
<li><p><code>dilation</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x5377;&#x79EF;&#x6838;&#x4E2D;&#x5404;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#xFF1B;&#x8FD9;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x591A;&#x5B54;&#x7B97;&#x6CD5;(&#xE0; trous algorithm)&#x3002;&#x8FD9;&#x4E2A;&#x6982;&#x5FF5;&#x6709;&#x70B9;&#x96BE;&#x89E3;&#x91CA;&#xFF0C;&#x8FD9;&#x4E2A;&#x94FE;&#x63A5;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">link</a>&#x7528;&#x53EF;&#x89C6;&#x5316;&#x7684;&#x65B9;&#x6CD5;&#x5F88;&#x597D;&#x5730;&#x89E3;&#x91CA;&#x4E86;<code>dilation</code>&#x7684;&#x4F5C;&#x7528;&#x3002;</p>
</li>
<li><p><code>groups</code> &#x63A7;&#x5236;&#x4E86;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x4E4B;&#x95F4;&#x7684;&#x8FDE;&#x63A5;&#xFF08;connections&#xFF09;&#x7684;&#x6570;&#x91CF;&#x3002;<code>in_channels</code> &#x548C; <code>out_channels</code> &#x5FC5;&#x987B;&#x80FD;&#x88AB; <code>groups</code> &#x6574;&#x9664;&#x3002;&#x4E3E;&#x4E2A;&#x6817;&#x5B50;&#xFF0C;</p>
<p>&gt; *   &#x5F53; groups=1, &#x6B64;Conv3d&#x5C42;&#x4F1A;&#x4F7F;&#x7528;&#x4E00;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x8FDB;&#x884C;&#x5BF9;&#x6240;&#x6709;&#x8F93;&#x5165;&#x5230;&#x8F93;&#x51FA;&#x7684;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&gt; *   &#x5F53; groups=2, &#x6B64;&#x65F6;Conv3d&#x5C42;&#x4F1A;&#x4EA7;&#x751F;&#x4E24;&#x4E2A;&#x5E76;&#x5217;&#x7684;&#x5377;&#x79EF;&#x5C42;&#x3002;&#x540C;&#x65F6;&#xFF0C;&#x8F93;&#x5165;&#x901A;&#x9053;&#x88AB;&#x5206;&#x4E3A;&#x4E24;&#x534A;&#xFF0C;&#x4E24;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x5206;&#x522B;&#x5904;&#x7406;&#x4E00;&#x534A;&#x7684;&#x8F93;&#x5165;&#x901A;&#x9053;&#xFF0C;&#x540C;&#x65F6;&#x5404;&#x81EA;&#x4EA7;&#x751F;&#x4E00;&#x534A;&#x7684;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x3002;&#x6700;&#x540E;&#x8FD9;&#x4E24;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x4F1A;&#x88AB;concatenated&#x4E00;&#x8D77;&#xFF0C;&#x4F5C;&#x4E3A;&#x6B64;Conv3d&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x3002;</p>
<p>&gt; *   &#x5F53; groups= in_channels, &#x6BCF;&#x4E2A;&#x8F93;&#x5165;&#x901A;&#x9053;&#x90FD;&#x4F1A;&#x88AB;&#x5355;&#x72EC;&#x7684;&#x4E00;&#x7EC4;&#x5377;&#x79EF;&#x5C42;&#x5904;&#x7406;&#xFF0C;&#x8FD9;&#x4E2A;&#x7EC4;&#x7684;&#x5927;&#x5C0F;&#x662F; <img src="img/648a514da1dace3deacf3f078287e157.jpg" alt="">.</p>
</li>
</ul>
<p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code>&#x8FD9;&#x51E0;&#x4E2A;&#x53C2;&#x6570;&#x5747;&#x652F;&#x6301;&#x4E00;&#x4E0B;&#x8F93;&#x5165;&#x5F62;&#x5F0F;&#xFF1A;</p>
<blockquote>
<ul>
<li>&#x4E00;&#x4E2A; <code>int</code> &#x6570;&#x5B57; &#x2013; &#x4E09;&#x7EF4;&#x7EF4;&#x6570;&#x636E;&#x7684;&#x6DF1;&#x5EA6;&#xFF0C;&#x9AD8;&#x548C;&#x5BBD;&#x8FD9;&#x4E09;&#x4E2A;&#x7EF4;&#x5EA6;&#x90FD;&#x4F1A;&#x91C7;&#x7528;&#x8FD9;&#x4E00;&#x4E2A;&#x6570;&#x5B57;&#x3002;</li>
<li>&#x4E00;&#x4E2A;&#x7531;&#x4E09;&#x4E2A;int&#x6570;&#x5B57;&#x7EC4;&#x6210;&#x7684;<code>tuple</code>&#x2013; &#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x4E09;&#x7EF4;&#x6570;&#x636E;&#x7684;&#x6DF1;&#x5EA6;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x5143;&#x7EC4;&#x4E2D;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#xFF0C;&#x9AD8;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x5143;&#x7EC4;&#x4E2D;&#x7684;&#x7B2C;&#x4E8C;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#xFF0C;&#x5BBD;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x7B2C;&#x4E09;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#x3002;</li>
</ul>
</blockquote>
<p>Note</p>
<p>&#x53D6;&#x51B3;&#x4E8E;&#x4F60;&#x5377;&#x79EF;&#x6838;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x6709;&#x4E9B;&#x65F6;&#x5019;&#x8F93;&#x5165;&#x6570;&#x636E;&#x4E2D;&#x67D0;&#x4E9B;&#x5217;&#xFF08;&#x6700;&#x540E;&#x51E0;&#x5217;&#xFF09;&#x53EF;&#x80FD;&#x4E0D;&#x4F1A;&#x53C2;&#x4E0E;&#x8BA1;&#x7B97;&#xFF08;&#x6BD4;&#x5982;&#x5217;&#x6570;&#x6574;&#x9664;&#x5377;&#x79EF;&#x6838;&#x5927;&#x5C0F;&#x6709;&#x4F59;&#x6570;&#xFF0C;&#x800C;&#x53C8;&#x6CA1;&#x6709;padding&#xFF0C;&#x90A3;&#x6700;&#x540E;&#x7684;&#x4F59;&#x6570;&#x5217;&#x4E00;&#x822C;&#x4E0D;&#x4F1A;&#x53C2;&#x4E0E;&#x5377;&#x79EF;&#x8BA1;&#x7B97;&#xFF09;&#xFF0C;&#x8FD9;&#x4E3B;&#x8981;&#x662F;&#x56E0;&#x4E3A;pytorch&#x4E2D;&#x7684;&#x4E92;&#x76F8;&#x5173;&#x64CD;&#x4F5C;<a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">cross-correlation</a>&#x662F;&#x4FDD;&#x8BC1;&#x8BA1;&#x7B97;&#x6B63;&#x786E;&#x7684;&#x64CD;&#x4F5C;(valid operation)&#xFF0C; &#x800C;&#x4E0D;&#x662F;&#x6EE1;&#x64CD;&#x4F5C;(full operation)&#x3002;&#x6240;&#x4EE5;&#x5B9E;&#x9645;&#x64CD;&#x4F5C;&#x4E2D;&#xFF0C;&#x8FD8;&#x662F;&#x8981;&#x4EB2;&#x5C3D;&#x91CF;&#x9009;&#x62E9;&#x597D;&#x5408;&#x9002;&#x7684;padding&#x53C2;&#x6570;&#x54E6;&#x3002;</p>
<p>Note</p>
<p>&#x5F53;<code>groups == in_channels</code> &#x5E76;&#x4E14; <code>out_channels == K * in_channels</code>&#xFF08;&#x5176;&#x4E2D;K&#x662F;&#x6B63;&#x6574;&#x6570;&#xFF09;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x8FD9;&#x4E2A;&#x64CD;&#x4F5C;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x6DF1;&#x5EA6;&#x5377;&#x79EF;&#x3002;</p>
<p>&#x6362;&#x53E5;&#x8BDD;&#x8BF4;&#xFF0C;&#x5BF9;&#x4E8E;&#x4E00;&#x4E2A;&#x5927;&#x5C0F;&#x4E3A;  <img src="img/a8d71105bc4954eb54660bc5d37c23de.jpg" alt=""> &#x7684;&#x8F93;&#x5165;&#xFF0C;&#x8981;&#x6784;&#x5EFA;&#x4E00;&#x4E2A;&#x6DF1;&#x5EA6;&#x4E58;&#x6570;&#x4E3A;<code>K</code>&#x7684;&#x6DF1;&#x5EA6;&#x5377;&#x79EF;&#x5C42;&#xFF0C;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x4EE5;&#x4E0B;&#x53C2;&#x6570;&#x6765;&#x521B;&#x5EFA;&#xFF1A;<img src="img/8aee041e54a302b342d50912ce67f44b.jpg" alt="">&#x3002;</p>
<p>Note</p>
<p>&#x5F53;&#x7A0B;&#x5E8F;&#x7684;&#x8FD0;&#x884C;&#x73AF;&#x5883;&#x662F;&#x4F7F;&#x7528;&#x4E86;CuDNN&#x7684;CUDA&#x73AF;&#x5883;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4E00;&#x4E9B;&#x975E;&#x786E;&#x5B9A;&#x6027;&#x7684;&#x7B97;&#x6CD5;&#xFF08;nondeterministic algorithm&#xFF09;&#x53EF;&#x80FD;&#x4F1A;&#x88AB;&#x91C7;&#x7528;&#x4EE5;&#x63D0;&#x9AD8;&#x6574;&#x4E2A;&#x8BA1;&#x7B97;&#x7684;&#x6027;&#x80FD;&#x3002;&#x5982;&#x679C;&#x4E0D;&#x60F3;&#x4F7F;&#x7528;&#x8FD9;&#x4E9B;&#x975E;&#x786E;&#x5B9A;&#x6027;&#x7684;&#x7B97;&#x6CD5;&#xFF0C;&#x4F60;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x8BBE;&#x7F6E;<code>torch.backends.cudnn.deterministic = True</code>&#x6765;&#x8BA9;&#x6574;&#x4E2A;&#x8BA1;&#x7B97;&#x8FC7;&#x7A0B;&#x4FDD;&#x6301;&#x786E;&#x5B9A;&#x6027;&#xFF08;&#x53EF;&#x80FD;&#x4F1A;&#x635F;&#x5931;&#x4E00;&#x5B9A;&#x7684;&#x8BA1;&#x7B97;&#x6027;&#x80FD;&#xFF09;&#x3002;&#x5BF9;&#x4E8E;&#x540E;&#x7AEF;(background)&#xFF0C;&#x4F60;&#x53EF;&#x4EE5;&#x770B;&#x4E00;&#x4E0B;&#x8FD9;&#x4E00;&#x90E8;&#x5206;<a href="notes/randomness.html">Reproducibility</a>&#x4E86;&#x89E3;&#x5176;&#x76F8;&#x5173;&#x4FE1;&#x606F;&#x3002;</p>
<p>Parameters: </p>
<ul>
<li><strong>in_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; &#x8F93;&#x5165;&#x901A;&#x9053;&#x7684;&#x4E2A;&#x6570;</li>
<li><strong>out_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; &#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x7684;&#x4E2A;&#x6570;</li>
<li><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x5377;&#x79EF;&#x6838;&#x5927;&#x5C0F;</li>
<li><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x7684;&#x6B65;&#x957F;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; 1</li>
<li><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x8F93;&#x5165;&#x6570;&#x636E;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; 0</li>
<li><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5377;&#x79EF;&#x6838;&#x5404;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; 1</li>
<li><strong>groups</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; &#x8F93;&#x5165;&#x901A;&#x9053;&#x4E0E;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x4E4B;&#x95F4;&#x76F8;&#x4E92;&#x9694;&#x79BB;&#x7684;&#x8FDE;&#x63A5;&#x7684;&#x4E2A;&#x6570;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A;1</li>
<li><strong>bias</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5982;&#x679C;&#x88AB;&#x7F6E;&#x4E3A; <code>True</code>&#xFF0C;&#x5411;&#x8F93;&#x51FA;&#x589E;&#x52A0;&#x4E00;&#x4E2A;&#x504F;&#x5DEE;&#x91CF;&#xFF0C;&#x6B64;&#x504F;&#x5DEE;&#x662F;&#x53EF;&#x5B66;&#x4E60;&#x53C2;&#x6570;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A;<code>True</code></li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li><p>&#x8F93;&#x5165;: <img src="img/a8d71105bc4954eb54660bc5d37c23de.jpg" alt=""></p>
</li>
<li><p>&#x8F93;&#x51FA;: <img src="img/f05e8faaf90b4c16b23ca0165e8e09f4.jpg" alt=""> where</p>
<p><img src="img/bbc2662490bb72269672fe81af1fe003.jpg" alt=""></p>
<p><img src="img/b7ca056f55603d0632bb03bdf9435d47.jpg" alt=""></p>
<p><img src="img/d040a26cd9a91c4d230afd4c15d0e1e6.jpg" alt=""></p>
</li>
</ul>
<p>| &#x5185;&#x90E8;Variables: | </p>
<ul>
<li><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; Conv3d&#x6A21;&#x5757;&#x4E2D;&#x7684;&#x4E00;&#x4E2A;&#x5927;&#x5C0F;&#x4E3A; (out_channels, in_channels, kernel_size[0], kernel_size[1], kernel_size[2]) &#x7684;&#x6743;&#x91CD;&#x5F20;&#x91CF;&#xFF0C;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x53EF;&#x8BAD;&#x7EC3;&#x5B66;&#x4E60;(learnable)&#x3002;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x521D;&#x59CB;&#x503C;&#x7684;&#x91C7;&#x6837;&#x7A7A;&#x95F4;&#x662F;<img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt="">&#xFF0C;&#x5176;&#x4E2D;<img src="img/378f5c5b47c36239b817ad23a612a9f7.jpg" alt="">&#x3002;</li>
<li><strong>bias</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; &#x6A21;&#x5757;&#x7684;&#x504F;&#x5DEE;&#x9879;&#xFF0C;&#x5927;&#x5C0F;&#x4E3A;(out_channels)&#xFF0C;&#x53EF;&#x8BAD;&#x7EC3;&#x5B66;&#x4E60;&#x3002;&#x5982;&#x679C;&#x6784;&#x9020;Conv1d&#x65F6;&#x6784;&#x9020;&#x51FD;&#x6570;&#x4E2D;&#x7684;<code>bias</code> &#x88AB;&#x7F6E;&#x4E3A; <code>True</code>&#xFF0C;&#x90A3;&#x4E48;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x521D;&#x59CB;&#x503C;&#x7684;&#x91C7;&#x6837;&#x7A7A;&#x95F4;&#x662F; <img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt=""> &#xFF0C;&#x5176;&#x4E2D; <img src="img/378f5c5b47c36239b817ad23a612a9f7.jpg" alt="">&#x3002;</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With square kernels and equal stride</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Conv3d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># non-square kernels and unequal stride and with padding</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Conv3d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">4</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>, <span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="convtranspose1d">ConvTranspose1d</h3>
<pre><code class="lang-py">class torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)
</code></pre>
<p>&#x5229;&#x7528;&#x6307;&#x5B9A;&#x5927;&#x5C0F;&#x7684;&#x4E00;&#x7EF4;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x6838;&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4E00;&#x7EF4;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x8FDB;&#x884C;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#xFF08;&#x5F53;&#x7136;&#x6B64;&#x5377;&#x79EF;&#x4E5F;&#x662F;&#x4E92;&#x76F8;&#x5173;&#x64CD;&#x4F5C;&#xFF0C;cross-correlation&#xFF09;&#x64CD;&#x4F5C;&#x7684;&#x6A21;&#x5757;&#x3002;</p>
<p>&#x8BE5;&#x6A21;&#x5757;&#x53EF;&#x4EE5;&#x770B;&#x4F5C;&#x662F;Conv1d&#x76F8;&#x5BF9;&#x4E8E;&#x5176;&#x8F93;&#x5165;&#x7684;&#x68AF;&#x5EA6;(the gradient of Conv1d with respect to its input&#xFF0C; &#x76F4;&#x8BD1;)&#xFF0C; &#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x53C8;&#x88AB;&#x79F0;&#x4E3A;&#x5C0F;&#x6570;&#x6B65;&#x957F;&#x5377;&#x79EF;&#x6216;&#x662F;&#x53CD;&#x5377;&#x79EF;&#xFF08;&#x5C3D;&#x7BA1;&#x8FD9;&#x4E0D;&#x662F;&#x4E00;&#x4E2A;&#x771F;&#x6B63;&#x610F;&#x4E49;&#x4E0A;&#x7684;&#x53CD;&#x5377;&#x79EF;&#xFF09;&#x3002;</p>
<ul>
<li><p><code>stride</code> &#x63A7;&#x5236;&#x4E86;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x7684;&#x6B65;&#x957F;</p>
</li>
<li><p><code>padding</code> &#x63A7;&#x5236;&#x4E86;&#x8981;&#x5728;&#x8F93;&#x5165;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x7684;&#x5404;&#x8FB9;&#x4E0A;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#xFF0C;&#x4E0E;Conv1d&#x4E0D;&#x540C;&#x7684;&#x5730;&#x65B9;&#xFF0C;&#x6B64;padding&#x53C2;&#x6570;&#x4E0E;&#x5B9E;&#x9645;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x7684;&#x5173;&#x7CFB;&#x4E3A;<code>&#x5C42;&#x6570; = kernel_size - 1 - padding</code>&#xFF0C;&#x8BE6;&#x60C5;&#x8BF7;&#x89C1;&#x4E0B;&#x9762;&#x7684;note&#x3002;</p>
<ul>
<li><code>output_padding</code> &#x63A7;&#x5236;&#x4E86;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x8F93;&#x51FA;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x7684;&#x957F;&#x5EA6;&#x589E;&#x91CF;&#xFF0C;&#x4F46;&#x6CE8;&#x610F;&#x8FD9;&#x4E2A;&#x53C2;&#x6570;&#x4E0D;&#x662F;&#x8BF4;&#x8981;&#x5F80;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x7684;&#x8F93;&#x51FA;&#x4E0A;pad 0&#xFF0C;&#x800C;&#x662F;&#x76F4;&#x63A5;&#x63A7;&#x5236;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x4E3A;&#x6839;&#x636E;&#x6B64;&#x53C2;&#x6570;pad&#x540E;&#x7684;&#x5927;&#x5C0F;&#x3002;&#x66F4;&#x591A;&#x7684;&#x8BE6;&#x60C5;&#x8BF7;&#x89C1;&#x4E0B;&#x9762;&#x7684;note&#x3002;</li>
</ul>
</li>
<li><p><code>dilation</code> &#x63A7;&#x5236;&#x4E86;&#x5377;&#x79EF;&#x6838;&#x4E2D;&#x5404;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x7A7A;&#x95F4;&#x8DDD;&#x79BB;&#xFF1B;&#x8FD9;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x591A;&#x5B54;&#x7B97;&#x6CD5;(&#xE0; trous algorithm)&#x3002;&#x8FD9;&#x4E2A;&#x6982;&#x5FF5;&#x6709;&#x70B9;&#x96BE;&#x89E3;&#x91CA;&#xFF0C;&#x8FD9;&#x4E2A;&#x94FE;&#x63A5;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">link</a>&#x7528;&#x53EF;&#x89C6;&#x5316;&#x7684;&#x65B9;&#x6CD5;&#x5F88;&#x597D;&#x5730;&#x89E3;&#x91CA;&#x4E86;dilation&#x7684;&#x4F5C;&#x7528;&#x3002;</p>
</li>
<li><p><code>groups</code> &#x63A7;&#x5236;&#x4E86;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x4E4B;&#x95F4;&#x7684;&#x8FDE;&#x63A5;&#xFF08;connections&#xFF09;&#x7684;&#x6570;&#x91CF;&#x3002;<code>in_channels</code> &#x548C; <code>out_channels</code> &#x5FC5;&#x987B;&#x80FD;&#x88AB; <code>groups</code> &#x6574;&#x9664;&#x3002;&#x4E3E;&#x4E2A;&#x6817;&#x5B50;&#xFF0C;</p>
<p>&gt; *   &#x5F53; groups=1, &#x6B64;Conv1d&#x5C42;&#x4F1A;&#x4F7F;&#x7528;&#x4E00;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x8FDB;&#x884C;&#x6240;&#x6709;&#x8F93;&#x5165;&#x5230;&#x8F93;&#x51FA;&#x7684;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&gt; *   &#x5F53; groups=2, &#x6B64;&#x65F6;Conv1d&#x5C42;&#x4F1A;&#x4EA7;&#x751F;&#x4E24;&#x4E2A;&#x5E76;&#x5217;&#x7684;&#x5377;&#x79EF;&#x5C42;&#x3002;&#x540C;&#x65F6;&#xFF0C;&#x8F93;&#x5165;&#x901A;&#x9053;&#x88AB;&#x5206;&#x4E3A;&#x4E24;&#x534A;&#xFF0C;&#x4E24;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x5206;&#x522B;&#x5904;&#x7406;&#x4E00;&#x534A;&#x7684;&#x8F93;&#x5165;&#x901A;&#x9053;&#xFF0C;&#x540C;&#x65F6;&#x5404;&#x81EA;&#x4EA7;&#x751F;&#x4E00;&#x534A;&#x7684;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x3002;&#x6700;&#x540E;&#x8FD9;&#x4E24;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x4F1A;&#x88AB;concatenated&#x4E00;&#x8D77;&#xFF0C;&#x4F5C;&#x4E3A;&#x6B64;Conv1d&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x3002;</p>
<p>&gt; *   &#x5F53; groups= <code>in_channels</code>, &#x6BCF;&#x4E2A;&#x8F93;&#x5165;&#x901A;&#x9053;&#x90FD;&#x4F1A;&#x88AB;&#x5355;&#x72EC;&#x7684;&#x4E00;&#x7EC4;&#x5377;&#x79EF;&#x5C42;&#x5904;&#x7406;&#xFF0C;&#x8FD9;&#x4E2A;&#x7EC4;&#x7684;&#x5927;&#x5C0F;&#x662F;<img src="img/648a514da1dace3deacf3f078287e157.jpg" alt="">&#x3002;</p>
</li>
</ul>
<p>Note</p>
<p>&#x53D6;&#x51B3;&#x4E8E;&#x4F60;&#x5377;&#x79EF;&#x6838;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x6709;&#x4E9B;&#x65F6;&#x5019;&#x8F93;&#x5165;&#x6570;&#x636E;&#x4E2D;&#x67D0;&#x4E9B;&#x5217;&#xFF08;&#x6700;&#x540E;&#x51E0;&#x5217;&#xFF09;&#x53EF;&#x80FD;&#x4E0D;&#x4F1A;&#x53C2;&#x4E0E;&#x8BA1;&#x7B97;&#xFF08;&#x6BD4;&#x5982;&#x5217;&#x6570;&#x6574;&#x9664;&#x5377;&#x79EF;&#x6838;&#x5927;&#x5C0F;&#x6709;&#x4F59;&#x6570;&#xFF0C;&#x800C;&#x53C8;&#x6CA1;&#x6709;padding&#xFF0C;&#x90A3;&#x6700;&#x540E;&#x7684;&#x4F59;&#x6570;&#x5217;&#x4E00;&#x822C;&#x4E0D;&#x4F1A;&#x53C2;&#x4E0E;&#x5377;&#x79EF;&#x8BA1;&#x7B97;&#xFF09;&#xFF0C;&#x8FD9;&#x4E3B;&#x8981;&#x662F;&#x56E0;&#x4E3A;pytorch&#x4E2D;&#x7684;&#x4E92;&#x76F8;&#x5173;&#x64CD;&#x4F5C;<a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">cross-correlation</a>&#x662F;&#x4FDD;&#x8BC1;&#x8BA1;&#x7B97;&#x6B63;&#x786E;&#x7684;&#x64CD;&#x4F5C;(valid operation)&#xFF0C; &#x800C;&#x4E0D;&#x662F;&#x6EE1;&#x64CD;&#x4F5C;(full operation)&#x3002;&#x6240;&#x4EE5;&#x5B9E;&#x9645;&#x64CD;&#x4F5C;&#x4E2D;&#xFF0C;&#x8FD8;&#x662F;&#x8981;&#x4EB2;&#x5C3D;&#x91CF;&#x9009;&#x62E9;&#x597D;&#x5408;&#x9002;&#x7684;padding&#x53C2;&#x6570;&#x54E6;&#x3002;</p>
<p>Note</p>
<p><code>padding</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x8981;&#x5728;&#x8F93;&#x5165;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#xFF0C;&#x4E0E;&#x5728;Conv1d&#x4E2D;&#x4E0D;&#x540C;&#x7684;&#x662F;&#xFF0C;&#x5728;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x6B64;padding&#x53C2;&#x6570;&#x4E0E;&#x5B9E;&#x9645;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x7684;&#x5173;&#x7CFB;&#x4E3A;<code>&#x5C42;&#x6570; = kernel_size - 1 - padding</code>&#xFF0C; &#x8FD9;&#x6837;&#x8BBE;&#x7F6E;&#x7684;&#x4E3B;&#x8981;&#x539F;&#x56E0;&#x662F;&#x5F53;&#x4F7F;&#x7528;&#x76F8;&#x540C;&#x7684;&#x53C2;&#x6570;&#x6784;&#x5EFA;<a href="#torch.nn.Conv1d" title="torch.nn.Conv1d"><code>Conv1d</code></a> &#x548C;<a href="#torch.nn.ConvTranspose1d" title="torch.nn.ConvTranspose1d"><code>ConvTranspose1d</code></a>&#x6A21;&#x5757;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x8FD9;&#x79CD;&#x8BBE;&#x7F6E;&#x80FD;&#x591F;&#x5B9E;&#x73B0;&#x4E24;&#x4E2A;&#x6A21;&#x5757;&#x6709;&#x6B63;&#x597D;&#x76F8;&#x53CD;&#x7684;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x5373;Conv1d&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x662F;&#x5176;&#x5BF9;&#x5E94;&#x7684;ConvTranspose1d&#x6A21;&#x5757;&#x7684;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#xFF0C;&#x800C;ConvTranspose1d&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x53C8;&#x6070;&#x597D;&#x662F;&#x5176;&#x5BF9;&#x5E94;&#x7684;Conv1d&#x6A21;&#x5757;&#x7684;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x3002;&#x7136;&#x800C;&#xFF0C;&#x5F53;<code>stride &gt; 1</code>&#x7684;&#x65F6;&#x5019;&#xFF0C;<a href="#torch.nn.Conv1d" title="torch.nn.Conv1d"><code>Conv1d</code></a> &#x7684;&#x4E00;&#x4E2A;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x53EF;&#x80FD;&#x4F1A;&#x5BF9;&#x5E94;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#xFF0C;&#x4E0A;&#x4E00;&#x4E2A;note&#x4E2D;&#x5C31;&#x8BE6;&#x7EC6;&#x7684;&#x4ECB;&#x7ECD;&#x4E86;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#xFF0C;&#x8FD9;&#x6837;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#x8981;&#x4FDD;&#x6301;&#x524D;&#x9762;&#x63D0;&#x5230;&#x4E24;&#x79CD;&#x6A21;&#x5757;&#x7684;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x4FDD;&#x6301;&#x53CD;&#x5411;&#x4E00;&#x81F4;&#xFF0C;&#x90A3;&#x5C31;&#x8981;&#x7528;&#x5230; <code>output_padding</code>&#x53C2;&#x6570;&#x4E86;&#xFF0C;&#x8FD9;&#x4E2A;&#x53C2;&#x6570;&#x53EF;&#x4EE5;&#x589E;&#x52A0;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x8F93;&#x51FA;&#x7684;&#x67D0;&#x4E00;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x4EE5;&#x6B64;&#x6765;&#x8FBE;&#x5230;&#x524D;&#x9762;&#x63D0;&#x5230;&#x7684;&#x540C;&#x53C2;&#x6570;&#x6784;&#x5EFA;&#x7684;<a href="#torch.nn.Conv1d" title="torch.nn.Conv1d"><code>Conv1d</code></a> &#x548C;<a href="#torch.nn.ConvTranspose1d" title="torch.nn.ConvTranspose1d"><code>ConvTranspose1d</code></a>&#x6A21;&#x5757;&#x7684;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x65B9;&#x5411;&#x4E00;&#x81F4;&#x3002; &#x4F46;&#x6CE8;&#x610F;&#x8FD9;&#x4E2A;&#x53C2;&#x6570;&#x4E0D;&#x662F;&#x8BF4;&#x8981;&#x5F80;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x7684;&#x8F93;&#x51FA;&#x4E0A;pad 0&#xFF0C;&#x800C;&#x662F;&#x76F4;&#x63A5;&#x63A7;&#x5236;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x7684;&#x8F93;&#x51FA;&#x5404;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;&#x4E3A;&#x6839;&#x636E;&#x6B64;&#x53C2;&#x6570;pad&#x540E;&#x7684;&#x5927;&#x5C0F;&#x3002;</p>
<p>Note</p>
<p>&#x5F53;&#x7A0B;&#x5E8F;&#x7684;&#x8FD0;&#x884C;&#x73AF;&#x5883;&#x662F;&#x4F7F;&#x7528;&#x4E86;CuDNN&#x7684;CUDA&#x73AF;&#x5883;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4E00;&#x4E9B;&#x975E;&#x786E;&#x5B9A;&#x6027;&#x7684;&#x7B97;&#x6CD5;&#xFF08;nondeterministic algorithm&#xFF09;&#x53EF;&#x80FD;&#x4F1A;&#x88AB;&#x91C7;&#x7528;&#x4EE5;&#x63D0;&#x9AD8;&#x6574;&#x4E2A;&#x8BA1;&#x7B97;&#x7684;&#x6027;&#x80FD;&#x3002;&#x5982;&#x679C;&#x4E0D;&#x60F3;&#x4F7F;&#x7528;&#x8FD9;&#x4E9B;&#x975E;&#x786E;&#x5B9A;&#x6027;&#x7684;&#x7B97;&#x6CD5;&#xFF0C;&#x4F60;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x8BBE;&#x7F6E;<code>torch.backends.cudnn.deterministic = True</code>&#x6765;&#x8BA9;&#x6574;&#x4E2A;&#x8BA1;&#x7B97;&#x8FC7;&#x7A0B;&#x4FDD;&#x6301;&#x786E;&#x5B9A;&#x6027;&#xFF08;&#x53EF;&#x80FD;&#x4F1A;&#x635F;&#x5931;&#x4E00;&#x5B9A;&#x7684;&#x8BA1;&#x7B97;&#x6027;&#x80FD;&#xFF09;&#x3002;&#x5BF9;&#x4E8E;&#x540E;&#x7AEF;(background)&#xFF0C;&#x4F60;&#x53EF;&#x4EE5;&#x770B;&#x4E00;&#x4E0B;&#x8FD9;&#x4E00;&#x90E8;&#x5206;<a href="notes/randomness.html">Reproducibility</a>&#x4E86;&#x89E3;&#x5176;&#x76F8;&#x5173;&#x4FE1;&#x606F;&#x3002;</p>
<p>Parameters: </p>
<ul>
<li><strong>in_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; &#x8F93;&#x5165;&#x901A;&#x9053;&#x7684;&#x4E2A;&#x6570;</li>
<li><strong>out_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; &#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x7684;&#x4E2A;&#x6570;</li>
<li><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x5377;&#x79EF;&#x6838;&#x5927;&#x5C0F;</li>
<li><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x7684;&#x6B65;&#x957F;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; 1</li>
<li><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; <code>kernel_size - 1 - padding</code> &#x5C42; 0 &#x4F1A;&#x88AB;&#x8865;&#x9F50;&#x5230;&#x8F93;&#x5165;&#x6570;&#x636E;&#x7684;&#x5404;&#x8FB9;&#x4E0A;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; 0</li>
<li><strong>output_padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x8F93;&#x51FA;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x8981;&#x589E;&#x52A0;&#x7684;&#x5927;&#x5C0F;&#x3002;&#x9ED8;&#x8BA4;&#xFF1A;0 </li>
<li><strong>groups</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; &#x8F93;&#x5165;&#x901A;&#x9053;&#x4E0E;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x4E4B;&#x95F4;&#x76F8;&#x4E92;&#x9694;&#x79BB;&#x7684;&#x8FDE;&#x63A5;&#x7684;&#x4E2A;&#x6570;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A;1</li>
<li><strong>bias</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5982;&#x679C;&#x88AB;&#x7F6E;&#x4E3A; <code>True</code>&#xFF0C;&#x5411;&#x8F93;&#x51FA;&#x589E;&#x52A0;&#x4E00;&#x4E2A;&#x504F;&#x5DEE;&#x91CF;&#xFF0C;&#x6B64;&#x504F;&#x5DEE;&#x662F;&#x53EF;&#x5B66;&#x4E60;&#x53C2;&#x6570;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A;<code>True</code></li>
<li><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5377;&#x79EF;&#x6838;&#x5404;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; 1</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li><p>&#x8F93;&#x5165;: <img src="img/7db3e5e5d600c81e77756d5eee050505.jpg" alt=""></p>
</li>
<li><p>&#x8F93;&#x51FA;: <img src="img/3423094375906aa21d1b2e095e95c230.jpg" alt=""> &#x5176;&#x4E2D;&#xFF0C;</p>
<p><img src="img/c37a7e44707d3c08522f44ab4e4d6841.jpg" alt=""></p>
</li>
</ul>
<p>| Variables: | </p>
<ul>
<li><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013;  &#x6A21;&#x5757;&#x4E2D;&#x7684;&#x4E00;&#x4E2A;&#x5927;&#x5C0F;&#x4E3A; (in_channels, out_channels, kernel_size[0])&#x7684;&#x6743;&#x91CD;&#x5F20;&#x91CF;&#xFF0C;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x53EF;&#x8BAD;&#x7EC3;&#x5B66;&#x4E60;(learnable)&#x3002;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x521D;&#x59CB;&#x503C;&#x7684;&#x91C7;&#x6837;&#x7A7A;&#x95F4;&#x662F;<img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt="">&#xFF0C;&#x5176;&#x4E2D; <img src="img/69aab1ce658aabc9a2d986ae8281e2ad.jpg" alt="">&#x3002;</li>
<li><strong>bias</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; &#x6A21;&#x5757;&#x7684;&#x504F;&#x5DEE;&#x9879;&#xFF0C;&#x5927;&#x5C0F;&#x4E3A; (out_channels)&#xFF0C; &#x5982;&#x679C;&#x6784;&#x9020;&#x51FD;&#x6570;&#x4E2D;&#x7684; <code>bias</code> &#x88AB;&#x7F6E;&#x4E3A; <code>True</code>&#xFF0C;&#x90A3;&#x4E48;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x521D;&#x59CB;&#x503C;&#x7684;&#x91C7;&#x6837;&#x7A7A;&#x95F4;&#x662F; <img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt=""> &#xFF0C;&#x5176;&#x4E2D; <img src="img/69aab1ce658aabc9a2d986ae8281e2ad.jpg" alt="">&#x3002;</li>
</ul>
<h3 id="convtranspose2d">ConvTranspose2d</h3>
<pre><code class="lang-py">class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)
</code></pre>
<p>&#x5229;&#x7528;&#x6307;&#x5B9A;&#x5927;&#x5C0F;&#x7684;&#x4E8C;&#x7EF4;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x6838;&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4E8C;&#x7EF4;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x8FDB;&#x884C;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#xFF08;&#x5F53;&#x7136;&#x6B64;&#x5377;&#x79EF;&#x4E5F;&#x662F;&#x4E92;&#x76F8;&#x5173;&#x64CD;&#x4F5C;&#xFF0C;cross-correlation&#xFF09;&#x64CD;&#x4F5C;&#x7684;&#x6A21;&#x5757;&#x3002;</p>
<p>&#x8BE5;&#x6A21;&#x5757;&#x53EF;&#x4EE5;&#x770B;&#x4F5C;&#x662F;Conv2d&#x76F8;&#x5BF9;&#x4E8E;&#x5176;&#x8F93;&#x5165;&#x7684;&#x68AF;&#x5EA6;(the gradient of Conv2d with respect to its input&#xFF0C; &#x76F4;&#x8BD1;)&#xFF0C; &#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x53C8;&#x88AB;&#x79F0;&#x4E3A;&#x5C0F;&#x6570;&#x6B65;&#x957F;&#x5377;&#x79EF;&#x6216;&#x662F;&#x53CD;&#x5377;&#x79EF;&#xFF08;&#x5C3D;&#x7BA1;&#x8FD9;&#x4E0D;&#x662F;&#x4E00;&#x4E2A;&#x771F;&#x6B63;&#x610F;&#x4E49;&#x4E0A;&#x7684;&#x53CD;&#x5377;&#x79EF;&#xFF09;&#x3002;</p>
<ul>
<li><p><code>stride</code> &#x63A7;&#x5236;&#x4E86;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x7684;&#x6B65;&#x957F; </p>
</li>
<li><p><code>padding</code> &#x63A7;&#x5236;&#x4E86;&#x8981;&#x5728;&#x8F93;&#x5165;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x7684;&#x5404;&#x8FB9;&#x4E0A;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#xFF0C;&#x4E0E;Conv1d&#x4E0D;&#x540C;&#x7684;&#x5730;&#x65B9;&#xFF0C;&#x6B64;padding&#x53C2;&#x6570;&#x4E0E;&#x5B9E;&#x9645;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x7684;&#x5173;&#x7CFB;&#x4E3A;<code>&#x5C42;&#x6570; = kernel_size - 1 - padding</code>&#xFF0C;&#x8BE6;&#x60C5;&#x8BF7;&#x89C1;&#x4E0B;&#x9762;&#x7684;note&#x3002;</p>
<ul>
<li><code>output_padding</code> &#x63A7;&#x5236;&#x4E86;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x8F93;&#x51FA;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x7684;&#x957F;&#x5EA6;&#x589E;&#x91CF;&#xFF0C;&#x4F46;&#x6CE8;&#x610F;&#x8FD9;&#x4E2A;&#x53C2;&#x6570;&#x4E0D;&#x662F;&#x8BF4;&#x8981;&#x5F80;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x7684;&#x8F93;&#x51FA;&#x4E0A;pad 0&#xFF0C;&#x800C;&#x662F;&#x76F4;&#x63A5;&#x63A7;&#x5236;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x4E3A;&#x6839;&#x636E;&#x6B64;&#x53C2;&#x6570;pad&#x540E;&#x7684;&#x5927;&#x5C0F;&#x3002;&#x66F4;&#x591A;&#x7684;&#x8BE6;&#x60C5;&#x8BF7;&#x89C1;&#x4E0B;&#x9762;&#x7684;note&#x3002;</li>
</ul>
</li>
<li><p><code>dilation</code> &#x63A7;&#x5236;&#x4E86;&#x5377;&#x79EF;&#x6838;&#x4E2D;&#x5404;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x7A7A;&#x95F4;&#x8DDD;&#x79BB;&#xFF1B;&#x8FD9;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x591A;&#x5B54;&#x7B97;&#x6CD5;(&#xE0; trous algorithm)&#x3002;&#x8FD9;&#x4E2A;&#x6982;&#x5FF5;&#x6709;&#x70B9;&#x96BE;&#x89E3;&#x91CA;&#xFF0C;&#x8FD9;&#x4E2A;&#x94FE;&#x63A5;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">link</a>&#x7528;&#x53EF;&#x89C6;&#x5316;&#x7684;&#x65B9;&#x6CD5;&#x5F88;&#x597D;&#x5730;&#x89E3;&#x91CA;&#x4E86;dilation&#x7684;&#x4F5C;&#x7528;&#x3002;</p>
</li>
<li><p><code>groups</code> &#x63A7;&#x5236;&#x4E86;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x4E4B;&#x95F4;&#x7684;&#x8FDE;&#x63A5;&#xFF08;connections&#xFF09;&#x7684;&#x6570;&#x91CF;&#x3002;<code>in_channels</code> &#x548C; <code>out_channels</code> &#x5FC5;&#x987B;&#x80FD;&#x88AB; <code>groups</code> &#x6574;&#x9664;&#x3002;&#x4E3E;&#x4E2A;&#x6817;&#x5B50;&#xFF0C;</p>
<p>&gt; *   &#x5F53; groups=1, &#x6B64;Conv1d&#x5C42;&#x4F1A;&#x4F7F;&#x7528;&#x4E00;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x8FDB;&#x884C;&#x6240;&#x6709;&#x8F93;&#x5165;&#x5230;&#x8F93;&#x51FA;&#x7684;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&gt; *   &#x5F53; groups=2, &#x6B64;&#x65F6;Conv1d&#x5C42;&#x4F1A;&#x4EA7;&#x751F;&#x4E24;&#x4E2A;&#x5E76;&#x5217;&#x7684;&#x5377;&#x79EF;&#x5C42;&#x3002;&#x540C;&#x65F6;&#xFF0C;&#x8F93;&#x5165;&#x901A;&#x9053;&#x88AB;&#x5206;&#x4E3A;&#x4E24;&#x534A;&#xFF0C;&#x4E24;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x5206;&#x522B;&#x5904;&#x7406;&#x4E00;&#x534A;&#x7684;&#x8F93;&#x5165;&#x901A;&#x9053;&#xFF0C;&#x540C;&#x65F6;&#x5404;&#x81EA;&#x4EA7;&#x751F;&#x4E00;&#x534A;&#x7684;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x3002;&#x6700;&#x540E;&#x8FD9;&#x4E24;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x4F1A;&#x88AB;concatenated&#x4E00;&#x8D77;&#xFF0C;&#x4F5C;&#x4E3A;&#x6B64;Conv1d&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x3002;</p>
<p>&gt; *   &#x5F53; groups= <code>in_channels</code>, &#x6BCF;&#x4E2A;&#x8F93;&#x5165;&#x901A;&#x9053;&#x90FD;&#x4F1A;&#x88AB;&#x5355;&#x72EC;&#x7684;&#x4E00;&#x7EC4;&#x5377;&#x79EF;&#x5C42;&#x5904;&#x7406;&#xFF0C;&#x8FD9;&#x4E2A;&#x7EC4;&#x7684;&#x5927;&#x5C0F;&#x662F;<img src="img/648a514da1dace3deacf3f078287e157.jpg" alt="">&#x3002;</p>
</li>
</ul>
<p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>output_padding</code> &#x8FD9;&#x51E0;&#x4E2A;&#x53C2;&#x6570;&#x5747;&#x652F;&#x6301;&#x4E00;&#x4E0B;&#x8F93;&#x5165;&#x5F62;&#x5F0F;&#xFF1A;</p>
<blockquote>
<ul>
<li>&#x4E00;&#x4E2A; <code>int</code> &#x6570;&#x5B57; &#x2013; &#x4E8C;&#x7EF4;&#x7EF4;&#x6570;&#x636E;&#x7684;&#x9AD8;&#x548C;&#x5BBD;&#x8FD9;&#x4E24;&#x4E2A;&#x7EF4;&#x5EA6;&#x90FD;&#x4F1A;&#x91C7;&#x7528;&#x8FD9;&#x4E00;&#x4E2A;&#x6570;&#x5B57;&#x3002;</li>
<li>&#x4E00;&#x4E2A;&#x7531;&#x4E24;&#x4E2A;int&#x6570;&#x5B57;&#x7EC4;&#x6210;&#x7684;<code>tuple</code>&#x2013; &#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x4E8C;&#x7EF4;&#x6570;&#x636E;&#x7684;&#x9AD8;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x5143;&#x7EC4;&#x4E2D;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#xFF0C;&#x5BBD;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x7B2C;&#x4E8C;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#x3002;</li>
</ul>
</blockquote>
<p>Note</p>
<p>&#x53D6;&#x51B3;&#x4E8E;&#x4F60;&#x5377;&#x79EF;&#x6838;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x6709;&#x4E9B;&#x65F6;&#x5019;&#x8F93;&#x5165;&#x6570;&#x636E;&#x4E2D;&#x67D0;&#x4E9B;&#x5217;&#xFF08;&#x6700;&#x540E;&#x51E0;&#x5217;&#xFF09;&#x53EF;&#x80FD;&#x4E0D;&#x4F1A;&#x53C2;&#x4E0E;&#x8BA1;&#x7B97;&#xFF08;&#x6BD4;&#x5982;&#x5217;&#x6570;&#x6574;&#x9664;&#x5377;&#x79EF;&#x6838;&#x5927;&#x5C0F;&#x6709;&#x4F59;&#x6570;&#xFF0C;&#x800C;&#x53C8;&#x6CA1;&#x6709;padding&#xFF0C;&#x90A3;&#x6700;&#x540E;&#x7684;&#x4F59;&#x6570;&#x5217;&#x4E00;&#x822C;&#x4E0D;&#x4F1A;&#x53C2;&#x4E0E;&#x5377;&#x79EF;&#x8BA1;&#x7B97;&#xFF09;&#xFF0C;&#x8FD9;&#x4E3B;&#x8981;&#x662F;&#x56E0;&#x4E3A;pytorch&#x4E2D;&#x7684;&#x4E92;&#x76F8;&#x5173;&#x64CD;&#x4F5C;<a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">cross-correlation</a>&#x662F;&#x4FDD;&#x8BC1;&#x8BA1;&#x7B97;&#x6B63;&#x786E;&#x7684;&#x64CD;&#x4F5C;(valid operation)&#xFF0C; &#x800C;&#x4E0D;&#x662F;&#x6EE1;&#x64CD;&#x4F5C;(full operation)&#x3002;&#x6240;&#x4EE5;&#x5B9E;&#x9645;&#x64CD;&#x4F5C;&#x4E2D;&#xFF0C;&#x8FD8;&#x662F;&#x8981;&#x4EB2;&#x5C3D;&#x91CF;&#x9009;&#x62E9;&#x597D;&#x5408;&#x9002;&#x7684;padding&#x53C2;&#x6570;&#x54E6;&#x3002;</p>
<p>Note</p>
<p><code>padding</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x8981;&#x5728;&#x8F93;&#x5165;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#xFF0C;&#x4E0E;&#x5728;Conv1d&#x4E2D;&#x4E0D;&#x540C;&#x7684;&#x662F;&#xFF0C;&#x5728;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x6B64;padding&#x53C2;&#x6570;&#x4E0E;&#x5B9E;&#x9645;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x7684;&#x5173;&#x7CFB;&#x4E3A;<code>&#x5C42;&#x6570; = kernel_size - 1 - padding</code>&#xFF0C; &#x8FD9;&#x6837;&#x8BBE;&#x7F6E;&#x7684;&#x4E3B;&#x8981;&#x539F;&#x56E0;&#x662F;&#x5F53;&#x4F7F;&#x7528;&#x76F8;&#x540C;&#x7684;&#x53C2;&#x6570;&#x6784;&#x5EFA;<a href="#torch.nn.Conv2d" title="torch.nn.Conv2d"><code>Conv2d</code></a> &#x548C;<a href="#torch.nn.ConvTranspose2d" title="torch.nn.ConvTranspose2d"><code>ConvTranspose2d</code></a>&#x6A21;&#x5757;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x8FD9;&#x79CD;&#x8BBE;&#x7F6E;&#x80FD;&#x591F;&#x5B9E;&#x73B0;&#x4E24;&#x4E2A;&#x6A21;&#x5757;&#x6709;&#x6B63;&#x597D;&#x76F8;&#x53CD;&#x7684;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x5373;Conv2d&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x662F;&#x5176;&#x5BF9;&#x5E94;&#x7684;ConvTranspose2d&#x6A21;&#x5757;&#x7684;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#xFF0C;&#x800C;ConvTranspose2d&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x53C8;&#x6070;&#x597D;&#x662F;&#x5176;&#x5BF9;&#x5E94;&#x7684;Conv2d&#x6A21;&#x5757;&#x7684;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x3002;&#x7136;&#x800C;&#xFF0C;&#x5F53;<code>stride &gt; 1</code>&#x7684;&#x65F6;&#x5019;&#xFF0C;<a href="#torch.nn.Conv2d" title="torch.nn.Conv2d"><code>Conv2d</code></a> &#x7684;&#x4E00;&#x4E2A;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x53EF;&#x80FD;&#x4F1A;&#x5BF9;&#x5E94;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#xFF0C;&#x4E0A;&#x4E00;&#x4E2A;note&#x4E2D;&#x5C31;&#x8BE6;&#x7EC6;&#x7684;&#x4ECB;&#x7ECD;&#x4E86;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#xFF0C;&#x8FD9;&#x6837;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#x8981;&#x4FDD;&#x6301;&#x524D;&#x9762;&#x63D0;&#x5230;&#x4E24;&#x79CD;&#x6A21;&#x5757;&#x7684;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x4FDD;&#x6301;&#x53CD;&#x5411;&#x4E00;&#x81F4;&#xFF0C;&#x90A3;&#x5C31;&#x8981;&#x7528;&#x5230; <code>output_padding</code>&#x53C2;&#x6570;&#x4E86;&#xFF0C;&#x8FD9;&#x4E2A;&#x53C2;&#x6570;&#x53EF;&#x4EE5;&#x589E;&#x52A0;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x8F93;&#x51FA;&#x7684;&#x67D0;&#x4E00;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x4EE5;&#x6B64;&#x6765;&#x8FBE;&#x5230;&#x524D;&#x9762;&#x63D0;&#x5230;&#x7684;&#x540C;&#x53C2;&#x6570;&#x6784;&#x5EFA;&#x7684;<a href="#torch.nn.Conv2d" title="torch.nn.Conv2d"><code>Conv2d</code></a> &#x548C;<a href="#torch.nn.ConvTranspose2d" title="torch.nn.ConvTranspose2d"><code>ConvTranspose2d</code></a>&#x6A21;&#x5757;&#x7684;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x65B9;&#x5411;&#x4E00;&#x81F4;&#x3002; &#x4F46;&#x6CE8;&#x610F;&#x8FD9;&#x4E2A;&#x53C2;&#x6570;&#x4E0D;&#x662F;&#x8BF4;&#x8981;&#x5F80;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x7684;&#x8F93;&#x51FA;&#x4E0A;pad 0&#xFF0C;&#x800C;&#x662F;&#x76F4;&#x63A5;&#x63A7;&#x5236;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x7684;&#x8F93;&#x51FA;&#x5404;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;&#x4E3A;&#x6839;&#x636E;&#x6B64;&#x53C2;&#x6570;pad&#x540E;&#x7684;&#x5927;&#x5C0F;&#x3002;</p>
<p>Note</p>
<p>&#x5F53;&#x7A0B;&#x5E8F;&#x7684;&#x8FD0;&#x884C;&#x73AF;&#x5883;&#x662F;&#x4F7F;&#x7528;&#x4E86;CuDNN&#x7684;CUDA&#x73AF;&#x5883;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4E00;&#x4E9B;&#x975E;&#x786E;&#x5B9A;&#x6027;&#x7684;&#x7B97;&#x6CD5;&#xFF08;nondeterministic algorithm&#xFF09;&#x53EF;&#x80FD;&#x4F1A;&#x88AB;&#x91C7;&#x7528;&#x4EE5;&#x63D0;&#x9AD8;&#x6574;&#x4E2A;&#x8BA1;&#x7B97;&#x7684;&#x6027;&#x80FD;&#x3002;&#x5982;&#x679C;&#x4E0D;&#x60F3;&#x4F7F;&#x7528;&#x8FD9;&#x4E9B;&#x975E;&#x786E;&#x5B9A;&#x6027;&#x7684;&#x7B97;&#x6CD5;&#xFF0C;&#x4F60;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x8BBE;&#x7F6E;<code>torch.backends.cudnn.deterministic = True</code>&#x6765;&#x8BA9;&#x6574;&#x4E2A;&#x8BA1;&#x7B97;&#x8FC7;&#x7A0B;&#x4FDD;&#x6301;&#x786E;&#x5B9A;&#x6027;&#xFF08;&#x53EF;&#x80FD;&#x4F1A;&#x635F;&#x5931;&#x4E00;&#x5B9A;&#x7684;&#x8BA1;&#x7B97;&#x6027;&#x80FD;&#xFF09;&#x3002;&#x5BF9;&#x4E8E;&#x540E;&#x7AEF;(background)&#xFF0C;&#x4F60;&#x53EF;&#x4EE5;&#x770B;&#x4E00;&#x4E0B;&#x8FD9;&#x4E00;&#x90E8;&#x5206;<a href="notes/randomness.html">Reproducibility</a>&#x4E86;&#x89E3;&#x5176;&#x76F8;&#x5173;&#x4FE1;&#x606F;&#x3002;</p>
<p>Parameters:</p>
<ul>
<li><strong>in_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; &#x8F93;&#x5165;&#x901A;&#x9053;&#x7684;&#x4E2A;&#x6570;</li>
<li><strong>out_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; &#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x7684;&#x4E2A;&#x6570;</li>
<li><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x5377;&#x79EF;&#x6838;&#x5927;&#x5C0F;</li>
<li><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x7684;&#x6B65;&#x957F;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; 1</li>
<li><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; <code>kernel_size - 1 - padding</code> &#x5C42; 0 &#x4F1A;&#x88AB;&#x8865;&#x9F50;&#x5230;&#x8F93;&#x5165;&#x6570;&#x636E;&#x7684;&#x5404;&#x8FB9;&#x4E0A;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; 0</li>
<li><strong>output_padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x8F93;&#x51FA;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x8981;&#x589E;&#x52A0;&#x7684;&#x5927;&#x5C0F;&#x3002;&#x9ED8;&#x8BA4;&#xFF1A;0 </li>
<li><strong>groups</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; &#x8F93;&#x5165;&#x901A;&#x9053;&#x4E0E;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x4E4B;&#x95F4;&#x76F8;&#x4E92;&#x9694;&#x79BB;&#x7684;&#x8FDE;&#x63A5;&#x7684;&#x4E2A;&#x6570;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A;1</li>
<li><strong>bias</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5982;&#x679C;&#x88AB;&#x7F6E;&#x4E3A; <code>True</code>&#xFF0C;&#x5411;&#x8F93;&#x51FA;&#x589E;&#x52A0;&#x4E00;&#x4E2A;&#x504F;&#x5DEE;&#x91CF;&#xFF0C;&#x6B64;&#x504F;&#x5DEE;&#x662F;&#x53EF;&#x5B66;&#x4E60;&#x53C2;&#x6570;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A;<code>True</code></li>
<li><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5377;&#x79EF;&#x6838;&#x5404;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; 1</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/0385ad868fed790d36381b9e8788c18b.jpg" alt=""></li>
<li>&#x8F93;&#x51FA;: <img src="img/d3edfe8a9bbdd73ba5c4b566353777f0.jpg" alt=""> &#x5176;&#x4E2D;</li>
</ul>
<p><img src="img/a2616e3fb8e8e919b799c2e62921c374.jpg" alt=""></p>
<p><img src="img/dee6540c49e827b0ececaf0154154b54.jpg" alt=""></p>
<p>| Variables: | </p>
<ul>
<li><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013;  &#x6A21;&#x5757;&#x4E2D;&#x7684;&#x4E00;&#x4E2A;&#x5927;&#x5C0F;&#x4E3A; (in_channels, out_channels, kernel_size[0], kernel_size[1])&#x7684;&#x6743;&#x91CD;&#x5F20;&#x91CF;&#xFF0C;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x53EF;&#x8BAD;&#x7EC3;&#x5B66;&#x4E60;(learnable)&#x3002;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x521D;&#x59CB;&#x503C;&#x7684;&#x91C7;&#x6837;&#x7A7A;&#x95F4;&#x662F;<img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt="">&#xFF0C;&#x5176;&#x4E2D; <img src="img/c12e2153347b696ebb784e5675cc566e.jpg" alt="">&#x3002;</li>
<li><strong>bias</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; &#x6A21;&#x5757;&#x7684;&#x504F;&#x5DEE;&#x9879;&#xFF0C;&#x5927;&#x5C0F;&#x4E3A; (out_channels)&#xFF0C; &#x5982;&#x679C;&#x6784;&#x9020;&#x51FD;&#x6570;&#x4E2D;&#x7684; <code>bias</code> &#x88AB;&#x7F6E;&#x4E3A; <code>True</code>&#xFF0C;&#x90A3;&#x4E48;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x521D;&#x59CB;&#x503C;&#x7684;&#x91C7;&#x6837;&#x7A7A;&#x95F4;&#x662F; <img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt=""> &#xFF0C;&#x5176;&#x4E2D; <img src="img/c12e2153347b696ebb784e5675cc566e.jpg" alt="">&#x3002;</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With square kernels and equal stride</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ConvTranspose2d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># non-square kernels and unequal stride and with padding</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ConvTranspose2d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">5</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">4</span>, <span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>, <span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># exact output size can be also specified as an argument</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>downsample = nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>upsample = nn.ConvTranspose2d(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>h = downsample(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>h.size()
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>output = upsample(h, output_size=input.size())
<span class="hljs-meta">&gt;&gt;&gt; </span>output.size()
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])
</code></pre>
<h3 id="convtranspose3d">ConvTranspose3d</h3>
<pre><code class="lang-py">class torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)
</code></pre>
<p>&#x5229;&#x7528;&#x6307;&#x5B9A;&#x5927;&#x5C0F;&#x7684;&#x4E09;&#x7EF4;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x6838;&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4E09;&#x7EF4;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x8FDB;&#x884C;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#xFF08;&#x5F53;&#x7136;&#x6B64;&#x5377;&#x79EF;&#x4E5F;&#x662F;&#x4E92;&#x76F8;&#x5173;&#x64CD;&#x4F5C;&#xFF0C;cross-correlation&#xFF09;&#x64CD;&#x4F5C;&#x7684;&#x6A21;&#x5757;&#x3002;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x7684;&#x64CD;&#x4F5C;&#x672C;&#x8D28;&#x662F;&#x5C06;&#x5404;&#x901A;&#x9053;&#x8F93;&#x5165;&#x4E0E;&#x5377;&#x79EF;&#x6838;&#x505A;&#x4E58;&#x6CD5;&#xFF0C;&#x7136;&#x540E;&#x8FD4;&#x56DE;&#x5404;&#x901A;&#x9053;&#x4E0E;&#x6B64;&#x5377;&#x79EF;&#x6838;&#x4E58;&#x79EF;&#x7ED3;&#x679C;&#x4E4B;&#x548C;&#xFF08;&#x5377;&#x79EF;&#x7684;&#x5B9A;&#x4E49;&#xFF09;&#x3002;</p>
<p>&#x8BE5;&#x6A21;&#x5757;&#x53EF;&#x4EE5;&#x770B;&#x4F5C;&#x662F;Conv3d&#x76F8;&#x5BF9;&#x4E8E;&#x5176;&#x8F93;&#x5165;&#x7684;&#x68AF;&#x5EA6;(the gradient of Conv3d with respect to its input&#xFF0C; &#x76F4;&#x8BD1;)&#xFF0C; &#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x53C8;&#x88AB;&#x79F0;&#x4E3A;&#x5C0F;&#x6570;&#x6B65;&#x957F;&#x5377;&#x79EF;&#x6216;&#x662F;&#x53CD;&#x5377;&#x79EF;&#xFF08;&#x5C3D;&#x7BA1;&#x8FD9;&#x4E0D;&#x662F;&#x4E00;&#x4E2A;&#x771F;&#x6B63;&#x610F;&#x4E49;&#x4E0A;&#x7684;&#x53CD;&#x5377;&#x79EF;&#xFF09;&#x3002;</p>
<ul>
<li><p><code>stride</code> &#x63A7;&#x5236;&#x4E86;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x7684;&#x6B65;&#x957F; </p>
</li>
<li><p><code>padding</code> &#x63A7;&#x5236;&#x4E86;&#x8981;&#x5728;&#x8F93;&#x5165;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x7684;&#x5404;&#x8FB9;&#x4E0A;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#xFF0C;&#x4E0E;Conv1d&#x4E0D;&#x540C;&#x7684;&#x5730;&#x65B9;&#xFF0C;&#x6B64;padding&#x53C2;&#x6570;&#x4E0E;&#x5B9E;&#x9645;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x7684;&#x5173;&#x7CFB;&#x4E3A;<code>&#x5C42;&#x6570; = kernel_size - 1 - padding</code>&#xFF0C;&#x8BE6;&#x60C5;&#x8BF7;&#x89C1;&#x4E0B;&#x9762;&#x7684;note&#x3002;</p>
<ul>
<li><code>output_padding</code> &#x63A7;&#x5236;&#x4E86;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x8F93;&#x51FA;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x7684;&#x957F;&#x5EA6;&#x589E;&#x91CF;&#xFF0C;&#x4F46;&#x6CE8;&#x610F;&#x8FD9;&#x4E2A;&#x53C2;&#x6570;&#x4E0D;&#x662F;&#x8BF4;&#x8981;&#x5F80;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x7684;&#x8F93;&#x51FA;&#x4E0A;pad 0&#xFF0C;&#x800C;&#x662F;&#x76F4;&#x63A5;&#x63A7;&#x5236;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x4E3A;&#x6839;&#x636E;&#x6B64;&#x53C2;&#x6570;pad&#x540E;&#x7684;&#x5927;&#x5C0F;&#x3002;&#x66F4;&#x591A;&#x7684;&#x8BE6;&#x60C5;&#x8BF7;&#x89C1;&#x4E0B;&#x9762;&#x7684;note&#x3002;</li>
</ul>
</li>
<li><p><code>dilation</code> &#x63A7;&#x5236;&#x4E86;&#x5377;&#x79EF;&#x6838;&#x4E2D;&#x5404;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x7A7A;&#x95F4;&#x8DDD;&#x79BB;&#xFF1B;&#x8FD9;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x591A;&#x5B54;&#x7B97;&#x6CD5;(&#xE0; trous algorithm)&#x3002;&#x8FD9;&#x4E2A;&#x6982;&#x5FF5;&#x6709;&#x70B9;&#x96BE;&#x89E3;&#x91CA;&#xFF0C;&#x8FD9;&#x4E2A;&#x94FE;&#x63A5;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">link</a>&#x7528;&#x53EF;&#x89C6;&#x5316;&#x7684;&#x65B9;&#x6CD5;&#x5F88;&#x597D;&#x5730;&#x89E3;&#x91CA;&#x4E86;dilation&#x7684;&#x4F5C;&#x7528;&#x3002;</p>
</li>
<li><p><code>groups</code> &#x63A7;&#x5236;&#x4E86;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x4E4B;&#x95F4;&#x7684;&#x8FDE;&#x63A5;&#xFF08;connections&#xFF09;&#x7684;&#x6570;&#x91CF;&#x3002;<code>in_channels</code> &#x548C; <code>out_channels</code> &#x5FC5;&#x987B;&#x80FD;&#x88AB; <code>groups</code> &#x6574;&#x9664;&#x3002;&#x4E3E;&#x4E2A;&#x6817;&#x5B50;&#xFF0C;</p>
<p>&gt; *   &#x5F53; groups=1, &#x6B64;Conv1d&#x5C42;&#x4F1A;&#x4F7F;&#x7528;&#x4E00;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x8FDB;&#x884C;&#x6240;&#x6709;&#x8F93;&#x5165;&#x5230;&#x8F93;&#x51FA;&#x7684;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&gt; *   &#x5F53; groups=2, &#x6B64;&#x65F6;Conv1d&#x5C42;&#x4F1A;&#x4EA7;&#x751F;&#x4E24;&#x4E2A;&#x5E76;&#x5217;&#x7684;&#x5377;&#x79EF;&#x5C42;&#x3002;&#x540C;&#x65F6;&#xFF0C;&#x8F93;&#x5165;&#x901A;&#x9053;&#x88AB;&#x5206;&#x4E3A;&#x4E24;&#x534A;&#xFF0C;&#x4E24;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x5206;&#x522B;&#x5904;&#x7406;&#x4E00;&#x534A;&#x7684;&#x8F93;&#x5165;&#x901A;&#x9053;&#xFF0C;&#x540C;&#x65F6;&#x5404;&#x81EA;&#x4EA7;&#x751F;&#x4E00;&#x534A;&#x7684;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x3002;&#x6700;&#x540E;&#x8FD9;&#x4E24;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x4F1A;&#x88AB;concatenated&#x4E00;&#x8D77;&#xFF0C;&#x4F5C;&#x4E3A;&#x6B64;Conv1d&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x3002;</p>
<p>&gt; *   &#x5F53; groups= <code>in_channels</code>, &#x6BCF;&#x4E2A;&#x8F93;&#x5165;&#x901A;&#x9053;&#x90FD;&#x4F1A;&#x88AB;&#x5355;&#x72EC;&#x7684;&#x4E00;&#x7EC4;&#x5377;&#x79EF;&#x5C42;&#x5904;&#x7406;&#xFF0C;&#x8FD9;&#x4E2A;&#x7EC4;&#x7684;&#x5927;&#x5C0F;&#x662F;<img src="img/648a514da1dace3deacf3f078287e157.jpg" alt="">&#x3002;</p>
</li>
</ul>
<p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>output_padding</code> &#x8FD9;&#x51E0;&#x4E2A;&#x53C2;&#x6570;&#x5747;&#x652F;&#x6301;&#x4E00;&#x4E0B;&#x8F93;&#x5165;&#x5F62;&#x5F0F;&#xFF1A;</p>
<blockquote>
<ul>
<li>&#x4E00;&#x4E2A; <code>int</code> &#x6570;&#x5B57; &#x2013; &#x4E09;&#x7EF4;&#x7EF4;&#x6570;&#x636E;&#x7684;&#x6DF1;&#x5EA6;&#xFF0C;&#x9AD8;&#x548C;&#x5BBD;&#x8FD9;&#x4E24;&#x4E2A;&#x7EF4;&#x5EA6;&#x90FD;&#x4F1A;&#x91C7;&#x7528;&#x8FD9;&#x4E00;&#x4E2A;&#x6570;&#x5B57;&#x3002;</li>
<li>&#x4E00;&#x4E2A;&#x7531;&#x4E09;&#x4E2A;int&#x6570;&#x5B57;&#x7EC4;&#x6210;&#x7684;<code>tuple</code>&#x2013; &#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x4E09;&#x7EF4;&#x6570;&#x636E;&#x7684;&#x6DF1;&#x5EA6;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x5143;&#x7EC4;&#x4E2D;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#xFF0C;&#x9AD8;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x5143;&#x7EC4;&#x4E2D;&#x7684;&#x7B2C;&#x4E8C;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#xFF0C;&#x5BBD;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x7B2C;&#x4E09;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#x3002;</li>
</ul>
</blockquote>
<p>Note</p>
<p>&#x53D6;&#x51B3;&#x4E8E;&#x4F60;&#x5377;&#x79EF;&#x6838;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x6709;&#x4E9B;&#x65F6;&#x5019;&#x8F93;&#x5165;&#x6570;&#x636E;&#x4E2D;&#x67D0;&#x4E9B;&#x5217;&#xFF08;&#x6700;&#x540E;&#x51E0;&#x5217;&#xFF09;&#x53EF;&#x80FD;&#x4E0D;&#x4F1A;&#x53C2;&#x4E0E;&#x8BA1;&#x7B97;&#xFF08;&#x6BD4;&#x5982;&#x5217;&#x6570;&#x6574;&#x9664;&#x5377;&#x79EF;&#x6838;&#x5927;&#x5C0F;&#x6709;&#x4F59;&#x6570;&#xFF0C;&#x800C;&#x53C8;&#x6CA1;&#x6709;padding&#xFF0C;&#x90A3;&#x6700;&#x540E;&#x7684;&#x4F59;&#x6570;&#x5217;&#x4E00;&#x822C;&#x4E0D;&#x4F1A;&#x53C2;&#x4E0E;&#x5377;&#x79EF;&#x8BA1;&#x7B97;&#xFF09;&#xFF0C;&#x8FD9;&#x4E3B;&#x8981;&#x662F;&#x56E0;&#x4E3A;pytorch&#x4E2D;&#x7684;&#x4E92;&#x76F8;&#x5173;&#x64CD;&#x4F5C;<a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank">cross-correlation</a>&#x662F;&#x4FDD;&#x8BC1;&#x8BA1;&#x7B97;&#x6B63;&#x786E;&#x7684;&#x64CD;&#x4F5C;(valid operation)&#xFF0C; &#x800C;&#x4E0D;&#x662F;&#x6EE1;&#x64CD;&#x4F5C;(full operation)&#x3002;&#x6240;&#x4EE5;&#x5B9E;&#x9645;&#x64CD;&#x4F5C;&#x4E2D;&#xFF0C;&#x8FD8;&#x662F;&#x8981;&#x4EB2;&#x5C3D;&#x91CF;&#x9009;&#x62E9;&#x597D;&#x5408;&#x9002;&#x7684;padding&#x53C2;&#x6570;&#x54E6;&#x3002;</p>
<p>Note</p>
<p><code>padding</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x8981;&#x5728;&#x8F93;&#x5165;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#xFF0C;&#x4E0E;&#x5728;Conv3d&#x4E2D;&#x4E0D;&#x540C;&#x7684;&#x662F;&#xFF0C;&#x5728;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x6B64;padding&#x53C2;&#x6570;&#x4E0E;&#x5B9E;&#x9645;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x7684;&#x5173;&#x7CFB;&#x4E3A;<code>&#x5C42;&#x6570; = kernel_size - 1 - padding</code>&#xFF0C; &#x8FD9;&#x6837;&#x8BBE;&#x7F6E;&#x7684;&#x4E3B;&#x8981;&#x539F;&#x56E0;&#x662F;&#x5F53;&#x4F7F;&#x7528;&#x76F8;&#x540C;&#x7684;&#x53C2;&#x6570;&#x6784;&#x5EFA;<a href="#torch.nn.Conv3d" title="torch.nn.Conv3d"><code>Conv3d</code></a> &#x548C;<a href="#torch.nn.ConvTranspose3d" title="torch.nn.ConvTranspose3d"><code>ConvTranspose3d</code></a>&#x6A21;&#x5757;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x8FD9;&#x79CD;&#x8BBE;&#x7F6E;&#x80FD;&#x591F;&#x5B9E;&#x73B0;&#x4E24;&#x4E2A;&#x6A21;&#x5757;&#x6709;&#x6B63;&#x597D;&#x76F8;&#x53CD;&#x7684;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x5373;Conv3d&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x662F;&#x5176;&#x5BF9;&#x5E94;&#x7684;ConvTranspose3d&#x6A21;&#x5757;&#x7684;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#xFF0C;&#x800C;ConvTranspose3d&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x53C8;&#x6070;&#x597D;&#x662F;&#x5176;&#x5BF9;&#x5E94;&#x7684;Conv3d&#x6A21;&#x5757;&#x7684;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x3002;&#x7136;&#x800C;&#xFF0C;&#x5F53;<code>stride &gt; 1</code>&#x7684;&#x65F6;&#x5019;&#xFF0C;<a href="#torch.nn.Conv3d" title="torch.nn.Conv3d"><code>Conv3d</code></a> &#x7684;&#x4E00;&#x4E2A;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x53EF;&#x80FD;&#x4F1A;&#x5BF9;&#x5E94;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#xFF0C;&#x4E0A;&#x4E00;&#x4E2A;note&#x4E2D;&#x5C31;&#x8BE6;&#x7EC6;&#x7684;&#x4ECB;&#x7ECD;&#x4E86;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#xFF0C;&#x8FD9;&#x6837;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#x8981;&#x4FDD;&#x6301;&#x524D;&#x9762;&#x63D0;&#x5230;&#x4E24;&#x79CD;&#x6A21;&#x5757;&#x7684;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x4FDD;&#x6301;&#x53CD;&#x5411;&#x4E00;&#x81F4;&#xFF0C;&#x90A3;&#x5C31;&#x8981;&#x7528;&#x5230; <code>output_padding</code>&#x53C2;&#x6570;&#x4E86;&#xFF0C;&#x8FD9;&#x4E2A;&#x53C2;&#x6570;&#x53EF;&#x4EE5;&#x589E;&#x52A0;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x8F93;&#x51FA;&#x7684;&#x67D0;&#x4E00;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x4EE5;&#x6B64;&#x6765;&#x8FBE;&#x5230;&#x524D;&#x9762;&#x63D0;&#x5230;&#x7684;&#x540C;&#x53C2;&#x6570;&#x6784;&#x5EFA;&#x7684;<a href="#torch.nn.Conv3d" title="torch.nn.Conv3d"><code>Conv3d</code></a> &#x548C;<a href="#torch.nn.ConvTranspose3d" title="torch.nn.ConvTranspose3d"><code>ConvTranspose3d</code></a>&#x6A21;&#x5757;&#x7684;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x65B9;&#x5411;&#x4E00;&#x81F4;&#x3002; &#x4F46;&#x6CE8;&#x610F;&#x8FD9;&#x4E2A;&#x53C2;&#x6570;&#x4E0D;&#x662F;&#x8BF4;&#x8981;&#x5F80;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x7684;&#x8F93;&#x51FA;&#x4E0A;pad 0&#xFF0C;&#x800C;&#x662F;&#x76F4;&#x63A5;&#x63A7;&#x5236;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x7684;&#x8F93;&#x51FA;&#x5404;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;&#x4E3A;&#x6839;&#x636E;&#x6B64;&#x53C2;&#x6570;pad&#x540E;&#x7684;&#x5927;&#x5C0F;&#x3002;</p>
<p>Note</p>
<p>&#x5F53;&#x7A0B;&#x5E8F;&#x7684;&#x8FD0;&#x884C;&#x73AF;&#x5883;&#x662F;&#x4F7F;&#x7528;&#x4E86;CuDNN&#x7684;CUDA&#x73AF;&#x5883;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4E00;&#x4E9B;&#x975E;&#x786E;&#x5B9A;&#x6027;&#x7684;&#x7B97;&#x6CD5;&#xFF08;nondeterministic algorithm&#xFF09;&#x53EF;&#x80FD;&#x4F1A;&#x88AB;&#x91C7;&#x7528;&#x4EE5;&#x63D0;&#x9AD8;&#x6574;&#x4E2A;&#x8BA1;&#x7B97;&#x7684;&#x6027;&#x80FD;&#x3002;&#x5982;&#x679C;&#x4E0D;&#x60F3;&#x4F7F;&#x7528;&#x8FD9;&#x4E9B;&#x975E;&#x786E;&#x5B9A;&#x6027;&#x7684;&#x7B97;&#x6CD5;&#xFF0C;&#x4F60;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x8BBE;&#x7F6E;<code>torch.backends.cudnn.deterministic = True</code>&#x6765;&#x8BA9;&#x6574;&#x4E2A;&#x8BA1;&#x7B97;&#x8FC7;&#x7A0B;&#x4FDD;&#x6301;&#x786E;&#x5B9A;&#x6027;&#xFF08;&#x53EF;&#x80FD;&#x4F1A;&#x635F;&#x5931;&#x4E00;&#x5B9A;&#x7684;&#x8BA1;&#x7B97;&#x6027;&#x80FD;&#xFF09;&#x3002;&#x5BF9;&#x4E8E;&#x540E;&#x7AEF;(background)&#xFF0C;&#x4F60;&#x53EF;&#x4EE5;&#x770B;&#x4E00;&#x4E0B;&#x8FD9;&#x4E00;&#x90E8;&#x5206;<a href="notes/randomness.html">Reproducibility</a>&#x4E86;&#x89E3;&#x5176;&#x76F8;&#x5173;&#x4FE1;&#x606F;&#x3002;</p>
<p>Parameters:</p>
<ul>
<li><strong>in_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; &#x8F93;&#x5165;&#x901A;&#x9053;&#x7684;&#x4E2A;&#x6570;</li>
<li><strong>out_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; &#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x7684;&#x4E2A;&#x6570;</li>
<li><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x5377;&#x79EF;&#x6838;&#x5927;&#x5C0F;</li>
<li><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x7684;&#x6B65;&#x957F;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; 1</li>
<li><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; <code>kernel_size - 1 - padding</code> &#x5C42; 0 &#x4F1A;&#x88AB;&#x8865;&#x9F50;&#x5230;&#x8F93;&#x5165;&#x6570;&#x636E;&#x7684;&#x5404;&#x8FB9;&#x4E0A;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; 0</li>
<li><strong>output_padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x8F93;&#x51FA;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x8981;&#x589E;&#x52A0;&#x7684;&#x5927;&#x5C0F;&#x3002;&#x9ED8;&#x8BA4;&#xFF1A;0 </li>
<li><strong>groups</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; &#x8F93;&#x5165;&#x901A;&#x9053;&#x4E0E;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x4E4B;&#x95F4;&#x76F8;&#x4E92;&#x9694;&#x79BB;&#x7684;&#x8FDE;&#x63A5;&#x7684;&#x4E2A;&#x6570;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A;1</li>
<li><strong>bias</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5982;&#x679C;&#x88AB;&#x7F6E;&#x4E3A; <code>True</code>&#xFF0C;&#x5411;&#x8F93;&#x51FA;&#x589E;&#x52A0;&#x4E00;&#x4E2A;&#x504F;&#x5DEE;&#x91CF;&#xFF0C;&#x6B64;&#x504F;&#x5DEE;&#x662F;&#x53EF;&#x5B66;&#x4E60;&#x53C2;&#x6570;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A;<code>True</code></li>
<li><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5377;&#x79EF;&#x6838;&#x5404;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; 1</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/a8d71105bc4954eb54660bc5d37c23de.jpg" alt=""></li>
<li>&#x8F93;&#x51FA;: <img src="img/f05e8faaf90b4c16b23ca0165e8e09f4.jpg" alt=""> &#x5176;&#x4E2D;</li>
</ul>
<p><img src="img/35234de680c85870881b7f5d9e8de589.jpg" alt=""></p>
<p><img src="img/044bc4ee93fc4a1725b5b5dc5840b408.jpg" alt=""></p>
<p><img src="img/133b249f21b73617ee100c4c072eee15.jpg" alt=""></p>
<p>| Variables: | </p>
<ul>
<li><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013;  &#x6A21;&#x5757;&#x4E2D;&#x7684;&#x4E00;&#x4E2A;&#x5927;&#x5C0F;&#x4E3A; (in_channels, out_channels, kernel_size[0], kernel_size[1], kernel_size[2])&#x7684;&#x6743;&#x91CD;&#x5F20;&#x91CF;&#xFF0C;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x53EF;&#x8BAD;&#x7EC3;&#x5B66;&#x4E60;(learnable)&#x3002;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x521D;&#x59CB;&#x503C;&#x7684;&#x91C7;&#x6837;&#x7A7A;&#x95F4;&#x662F;<img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt="">&#xFF0C;&#x5176;&#x4E2D; <img src="img/378f5c5b47c36239b817ad23a612a9f7.jpg" alt="">&#x3002;</li>
<li><strong>bias</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; &#x6A21;&#x5757;&#x7684;&#x504F;&#x5DEE;&#x9879;&#xFF0C;&#x5927;&#x5C0F;&#x4E3A; (out_channels)&#xFF0C; &#x5982;&#x679C;&#x6784;&#x9020;&#x51FD;&#x6570;&#x4E2D;&#x7684; <code>bias</code> &#x88AB;&#x7F6E;&#x4E3A; <code>True</code>&#xFF0C;&#x90A3;&#x4E48;&#x8FD9;&#x4E9B;&#x6743;&#x91CD;&#x7684;&#x521D;&#x59CB;&#x503C;&#x7684;&#x91C7;&#x6837;&#x7A7A;&#x95F4;&#x662F; <img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt="">&#xFF0C;&#x5176;&#x4E2D; <img src="img/378f5c5b47c36239b817ad23a612a9f7.jpg" alt="">&#x3002;</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With square kernels and equal stride</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ConvTranspose3d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># non-square kernels and unequal stride and with padding</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ConvTranspose3d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>, <span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="unfold">Unfold</h3>
<pre><code class="lang-py">class torch.nn.Unfold(kernel_size, dilation=1, padding=0, stride=1)
</code></pre>
<p>&#x5C06;&#x4E00;&#x4E2A;batch&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x5C55;&#x5F00;&#x6210;&#x7531;&#x591A;&#x4E2A;&#x6ED1;&#x52A8;&#x5C40;&#x90E8;&#x5757;&#x7EC4;&#x6210;&#x7684;&#x5F62;&#x5F0F;&#x3002;&#xFF08;im2col&#x7684;&#x6269;&#x5C55;&#x6A21;&#x5757;&#xFF0C;&#x8D77;&#x5230;&#x57FA;&#x672C;&#x7C7B;&#x4F3C;im2col&#x7684;&#x4F5C;&#x7528;&#xFF09;</p>
<p>&#x4EE5;&#x4E00;&#x4E2A;&#x5927;&#x5C0F;&#x4E3A;<img src="img/2468b226c29a7e754a9c20f0214fa85f.jpg" alt="">&#x7684;&#x6279;&#x6B21;&#x5316;(batched)&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E3A;&#x4F8B;&#xFF0C;&#x5176;&#x4E2D;<img src="img/9341d9048ac485106d2b2ee8de14876f.jpg" alt="">&#x662F;batch&#x7684;&#x5927;&#x5C0F;&#xFF0C;<img src="img/6c8feca3b2da3d6cf371417edff4be4f.jpg" alt="">&#x662F;&#x901A;&#x9053;&#x6570;&#x91CF;&#xFF0C;<img src="img/28ec51e742166ea3400be6e7343bbfa5.jpg" alt="">&#x4EE3;&#x8868;&#x4E86;&#x4EFB;&#x610F;&#x7A7A;&#x95F4;&#x7EF4;&#x5EA6;&#x3002;&#x90A3;Unfold&#x8FD9;&#x4E2A;&#x64CD;&#x4F5C;&#x5728;&#x6B64;&#x5F20;&#x91CF;&#x4E0A;&#x7684;&#x64CD;&#x4F5C;&#x5C31;&#x662F;&#xFF0C;&#x5C06;&#x8FD9;&#x4E2A;&#x5F20;&#x91CF;&#x5C55;&#x5F00;&#x6210;&#x7531;&#x591A;&#x4E2A;<code>kernel_size</code>&#x5927;&#x5C0F;&#x7684;&#x6ED1;&#x52A8;&#x5757;&#x7EC4;&#x6210;&#x7684;&#x5927;&#x5C0F;&#x4E3A;<img src="img/4e1cad10fa9480fa82adbe59a5ae81fa.jpg" alt="">&#x7684;&#x4E09;&#x7EF4;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;<img src="img/a8846766f2e1b47021f1520993773ccb.jpg" alt="">&#x662F;&#x6BCF;&#x4E2A;&#x5757;&#x4E2D;&#x6570;&#x7684;&#x4E2A;&#x6570;&#xFF08;&#x6BCF;&#x4E2A;&#x5757;&#x6709;<img src="img/8c7a54ca7193bc3a6c5ace8c3b07d24c.jpg" alt="">&#x4E2A;&#x7A7A;&#x95F4;&#x4F4D;&#x7F6E;&#xFF0C;&#x6BCF;&#x4E2A;&#x7A7A;&#x95F4;&#x4F4D;&#x7F6E;&#x5B58;&#x50A8;&#x4E00;&#x4E2A;&#x901A;&#x9053;&#x5927;&#x5C0F;&#x4E3A;<img src="img/6c8feca3b2da3d6cf371417edff4be4f.jpg" alt="">&#x7684;&#x5411;&#x91CF;&#xFF09;&#xFF0C;<img src="img/db4a9fef02111450bf98261889de550c.jpg" alt="">&#x662F;&#x5757;&#x7684;&#x4E2A;&#x6570;&#xFF1A;</p>
<p><img src="img/1d2c6a9103e2b33f725602aebf90364e.jpg" alt="">
&#xFF08;&#x8FD9;&#x5F20;&#x56FE;&#x6709;&#x95EE;&#x9898;&#x554A;&#xFF0C;&#x7F16;&#x8F91;&#x6574;&#x7406;&#x7684;&#x65F6;&#x5019;&#x6CE8;&#x610F;&#x4FEE;&#x6B63;&#x4E00;&#x4E0B;&#xFF09;</p>
<p>&#x5176;&#x4E2D; <img src="img/42a2dca8a9cb6104321cf29ae30fd56a.jpg" alt=""> &#x662F;&#x7531;&#x4E0A;&#x9762;&#x4F8B;&#x5B50;&#x4E2D;&#x7684;<code>input</code>&#x5404;&#x7A7A;&#x95F4;&#x7EF4;&#x5EA6;&#x7EC4;&#x6210;&#x7684;&#xFF0C;<img src="img/9566974d45a96737f7e0ecf302d877b8.jpg" alt="">&#x904D;&#x5386;&#x4E86;&#x5404;&#x4E2A;&#x7A7A;&#x95F4;&#x7EF4;&#x5EA6;&#x3002;</p>
<p>&#x56E0;&#x6B64;&#xFF0C;&#x7D22;&#x5F15;Fold&#x64CD;&#x4F5C;&#x7684;<code>output</code>&#x7684;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x7EF4;&#x5EA6;&#x7B49;&#x4EF7;&#x4E8E;&#x7D22;&#x5F15;&#x67D0;&#x4E00;&#x4E2A;block&#xFF0C;&#x800C;&#x7D22;&#x5F15;&#x64CD;&#x4F5C;&#x7684;&#x8FD4;&#x56DE;&#x503C;&#x662F;&#x8FD9;&#x4E2A;&#x7D22;&#x5F15;&#x5230;&#x7684;block&#x4E2D;&#x7684;&#x6240;&#x6709;&#x503C;&#x3002;</p>
<p><code>padding</code>, <code>stride</code> &#x548C; <code>dilation</code> &#x53C2;&#x6570;&#x6307;&#x660E;&#x4E86;&#x6ED1;&#x52A8;&#x5757;&#x7684;&#x76F8;&#x5173;&#x6027;&#x8D28;&#x3002;</p>
<ul>
<li><code>stride</code> &#x63A7;&#x5236;&#x4E86;&#x6ED1;&#x52A8;&#x5757;&#x7684;&#x6B65;&#x957F;&#x3002;</li>
<li><code>padding</code> &#x63A7;&#x5236;&#x4E86;&#x5728;&#x53D8;&#x5F62;&#x4E4B;&#x524D;&#x8981;&#x5411;input&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8865;&#x9F50;&#x7684;0&#x7684;&#x5C42;&#x6570;&#x3002; </li>
<li><code>dilation</code> &#x63A7;&#x5236;&#x4E86;&#x5377;&#x79EF;&#x6838;&#x4E2D;&#x5404;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x7A7A;&#x95F4;&#x8DDD;&#x79BB;&#xFF1B;&#x8FD9;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x591A;&#x5B54;&#x7B97;&#x6CD5;(&#xE0; trous algorithm)&#x3002;&#x8FD9;&#x4E2A;&#x6982;&#x5FF5;&#x6709;&#x70B9;&#x96BE;&#x89E3;&#x91CA;&#xFF0C;&#x8FD9;&#x4E2A;&#x94FE;&#x63A5;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">link</a>&#x7528;&#x53EF;&#x89C6;&#x5316;&#x7684;&#x65B9;&#x6CD5;&#x5F88;&#x597D;&#x5730;&#x89E3;&#x91CA;&#x4E86;dilation&#x7684;&#x4F5C;&#x7528;&#x3002;</li>
</ul>
<p>Parameters: </p>
<ul>
<li><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x6ED1;&#x52A8;&#x5757;&#x7684;&#x5927;&#x5C0F;</li>
<li><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x6ED1;&#x52A8;&#x5757;&#x5728;&#x8F93;&#x5165;&#x5404;&#x7EF4;&#x5EA6;&#x4E0A;&#x7684;&#x6B65;&#x957F;&#x3002;&#x9ED8;&#x8BA4;: 1</li>
<li><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5728;&#x8F93;&#x5165;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x3002;</li>
<li><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x63A7;&#x5236;&#x4E86;&#x5404;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#xFF08;&#x6CA1;&#x6709;&#x6307;&#x660E;&#x5143;&#x7D20;&#x5177;&#x4F53;&#x6307;&#x7684;&#x662F;&#x8C01;&#x7684;&#x5143;&#x7D20;&#xFF0C;&#x731C;&#x6D4B;&#x662F;&#x8F93;&#x51FA;&#x7684;&#xFF09;&#x3002;&#x9ED8;&#x8BA4;&#xFF1A;1 </li>
</ul>
<ul>
<li>&#x5982;&#x679C; <code>kernel_size</code>, <code>dilation</code>, <code>padding</code> &#x6216;&#x8005; <code>stride</code>&#x7684;&#x503C;&#x662F;&#x4E00;&#x4E2A;int&#xFF0C;&#x6216;&#x662F;&#x4E00;&#x4E2A;&#x957F;&#x5EA6;&#x4E3A;1&#x7684;int&#x5143;&#x7EC4;&#xFF0C;&#x5728;&#x76F8;&#x5173;&#x64CD;&#x4F5C;&#x7684;&#x65F6;&#x5019;&#x5404;&#x4E2A;&#x7A7A;&#x95F4;&#x7EF4;&#x5EA6;&#x4E0A;&#x90FD;&#x4F1A;&#x4F7F;&#x7528;&#x8FD9;&#x540C;&#x4E00;&#x4E2A;&#x503C;&#x3002; </li>
<li>&#x5982;&#x679C;&#x8F93;&#x51FA;&#x5411;&#x91CF;&#x6709;&#x4E24;&#x4E2A;&#x7A7A;&#x95F4;&#x7EF4;&#x5EA6;&#xFF0C;&#x90A3;&#x4E48;&#x6B64;Fold&#x64CD;&#x4F5C;&#x6709;&#x65F6;&#x53C8;&#x88AB;&#x79F0;&#x4E3A;<code>im2col</code>&#x3002;</li>
</ul>
<p>Note
<a href="#torch.nn.Fold" title="torch.nn.Fold"><code>Fold</code></a>&#x5728;&#x6267;&#x884C;&#x7C7B;<code>col2im</code>&#x7684;&#x64CD;&#x4F5C;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4E3B;&#x8981;&#x662F;&#x662F;&#x901A;&#x8FC7;&#x96C6;&#x6210;&#x6B64;im&#xFF08;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#xFF09;&#x5206;&#x88C2;&#x51FA;&#x6240;&#x6709;&#x5BF9;&#x5E94;&#x4F4D;&#x7F6E;&#x7684;col&#xFF08;&#x8F93;&#x5165;&#x7684;&#x6ED1;&#x52A8;&#x5757;&#xFF09;&#x6765;&#x590D;&#x539F;&#x539F;im&#x3002;&#x800C;<a href="#torch.nn.Unfold" title="torch.nn.Unfold"><code>Unfold</code></a>&#x5219;&#x662F;&#x901A;&#x8FC7;&#x4ECE;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E2D;&#x4E0D;&#x65AD;&#x62F7;&#x8D1D;&#x6570;&#x503C;&#x5230;&#x76F8;&#x5E94;&#x7684;block&#x4E2D;&#x6765;&#x751F;&#x6210;&#x7531;&#x6ED1;&#x52A8;&#x5757;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x3002;&#x6240;&#x4EE5;&#xFF0C;&#x5982;&#x679C;&#x6ED1;&#x52A8;&#x5757;&#x4E4B;&#x95F4;&#x5982;&#x679C;&#x6709;&#x6570;&#x503C;&#x91CD;&#x53E0;&#xFF0C;&#x90A3;&#x8FD9;&#x4E9B;&#x6ED1;&#x52A8;&#x5757;&#x4E4B;&#x95F4;&#x5E76;&#x4E0D;&#x662F;&#x4E92;&#x9006;&#x7684;&#x3002;</p>
<p>Warning</p>
<p>&#x76EE;&#x524D;&#xFF0C;&#x53EA;&#x6709;&#x56DB;&#x7EF4;&#x5F20;&#x91CF;&#xFF08;&#x6BD4;&#x5982;&#x6279;&#x6B21;&#x5316;&#x7684;&#x56FE;&#x50CF;&#x5F20;&#x91CF;&#xFF09;&#x652F;&#x6301;&#x8FD9;&#x4E2A;&#x64CD;&#x4F5C;&#x3002;</p>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/2468b226c29a7e754a9c20f0214fa85f.jpg" alt=""></li>
<li>&#x8F93;&#x51FA;: <img src="img/4e1cad10fa9480fa82adbe59a5ae81fa.jpg" alt=""></li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>unfold = nn.Unfold(kernel_size=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = unfold(input)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># each patch contains 30 values (2x3=6 vectors, each of 5 channels)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 4 blocks (2x3 kernels) in total in the 3x4 input</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>output.size()
torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">30</span>, <span class="hljs-number">4</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inp = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>, <span class="hljs-number">12</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>w = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inp_unf = torch.nn.functional.unfold(inp, (<span class="hljs-number">4</span>, <span class="hljs-number">5</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>out_unf = inp_unf.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).matmul(w.view(w.size(<span class="hljs-number">0</span>), <span class="hljs-number">-1</span>).t()).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>out = torch.nn.functional.fold(out_unf, (<span class="hljs-number">7</span>, <span class="hljs-number">8</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># or equivalently (and avoiding a copy),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># out = out_unf.view(1, 2, 7, 8)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>(torch.nn.functional.conv2d(inp, w) - out).abs().max()
tensor(<span class="hljs-number">1.9073e-06</span>)
</code></pre>
<h3 id="fold">Fold</h3>
<pre><code class="lang-py">class torch.nn.Fold(output_size, kernel_size, dilation=1, padding=0, stride=1)
</code></pre>
<p>&#x5C06;&#x7531;&#x6ED1;&#x52A8;&#x5C40;&#x90E8;&#x5757;&#x7EC4;&#x6210;&#x7684;&#x6570;&#x7EC4;&#x96C6;&#x5408;&#x4E3A;&#x4E00;&#x4E2A;&#x5927;&#x5F20;&#x91CF;&#x3002;(&#x7C7B;col2im)</p>
<p>&#x8003;&#x8651;&#x4E00;&#x4E2A;&#x5305;&#x542B;&#x4E86;&#x5F88;&#x591A;&#x4E2A;&#x6ED1;&#x52A8;&#x5C40;&#x90E8;&#x5757;&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#xFF0C;&#x6BD4;&#x5982;&#xFF0C;&#x4E00;&#x6279;&#x56FE;&#x50CF;&#x5206;&#x5272;&#x5757;(patches of images)&#x7684;&#x96C6;&#x5408;&#xFF0C;&#x5927;&#x5C0F;&#x4E3A;<img src="img/9e56ff5e3827b936da5cfa3a5258b12e.jpg" alt="">&#xFF0C;&#x5176;&#x4E2D;<img src="img/9341d9048ac485106d2b2ee8de14876f.jpg" alt="">&#x662F;batch&#x5927;&#x5C0F;&#xFF0C; <img src="img/a8846766f2e1b47021f1520993773ccb.jpg" alt=""> &#x662F;&#x4E00;&#x4E2A;&#x5757;&#x4E2D;&#x7684;&#x6570;&#x503C;&#x4E2A;&#x6570;&#xFF08;&#x6BCF;&#x4E2A;&#x5757;&#x6709;<img src="img/8c7a54ca7193bc3a6c5ace8c3b07d24c.jpg" alt="">&#x4E2A;&#x7A7A;&#x95F4;&#x4F4D;&#x7F6E;&#xFF0C;&#x6BCF;&#x4E2A;&#x7A7A;&#x95F4;&#x4F4D;&#x7F6E;&#x5B58;&#x50A8;&#x4E00;&#x4E2A;&#x901A;&#x9053;&#x5927;&#x5C0F;&#x4E3A;<img src="img/6c8feca3b2da3d6cf371417edff4be4f.jpg" alt="">&#x7684;&#x5411;&#x91CF;&#xFF09;&#xFF0C;<img src="img/db4a9fef02111450bf98261889de550c.jpg" alt="">&#x662F;&#x6ED1;&#x52A8;&#x5757;&#x7684;&#x4E2A;&#x6570;&#x3002;&#xFF08;&#x8FD9;&#x4E9B;&#x5927;&#x5C0F;&#x53C2;&#x6570;&#x4E25;&#x683C;&#x9075;&#x5FAA;&#x4E86;<a href="#torch.nn.Unfold" title="torch.nn.Unfold"><code>Unfold</code></a>&#x64CD;&#x4F5C;&#x7684;&#x8F93;&#x51FA;&#x5411;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x89C4;&#x5B9A;&#x3002;&#xFF09;Fold&#x64CD;&#x4F5C;&#x901A;&#x8FC7;&#x6C42;&#x548C;&#x91CD;&#x53E0;&#x503C;&#x7684;&#x65B9;&#x5F0F;&#x6765;&#x5C06;&#x8FD9;&#x4E9B;&#x5C40;&#x90E8;&#x5757;&#x96C6;&#x5408;&#x4E3A;&#x4E00;&#x4E2A;&#x5927;&#x5C0F;&#x4E3A;<img src="img/c2176aae9e099eeee07cc00c4dc7b7e7.jpg" alt="">&#x7684;<code>output</code>&#x5F20;&#x91CF;&#x3002;&#x4E0E; <a href="#torch.nn.Unfold" title="torch.nn.Unfold"><code>Unfold</code></a>&#x7C7B;&#x4F3C;&#xFF0C;&#x8FD9;&#x4E9B;&#x53C2;&#x6570;&#x5FC5;&#x987B;&#x6EE1;&#x8DB3;&#xFF1A;</p>
<p><img src="img/465bba7070e80a7e5964f46f7f5ed8bb.jpg" alt=""></p>
<p>&#x5176;&#x4E2D;<img src="img/9566974d45a96737f7e0ecf302d877b8.jpg" alt="">&#x904D;&#x5386;&#x4E86;&#x5404;&#x4E2A;&#x7A7A;&#x95F4;&#x7EF4;&#x5EA6;&#x3002;</p>
<ul>
<li><code>output_size</code> &#x63CF;&#x8FF0;&#x4E86;&#x8981;&#x751F;&#x6210;&#x7684;output&#x7684;&#x5404;&#x7A7A;&#x95F4;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;&#x3002;&#x6709;&#x65F6;&#xFF0C;&#x540C;&#x6837;&#x6570;&#x91CF;&#x7684;&#x6ED1;&#x52A8;&#x5757;&#xFF0C;&#x53EF;&#x80FD;&#x4F1A;&#x4EA7;&#x751F;&#x591A;&#x79CD;<code>input</code>&#x7684;&#x5F62;&#x72B6;&#xFF0C;&#x6BD4;&#x5982;&#xFF0C;&#x5F53;<code>stride &gt; 0</code>&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x8FD9;&#x65F6;&#x5019;&#xFF0C;&#x8BBE;&#x7F6E;<code>output_size</code>&#x53C2;&#x6570;&#x5C31;&#x4F1A;&#x663E;&#x5F97;&#x6781;&#x4E3A;&#x91CD;&#x8981;&#x3002;</li>
</ul>
<p><code>padding</code>, <code>stride</code> &#x548C; <code>dilation</code> &#x53C2;&#x6570;&#x6307;&#x660E;&#x4E86;&#x6ED1;&#x52A8;&#x5757;&#x7684;&#x76F8;&#x5173;&#x6027;&#x8D28;&#x3002;</p>
<ul>
<li><code>stride</code> &#x63A7;&#x5236;&#x4E86;&#x6ED1;&#x52A8;&#x5757;&#x7684;&#x6B65;&#x957F;&#x3002;</li>
<li><code>padding</code> &#x63A7;&#x5236;&#x4E86;&#x5728;&#x53D8;&#x5F62;&#x4E4B;&#x524D;&#x8981;&#x5411;input&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8865;&#x9F50;&#x7684;0&#x7684;&#x5C42;&#x6570;&#x3002; </li>
<li><code>dilation</code> &#x63A7;&#x5236;&#x4E86;&#x5377;&#x79EF;&#x6838;&#x4E2D;&#x5404;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x7A7A;&#x95F4;&#x8DDD;&#x79BB;&#xFF1B;&#x8FD9;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x591A;&#x5B54;&#x7B97;&#x6CD5;(&#xE0; trous algorithm)&#x3002;&#x8FD9;&#x4E2A;&#x6982;&#x5FF5;&#x6709;&#x70B9;&#x96BE;&#x89E3;&#x91CA;&#xFF0C;&#x8FD9;&#x4E2A;&#x94FE;&#x63A5;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">link</a>&#x7528;&#x53EF;&#x89C6;&#x5316;&#x7684;&#x65B9;&#x6CD5;&#x5F88;&#x597D;&#x5730;&#x89E3;&#x91CA;&#x4E86;dilation&#x7684;&#x4F5C;&#x7528;&#x3002;</li>
</ul>
<p>Parameters: </p>
<ul>
<li><p><strong>output_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013;  &#x8F93;&#x51FA;&#x5411;&#x91CF;&#x7684;&#x5404;&#x7A7A;&#x95F4;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F; (i.e., <code>input.sizes()[2:]</code>)</p>
</li>
<li><p><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x6ED1;&#x52A8;&#x5757;&#x7684;&#x5927;&#x5C0F;</p>
</li>
<li><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x6ED1;&#x52A8;&#x5757;&#x5728;&#x8F93;&#x5165;&#x5404;&#x7EF4;&#x5EA6;&#x4E0A;&#x7684;&#x6B65;&#x957F;&#x3002;&#x9ED8;&#x8BA4;: 1</li>
<li><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x5728;&#x8F93;&#x5165;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x3002;</li>
<li><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; &#x63A7;&#x5236;&#x4E86;&#x5404;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#xFF08;&#x6CA1;&#x6709;&#x6307;&#x660E;&#x5143;&#x7D20;&#x5177;&#x4F53;&#x6307;&#x7684;&#x662F;&#x8C01;&#x7684;&#x5143;&#x7D20;&#xFF0C;&#x731C;&#x6D4B;&#x662F;&#x8F93;&#x51FA;&#x7684;&#xFF09;&#x3002;&#x9ED8;&#x8BA4;&#xFF1A;1 </li>
</ul>
<ul>
<li>&#x5982;&#x679C;<code>output_size</code>&#xFF0C; <code>kernel_size</code>, <code>dilation</code>, <code>padding</code> &#x6216;&#x8005; <code>stride</code>&#x662F;&#x4E00;&#x4E2A;int&#x6216;&#x8005;&#x957F;&#x5EA6;&#x4E3A;1&#x7684;int&#x5143;&#x7EC4;&#xFF0C;&#x5728;&#x76F8;&#x5173;&#x64CD;&#x4F5C;&#x7684;&#x65F6;&#x5019;&#x5404;&#x4E2A;&#x7A7A;&#x95F4;&#x7EF4;&#x5EA6;&#x4E0A;&#x90FD;&#x4F1A;&#x4F7F;&#x7528;&#x8FD9;&#x540C;&#x4E00;&#x4E2A;&#x503C;&#x3002; </li>
<li>&#x5982;&#x679C;&#x6B64;&#x8F93;&#x51FA;&#x5411;&#x91CF;&#x7684;&#x7A7A;&#x95F4;&#x7EF4;&#x5EA6;&#x6570;&#x4E3A;2&#xFF0C;&#x90A3;&#x4E48;&#x6B64;Fold&#x64CD;&#x4F5C;&#x6709;&#x65F6;&#x53C8;&#x88AB;&#x79F0;&#x4E3A;<code>col2im</code>&#x3002;</li>
</ul>
<p>Note
<a href="#torch.nn.Fold" title="torch.nn.Fold"><code>Fold</code></a>&#x5728;&#x6267;&#x884C;&#x7C7B;<code>col2im</code>&#x7684;&#x64CD;&#x4F5C;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4E3B;&#x8981;&#x662F;&#x662F;&#x901A;&#x8FC7;&#x96C6;&#x6210;&#x6B64;im&#xFF08;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#xFF09;&#x5206;&#x88C2;&#x51FA;&#x6240;&#x6709;&#x5BF9;&#x5E94;&#x4F4D;&#x7F6E;&#x7684;col&#xFF08;&#x8F93;&#x5165;&#x7684;&#x6ED1;&#x52A8;&#x5757;&#xFF09;&#x6765;&#x590D;&#x539F;&#x539F;im&#x3002;&#x800C;<a href="#torch.nn.Unfold" title="torch.nn.Unfold"><code>Unfold</code></a>&#x5219;&#x662F;&#x901A;&#x8FC7;&#x4ECE;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E2D;&#x4E0D;&#x65AD;&#x62F7;&#x8D1D;&#x6570;&#x503C;&#x5230;&#x76F8;&#x5E94;&#x7684;block&#x4E2D;&#x6765;&#x751F;&#x6210;&#x7531;&#x6ED1;&#x52A8;&#x5757;&#x7EC4;&#x6210;&#x7684;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x3002;&#x6240;&#x4EE5;&#xFF0C;&#x5982;&#x679C;&#x6ED1;&#x52A8;&#x5757;&#x4E4B;&#x95F4;&#x5982;&#x679C;&#x6709;&#x6570;&#x503C;&#x91CD;&#x53E0;&#xFF0C;&#x90A3;&#x8FD9;&#x4E9B;&#x6ED1;&#x52A8;&#x5757;&#x4E4B;&#x95F4;&#x5E76;&#x4E0D;&#x662F;&#x4E92;&#x9006;&#x7684;&#x3002;</p>
<p>Warning</p>
<p>&#x76EE;&#x524D;&#xFF0C;&#x53EA;&#x6709;&#x56DB;&#x7EF4;&#x5F20;&#x91CF;&#xFF08;&#x6BD4;&#x5982;&#x6279;&#x6B21;&#x5316;&#x7684;&#x56FE;&#x50CF;&#x5F20;&#x91CF;&#xFF09;&#x652F;&#x6301;&#x8FD9;&#x4E2A;&#x64CD;&#x4F5C;&#x3002;</p>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/4e1cad10fa9480fa82adbe59a5ae81fa.jpg" alt=""></li>
<li>&#x8F93;&#x51FA;: <img src="img/c2176aae9e099eeee07cc00c4dc7b7e7.jpg" alt=""> </li>
</ul>
<p>&#x4E3E;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>fold = nn.Fold(output_size=(<span class="hljs-number">4</span>, <span class="hljs-number">5</span>), kernel_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span> * <span class="hljs-number">2</span> * <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = fold(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>output.size()
</code></pre>
<p><code>&#x5377;&#x79EF;&#x5C42;&#x90E8;&#x5206;Fold &#x4E0E; Unfold &#x662F;1.0&#x65B0;&#x589E;&#x7684;&#x5185;&#x5BB9;&#xFF0C;&#x731C;&#x6D4B;&#x5176;&#x4E3B;&#x8981;&#x76EE;&#x7684;&#x662F;&#x5F00;&#x653E;col2im&#x548C;im2col&#x8FD9;&#x4E24;&#x4E2A;&#x901A;&#x8FC7;&#x77E9;&#x9635;&#x4E58;&#x6CD5;&#x5B9E;&#x73B0;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x7684;&#x524D;&#x5E8F;&#x63A5;&#x53E3;&#xFF0C;&#x8981;&#x597D;&#x597D;&#x7406;&#x89E3;&#x8FD9;&#x90E8;&#x5206;&#x53EF;&#x80FD;&#x8981;&#x4E86;&#x89E3;&#x4E00;&#x4E0B;&#x73B0;&#x5728;&#x4E3B;&#x6D41;&#x6846;&#x67B6;&#x901A;&#x8FC7;&#x5927;&#x77E9;&#x9635;&#x4E58;&#x6CD5;&#x6765;&#x5B9E;&#x73B0;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x8FD9;&#x4E00;&#x901A;&#x7528;&#x505A;&#x6CD5;&#x4E86;&#xFF0C;&#x8FD9;&#x4E00;&#x7BC7;&#x6587;&#x7AE0;&#x5C31;&#x4ECB;&#x7ECD;&#x7684;&#x5F88;&#x597D;[Implementing convolution as a matrix multiplication](https://buptldy.github.io/2016/10/01/2016-10-01-im2col/)&#xFF0C;&#x8FD9;&#x4E00;&#x6BB5;&#x5982;&#x679C;&#x611F;&#x89C9;&#x6211;&#x7684;&#x76F4;&#x8BD1;&#x6666;&#x6DA9;&#x96BE;&#x61C2;&#xFF0C;&#x90A3;&#x6211;&#x6DF1;&#x611F;&#x62B1;&#x6B49;&#x5E76;&#x5EFA;&#x8BAE;&#x770B;&#x4E00;&#x4E0B;&#x82F1;&#x6587;&#x539F;&#x7248;&#xFF0C;&#x867D;&#x7136;&#x6211;&#x89C9;&#x5F97;&#x82F1;&#x6587;&#x539F;&#x7248;&#x4ECB;&#x7ECD;&#x7684;&#x4E5F;&#x662F;&#x6666;&#x6DA9;&#x96BE;&#x61C2;</code></p>
<h2 id="&#x6C60;&#x5316;&#x5C42;&#xFF08;pooling-layers&#xFF09;">&#x6C60;&#x5316;&#x5C42;&#xFF08;Pooling layers&#xFF09;</h2>
<h3 id="maxpool1d">MaxPool1d</h3>
<pre><code class="lang-py">class torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
</code></pre>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4FE1;&#x53F7;&#x6267;&#x884C;&#x4E00;&#x7EF4;&#x6700;&#x5927;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x6700;&#x7B80;&#x5355;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5BF9;&#x4E8E;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x4E3A; <img src="img/5816e96aa78b7425cf792435bba8bc29.jpg" alt=""> &#xFF0C;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x4E3A;<img src="img/d131773750846713475c600aa8cd917a.jpg" alt="">&#x7684;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#xFF0C;&#x6B64;&#x6C60;&#x5316;&#x8FC7;&#x7A0B;&#x53EF;&#x8868;&#x8FF0;&#x5982;&#x4E0B;&#xFF1A;</p>
<p><img src="img/9e414c5b7df992e54f3227bb130be349.jpg" alt=""></p>
<p><code>padding</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x8981;&#x5728;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x3002;
<code>dilation</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x6C60;&#x5316;&#x6838;&#x4E2D;&#x5404;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#xFF1B;&#x8FD9;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x591A;&#x5B54;&#x7B97;&#x6CD5;(&#xE0; trous algorithm)&#x3002;&#x8FD9;&#x4E2A;&#x6982;&#x5FF5;&#x6709;&#x70B9;&#x96BE;&#x89E3;&#x91CA;&#xFF0C;&#x8FD9;&#x4E2A;&#x94FE;&#x63A5;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">link</a>&#x7528;&#x53EF;&#x89C6;&#x5316;&#x7684;&#x65B9;&#x6CD5;&#x5F88;&#x597D;&#x5730;&#x89E3;&#x91CA;&#x4E86;<code>dilation</code>&#x7684;&#x4F5C;&#x7528;&#x3002;</p>
<p>Parameters: </p>
<ul>
<li><strong>kernel_size</strong> &#x2013; &#x6700;&#x5927;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x7684;&#x6ED1;&#x52A8;&#x7A97;&#x5927;&#x5C0F;</li>
<li><strong>stride</strong> &#x2013; &#x6ED1;&#x52A8;&#x7A97;&#x7684;&#x6B65;&#x957F;&#xFF0C;&#x9ED8;&#x8BA4;&#x503C;&#x662F; <code>kernel_size</code></li>
<li><strong>padding</strong> &#x2013; &#x8981;&#x5728;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;</li>
<li><strong>dilation</strong> &#x2013; &#x6ED1;&#x52A8;&#x7A97;&#x4E2D;&#x5404;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;</li>
<li><strong>return_indices</strong> &#x2013; &#x5982;&#x679C;&#x6B64;&#x53C2;&#x6570;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>&#xFF0C; &#x90A3;&#x4E48;&#x6B64;&#x6C60;&#x5316;&#x5C42;&#x5728;&#x8FD4;&#x56DE;&#x8F93;&#x51FA;&#x4FE1;&#x53F7;&#x7684;&#x540C;&#x65F6;&#x8FD8;&#x4F1A;&#x8FD4;&#x56DE;&#x4E00;&#x8FDE;&#x4E32;&#x6ED1;&#x52A8;&#x7A97;&#x6700;&#x5927;&#x503C;&#x7684;&#x7D22;&#x5F15;&#x4F4D;&#x7F6E;&#xFF0C;&#x5373;&#x6BCF;&#x4E2A;&#x6ED1;&#x52A8;&#x7A97;&#x7684;&#x6700;&#x5927;&#x503C;&#x4F4D;&#x7F6E;&#x4FE1;&#x606F;&#x3002;&#x8FD9;&#x4E9B;&#x4FE1;&#x606F;&#x53EF;&#x4EE5;&#x5728;&#x540E;&#x9762;&#x7684;&#x4E0A;&#x91C7;&#x6837;<a href="#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code>torch.nn.MaxUnpool1d</code></a>&#x4E2D;&#x88AB;&#x7528;&#x5230;&#x3002;</li>
<li><strong>ceil_mode</strong> &#x2013; &#x5982;&#x679C;&#x6B64;&#x53C2;&#x6570;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;True&#xFF0C;&#x8BA1;&#x7B97;&#x8F93;&#x51FA;&#x4FE1;&#x53F7;&#x5927;&#x5C0F;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4F1A;&#x4F7F;&#x7528;&#x5411;&#x4E0A;&#x53D6;&#x6574;&#xFF0C;&#x4EE3;&#x66FF;&#x9ED8;&#x8BA4;&#x7684;&#x5411;&#x4E0B;&#x53D6;&#x6574;&#x7684;&#x64CD;&#x4F5C;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li><p>&#x8F93;&#x5165;: <img src="img/3ceb415a2a1558bab9998c277f780ec3.jpg" alt=""></p>
</li>
<li><p>&#x8F93;&#x51FA;: <img src="img/d131773750846713475c600aa8cd917a.jpg" alt=""> &#x5176;&#x4E2D;</p>
<p><img src="img/ff16cce6b4741640e8adc0a271cd4592.jpg" alt=""></p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of size=3, stride=2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.MaxPool1d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="maxpool2d">MaxPool2d</h3>
<pre><code class="lang-py">class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
</code></pre>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4FE1;&#x53F7;&#x6267;&#x884C;&#x4E8C;&#x7EF4;&#x6700;&#x5927;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x6700;&#x7B80;&#x5355;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5BF9;&#x4E8E;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x4E3A; <img src="img/23f8772594b27bd387be708fe9c085e1.jpg" alt=""> &#xFF0C;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x4E3A;<img src="img/a0ef05f779873fc4dcbf020b1ea14754.jpg" alt="">&#xFF0C;<code>kernel_size</code>&#x4E3A;<img src="img/6384e001ad4c0989683deb86f6ffbd2f.jpg" alt="">&#x7684;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#xFF0C;&#x6B64;&#x6C60;&#x5316;&#x8FC7;&#x7A0B;&#x53EF;&#x8868;&#x8FF0;&#x5982;&#x4E0B;&#xFF1A;</p>
<p><img src="img/caa8cbcbb8bbbbc6b0e47f9daa80ab12.jpg" alt=""></p>
<p><code>padding</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x8981;&#x5728;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x3002;
<code>dilation</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x6C60;&#x5316;&#x6838;&#x4E2D;&#x5404;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#xFF1B;&#x8FD9;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x591A;&#x5B54;&#x7B97;&#x6CD5;(&#xE0; trous algorithm)&#x3002;&#x8FD9;&#x4E2A;&#x6982;&#x5FF5;&#x6709;&#x70B9;&#x96BE;&#x89E3;&#x91CA;&#xFF0C;&#x8FD9;&#x4E2A;&#x94FE;&#x63A5;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">link</a>&#x7528;&#x53EF;&#x89C6;&#x5316;&#x7684;&#x65B9;&#x6CD5;&#x5F88;&#x597D;&#x5730;&#x89E3;&#x91CA;&#x4E86;<code>dilation</code>&#x7684;&#x4F5C;&#x7528;&#x3002;</p>
<p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code> &#x7B49;&#x53C2;&#x6570;&#x5747;&#x652F;&#x6301;&#x4EE5;&#x4E0B;&#x7C7B;&#x578B;&#x8F93;&#x5165;&#xFF1A;</p>
<blockquote>
<ul>
<li>&#x4E00;&#x4E2A;&#x5355;&#x72EC;&#x7684; <code>int</code> &#x2013; &#x6B64;&#x65F6;&#x8FD9;&#x4E2A;<code>int</code>&#x4F1A;&#x540C;&#x65F6;&#x63A7;&#x5236;&#x6C60;&#x5316;&#x6ED1;&#x52A8;&#x7A97;&#x7684;&#x5BBD;&#x548C;&#x9AD8;&#x8FD9;&#x4E24;&#x4E2A;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;</li>
<li>&#x4E00;&#x4E2A;&#x7531;&#x4E24;&#x4E2A;<code>int</code>&#x7EC4;&#x6210;&#x7684;<code>tuple</code> &#x2013; &#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x9AD8;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x5143;&#x7EC4;&#x4E2D;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#xFF0C;&#x5BBD;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x7B2C;&#x4E8C;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#x3002;</li>
</ul>
</blockquote>
<p>Parameters: </p>
<ul>
<li><strong>kernel_size</strong> &#x2013; &#x6700;&#x5927;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x7684;&#x6ED1;&#x52A8;&#x7A97;&#x5927;&#x5C0F;</li>
<li><strong>stride</strong> &#x2013; &#x6ED1;&#x52A8;&#x7A97;&#x7684;&#x6B65;&#x957F;&#xFF0C;&#x9ED8;&#x8BA4;&#x503C;&#x662F; <code>kernel_size</code></li>
<li><strong>padding</strong> &#x2013; &#x8981;&#x5728;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;</li>
<li><strong>dilation</strong> &#x2013; &#x6ED1;&#x52A8;&#x7A97;&#x4E2D;&#x5404;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;</li>
<li><strong>return_indices</strong> &#x2013; &#x5982;&#x679C;&#x6B64;&#x53C2;&#x6570;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>&#xFF0C; &#x90A3;&#x4E48;&#x6B64;&#x6C60;&#x5316;&#x5C42;&#x5728;&#x8FD4;&#x56DE;&#x8F93;&#x51FA;&#x4FE1;&#x53F7;&#x7684;&#x540C;&#x65F6;&#x8FD8;&#x4F1A;&#x8FD4;&#x56DE;&#x4E00;&#x8FDE;&#x4E32;&#x6ED1;&#x52A8;&#x7A97;&#x6700;&#x5927;&#x503C;&#x7684;&#x7D22;&#x5F15;&#x4F4D;&#x7F6E;&#xFF0C;&#x5373;&#x6BCF;&#x4E2A;&#x6ED1;&#x52A8;&#x7A97;&#x7684;&#x6700;&#x5927;&#x503C;&#x4F4D;&#x7F6E;&#x4FE1;&#x606F;&#x3002;&#x8FD9;&#x4E9B;&#x4FE1;&#x606F;&#x53EF;&#x4EE5;&#x5728;&#x540E;&#x9762;&#x7684;&#x4E0A;&#x91C7;&#x6837;<a href="#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code>torch.nn.MaxUnpool2d</code></a>&#x4E2D;&#x88AB;&#x7528;&#x5230;&#x3002;</li>
<li><strong>ceil_mode</strong> &#x2013; &#x5982;&#x679C;&#x6B64;&#x53C2;&#x6570;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;True&#xFF0C;&#x8BA1;&#x7B97;&#x8F93;&#x51FA;&#x4FE1;&#x53F7;&#x5927;&#x5C0F;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4F1A;&#x4F7F;&#x7528;&#x5411;&#x4E0A;&#x53D6;&#x6574;&#xFF0C;&#x4EE3;&#x66FF;&#x9ED8;&#x8BA4;&#x7684;&#x5411;&#x4E0B;&#x53D6;&#x6574;&#x7684;&#x64CD;&#x4F5C;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li><p>&#x8F93;&#x5165;: <img src="img/ff71b16eb10237262566c6907acaaf1f.jpg" alt=""></p>
</li>
<li><p>&#x8F93;&#x51FA;: <img src="img/a0ef05f779873fc4dcbf020b1ea14754.jpg" alt="">, &#x5176;&#x4E2D;</p>
<p><img src="img/991d42318f90dcb68b26938c542b8457.jpg" alt=""></p>
<p><img src="img/1e35edf42ee6921adb435b5ca638d406.jpg" alt=""></p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of square window of size=3, stride=2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of non-square window</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.MaxPool2d((<span class="hljs-number">3</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>, <span class="hljs-number">32</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="maxpool3d">MaxPool3d</h3>
<pre><code class="lang-py">class torch.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
</code></pre>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4FE1;&#x53F7;&#x6267;&#x884C;&#x4E09;&#x7EF4;&#x6700;&#x5927;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x6700;&#x7B80;&#x5355;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5BF9;&#x4E8E;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x4E3A; <img src="img/f5a45f7b445db562b21cfcb525637aab.jpg" alt=""> &#xFF0C;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x4E3A;<img src="img/41ca4c8d4c65c979d2d643c6f62ea280.jpg" alt="">&#xFF0C;<code>kernel_size</code>&#x4E3A; <img src="img/f5dcdebf9a81b9d15227749ae7535eb7.jpg" alt=""> &#x7684;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#xFF0C;&#x6B64;&#x6C60;&#x5316;&#x8FC7;&#x7A0B;&#x53EF;&#x8868;&#x8FF0;&#x5982;&#x4E0B;&#xFF1A;</p>
<p><img src="img/f0f7a770dcfb802e7fc0f8995cfad3d7.jpg" alt=""></p>
<p><code>padding</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x8981;&#x5728;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x3002;
<code>dilation</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x6C60;&#x5316;&#x6838;&#x4E2D;&#x5404;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#xFF1B;&#x8FD9;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A;&#x591A;&#x5B54;&#x7B97;&#x6CD5;(&#xE0; trous algorithm)&#x3002;&#x8FD9;&#x4E2A;&#x6982;&#x5FF5;&#x6709;&#x70B9;&#x96BE;&#x89E3;&#x91CA;&#xFF0C;&#x8FD9;&#x4E2A;&#x94FE;&#x63A5;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">link</a>&#x7528;&#x53EF;&#x89C6;&#x5316;&#x7684;&#x65B9;&#x6CD5;&#x5F88;&#x597D;&#x5730;&#x89E3;&#x91CA;&#x4E86;<code>dilation</code>&#x7684;&#x4F5C;&#x7528;&#x3002;</p>
<p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code> &#x7B49;&#x53C2;&#x6570;&#x5747;&#x652F;&#x6301;&#x4EE5;&#x4E0B;&#x7C7B;&#x578B;&#x8F93;&#x5165;&#xFF1A;</p>
<blockquote>
<ul>
<li>&#x4E00;&#x4E2A;&#x5355;&#x72EC;&#x7684; <code>int</code> &#x2013; &#x6B64;&#x65F6;&#x8FD9;&#x4E2A;<code>int</code>&#x4F1A;&#x540C;&#x65F6;&#x63A7;&#x5236;&#x6C60;&#x5316;&#x6ED1;&#x52A8;&#x7A97;&#x7684;&#x6DF1;&#x5EA6;&#xFF0C;&#x5BBD;&#x548C;&#x9AD8;&#x8FD9;&#x4E09;&#x4E2A;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;</li>
<li>&#x4E00;&#x4E2A;&#x7531;&#x4E09;&#x4E2A;<code>int</code>&#x7EC4;&#x6210;&#x7684;<code>tuple</code> &#x2013; &#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6DF1;&#x5EA6;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x5143;&#x7EC4;&#x4E2D;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#xFF0C;&#x9AD8;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x5143;&#x7EC4;&#x4E2D;&#x7684;&#x7B2C;&#x4E8C;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#xFF0C;&#x5BBD;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x7B2C;&#x4E09;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#x3002;</li>
</ul>
</blockquote>
<p>Parameters: </p>
<ul>
<li><strong>kernel_size</strong> &#x2013; &#x6700;&#x5927;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x7684;&#x6ED1;&#x52A8;&#x7A97;&#x5927;&#x5C0F;</li>
<li><strong>stride</strong> &#x2013; &#x6ED1;&#x52A8;&#x7A97;&#x7684;&#x6B65;&#x957F;&#xFF0C;&#x9ED8;&#x8BA4;&#x503C;&#x662F; <code>kernel_size</code></li>
<li><strong>padding</strong> &#x2013; &#x8981;&#x5728;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;</li>
<li><strong>dilation</strong> &#x2013; &#x6ED1;&#x52A8;&#x7A97;&#x4E2D;&#x5404;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;</li>
<li><strong>return_indices</strong> &#x2013; &#x5982;&#x679C;&#x6B64;&#x53C2;&#x6570;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>&#xFF0C; &#x90A3;&#x4E48;&#x6B64;&#x6C60;&#x5316;&#x5C42;&#x5728;&#x8FD4;&#x56DE;&#x8F93;&#x51FA;&#x4FE1;&#x53F7;&#x7684;&#x540C;&#x65F6;&#x8FD8;&#x4F1A;&#x8FD4;&#x56DE;&#x4E00;&#x8FDE;&#x4E32;&#x6ED1;&#x52A8;&#x7A97;&#x6700;&#x5927;&#x503C;&#x7684;&#x7D22;&#x5F15;&#x4F4D;&#x7F6E;&#xFF0C;&#x5373;&#x6BCF;&#x4E2A;&#x6ED1;&#x52A8;&#x7A97;&#x7684;&#x6700;&#x5927;&#x503C;&#x4F4D;&#x7F6E;&#x4FE1;&#x606F;&#x3002;&#x8FD9;&#x4E9B;&#x4FE1;&#x606F;&#x53EF;&#x4EE5;&#x5728;&#x540E;&#x9762;&#x7684;&#x4E0A;&#x91C7;&#x6837;<a href="#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code>torch.nn.MaxUnpool3d</code></a>&#x4E2D;&#x88AB;&#x7528;&#x5230;&#x3002;</li>
<li><strong>ceil_mode</strong> &#x2013; &#x5982;&#x679C;&#x6B64;&#x53C2;&#x6570;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;True&#xFF0C;&#x8BA1;&#x7B97;&#x8F93;&#x51FA;&#x4FE1;&#x53F7;&#x5927;&#x5C0F;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4F1A;&#x4F7F;&#x7528;&#x5411;&#x4E0A;&#x53D6;&#x6574;&#xFF0C;&#x4EE3;&#x66FF;&#x9ED8;&#x8BA4;&#x7684;&#x5411;&#x4E0B;&#x53D6;&#x6574;&#x7684;&#x64CD;&#x4F5C;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li><p>&#x8F93;&#x5165;: <img src="img/c187d190013d0785320e3412fe8cd669.jpg" alt=""></p>
</li>
<li><p>&#x8F93;&#x51FA;: <img src="img/41ca4c8d4c65c979d2d643c6f62ea280.jpg" alt="">, &#x5176;&#x4E2D;</p>
<p><img src="img/0e49f319aa911192458f7b02321eff3a.jpg" alt=""></p>
<p><img src="img/b8fbd329439d7eba62abdf0df19f464d.jpg" alt=""></p>
<p><img src="img/eb1d0c30d1cf681f38e8391bd7d03dff.jpg" alt=""></p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of square window of size=3, stride=2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.MaxPool3d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of non-square window</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.MaxPool3d((<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>,<span class="hljs-number">44</span>, <span class="hljs-number">31</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="maxunpool1d">MaxUnpool1d</h3>
<pre><code class="lang-py">class torch.nn.MaxUnpool1d(kernel_size, stride=None, padding=0)
</code></pre>
<p><a href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code>MaxPool1d</code></a>&#x7684;&#x9006;&#x8FC7;&#x7A0B;&#xFF0C;&#x4E0D;&#x8FC7;&#x5E76;&#x4E0D;&#x662F;&#x5B8C;&#x5168;&#x7684;&#x9006;&#x8FC7;&#x7A0B;&#xFF0C;&#x56E0;&#x4E3A;&#x5728;<a href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code>MaxPool1d</code></a>&#x7684;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x6C60;&#x5316;&#x7A97;&#x533A;&#x57DF;&#x5185;&#x7684;&#x975E;&#x6700;&#x5927;&#x503C;&#x90FD;&#x5DF2;&#x7ECF;&#x4E22;&#x5931;&#x3002; <a href="#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code>MaxUnpool1d</code></a>&#x7684;&#x8F93;&#x5165;&#x662F;<a href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code>MaxPool1d</code></a>&#x7684;&#x8F93;&#x51FA;&#xFF0C;&#x5176;&#x4E2D;&#x4E5F;&#x5305;&#x62EC;&#x5305;&#x62EC;&#x6ED1;&#x52A8;&#x7A97;&#x6700;&#x5927;&#x503C;&#x7684;&#x7D22;&#x5F15;&#xFF08;&#x5373;return_indices&#x6240;&#x63A7;&#x5236;&#x7684;&#x8F93;&#x51FA;&#xFF09;&#xFF0C;&#x9006;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x7684;&#x8FC7;&#x7A0B;&#x5C31;&#x662F;&#x5C06;<a href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code>MaxPool1d</code></a>&#x8FC7;&#x7A0B;&#x4E2D;&#x4EA7;&#x751F;&#x7684;&#x6700;&#x5927;&#x503C;&#x63D2;&#x56DE;&#x5230;&#x539F;&#x6765;&#x7684;&#x4F4D;&#x7F6E;&#xFF0C;&#x5E76;&#x5C06;&#x975E;&#x6700;&#x5927;&#x503C;&#x533A;&#x57DF;&#x7F6E;&#x4E3A;0&#x3002;</p>
<p>Note</p>
<p><a href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code>MaxPool1d</code></a>&#x64CD;&#x4F5C;&#x53EF;&#x4EE5;&#x5C06;&#x591A;&#x4E2A;&#x5927;&#x5C0F;&#x4E0D;&#x540C;&#x7684;&#x8F93;&#x5165;&#x6620;&#x5C04;&#x5230;&#x76F8;&#x540C;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x7684;&#x53CD;&#x8FC7;&#x7A0B;&#xFF0C;<a href="#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code>MaxUnpool1d</code></a>&#x7684;&#x4E0A;&#x91C7;&#x6837;&#x8FC7;&#x7A0B;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x5C31;&#x4E0D;&#x552F;&#x4E00;&#x4E86;&#x3002;&#x4E3A;&#x4E86;&#x9002;&#x5E94;&#x8FD9;&#x4E00;&#x70B9;&#xFF0C;&#x53EF;&#x4EE5;&#x5728;&#x8BBE;&#x7F6E;&#x63A7;&#x5236;&#x4E0A;&#x91C7;&#x6837;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x7684;&#xFF08;<code>output_size</code>&#xFF09;&#x53C2;&#x6570;&#x3002; &#x5177;&#x4F53;&#x7528;&#x6CD5;&#xFF0C;&#x8BF7;&#x53C2;&#x9605;&#x4E0B;&#x9762;&#x7684;&#x8F93;&#x5165;&#x548C;&#x793A;&#x4F8B;</p>
<p>Parameters: </p>
<ul>
<li><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x6700;&#x5927;&#x6C60;&#x5316;&#x7A97;&#x7684;&#x5927;&#x5C0F;</li>
<li><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x6700;&#x5927;&#x6C60;&#x5316;&#x7A97;&#x7684;&#x6B65;&#x957F;&#x3002;&#x9ED8;&#x8BA4;<code>kernel_size</code></li>
<li><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;</li>
</ul>
<pre><code class="lang-py">Inputs:
</code></pre>
<ul>
<li><code>input</code>: &#x8981;&#x6267;&#x884C;&#x4E0A;&#x91C7;&#x6837;&#x64CD;&#x4F5C;&#x7684;&#x5F20;&#x91CF;</li>
<li><code>indices</code>: <a href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code>MaxPool1d</code></a>&#x6C60;&#x5316;&#x8FC7;&#x7A0B;&#x4E2D;&#x8F93;&#x51FA;&#x7684;&#x6C60;&#x5316;&#x7A97;&#x6700;&#x5927;&#x503C;&#x7684;&#x4F4D;&#x7F6E;&#x7D22;&#x5F15;</li>
<li><code>output_size</code> (&#x9009;&#x586B;): &#x6307;&#x5B9A;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li><p>&#x8F93;&#x5165;: <img src="img/ccc1792005f1eb97a439118aeba930e9.jpg" alt=""></p>
</li>
<li><p>&#x8F93;&#x51FA;: <img src="img/1b0403b4ee318895368afc8fa37b9407.jpg" alt="">, &#x5176;&#x4E2D;</p>
<p><img src="img/9618fc866026e724d16c5481dd67dc4c.jpg" alt=""></p>
<p>&#x4E5F;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;<code>output_size</code>&#x6307;&#x5B9A;&#x8F93;&#x51FA;&#x7684;&#x5927;&#x5C0F;</p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>pool = nn.MaxPool1d(<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, return_indices=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>unpool = nn.MaxUnpool1d(<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.tensor([[[<span class="hljs-number">1.</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>]]])
<span class="hljs-meta">&gt;&gt;&gt; </span>output, indices = pool(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>unpool(output, indices)
tensor([[[ <span class="hljs-number">0.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">6.</span>,  <span class="hljs-number">0.</span>, <span class="hljs-number">8.</span>]]])

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Example showcasing the use of output_size</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.tensor([[[<span class="hljs-number">1.</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]]])
<span class="hljs-meta">&gt;&gt;&gt; </span>output, indices = pool(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>unpool(output, indices, output_size=input.size())
tensor([[[ <span class="hljs-number">0.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">6.</span>,  <span class="hljs-number">0.</span>, <span class="hljs-number">8.</span>,  <span class="hljs-number">0.</span>]]])

<span class="hljs-meta">&gt;&gt;&gt; </span>unpool(output, indices)
tensor([[[ <span class="hljs-number">0.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">6.</span>,  <span class="hljs-number">0.</span>, <span class="hljs-number">8.</span>]]])
</code></pre>
<h3 id="maxunpool2d">MaxUnpool2d</h3>
<pre><code class="lang-py">class torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0)
</code></pre>
<p><a href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code>MaxPool2d</code></a>&#x7684;&#x9006;&#x8FC7;&#x7A0B;&#xFF0C;&#x4E0D;&#x8FC7;&#x5E76;&#x4E0D;&#x662F;&#x5B8C;&#x5168;&#x7684;&#x9006;&#x8FC7;&#x7A0B;&#xFF0C;&#x56E0;&#x4E3A;&#x5728;<a href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code>MaxPool2d</code></a>&#x7684;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x6C60;&#x5316;&#x7A97;&#x533A;&#x57DF;&#x5185;&#x7684;&#x975E;&#x6700;&#x5927;&#x503C;&#x90FD;&#x5DF2;&#x7ECF;&#x4E22;&#x5931;&#x3002; <a href="#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code>MaxUnpool2d</code></a>&#x7684;&#x8F93;&#x5165;&#x662F;<a href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code>MaxPool2d</code></a>&#x7684;&#x8F93;&#x51FA;&#xFF0C;&#x5176;&#x4E2D;&#x4E5F;&#x5305;&#x62EC;&#x5305;&#x62EC;&#x6ED1;&#x52A8;&#x7A97;&#x6700;&#x5927;&#x503C;&#x7684;&#x7D22;&#x5F15;&#xFF08;&#x5373;return_indices&#x6240;&#x63A7;&#x5236;&#x7684;&#x8F93;&#x51FA;&#xFF09;&#xFF0C;&#x9006;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x7684;&#x8FC7;&#x7A0B;&#x5C31;&#x662F;&#x5C06;<a href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code>MaxPool2d</code></a>&#x8FC7;&#x7A0B;&#x4E2D;&#x4EA7;&#x751F;&#x7684;&#x6700;&#x5927;&#x503C;&#x63D2;&#x56DE;&#x5230;&#x539F;&#x6765;&#x7684;&#x4F4D;&#x7F6E;&#xFF0C;&#x5E76;&#x5C06;&#x975E;&#x6700;&#x5927;&#x503C;&#x533A;&#x57DF;&#x7F6E;&#x4E3A;0&#x3002;</p>
<p>Note</p>
<p><a href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code>MaxPool2d</code></a>&#x64CD;&#x4F5C;&#x53EF;&#x4EE5;&#x5C06;&#x591A;&#x4E2A;&#x5927;&#x5C0F;&#x4E0D;&#x540C;&#x7684;&#x8F93;&#x5165;&#x6620;&#x5C04;&#x5230;&#x76F8;&#x540C;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x7684;&#x53CD;&#x8FC7;&#x7A0B;&#xFF0C;<a href="#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code>MaxUnpool2d</code></a>&#x7684;&#x4E0A;&#x91C7;&#x6837;&#x8FC7;&#x7A0B;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x5C31;&#x4E0D;&#x552F;&#x4E00;&#x4E86;&#x3002;&#x4E3A;&#x4E86;&#x9002;&#x5E94;&#x8FD9;&#x4E00;&#x70B9;&#xFF0C;&#x53EF;&#x4EE5;&#x5728;&#x8BBE;&#x7F6E;&#x63A7;&#x5236;&#x4E0A;&#x91C7;&#x6837;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x7684;&#xFF08;<code>output_size</code>&#xFF09;&#x53C2;&#x6570;&#x3002; &#x5177;&#x4F53;&#x7528;&#x6CD5;&#xFF0C;&#x8BF7;&#x53C2;&#x9605;&#x4E0B;&#x9762;&#x7684;&#x8F93;&#x5165;&#x548C;&#x793A;&#x4F8B;</p>
<p>Parameters: </p>
<ul>
<li><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x6700;&#x5927;&#x6C60;&#x5316;&#x7A97;&#x7684;&#x5927;&#x5C0F;</li>
<li><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x6700;&#x5927;&#x6C60;&#x5316;&#x7A97;&#x7684;&#x6B65;&#x957F;&#x3002;&#x9ED8;&#x8BA4;<code>kernel_size</code></li>
<li><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;</li>
</ul>
<pre><code class="lang-py">Inputs:
</code></pre>
<ul>
<li><code>input</code>: &#x8981;&#x6267;&#x884C;&#x4E0A;&#x91C7;&#x6837;&#x64CD;&#x4F5C;&#x7684;&#x5F20;&#x91CF;</li>
<li><code>indices</code>: <a href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code>MaxPool2d</code></a>&#x6C60;&#x5316;&#x8FC7;&#x7A0B;&#x4E2D;&#x8F93;&#x51FA;&#x7684;&#x6C60;&#x5316;&#x7A97;&#x6700;&#x5927;&#x503C;&#x7684;&#x4F4D;&#x7F6E;&#x7D22;&#x5F15;</li>
<li><code>output_size</code> (&#x9009;&#x586B;): &#x6307;&#x5B9A;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li><p>&#x8F93;&#x5165;: <img src="img/ff71b16eb10237262566c6907acaaf1f.jpg" alt=""></p>
</li>
<li><p>&#x8F93;&#x51FA;: <img src="img/a0ef05f779873fc4dcbf020b1ea14754.jpg" alt="">, &#x5176;&#x4E2D;</p>
<p><img src="img/f6dd707e18ccbf75f607d05338443e87.jpg" alt=""></p>
<p><img src="img/ac5d54ef9922f9e0dbe2dc916bf9d80b.jpg" alt=""></p>
<p>&#x4E5F;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;<code>output_size</code>&#x6307;&#x5B9A;&#x8F93;&#x51FA;&#x7684;&#x5927;&#x5C0F;</p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>pool = nn.MaxPool2d(<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, return_indices=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>unpool = nn.MaxUnpool2d(<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.tensor([[[[ <span class="hljs-number">1.</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>],
 [ <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>],
 [ <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>],
 [<span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>, <span class="hljs-number">16</span>]]]])
<span class="hljs-meta">&gt;&gt;&gt; </span>output, indices = pool(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>unpool(output, indices)
tensor([[[[  <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>],
 [  <span class="hljs-number">0.</span>,   <span class="hljs-number">6.</span>,   <span class="hljs-number">0.</span>,   <span class="hljs-number">8.</span>],
 [  <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>],
 [  <span class="hljs-number">0.</span>,  <span class="hljs-number">14.</span>,   <span class="hljs-number">0.</span>,  <span class="hljs-number">16.</span>]]]])

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># specify a different output size than input size</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>unpool(output, indices, output_size=torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>]))
tensor([[[[  <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>],
 [  <span class="hljs-number">6.</span>,   <span class="hljs-number">0.</span>,   <span class="hljs-number">8.</span>,   <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>],
 [  <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>,  <span class="hljs-number">14.</span>,   <span class="hljs-number">0.</span>],
 [ <span class="hljs-number">16.</span>,   <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>],
 [  <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>,   <span class="hljs-number">0.</span>]]]])
</code></pre>
<h3 id="maxunpool3d">MaxUnpool3d</h3>
<pre><code class="lang-py">class torch.nn.MaxUnpool3d(kernel_size, stride=None, padding=0)
</code></pre>
<p><a href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code>MaxPool3d</code></a>&#x7684;&#x9006;&#x8FC7;&#x7A0B;&#xFF0C;&#x4E0D;&#x8FC7;&#x5E76;&#x4E0D;&#x662F;&#x5B8C;&#x5168;&#x7684;&#x9006;&#x8FC7;&#x7A0B;&#xFF0C;&#x56E0;&#x4E3A;&#x5728;<a href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code>MaxPool3d</code></a>&#x7684;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x6C60;&#x5316;&#x7A97;&#x533A;&#x57DF;&#x5185;&#x7684;&#x975E;&#x6700;&#x5927;&#x503C;&#x90FD;&#x5DF2;&#x7ECF;&#x4E22;&#x5931;&#x3002; <a href="#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code>MaxUnpool3d</code></a>&#x7684;&#x8F93;&#x5165;&#x662F;<a href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code>MaxPool3d</code></a>&#x7684;&#x8F93;&#x51FA;&#xFF0C;&#x5176;&#x4E2D;&#x4E5F;&#x5305;&#x62EC;&#x5305;&#x62EC;&#x6ED1;&#x52A8;&#x7A97;&#x6700;&#x5927;&#x503C;&#x7684;&#x7D22;&#x5F15;&#xFF08;&#x5373;return_indices&#x6240;&#x63A7;&#x5236;&#x7684;&#x8F93;&#x51FA;&#xFF09;&#xFF0C;&#x9006;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x7684;&#x8FC7;&#x7A0B;&#x5C31;&#x662F;&#x5C06;<a href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code>MaxPool3d</code></a>&#x8FC7;&#x7A0B;&#x4E2D;&#x4EA7;&#x751F;&#x7684;&#x6700;&#x5927;&#x503C;&#x63D2;&#x56DE;&#x5230;&#x539F;&#x6765;&#x7684;&#x4F4D;&#x7F6E;&#xFF0C;&#x5E76;&#x5C06;&#x975E;&#x6700;&#x5927;&#x503C;&#x533A;&#x57DF;&#x7F6E;&#x4E3A;0&#x3002;</p>
<p>Note</p>
<p><a href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code>MaxPool3d</code></a>&#x64CD;&#x4F5C;&#x53EF;&#x4EE5;&#x5C06;&#x591A;&#x4E2A;&#x5927;&#x5C0F;&#x4E0D;&#x540C;&#x7684;&#x8F93;&#x5165;&#x6620;&#x5C04;&#x5230;&#x76F8;&#x540C;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x7684;&#x53CD;&#x8FC7;&#x7A0B;&#xFF0C;<a href="#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code>MaxUnpool3d</code></a>&#x7684;&#x4E0A;&#x91C7;&#x6837;&#x8FC7;&#x7A0B;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x5C31;&#x4E0D;&#x552F;&#x4E00;&#x4E86;&#x3002;&#x4E3A;&#x4E86;&#x9002;&#x5E94;&#x8FD9;&#x4E00;&#x70B9;&#xFF0C;&#x53EF;&#x4EE5;&#x5728;&#x8BBE;&#x7F6E;&#x63A7;&#x5236;&#x4E0A;&#x91C7;&#x6837;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x7684;&#xFF08;<code>output_size</code>&#xFF09;&#x53C2;&#x6570;&#x3002; &#x5177;&#x4F53;&#x7528;&#x6CD5;&#xFF0C;&#x8BF7;&#x53C2;&#x9605;&#x4E0B;&#x9762;&#x7684;&#x8F93;&#x5165;&#x548C;&#x793A;&#x4F8B;</p>
<p>Parameters: </p>
<ul>
<li><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x6700;&#x5927;&#x6C60;&#x5316;&#x7A97;&#x7684;&#x5927;&#x5C0F;</li>
<li><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x6700;&#x5927;&#x6C60;&#x5316;&#x7A97;&#x7684;&#x6B65;&#x957F;&#x3002;&#x9ED8;&#x8BA4;<code>kernel_size</code></li>
<li><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;</li>
</ul>
<pre><code class="lang-py">Inputs:
</code></pre>
<ul>
<li><code>input</code>: &#x8981;&#x6267;&#x884C;&#x4E0A;&#x91C7;&#x6837;&#x64CD;&#x4F5C;&#x7684;&#x5F20;&#x91CF;</li>
<li><code>indices</code>: <a href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code>MaxPool3d</code></a>&#x6C60;&#x5316;&#x8FC7;&#x7A0B;&#x4E2D;&#x8F93;&#x51FA;&#x7684;&#x6C60;&#x5316;&#x7A97;&#x6700;&#x5927;&#x503C;&#x7684;&#x4F4D;&#x7F6E;&#x7D22;&#x5F15;</li>
<li><code>output_size</code> (&#x9009;&#x586B;): &#x6307;&#x5B9A;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li><p>&#x8F93;&#x5165;: <img src="img/c187d190013d0785320e3412fe8cd669.jpg" alt=""></p>
</li>
<li><p>&#x8F93;&#x51FA;: <img src="img/41ca4c8d4c65c979d2d643c6f62ea280.jpg" alt="">, &#x5176;&#x4E2D;</p>
<p><img src="img/190cbccb4ab554a9b19bfc3df956f982.jpg" alt=""></p>
<p><img src="img/785a4e892f32eee65446b4e269fc452b.jpg" alt=""></p>
<p><img src="img/e666e9d78ffab5d03b4cf1adf1a6e331.jpg" alt=""></p>
<p>&#x4E5F;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;<code>output_size</code>&#x6307;&#x5B9A;&#x8F93;&#x51FA;&#x7684;&#x5927;&#x5C0F;</p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of square window of size=3, stride=2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pool = nn.MaxPool3d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, return_indices=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>unpool = nn.MaxUnpool3d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output, indices = pool(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">51</span>, <span class="hljs-number">33</span>, <span class="hljs-number">15</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>unpooled_output = unpool(output, indices)
<span class="hljs-meta">&gt;&gt;&gt; </span>unpooled_output.size()
torch.Size([<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">51</span>, <span class="hljs-number">33</span>, <span class="hljs-number">15</span>])
</code></pre>
<h3 id="avgpool1d">AvgPool1d</h3>
<pre><code class="lang-py">class torch.nn.AvgPool1d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)
</code></pre>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4FE1;&#x53F7;&#x6267;&#x884C;&#x4E00;&#x7EF4;&#x5E73;&#x5747;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x6700;&#x7B80;&#x5355;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5BF9;&#x4E8E;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x4E3A; <img src="img/5816e96aa78b7425cf792435bba8bc29.jpg" alt=""> &#xFF0C;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x4E3A;<img src="img/d131773750846713475c600aa8cd917a.jpg" alt="">&#xFF0C;<code>kernel_size</code>&#x4E3A;<img src="img/a1c2f8d5b1226e67bdb44b12a6ddf18b.jpg" alt="">&#x7684;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#xFF0C;&#x6B64;&#x6C60;&#x5316;&#x8FC7;&#x7A0B;&#x53EF;&#x8868;&#x8FF0;&#x5982;&#x4E0B;&#xFF1A;</p>
<p><img src="img/5df0036df168f4a16d4437d91968f640.jpg" alt=""></p>
<p><code>padding</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x8981;&#x5728;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x3002;</p>
<p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code> &#x7B49;&#x53C2;&#x6570;&#x5747;&#x652F;&#x6301;&#x8F93;&#x5165;&#x4E00;&#x4E2A;int&#x6216;&#x8005;&#x7531;&#x4E00;&#x4E2A;int&#x7EC4;&#x6210;&#x7684;tuple&#x3002;</p>
<p>Parameters: </p>
<ul>
<li><strong>kernel_size</strong> &#x2013; &#x5E73;&#x5747;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x7684;&#x6ED1;&#x52A8;&#x7A97;&#x5927;&#x5C0F;</li>
<li><strong>stride</strong> &#x2013; &#x6ED1;&#x52A8;&#x7A97;&#x7684;&#x6B65;&#x957F;&#xFF0C;&#x9ED8;&#x8BA4;&#x503C;&#x662F; <code>kernel_size</code></li>
<li><strong>padding</strong> &#x2013; &#x8981;&#x5728;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;</li>
<li><strong>ceil_mode</strong> &#x2013; &#x5982;&#x679C;&#x6B64;&#x53C2;&#x6570;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;True&#xFF0C;&#x8BA1;&#x7B97;&#x8F93;&#x51FA;&#x4FE1;&#x53F7;&#x5927;&#x5C0F;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4F1A;&#x4F7F;&#x7528;&#x5411;&#x4E0A;&#x53D6;&#x6574;&#xFF0C;&#x4EE3;&#x66FF;&#x9ED8;&#x8BA4;&#x7684;&#x5411;&#x4E0B;&#x53D6;&#x6574;&#x7684;&#x64CD;&#x4F5C;</li>
<li><strong>count_include_pad</strong> &#x2013; &#x5982;&#x679C;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;True, &#x90A3;&#x4E48;&#x5728;&#x8FDB;&#x884C;&#x5E73;&#x5747;&#x8FD0;&#x7B97;&#x7684;&#x65F6;&#x5019;&#x4E5F;&#x4F1A;&#x5C06;&#x7528;&#x4E8E;&#x8865;&#x9F50;&#x7684;0&#x52A0;&#x5165;&#x8FD0;&#x7B97;&#x3002;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li><p>&#x8F93;&#x5165;: <img src="img/3ceb415a2a1558bab9998c277f780ec3.jpg" alt=""></p>
</li>
<li><p>&#x8F93;&#x51FA;: <img src="img/d131773750846713475c600aa8cd917a.jpg" alt="">, &#x5176;&#x4E2D;</p>
<p><img src="img/ad61a9298a545292682229fef2f1a910.jpg" alt=""></p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool with window of size=3, stride=2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AvgPool1d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>m(torch.tensor([[[<span class="hljs-number">1.</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>]]]))
tensor([[[ <span class="hljs-number">2.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">6.</span>]]])
</code></pre>
<h3 id="avgpool2d">AvgPool2d</h3>
<pre><code class="lang-py">class torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)
</code></pre>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4FE1;&#x53F7;&#x6267;&#x884C;&#x4E8C;&#x7EF4;&#x5E73;&#x5747;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x3002;
&#x6700;&#x7B80;&#x5355;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5BF9;&#x4E8E;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x4E3A;  <img src="img/23f8772594b27bd387be708fe9c085e1.jpg" alt=""> &#xFF0C;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x4E3A;<img src="img/a0ef05f779873fc4dcbf020b1ea14754.jpg" alt="">&#xFF0C;<code>kernel_size</code>&#x4E3A;<img src="img/6384e001ad4c0989683deb86f6ffbd2f.jpg" alt="">&#x7684;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#xFF0C;&#x6B64;&#x6C60;&#x5316;&#x8FC7;&#x7A0B;&#x53EF;&#x8868;&#x8FF0;&#x5982;&#x4E0B;&#xFF1A;</p>
<p><img src="img/b7a0e1d0a42a3626724c14d89a10a44f.jpg" alt=""></p>
<p><code>padding</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x8981;&#x5728;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x3002;</p>
<p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>&#x7B49;&#x53C2;&#x6570;&#x5747;&#x652F;&#x6301;&#x4EE5;&#x4E0B;&#x7C7B;&#x578B;&#x8F93;&#x5165;&#xFF1A;</p>
<blockquote>
<ul>
<li>&#x4E00;&#x4E2A;&#x5355;&#x72EC;&#x7684; <code>int</code> &#x2013; &#x6B64;&#x65F6;&#x8FD9;&#x4E2A;<code>int</code>&#x4F1A;&#x540C;&#x65F6;&#x63A7;&#x5236;&#x6C60;&#x5316;&#x6ED1;&#x52A8;&#x7A97;&#x7684;&#x5BBD;&#x548C;&#x9AD8;&#x8FD9;&#x4E24;&#x4E2A;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;</li>
<li>&#x4E00;&#x4E2A;&#x7531;&#x4E24;&#x4E2A;<code>int</code>&#x7EC4;&#x6210;&#x7684;<code>tuple</code> &#x2013; &#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x9AD8;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x5143;&#x7EC4;&#x4E2D;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#xFF0C;&#x5BBD;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x7B2C;&#x4E8C;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#x3002;</li>
</ul>
</blockquote>
<p>Parameters: </p>
<ul>
<li><strong>kernel_size</strong> &#x2013; &#x5E73;&#x5747;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x7684;&#x6ED1;&#x52A8;&#x7A97;&#x5927;&#x5C0F;</li>
<li><strong>stride</strong> &#x2013; &#x6ED1;&#x52A8;&#x7A97;&#x7684;&#x6B65;&#x957F;&#xFF0C;&#x9ED8;&#x8BA4;&#x503C;&#x662F; <code>kernel_size</code></li>
<li><strong>padding</strong> &#x2013; &#x8981;&#x5728;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;</li>
<li><strong>ceil_mode</strong> &#x2013; &#x5982;&#x679C;&#x6B64;&#x53C2;&#x6570;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;True&#xFF0C;&#x8BA1;&#x7B97;&#x8F93;&#x51FA;&#x4FE1;&#x53F7;&#x5927;&#x5C0F;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4F1A;&#x4F7F;&#x7528;&#x5411;&#x4E0A;&#x53D6;&#x6574;&#xFF0C;&#x4EE3;&#x66FF;&#x9ED8;&#x8BA4;&#x7684;&#x5411;&#x4E0B;&#x53D6;&#x6574;&#x7684;&#x64CD;&#x4F5C;</li>
<li><strong>count_include_pad</strong> &#x2013; &#x5982;&#x679C;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;True, &#x90A3;&#x4E48;&#x5728;&#x8FDB;&#x884C;&#x5E73;&#x5747;&#x8FD0;&#x7B97;&#x7684;&#x65F6;&#x5019;&#x4E5F;&#x4F1A;&#x5C06;&#x7528;&#x4E8E;&#x8865;&#x9F50;&#x7684;0&#x52A0;&#x5165;&#x8FD0;&#x7B97;&#x3002;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li><p>&#x8F93;&#x5165;: <img src="img/ff71b16eb10237262566c6907acaaf1f.jpg" alt=""></p>
</li>
<li><p>&#x8F93;&#x51FA;: <img src="img/a0ef05f779873fc4dcbf020b1ea14754.jpg" alt="">, &#x5176;&#x4E2D;</p>
<p><img src="img/8b8b2b1a77c4f104a936efb1708366ef.jpg" alt=""></p>
<p><img src="img/2792347200dabe493ae8baee428f9bf8.jpg" alt=""></p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of square window of size=3, stride=2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AvgPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of non-square window</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AvgPool2d((<span class="hljs-number">3</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>, <span class="hljs-number">32</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="avgpool3d">AvgPool3d</h3>
<pre><code class="lang-py">class torch.nn.AvgPool3d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)
</code></pre>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4FE1;&#x53F7;&#x6267;&#x884C;&#x4E09;&#x7EF4;&#x5E73;&#x5747;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x3002;
&#x6700;&#x7B80;&#x5355;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5BF9;&#x4E8E;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x4E3A;<img src="img/f5a45f7b445db562b21cfcb525637aab.jpg" alt="">&#xFF0C;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x4E3A;<img src="img/41ca4c8d4c65c979d2d643c6f62ea280.jpg" alt="">&#xFF0C;<code>kernel_size</code>&#x4E3A;<img src="img/f5dcdebf9a81b9d15227749ae7535eb7.jpg" alt="">&#x7684;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#xFF0C;&#x6B64;&#x6C60;&#x5316;&#x8FC7;&#x7A0B;&#x53EF;&#x8868;&#x8FF0;&#x5982;&#x4E0B;&#xFF1A;</p>
<p><img src="img/79acedd31cd18baac8d97ab96a7092e0.jpg" alt=""></p>
<p><code>padding</code> &#x53C2;&#x6570;&#x63A7;&#x5236;&#x4E86;&#x8981;&#x5728;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;&#x3002;</p>
<p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>&#x7B49;&#x53C2;&#x6570;&#x5747;&#x652F;&#x6301;&#x4EE5;&#x4E0B;&#x7C7B;&#x578B;&#x8F93;&#x5165;&#xFF1A;</p>
<blockquote>
<ul>
<li>&#x4E00;&#x4E2A;&#x5355;&#x72EC;&#x7684; <code>int</code> &#x2013; &#x6B64;&#x65F6;&#x8FD9;&#x4E2A;<code>int</code>&#x4F1A;&#x540C;&#x65F6;&#x63A7;&#x5236;&#x6C60;&#x5316;&#x6ED1;&#x52A8;&#x7A97;&#x7684;&#x6DF1;&#x5EA6;&#xFF0C;&#x5BBD;&#x548C;&#x9AD8;&#x8FD9;&#x4E24;&#x4E2A;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;</li>
<li>&#x4E00;&#x4E2A;&#x7531;&#x4E09;&#x4E2A;<code>int</code>&#x7EC4;&#x6210;&#x7684;<code>tuple</code> &#x2013; &#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6DF1;&#x5EA6;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x5143;&#x7EC4;&#x4E2D;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#xFF0C;&#x9AD8;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x5143;&#x7EC4;&#x4E2D;&#x7684;&#x7B2C;&#x4E8C;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#xFF0C;&#x5BBD;&#x8FD9;&#x4E00;&#x7EF4;&#x5EA6;&#x4F1A;&#x91C7;&#x7528;&#x7B2C;&#x4E09;&#x4E2A;<code>int</code>&#x6570;&#x5B57;&#x3002;</li>
</ul>
</blockquote>
<p>Parameters: </p>
<ul>
<li><strong>kernel_size</strong> &#x2013; &#x5E73;&#x5747;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x7684;&#x6ED1;&#x52A8;&#x7A97;&#x5927;&#x5C0F;</li>
<li><strong>stride</strong> &#x2013; &#x6ED1;&#x52A8;&#x7A97;&#x7684;&#x6B65;&#x957F;&#xFF0C;&#x9ED8;&#x8BA4;&#x503C;&#x662F; <code>kernel_size</code></li>
<li><strong>padding</strong> &#x2013; &#x8981;&#x5728;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5404;&#x7EF4;&#x5EA6;&#x5404;&#x8FB9;&#x4E0A;&#x8981;&#x8865;&#x9F50;0&#x7684;&#x5C42;&#x6570;</li>
<li><strong>ceil_mode</strong> &#x2013; &#x5982;&#x679C;&#x6B64;&#x53C2;&#x6570;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;True&#xFF0C;&#x8BA1;&#x7B97;&#x8F93;&#x51FA;&#x4FE1;&#x53F7;&#x5927;&#x5C0F;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4F1A;&#x4F7F;&#x7528;&#x5411;&#x4E0A;&#x53D6;&#x6574;&#xFF0C;&#x4EE3;&#x66FF;&#x9ED8;&#x8BA4;&#x7684;&#x5411;&#x4E0B;&#x53D6;&#x6574;&#x7684;&#x64CD;&#x4F5C;</li>
<li><strong>count_include_pad</strong> &#x2013; &#x5982;&#x679C;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;True, &#x90A3;&#x4E48;&#x5728;&#x8FDB;&#x884C;&#x5E73;&#x5747;&#x8FD0;&#x7B97;&#x7684;&#x65F6;&#x5019;&#x4E5F;&#x4F1A;&#x5C06;&#x7528;&#x4E8E;&#x8865;&#x9F50;&#x7684;0&#x52A0;&#x5165;&#x8FD0;&#x7B97;&#x3002;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li><p>&#x8F93;&#x5165;: <img src="img/c187d190013d0785320e3412fe8cd669.jpg" alt=""></p>
</li>
<li><p>&#x8F93;&#x51FA;: <img src="img/41ca4c8d4c65c979d2d643c6f62ea280.jpg" alt="">, &#x5176;&#x4E2D;</p>
<p><img src="img/443035401ce7a1144122a862f34493cf.jpg" alt=""></p>
<p><img src="img/8488a299a75cb56e138e1dc5a24a10db.jpg" alt=""></p>
<p><img src="img/c799c115b670c02d039f828fe1afa443.jpg" alt=""></p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of square window of size=3, stride=2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AvgPool3d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of non-square window</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AvgPool3d((<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>,<span class="hljs-number">44</span>, <span class="hljs-number">31</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="fractionalmaxpool2d">FractionalMaxPool2d</h3>
<pre><code class="lang-py">class torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)
</code></pre>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4FE1;&#x53F7;&#x6267;&#x884C;&#x5C0F;&#x6570;&#x7EA7;&#x4E8C;&#x7EF4;&#x6700;&#x5927;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x3002;<code>&#x5C0F;&#x6570;&#x7EA7;</code>&#x6307;&#x7684;&#x662F;&#x6B64;&#x64CD;&#x4F5C;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x4E0E;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x6210;&#x6307;&#x5B9A;&#x7684;&#x5C0F;&#x6570;&#x500D;&#x6570;&#x5173;&#x7CFB;&#x3002;</p>
<p>Ben Graham&#x7684;&#x8FD9;&#x7BC7;&#x6587;&#x7AE0;<a href="http://arxiv.org/abs/1412.6071" target="_blank">Fractional MaxPooling</a>&#x4E2D;&#x8BE6;&#x7EC6;&#x5730;&#x4ECB;&#x7ECD;&#x4E86;&#x5C0F;&#x6570;&#x7EA7;&#x4E8C;&#x7EF4;&#x6700;&#x5927;&#x6C60;&#x5316;&#x7684;&#x57FA;&#x672C;&#x601D;&#x60F3;&#x548C;&#x6280;&#x672F;&#x7EC6;&#x8282;&#x3002;</p>
<p>&#x5C0F;&#x6570;&#x7EA7;&#x4E8C;&#x7EF4;&#x6700;&#x5927;&#x6C60;&#x5316;&#x7684;&#x57FA;&#x672C;&#x601D;&#x60F3;&#x5C31;&#x662F;&#x5C06;&#x6700;&#x5927;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x5E94;&#x7528;&#x4E8E;<img src="img/52ec12db6613ee8a0f6f41143ab2e8a2.jpg" alt="">&#x4E2A;&#x7531;&#x968F;&#x673A;&#x6B65;&#x957F;&#x5927;&#x5C0F;&#x91C7;&#x96C6;&#x7684;&#x533A;&#x57DF;&#x4E2D;&#xFF0C;&#x8FD9;&#x4E9B;&#x6B65;&#x957F;&#x5927;&#x5C0F;&#x662F;&#x7531;&#x8F93;&#x51FA;&#x76EE;&#x6807;&#x7684;&#x5927;&#x5C0F;&#x51B3;&#x5B9A;&#x7684;&#x3002;&#x5C0F;&#x6570;&#x7EA7;&#x4E8C;&#x7EF4;&#x6700;&#x5927;&#x6C60;&#x5316;&#x7684;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#x7684;&#x6570;&#x91CF;&#x7B49;&#x4E8E;&#x8F93;&#x5165;&#x901A;&#x9053;&#x7684;&#x6570;&#x91CF;&#x3002;</p>
<p>Parameters: </p>
<ul>
<li><strong>kernel_size</strong> &#x2013; &#x6267;&#x884C;&#x6700;&#x5927;&#x64CD;&#x4F5C;&#x7684;&#x7A97;&#x53E3;&#x5927;&#x5C0F;&#x3002;&#x652F;&#x6301;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x5305;&#x62EC;&#x4E00;&#x4E2A;&#x5355;&#x72EC;&#x7684;&#x6570;&#x5B57;k(&#x751F;&#x6210;&#x4E00;&#x4E2A;&#x5927;&#x5C0F;&#x4E3A;k x k&#x7684;&#x6B63;&#x65B9;&#x5F62;kernal)&#xFF0C;&#x6216;&#x8005;&#x4E00;&#x4E2A;&#x5143;&#x7EC4; <code>(kh x kw)</code></li>
<li><strong>output_size</strong> &#x2013; &#x6C60;&#x5316;&#x8F93;&#x51FA;&#x76EE;&#x6807;&#x5927;&#x5C0F;&#xFF0C;&#x5177;&#x4F53;&#x5F62;&#x5F0F;&#x662F; <code>oH x oW</code>&#x3002;&#x652F;&#x6301;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x5305;&#x62EC;&#x4E00;&#x4E2A;&#x5355;&#x72EC;&#x7684;&#x6570;&#x5B57;<code>oH</code>&#xFF0C;&#x6216;&#x8005;&#x4E00;&#x4E2A;&#x5143;&#x7EC4; <code>(oH, oW)</code>&#xFF0C;&#x6CE8;&#x610F;&#x6B64;&#x5904;<code>oH x oW</code>&#x4E0E;<code>kernal_size</code>&#x4E2D;&#x7684;<code>kh x ow</code>&#x76F8;&#x547C;&#x5E94;&#xFF0C;&#x4E24;&#x8005;&#x6210;&#x4E00;&#x5B9A;&#x7684;&#x5C0F;&#x6570;&#x7EA7;&#x500D;&#x6570;&#x5173;&#x7CFB;</li>
<li><strong>output_ratio</strong> &#x2013; &#x5982;&#x679C;&#x60F3;&#x8BA9;&#x8F93;&#x51FA;&#x76EE;&#x6807;&#x7684;&#x5927;&#x5C0F;&#x662F;&#x8F93;&#x5165;&#x76EE;&#x6807;&#x5927;&#x5C0F;&#x7684;ratio&#x500D;&#xFF0C;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x8BBE;&#x7F6E;&#x6B64;&#x53C2;&#x6570;&#x6765;&#x5B9E;&#x73B0;&#x3002;&#x6B64;&#x53C2;&#x6570;&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;&#x5C0F;&#x6570;&#x6570;&#x5B57;&#x6216;&#x8005;&#x5C0F;&#x6570;&#x5143;&#x7EC4;&#xFF0C;&#x6570;&#x5B57;&#x8303;&#x56F4;&#x662F;(0, 1)</li>
<li><strong>return_indices</strong> &#x2013; &#x5982;&#x679C;&#x6B64;&#x53C2;&#x6570;&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>, &#x90A3;&#x4E48;&#x5728;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x7ED3;&#x675F;&#x540E;&#xFF0C;&#x8FD4;&#x56DE;&#x6C60;&#x5316;&#x8F93;&#x51FA;&#x7ED3;&#x679C;&#x7684;&#x540C;&#x65F6;&#x4E5F;&#x4F1A;&#x8FD4;&#x56DE;&#x6BCF;&#x4E2A;&#x6C60;&#x5316;&#x533A;&#x57DF;&#x4E2D;&#xFF0C;&#x6700;&#x5927;&#x503C;&#x7684;&#x4F4D;&#x7F6E;&#x4FE1;&#x606F;&#x3002;&#x8FD9;&#x4E9B;&#x4FE1;&#x606F;&#x5728;<code>nn.MaxUnpool2d()</code>&#x53EF;&#x4EE5;&#x88AB;&#x7528;&#x5230;&#x3002;&#x6B64;&#x53C2;&#x6570;&#x9ED8;&#x8BA4;&#x4E3A;<code>False</code></li>
</ul>
<p>&#x4F8B;&#x5B50;</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of square window of size=3, and target output size 13x12</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.FractionalMaxPool2d(<span class="hljs-number">3</span>, output_size=(<span class="hljs-number">13</span>, <span class="hljs-number">12</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of square window and target output size being half of input image size</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.FractionalMaxPool2d(<span class="hljs-number">3</span>, output_ratio=(<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>, <span class="hljs-number">32</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="lppool1d">LPPool1d</h3>
<pre><code class="lang-py">class torch.nn.LPPool1d(norm_type, kernel_size, stride=None, ceil_mode=False)
</code></pre>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4FE1;&#x53F7;&#x6267;&#x884C;&#x4E00;&#x7EF4;&#x5E42;&#x5E73;&#x5747;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x5BF9;&#x4E8E;&#x6BCF;&#x4E2A;&#x6C60;&#x5316;&#x7A97;&#x53E3;&#xFF0C;&#x6B64;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x7684;&#x8BA1;&#x7B97;&#x65B9;&#x5F0F;&#x5982;&#x4E0B;&#xFF1A;</p>
<p><img src="img/e4451f809255881ee286970ddf3fb377.jpg" alt=""></p>
<ul>
<li>&#x5F53;p&#x4E3A;&#x65E0;&#x7A77;&#x5927;&#x7684;&#x65F6;&#x5019;&#x65F6;&#xFF0C;&#x7B49;&#x4EF7;&#x4E8E;&#x6700;&#x5927;&#x6C60;&#x5316;&#x64CD;&#x4F5C;</li>
<li>&#x5F53;<code>p=1</code>&#x65F6;&#xFF0C;&#x7B49;&#x4EF7;&#x4E8E;&#x6C42;&#x548C;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#xFF08;&#x4E00;&#x5B9A;&#x7A0B;&#x5EA6;&#x4E0A;&#x7B49;&#x4EF7;&#x4E8E;&#x5E73;&#x5747;&#x6C60;&#x5316;&#xFF09;</li>
</ul>
<p>Note</p>
<p>&#x5982;&#x679C;&#x67D0;&#x4E2A;&#x7279;&#x6B8A;&#x7684;&#x8F93;&#x5165;&#x5BFC;&#x81F4;&#x8FD9;&#x4E2A;&#x8F93;&#x5165;&#x5173;&#x4E8E;&#x5E42;&#x6307;&#x6570;<code>p</code>&#x7684;&#x6C42;&#x548C;&#x662F;0&#xFF0C;&#x90A3;&#x4E0A;&#x8FF0;&#x6C60;&#x5316;&#x51FD;&#x6570;&#x5728;&#x8FD9;&#x4E00;&#x70B9;&#x662F;&#x6CA1;&#x6709;&#x610F;&#x4E49;&#x7684;&#x3002;&#x5728;&#x5B9E;&#x9645;&#x5B9E;&#x73B0;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x6B64;&#x70B9;&#x7684;&#x68AF;&#x5EA6;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;0&#x3002;</p>
<p>Parameters: </p>
<ul>
<li>kernel_size: &#x6C60;&#x5316;&#x7A97;&#x53E3;&#x7684;&#x5927;&#x5C0F;</li>
<li>stride&#xFF1A;&#x6C60;&#x5316;&#x7A97;&#x53E3;&#x79FB;&#x52A8;&#x7684;&#x6B65;&#x957F;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#x662F;<code>kernel_size</code></li>
<li>ceil_mode: &#x5F53;&#x6B64;&#x53C2;&#x6570;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>&#x65F6;&#xFF0C;&#x5728;&#x8BA1;&#x7B97;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x7684;&#x65F6;&#x5019;&#x5C06;&#x4F7F;&#x7528;&#x5411;&#x4E0B;&#x53D6;&#x6574;&#x4EE3;&#x66FF;&#x5411;&#x4E0A;&#x53D6;&#x6574;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li><p>&#x8F93;&#x5165;: <img src="img/3ceb415a2a1558bab9998c277f780ec3.jpg" alt=""></p>
</li>
<li><p>&#x8F93;&#x51FA;: <img src="img/d131773750846713475c600aa8cd917a.jpg" alt="">&#xFF0C;&#x5176;&#x4E2D;</p>
<p><img src="img/5d246e9891509c48081bc89191e64418.jpg" alt=""></p>
</li>
</ul>
<pre><code class="lang-py">&#x4F8B;&#x5B50;:
</code></pre>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># power-2 pool of window of length 3, with stride 2.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.LPPool1d(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="lppool2d">LPPool2d</h3>
<pre><code class="lang-py">class torch.nn.LPPool2d(norm_type, kernel_size, stride=None, ceil_mode=False)
</code></pre>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4FE1;&#x53F7;&#x6267;&#x884C;&#x4E8C;&#x7EF4;&#x5E42;&#x5E73;&#x5747;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x5BF9;&#x4E8E;&#x6BCF;&#x4E2A;&#x6C60;&#x5316;&#x7A97;&#x53E3;&#xFF0C;&#x6B64;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x7684;&#x8BA1;&#x7B97;&#x65B9;&#x5F0F;&#x5982;&#x4E0B;&#xFF1A;</p>
<p><img src="img/e4451f809255881ee286970ddf3fb377.jpg" alt=""></p>
<ul>
<li>&#x5F53;p&#x7B49;&#x4E8E;<img src="img/b0c1b8fa38555e0b1ca3265b84bb3974.jpg" alt="">&#x65F6;&#x5019;&#x65F6;&#xFF0C;&#x7B49;&#x4EF7;&#x4E8E;&#x6700;&#x5927;&#x6C60;&#x5316;&#x64CD;&#x4F5C;</li>
<li>&#x5F53;<code>p=1</code>&#x65F6;&#xFF0C;&#x7B49;&#x4EF7;&#x4E8E;&#x6C42;&#x548C;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#xFF08;&#x4E00;&#x5B9A;&#x7A0B;&#x5EA6;&#x4E0A;&#x7B49;&#x4EF7;&#x4E8E;&#x5E73;&#x5747;&#x6C60;&#x5316;&#xFF09;</li>
</ul>
<p>&#x53C2;&#x6570;<code>kernel_size</code>, <code>stride</code>&#x652F;&#x6301;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#xFF1A;</p>
<ul>
<li><code>int</code>&#xFF0C;&#x6C60;&#x5316;&#x7A97;&#x53E3;&#x7684;&#x5BBD;&#x548C;&#x9AD8;&#x76F8;&#x7B49;</li>
<li><code>tuple</code>&#x6570;&#x7EC4;&#xFF08;&#x4E24;&#x4E2A;&#x6570;&#x5B57;&#x7684;&#xFF09;&#xFF0C;&#x7B2C;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x662F;&#x6C60;&#x5316;&#x7A97;&#x53E3;&#x7684;&#x9AD8;&#xFF0C;&#x7B2C;&#x4E8C;&#x4E2A;&#x662F;&#x5BBD;</li>
</ul>
<p>Note</p>
<p>&#x5982;&#x679C;&#x67D0;&#x4E2A;&#x7279;&#x6B8A;&#x7684;&#x8F93;&#x5165;&#x5BFC;&#x81F4;&#x8FD9;&#x4E2A;&#x8F93;&#x5165;&#x5173;&#x4E8E;&#x5E42;&#x6307;&#x6570;<code>p</code>&#x7684;&#x6C42;&#x548C;&#x662F;0&#xFF0C;&#x90A3;&#x4E0A;&#x8FF0;&#x6C60;&#x5316;&#x51FD;&#x6570;&#x5728;&#x8FD9;&#x4E00;&#x70B9;&#x662F;&#x6CA1;&#x6709;&#x610F;&#x4E49;&#x7684;&#x3002;&#x5728;&#x5B9E;&#x9645;&#x5B9E;&#x73B0;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x6B64;&#x70B9;&#x7684;&#x68AF;&#x5EA6;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;0&#x3002;</p>
<p>Parameters: </p>
<ul>
<li>kernel_size: &#x6C60;&#x5316;&#x7A97;&#x53E3;&#x7684;&#x5927;&#x5C0F;</li>
<li>stride&#xFF1A;&#x6C60;&#x5316;&#x7A97;&#x53E3;&#x79FB;&#x52A8;&#x7684;&#x6B65;&#x957F;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#x662F;<code>kernel_size</code></li>
<li>ceil_mode: &#x5F53;&#x6B64;&#x53C2;&#x6570;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>&#x65F6;&#xFF0C;&#x5728;&#x8BA1;&#x7B97;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x7684;&#x65F6;&#x5019;&#x5C06;&#x4F7F;&#x7528;&#x5411;&#x4E0B;&#x53D6;&#x6574;&#x4EE3;&#x66FF;&#x5411;&#x4E0A;&#x53D6;&#x6574;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li><p>&#x8F93;&#x5165;: <img src="img/ff71b16eb10237262566c6907acaaf1f.jpg" alt=""></p>
</li>
<li><p>&#x8F93;&#x51FA;: <img src="img/a0ef05f779873fc4dcbf020b1ea14754.jpg" alt="">, &#x5176;&#x4E2D;</p>
<p><img src="img/44bfdaa5e6b603085c2da3eddb558556.jpg" alt=""></p>
<p><img src="img/d59b07475dea090e5f7110600d8f8bdc.jpg" alt=""></p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># power-2 pool of square window of size=3, stride=2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.LPPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of non-square window of power 1.2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.LPPool2d(<span class="hljs-number">1.2</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>, <span class="hljs-number">32</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="adaptivemaxpool1d">AdaptiveMaxPool1d</h3>
<pre><code class="lang-py">class torch.nn.AdaptiveMaxPool1d(output_size, return_indices=False)
</code></pre>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4FE1;&#x53F7;&#x8FDB;&#x884C;1&#x7EF4;&#x7684;&#x81EA;&#x9002;&#x5E94;&#x6700;&#x5927;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x6B64;&#x6C60;&#x5316;&#x5C42;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x6307;&#x5B9A;&#x8F93;&#x51FA;&#x5927;&#x5C0F;H&#xFF0C;&#x5C06;&#x4EFB;&#x610F;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x7684;&#x8F93;&#x5165;&#x5F3A;&#x884C;&#x7684;&#x6C60;&#x5316;&#x5230;&#x6307;&#x5B9A;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x3002;&#x4E0D;&#x8FC7;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#x7684;&#x901A;&#x9053;&#x6570;&#x4E0D;&#x4F1A;&#x53D8;&#x5316;&#x3002;</p>
<p>Parameters: </p>
<ul>
<li><strong>output_size</strong> &#x2013; &#x6307;&#x5B9A;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;H</li>
<li><strong>return_indices</strong> &#x2013; &#x5982;&#x679C;&#x6B64;&#x53C2;&#x6570;&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>, &#x90A3;&#x4E48;&#x5728;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x7ED3;&#x675F;&#x540E;&#xFF0C;&#x8FD4;&#x56DE;&#x6C60;&#x5316;&#x8F93;&#x51FA;&#x7ED3;&#x679C;&#x7684;&#x540C;&#x65F6;&#x4E5F;&#x4F1A;&#x8FD4;&#x56DE;&#x6BCF;&#x4E2A;&#x6C60;&#x5316;&#x533A;&#x57DF;&#x4E2D;&#xFF0C;&#x6700;&#x5927;&#x503C;&#x7684;&#x4F4D;&#x7F6E;&#x4FE1;&#x606F;&#x3002;&#x8FD9;&#x4E9B;&#x4FE1;&#x606F;&#x5728;<code>nn.MaxUnpool1d()</code>&#x53EF;&#x4EE5;&#x88AB;&#x7528;&#x5230;&#x3002;&#x6B64;&#x53C2;&#x6570;&#x9ED8;&#x8BA4;&#x4E3A;<code>False</code></li>
</ul>
<p>&#x4F8B;&#x5B50;</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 5</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveMaxPool1d(<span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="adaptivemaxpool2d">AdaptiveMaxPool2d</h3>
<pre><code class="lang-py">class torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False)
</code></pre>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4FE1;&#x53F7;&#x8FDB;&#x884C;2&#x7EF4;&#x7684;&#x81EA;&#x9002;&#x5E94;&#x6700;&#x5927;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x6B64;&#x6C60;&#x5316;&#x5C42;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x6307;&#x5B9A;&#x8F93;&#x51FA;&#x5927;&#x5C0F;H x W&#xFF0C;&#x5C06;&#x4EFB;&#x610F;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x7684;&#x8F93;&#x5165;&#x5F3A;&#x884C;&#x7684;&#x6C60;&#x5316;&#x5230;&#x6307;&#x5B9A;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x3002;&#x4E0D;&#x8FC7;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#x7684;&#x901A;&#x9053;&#x6570;&#x4E0D;&#x4F1A;&#x53D8;&#x5316;&#x3002;</p>
<p>Parameters: </p>
<ul>
<li><strong>output_size</strong> &#x2013; &#x6307;&#x5B9A;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;H x W&#x3002;&#x6B64;&#x53C2;&#x6570;&#x652F;&#x6301;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;&#x5143;&#x7EC4;(H, W)&#xFF0C;&#x53C8;&#x6216;&#x8005;&#x662F;&#x4E00;&#x4E2A;&#x5355;&#x72EC;&#x7684;<code>int</code> H&#xFF08;&#x7B49;&#x4EF7;&#x4E8E;H x H&#xFF09;&#x3002;H &#x548C; W&#x8FD9;&#x4E24;&#x4E2A;&#x53C2;&#x6570;&#x652F;&#x6301;&#x8F93;&#x5165;&#x4E00;&#x4E2A;<code>int</code>&#x53C8;&#x6216;&#x8005;&#x662F;<code>None</code>, <code>None</code>&#x8868;&#x793A;&#x6B64;&#x8F93;&#x51FA;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;&#x7B49;&#x4EF7;&#x4E8E;&#x8F93;&#x5165;&#x6570;&#x636E;&#x6B64;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;</li>
<li><strong>return_indices</strong> &#x2013; &#x5982;&#x679C;&#x6B64;&#x53C2;&#x6570;&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>, &#x90A3;&#x4E48;&#x5728;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x7ED3;&#x675F;&#x540E;&#xFF0C;&#x8FD4;&#x56DE;&#x6C60;&#x5316;&#x8F93;&#x51FA;&#x7ED3;&#x679C;&#x7684;&#x540C;&#x65F6;&#x4E5F;&#x4F1A;&#x8FD4;&#x56DE;&#x6BCF;&#x4E2A;&#x6C60;&#x5316;&#x533A;&#x57DF;&#x4E2D;&#xFF0C;&#x6700;&#x5927;&#x503C;&#x7684;&#x4F4D;&#x7F6E;&#x4FE1;&#x606F;&#x3002;&#x8FD9;&#x4E9B;&#x4FE1;&#x606F;&#x5728;<code>nn.MaxUnpool2d()</code>&#x53EF;&#x4EE5;&#x88AB;&#x7528;&#x5230;&#x3002;&#x6B64;&#x53C2;&#x6570;&#x9ED8;&#x8BA4;&#x4E3A;<code>False</code></li>
</ul>
<p>&#x4F8B;&#x5B50;</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 5x7</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveMaxPool2d((<span class="hljs-number">5</span>,<span class="hljs-number">7</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 7x7 (square)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveMaxPool2d(<span class="hljs-number">7</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 10x7</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveMaxPool2d((<span class="hljs-keyword">None</span>, <span class="hljs-number">7</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="adaptivemaxpool3d">AdaptiveMaxPool3d</h3>
<pre><code class="lang-py">class torch.nn.AdaptiveMaxPool3d(output_size, return_indices=False)
</code></pre>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4FE1;&#x53F7;&#x8FDB;&#x884C;3&#x7EF4;&#x7684;&#x81EA;&#x9002;&#x5E94;&#x6700;&#x5927;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x6B64;&#x6C60;&#x5316;&#x5C42;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x6307;&#x5B9A;&#x8F93;&#x51FA;&#x5927;&#x5C0F;D x H x W&#xFF0C;&#x5C06;&#x4EFB;&#x610F;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x7684;&#x8F93;&#x5165;&#x5F3A;&#x884C;&#x7684;&#x6C60;&#x5316;&#x5230;&#x6307;&#x5B9A;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x3002;&#x4E0D;&#x8FC7;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#x7684;&#x901A;&#x9053;&#x6570;&#x4E0D;&#x4F1A;&#x53D8;&#x5316;&#x3002;</p>
<p>Parameters: </p>
<ul>
<li><strong>output_size</strong> &#x2013; &#x6307;&#x5B9A;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;D x H x W&#x3002;&#x6B64;&#x53C2;&#x6570;&#x652F;&#x6301;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;&#x5143;&#x7EC4;(D, H, W)&#xFF0C;&#x53C8;&#x6216;&#x8005;&#x662F;&#x4E00;&#x4E2A;&#x5355;&#x72EC;&#x7684;<code>int</code> D&#xFF08;&#x7B49;&#x4EF7;&#x4E8E;D x D x D)&#x3002;D, H &#x548C; W&#x8FD9;&#x4E09;&#x4E2A;&#x53C2;&#x6570;&#x652F;&#x6301;&#x8F93;&#x5165;&#x4E00;&#x4E2A;<code>int</code>&#x53C8;&#x6216;&#x8005;&#x662F;<code>None</code>, <code>None</code>&#x8868;&#x793A;&#x6B64;&#x8F93;&#x51FA;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;&#x7B49;&#x4EF7;&#x4E8E;&#x8F93;&#x5165;&#x6570;&#x636E;&#x6B64;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;</li>
<li><strong>return_indices</strong> &#x2013; &#x5982;&#x679C;&#x6B64;&#x53C2;&#x6570;&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>, &#x90A3;&#x4E48;&#x5728;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x7ED3;&#x675F;&#x540E;&#xFF0C;&#x8FD4;&#x56DE;&#x6C60;&#x5316;&#x8F93;&#x51FA;&#x7ED3;&#x679C;&#x7684;&#x540C;&#x65F6;&#x4E5F;&#x4F1A;&#x8FD4;&#x56DE;&#x6BCF;&#x4E2A;&#x6C60;&#x5316;&#x533A;&#x57DF;&#x4E2D;&#xFF0C;&#x6700;&#x5927;&#x503C;&#x7684;&#x4F4D;&#x7F6E;&#x4FE1;&#x606F;&#x3002;&#x8FD9;&#x4E9B;&#x4FE1;&#x606F;&#x5728;<code>nn.MaxUnpool3d()</code>&#x53EF;&#x4EE5;&#x88AB;&#x7528;&#x5230;&#x3002;&#x6B64;&#x53C2;&#x6570;&#x9ED8;&#x8BA4;&#x4E3A;<code>False</code></li>
</ul>
<p>&#x4F8B;&#x5B50;</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 5x7x9</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveMaxPool3d((<span class="hljs-number">5</span>,<span class="hljs-number">7</span>,<span class="hljs-number">9</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 7x7x7 (cube)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveMaxPool3d(<span class="hljs-number">7</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>, <span class="hljs-number">8</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 7x9x8</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveMaxPool3d((<span class="hljs-number">7</span>, <span class="hljs-keyword">None</span>, <span class="hljs-keyword">None</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>, <span class="hljs-number">8</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="adaptiveavgpool1d">AdaptiveAvgPool1d</h3>
<pre><code class="lang-py">class torch.nn.AdaptiveAvgPool1d(output_size)
</code></pre>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4FE1;&#x53F7;&#x8FDB;&#x884C;1&#x7EF4;&#x7684;&#x81EA;&#x9002;&#x5E94;&#x5E73;&#x5747;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x6B64;&#x6C60;&#x5316;&#x5C42;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x6307;&#x5B9A;&#x8F93;&#x51FA;&#x5927;&#x5C0F;H&#xFF0C;&#x5C06;&#x4EFB;&#x610F;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x7684;&#x8F93;&#x5165;&#x5F3A;&#x884C;&#x7684;&#x6C60;&#x5316;&#x5230;&#x6307;&#x5B9A;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x3002;&#x4E0D;&#x8FC7;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#x7684;&#x901A;&#x9053;&#x6570;&#x4E0D;&#x4F1A;&#x53D8;&#x5316;&#x3002;</p>
<p>Parameters: </p>
<ul>
<li><strong>output_size</strong> &#x2013; &#x6307;&#x5B9A;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;H</li>
</ul>
<p>&#x4F8B;&#x5B50;</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 5</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveAvgPool1d(<span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="adaptiveavgpool2d">AdaptiveAvgPool2d</h3>
<pre><code class="lang-py">class torch.nn.AdaptiveAvgPool2d(output_size)
</code></pre>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4FE1;&#x53F7;&#x8FDB;&#x884C;2&#x7EF4;&#x7684;&#x81EA;&#x9002;&#x5E94;&#x5E73;&#x5747;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x6B64;&#x6C60;&#x5316;&#x5C42;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x6307;&#x5B9A;&#x8F93;&#x51FA;&#x5927;&#x5C0F;H x W&#xFF0C;&#x5C06;&#x4EFB;&#x610F;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x7684;&#x8F93;&#x5165;&#x5F3A;&#x884C;&#x7684;&#x6C60;&#x5316;&#x5230;&#x6307;&#x5B9A;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x3002;&#x4E0D;&#x8FC7;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#x7684;&#x901A;&#x9053;&#x6570;&#x4E0D;&#x4F1A;&#x53D8;&#x5316;&#x3002;</p>
<p>Parameters: </p>
<ul>
<li><strong>output_size</strong> &#x2013; &#x6307;&#x5B9A;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;H x W&#x3002;&#x6B64;&#x53C2;&#x6570;&#x652F;&#x6301;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;&#x5143;&#x7EC4;(H, W)&#xFF0C;&#x53C8;&#x6216;&#x8005;&#x662F;&#x4E00;&#x4E2A;&#x5355;&#x72EC;&#x7684;<code>int</code> H&#xFF08;&#x7B49;&#x4EF7;&#x4E8E;H x H&#xFF09;&#x3002;H &#x548C; W&#x8FD9;&#x4E24;&#x4E2A;&#x53C2;&#x6570;&#x652F;&#x6301;&#x8F93;&#x5165;&#x4E00;&#x4E2A;<code>int</code>&#x53C8;&#x6216;&#x8005;&#x662F;<code>None</code>, <code>None</code>&#x8868;&#x793A;&#x6B64;&#x8F93;&#x51FA;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;&#x7B49;&#x4EF7;&#x4E8E;&#x8F93;&#x5165;&#x6570;&#x636E;&#x6B64;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;</li>
</ul>
<p>&#x4F8B;&#x5B50;</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 5x7</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveAvgPool2d((<span class="hljs-number">5</span>,<span class="hljs-number">7</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 7x7 (square)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveAvgPool2d(<span class="hljs-number">7</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 10x7</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveMaxPool2d((<span class="hljs-keyword">None</span>, <span class="hljs-number">7</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="adaptiveavgpool3d">AdaptiveAvgPool3d</h3>
<pre><code class="lang-py">class torch.nn.AdaptiveAvgPool3d(output_size)
</code></pre>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x591A;&#x901A;&#x9053;&#x4FE1;&#x53F7;&#x8FDB;&#x884C;3&#x7EF4;&#x7684;&#x81EA;&#x9002;&#x5E94;&#x5E73;&#x5747;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x6B64;&#x6C60;&#x5316;&#x5C42;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x6307;&#x5B9A;&#x8F93;&#x51FA;&#x5927;&#x5C0F;D x H x W&#xFF0C;&#x5C06;&#x4EFB;&#x610F;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x7684;&#x8F93;&#x5165;&#x5F3A;&#x884C;&#x7684;&#x6C60;&#x5316;&#x5230;&#x6307;&#x5B9A;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x3002;&#x4E0D;&#x8FC7;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#x7684;&#x901A;&#x9053;&#x6570;&#x4E0D;&#x4F1A;&#x53D8;&#x5316;&#x3002;</p>
<p>Parameters: </p>
<ul>
<li><strong>output_size</strong> &#x2013; &#x6307;&#x5B9A;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;D x H x W&#x3002;&#x6B64;&#x53C2;&#x6570;&#x652F;&#x6301;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;&#x5143;&#x7EC4;(D, H, W)&#xFF0C;&#x53C8;&#x6216;&#x8005;&#x662F;&#x4E00;&#x4E2A;&#x5355;&#x72EC;&#x7684;<code>int</code> D&#xFF08;&#x7B49;&#x4EF7;&#x4E8E;D x D x D)&#x3002;D, H &#x548C; W&#x8FD9;&#x4E09;&#x4E2A;&#x53C2;&#x6570;&#x652F;&#x6301;&#x8F93;&#x5165;&#x4E00;&#x4E2A;<code>int</code>&#x53C8;&#x6216;&#x8005;&#x662F;<code>None</code>, <code>None</code>&#x8868;&#x793A;&#x6B64;&#x8F93;&#x51FA;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;&#x7B49;&#x4EF7;&#x4E8E;&#x8F93;&#x5165;&#x6570;&#x636E;&#x6B64;&#x7EF4;&#x5EA6;&#x7684;&#x5927;&#x5C0F;</li>
</ul>
<p>&#x4F8B;&#x5B50;</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 5x7x9</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveAvgPool3d((<span class="hljs-number">5</span>,<span class="hljs-number">7</span>,<span class="hljs-number">9</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 7x7x7 (cube)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveAvgPool3d(<span class="hljs-number">7</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>, <span class="hljs-number">8</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 7x9x8</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveMaxPool3d((<span class="hljs-number">7</span>, <span class="hljs-keyword">None</span>, <span class="hljs-keyword">None</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>, <span class="hljs-number">8</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h2 id="&#x586B;&#x5145;&#x5C42;&#xFF08;padding-layers&#xFF09;">&#x586B;&#x5145;&#x5C42;&#xFF08;Padding layers&#xFF09;</h2>
<h3 id="reflectionpad1d">ReflectionPad1d</h3>
<pre><code class="lang-py">class torch.nn.ReflectionPad1d(padding)
</code></pre>
<p>&#x4EE5;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x5404;&#x8FB9;&#x754C;&#x4E3A;&#x8F74;&#xFF0C;&#x901A;&#x8FC7;&#x5BF9;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x6570;&#x636E;&#x7684;&#x8FDB;&#x884C;&#x955C;&#x50CF;&#x590D;&#x5236;&#x7684;&#x65B9;&#x5F0F;&#x6765;&#x5BF9;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x8FDB;&#x884C;&#x586B;&#x5145;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x5BF9;&#x4E8E;<code>N</code>&#x7EF4;&#x7684;&#x586B;&#x5145;&#x64CD;&#x4F5C;&#xFF0C;&#x8C03;&#x7528;<a href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x8981;&#x586B;&#x5145;&#x7684;&#x8303;&#x56F4;&#x5927;&#x5C0F;&#x3002;&#x5982;&#x679C;&#x8F93;&#x5165;&#x6570;&#x636E;&#x662F;&#x4E00;&#x4E2A;<code>int</code>, &#x90A3;&#x5404;&#x4E2A;&#x8FB9;&#x754C;&#x4E0A;&#x90FD;&#x4F1A;&#x586B;&#x5145;&#x540C;&#x6837;&#x5927;&#x5C0F;&#x7684;&#x6570;&#x636E;&#x3002;&#x5982;&#x679C;&#x662F;&#x4E00;&#x4E2A;&#x4E24;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5143;&#x7EC4;&#xFF0C;&#x90A3;&#x4E48;&#x6309;&#x7167; (<img src="img/08b2cac9ee37dde4cec3d372ebbfa0bd.jpg" alt="">, <img src="img/9f56071a00e2baa50d7fa9bde997852d.jpg" alt="">)&#x7684;&#x5927;&#x5C0F;&#x8BBE;&#x5B9A;&#x6765;&#x5728;&#x5404;&#x8FB9;&#x4E0A;&#x586B;&#x5145;&#x3002;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/964aa6df63e83f4468aa090441f01972.jpg" alt=""></li>
<li>&#x8F93;&#x51FA;: <img src="img/ac2661719f40fc422e2b1590a1e7b4a4.jpg" alt=""> &#x5176;&#x4E2D; <img src="img/e2294a717e6d12035072d23c45273863.jpg" alt=""></li>
</ul>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ReflectionPad1d(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.arange(<span class="hljs-number">8</span>, dtype=torch.float).reshape(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input
tensor([[[<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>],
 [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">7.</span>]]])
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[<span class="hljs-number">2.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">1.</span>],
 [<span class="hljs-number">6.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">5.</span>]]])
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[<span class="hljs-number">2.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">1.</span>],
 [<span class="hljs-number">6.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">5.</span>]]])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># using different paddings for different sides</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ReflectionPad1d((<span class="hljs-number">3</span>, <span class="hljs-number">1</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[<span class="hljs-number">3.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">2.</span>],
 [<span class="hljs-number">7.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">6.</span>]]])
</code></pre>
<h3 id="reflectionpad2d">ReflectionPad2d</h3>
<pre><code class="lang-py">class torch.nn.ReflectionPad2d(padding)
</code></pre>
<p>&#x4EE5;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x5404;&#x8FB9;&#x754C;&#x4E3A;&#x8F74;&#xFF0C;&#x901A;&#x8FC7;&#x5BF9;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x6570;&#x636E;&#x7684;&#x8FDB;&#x884C;&#x955C;&#x50CF;&#x590D;&#x5236;&#x7684;&#x65B9;&#x5F0F;&#x6765;&#x5BF9;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x8FDB;&#x884C;&#x586B;&#x5145;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x5BF9;&#x4E8E;<code>N</code>&#x7EF4;&#x7684;&#x586B;&#x5145;&#x64CD;&#x4F5C;&#xFF0C;&#x8C03;&#x7528;<a href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x8981;&#x586B;&#x5145;&#x7684;&#x8303;&#x56F4;&#x5927;&#x5C0F;&#x3002;&#x5982;&#x679C;&#x8F93;&#x5165;&#x6570;&#x636E;&#x662F;&#x4E00;&#x4E2A;<code>int</code>, &#x90A3;&#x5404;&#x4E2A;&#x8FB9;&#x754C;&#x4E0A;&#x90FD;&#x4F1A;&#x586B;&#x5145;&#x540C;&#x6837;&#x5927;&#x5C0F;&#x7684;&#x6570;&#x636E;&#x3002;&#x5982;&#x679C;&#x662F;&#x4E00;&#x4E2A;&#x56DB;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5143;&#x7EC4;&#xFF0C;&#x90A3;&#x4E48;&#x6309;&#x7167;(<img src="img/08b2cac9ee37dde4cec3d372ebbfa0bd.jpg" alt="">, <img src="img/9f56071a00e2baa50d7fa9bde997852d.jpg" alt="">, <img src="img/65f6ce26141c225acd502a7bef164f66.jpg" alt="">, <img src="img/9a98061e27ba6ed06e846767b9c77c3a.jpg" alt="">)&#x7684;&#x5927;&#x5C0F;&#x8BBE;&#x5B9A;&#x6765;&#x5728;&#x5404;&#x8FB9;&#x4E0A;&#x586B;&#x5145;&#x3002;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li><p>&#x8F93;&#x5165;: <img src="img/ff71b16eb10237262566c6907acaaf1f.jpg" alt=""></p>
</li>
<li><p>&#x8F93;&#x51FA;: <img src="img/a0ef05f779873fc4dcbf020b1ea14754.jpg" alt=""> &#x5176;&#x4E2D;</p>
<p><img src="img/75aa7c4a6c84e0ccb8aa91592cf6a077.jpg" alt=""> <img src="img/e2294a717e6d12035072d23c45273863.jpg" alt=""></p>
</li>
</ul>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ReflectionPad2d(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.arange(<span class="hljs-number">9</span>, dtype=torch.float).reshape(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input
tensor([[[[<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>],
 [<span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>],
 [<span class="hljs-number">6.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">8.</span>]]]])
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[[<span class="hljs-number">8.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">8.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">6.</span>],
 [<span class="hljs-number">5.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">3.</span>],
 [<span class="hljs-number">2.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>],
 [<span class="hljs-number">5.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">3.</span>],
 [<span class="hljs-number">8.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">8.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">6.</span>],
 [<span class="hljs-number">5.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">3.</span>],
 [<span class="hljs-number">2.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>]]]])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># using different paddings for different sides</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ReflectionPad2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[[<span class="hljs-number">7.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">8.</span>, <span class="hljs-number">7.</span>],
 [<span class="hljs-number">4.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">4.</span>],
 [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">1.</span>],
 [<span class="hljs-number">4.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">4.</span>],
 [<span class="hljs-number">7.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">8.</span>, <span class="hljs-number">7.</span>]]]])
</code></pre>
<h3 id="replicationpad1d">ReplicationPad1d</h3>
<pre><code class="lang-py">class torch.nn.ReplicationPad1d(padding)
</code></pre>
<p>&#x901A;&#x8FC7;&#x590D;&#x5236;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x8FB9;&#x754C;&#x5143;&#x7D20;&#x7684;&#x65B9;&#x5F0F;&#x5BF9;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x8FDB;&#x884C;&#x586B;&#x5145;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x5BF9;&#x4E8E;<code>N</code>&#x7EF4;&#x7684;&#x586B;&#x5145;&#x64CD;&#x4F5C;&#xFF0C;&#x8C03;&#x7528;<a href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x8981;&#x586B;&#x5145;&#x7684;&#x8303;&#x56F4;&#x5927;&#x5C0F;&#x3002;&#x5982;&#x679C;&#x8F93;&#x5165;&#x6570;&#x636E;&#x662F;&#x4E00;&#x4E2A;<code>int</code>, &#x90A3;&#x5404;&#x4E2A;&#x8FB9;&#x754C;&#x4E0A;&#x90FD;&#x4F1A;&#x586B;&#x5145;&#x540C;&#x6837;&#x5927;&#x5C0F;&#x7684;&#x6570;&#x636E;&#x3002;&#x5982;&#x679C;&#x662F;&#x4E00;&#x4E2A;&#x4E24;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5143;&#x7EC4;&#xFF0C;&#x90A3;&#x4E48;&#x6309;&#x7167; (<img src="img/08b2cac9ee37dde4cec3d372ebbfa0bd.jpg" alt="">, <img src="img/9f56071a00e2baa50d7fa9bde997852d.jpg" alt="">)&#x7684;&#x5927;&#x5C0F;&#x8BBE;&#x5B9A;&#x6765;&#x5728;&#x5404;&#x8FB9;&#x4E0A;&#x586B;&#x5145;&#x3002;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/964aa6df63e83f4468aa090441f01972.jpg" alt=""></li>
<li>&#x8F93;&#x51FA;: <img src="img/ac2661719f40fc422e2b1590a1e7b4a4.jpg" alt=""> &#x5176;&#x4E2D; <img src="img/e2294a717e6d12035072d23c45273863.jpg" alt=""></li>
</ul>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ReplicationPad1d(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.arange(<span class="hljs-number">8</span>, dtype=torch.float).reshape(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input
tensor([[[<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>],
 [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">7.</span>]]])
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>],
 [<span class="hljs-number">4.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">7.</span>]]])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># using different paddings for different sides</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ReplicationPad1d((<span class="hljs-number">3</span>, <span class="hljs-number">1</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>],
 [<span class="hljs-number">4.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">7.</span>]]])
</code></pre>
<h3 id="replicationpad2d">ReplicationPad2d</h3>
<pre><code class="lang-py">class torch.nn.ReplicationPad2d(padding)
</code></pre>
<p>&#x901A;&#x8FC7;&#x590D;&#x5236;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x8FB9;&#x754C;&#x5143;&#x7D20;&#x7684;&#x65B9;&#x5F0F;&#x5BF9;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x8FDB;&#x884C;&#x586B;&#x5145;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x5BF9;&#x4E8E;<code>N</code>&#x7EF4;&#x7684;&#x586B;&#x5145;&#x64CD;&#x4F5C;&#xFF0C;&#x8C03;&#x7528;<a href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x8981;&#x586B;&#x5145;&#x7684;&#x8303;&#x56F4;&#x5927;&#x5C0F;&#x3002;&#x5982;&#x679C;&#x8F93;&#x5165;&#x6570;&#x636E;&#x662F;&#x4E00;&#x4E2A;<code>int</code>, &#x90A3;&#x5404;&#x4E2A;&#x8FB9;&#x754C;&#x4E0A;&#x90FD;&#x4F1A;&#x586B;&#x5145;&#x540C;&#x6837;&#x5927;&#x5C0F;&#x7684;&#x6570;&#x636E;&#x3002;&#x5982;&#x679C;&#x662F;&#x4E00;&#x4E2A;&#x56DB;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5143;&#x7EC4;&#xFF0C;&#x90A3;&#x4E48;&#x6309;&#x7167;(<img src="img/08b2cac9ee37dde4cec3d372ebbfa0bd.jpg" alt="">, <img src="img/9f56071a00e2baa50d7fa9bde997852d.jpg" alt="">, <img src="img/65f6ce26141c225acd502a7bef164f66.jpg" alt="">, <img src="img/9a98061e27ba6ed06e846767b9c77c3a.jpg" alt="">)&#x7684;&#x5927;&#x5C0F;&#x8BBE;&#x5B9A;&#x6765;&#x5728;&#x5404;&#x8FB9;&#x4E0A;&#x586B;&#x5145;&#x3002;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/ff71b16eb10237262566c6907acaaf1f.jpg" alt=""></li>
<li>&#x8F93;&#x51FA;: <img src="img/a0ef05f779873fc4dcbf020b1ea14754.jpg" alt=""> &#x5176;&#x4E2D; <img src="img/75aa7c4a6c84e0ccb8aa91592cf6a077.jpg" alt=""> <img src="img/e2294a717e6d12035072d23c45273863.jpg" alt=""></li>
</ul>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ReplicationPad2d(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.arange(<span class="hljs-number">9</span>, dtype=torch.float).reshape(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input
tensor([[[[<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>],
 [<span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>],
 [<span class="hljs-number">6.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">8.</span>]]]])
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>],
 [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>],
 [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>],
 [<span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>],
 [<span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">8.</span>, <span class="hljs-number">8.</span>, <span class="hljs-number">8.</span>],
 [<span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">8.</span>, <span class="hljs-number">8.</span>, <span class="hljs-number">8.</span>],
 [<span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">8.</span>, <span class="hljs-number">8.</span>, <span class="hljs-number">8.</span>]]]])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># using different paddings for different sides</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ReplicationPad2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>],
 [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>],
 [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>],
 [<span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>],
 [<span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">8.</span>, <span class="hljs-number">8.</span>]]]])
</code></pre>
<h3 id="replicationpad3d">ReplicationPad3d</h3>
<pre><code class="lang-py">class torch.nn.ReplicationPad3d(padding)
</code></pre>
<p>&#x901A;&#x8FC7;&#x590D;&#x5236;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x8FB9;&#x754C;&#x5143;&#x7D20;&#x7684;&#x65B9;&#x5F0F;&#x5BF9;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x8FDB;&#x884C;&#x586B;&#x5145;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x5BF9;&#x4E8E;<code>N</code>&#x7EF4;&#x7684;&#x586B;&#x5145;&#x64CD;&#x4F5C;&#xFF0C;&#x8C03;&#x7528;<a href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x8981;&#x586B;&#x5145;&#x7684;&#x8303;&#x56F4;&#x5927;&#x5C0F;&#x3002;&#x5982;&#x679C;&#x8F93;&#x5165;&#x6570;&#x636E;&#x662F;&#x4E00;&#x4E2A;<code>int</code>, &#x90A3;&#x5404;&#x4E2A;&#x8FB9;&#x754C;&#x4E0A;&#x90FD;&#x4F1A;&#x586B;&#x5145;&#x540C;&#x6837;&#x5927;&#x5C0F;&#x7684;&#x6570;&#x636E;&#x3002;&#x5982;&#x679C;&#x662F;&#x4E00;&#x4E2A;&#x516D;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5143;&#x7EC4;&#xFF0C;&#x90A3;&#x4E48;&#x6309;&#x7167;(<img src="img/08b2cac9ee37dde4cec3d372ebbfa0bd.jpg" alt="">, <img src="img/9f56071a00e2baa50d7fa9bde997852d.jpg" alt="">, <img src="img/65f6ce26141c225acd502a7bef164f66.jpg" alt="">, <img src="img/9a98061e27ba6ed06e846767b9c77c3a.jpg" alt="">, <img src="img/bffb266183e8fa640240e16a45076c34.jpg" alt="">, <img src="img/9138ac0ee6f6e96dfe795ead91ec0003.jpg" alt="">)&#x7684;&#x5927;&#x5C0F;&#x8BBE;&#x5B9A;&#x6765;&#x5728;&#x5404;&#x8FB9;&#x4E0A;&#x586B;&#x5145;&#x3002;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/c187d190013d0785320e3412fe8cd669.jpg" alt=""></li>
<li>&#x8F93;&#x51FA;: <img src="img/41ca4c8d4c65c979d2d643c6f62ea280.jpg" alt=""> &#x5176;&#x4E2D; <img src="img/006114d9c80210ede5da92f2f3a44bb7.jpg" alt=""> <img src="img/75aa7c4a6c84e0ccb8aa91592cf6a077.jpg" alt=""> <img src="img/e2294a717e6d12035072d23c45273863.jpg" alt=""></li>
</ul>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ReplicationPad3d(<span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">16</span>, <span class="hljs-number">3</span>, <span class="hljs-number">8</span>, <span class="hljs-number">320</span>, <span class="hljs-number">480</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># using different paddings for different sides</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ReplicationPad3d((<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="zeropad2d">ZeroPad2d</h3>
<pre><code class="lang-py">class torch.nn.ZeroPad2d(padding)
</code></pre>
<p>&#x901A;&#x8FC7;&#x5728;&#x5404;&#x8FB9;&#x4E0A;&#x586B;&#x5145;0&#x7684;&#x65B9;&#x5F0F;&#x5BF9;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x8FDB;&#x884C;&#x586B;&#x5145;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x5BF9;&#x4E8E;<code>N</code>&#x7EF4;&#x7684;&#x586B;&#x5145;&#x64CD;&#x4F5C;&#xFF0C;&#x8C03;&#x7528;<a href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x8981;&#x586B;&#x5145;&#x7684;&#x8303;&#x56F4;&#x5927;&#x5C0F;&#x3002;&#x5982;&#x679C;&#x8F93;&#x5165;&#x6570;&#x636E;&#x662F;&#x4E00;&#x4E2A;<code>int</code>, &#x90A3;&#x5404;&#x4E2A;&#x8FB9;&#x754C;&#x4E0A;&#x90FD;&#x4F1A;&#x586B;&#x5145;&#x540C;&#x6837;&#x5927;&#x5C0F;&#x7684;&#x6570;&#x636E;&#x3002;&#x5982;&#x679C;&#x662F;&#x4E00;&#x4E2A;&#x56DB;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5143;&#x7EC4;&#xFF0C;&#x90A3;&#x4E48;&#x6309;&#x7167;(<img src="img/08b2cac9ee37dde4cec3d372ebbfa0bd.jpg" alt="">, <img src="img/9f56071a00e2baa50d7fa9bde997852d.jpg" alt="">, <img src="img/65f6ce26141c225acd502a7bef164f66.jpg" alt="">, <img src="img/9a98061e27ba6ed06e846767b9c77c3a.jpg" alt="">)&#x7684;&#x5927;&#x5C0F;&#x8BBE;&#x5B9A;&#x6765;&#x5728;&#x5404;&#x8FB9;&#x4E0A;&#x586B;&#x5145;&#x3002;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/ff71b16eb10237262566c6907acaaf1f.jpg" alt=""></li>
<li>&#x8F93;&#x51FA;: <img src="img/a0ef05f779873fc4dcbf020b1ea14754.jpg" alt=""> &#x5176;&#x4E2D; <img src="img/75aa7c4a6c84e0ccb8aa91592cf6a077.jpg" alt=""> <img src="img/e2294a717e6d12035072d23c45273863.jpg" alt=""></li>
</ul>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ZeroPad2d(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input
tensor([[[[<span class="hljs-number">-0.1678</span>, <span class="hljs-number">-0.4418</span>,  <span class="hljs-number">1.9466</span>],
 [ <span class="hljs-number">0.9604</span>, <span class="hljs-number">-0.4219</span>, <span class="hljs-number">-0.5241</span>],
 [<span class="hljs-number">-0.9162</span>, <span class="hljs-number">-0.5436</span>, <span class="hljs-number">-0.6446</span>]]]])
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[[ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>, <span class="hljs-number">-0.1678</span>, <span class="hljs-number">-0.4418</span>,  <span class="hljs-number">1.9466</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.9604</span>, <span class="hljs-number">-0.4219</span>, <span class="hljs-number">-0.5241</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>, <span class="hljs-number">-0.9162</span>, <span class="hljs-number">-0.5436</span>, <span class="hljs-number">-0.6446</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>]]]])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># using different paddings for different sides</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ZeroPad2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[[ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">0.0000</span>, <span class="hljs-number">-0.1678</span>, <span class="hljs-number">-0.4418</span>,  <span class="hljs-number">1.9466</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.9604</span>, <span class="hljs-number">-0.4219</span>, <span class="hljs-number">-0.5241</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">0.0000</span>, <span class="hljs-number">-0.9162</span>, <span class="hljs-number">-0.5436</span>, <span class="hljs-number">-0.6446</span>,  <span class="hljs-number">0.0000</span>]]]])
</code></pre>
<h3 id="constantpad1d">ConstantPad1d</h3>
<pre><code class="lang-py">class torch.nn.ConstantPad1d(padding, value)
</code></pre>
<p>&#x901A;&#x8FC7;&#x5728;&#x5404;&#x8FB9;&#x4E0A;&#x586B;&#x5145;&#x56FA;&#x5B9A;&#x6570;&#x5B57;&#x7684;&#x65B9;&#x5F0F;&#x5BF9;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x8FDB;&#x884C;&#x586B;&#x5145;&#x64CD;&#x4F5C;</p>
<p>&#x5BF9;&#x4E8E;<code>N</code>&#x7EF4;&#x7684;&#x586B;&#x5145;&#x64CD;&#x4F5C;&#xFF0C;&#x8C03;&#x7528;<a href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x8981;&#x586B;&#x5145;&#x7684;&#x8303;&#x56F4;&#x5927;&#x5C0F;&#x3002;&#x5982;&#x679C;&#x8F93;&#x5165;&#x6570;&#x636E;&#x662F;&#x4E00;&#x4E2A;<code>int</code>, &#x90A3;&#x5404;&#x4E2A;&#x8FB9;&#x754C;&#x4E0A;&#x90FD;&#x4F1A;&#x586B;&#x5145;&#x540C;&#x6837;&#x5927;&#x5C0F;&#x7684;&#x6570;&#x636E;&#x3002;&#x5982;&#x679C;&#x662F;&#x4E00;&#x4E2A;&#x4E24;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5143;&#x7EC4;&#xFF0C;&#x90A3;&#x4E48;&#x6309;&#x7167; (<img src="img/08b2cac9ee37dde4cec3d372ebbfa0bd.jpg" alt="">, <img src="img/9f56071a00e2baa50d7fa9bde997852d.jpg" alt="">)&#x7684;&#x5927;&#x5C0F;&#x8BBE;&#x5B9A;&#x6765;&#x5728;&#x5404;&#x8FB9;&#x4E0A;&#x586B;&#x5145;&#x3002;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/964aa6df63e83f4468aa090441f01972.jpg" alt=""></li>
<li>&#x8F93;&#x51FA;: <img src="img/ac2661719f40fc422e2b1590a1e7b4a4.jpg" alt=""> &#x5176;&#x4E2D; <img src="img/e2294a717e6d12035072d23c45273863.jpg" alt=""></li>
</ul>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ConstantPad1d(<span class="hljs-number">2</span>, <span class="hljs-number">3.5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input
tensor([[[<span class="hljs-number">-1.0491</span>, <span class="hljs-number">-0.7152</span>, <span class="hljs-number">-0.0749</span>,  <span class="hljs-number">0.8530</span>],
 [<span class="hljs-number">-1.3287</span>,  <span class="hljs-number">1.8966</span>,  <span class="hljs-number">0.1466</span>, <span class="hljs-number">-0.2771</span>]]])
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>, <span class="hljs-number">-1.0491</span>, <span class="hljs-number">-0.7152</span>, <span class="hljs-number">-0.0749</span>,  <span class="hljs-number">0.8530</span>,  <span class="hljs-number">3.5000</span>,
 <span class="hljs-number">3.5000</span>],
 [ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>, <span class="hljs-number">-1.3287</span>,  <span class="hljs-number">1.8966</span>,  <span class="hljs-number">0.1466</span>, <span class="hljs-number">-0.2771</span>,  <span class="hljs-number">3.5000</span>,
 <span class="hljs-number">3.5000</span>]]])
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ConstantPad1d(<span class="hljs-number">2</span>, <span class="hljs-number">3.5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input
tensor([[[ <span class="hljs-number">1.6616</span>,  <span class="hljs-number">1.4523</span>, <span class="hljs-number">-1.1255</span>],
 [<span class="hljs-number">-3.6372</span>,  <span class="hljs-number">0.1182</span>, <span class="hljs-number">-1.8652</span>]]])
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">1.6616</span>,  <span class="hljs-number">1.4523</span>, <span class="hljs-number">-1.1255</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>],
 [ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>, <span class="hljs-number">-3.6372</span>,  <span class="hljs-number">0.1182</span>, <span class="hljs-number">-1.8652</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>]]])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># using different paddings for different sides</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ConstantPad1d((<span class="hljs-number">3</span>, <span class="hljs-number">1</span>), <span class="hljs-number">3.5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">1.6616</span>,  <span class="hljs-number">1.4523</span>, <span class="hljs-number">-1.1255</span>,  <span class="hljs-number">3.5000</span>],
 [ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>, <span class="hljs-number">-3.6372</span>,  <span class="hljs-number">0.1182</span>, <span class="hljs-number">-1.8652</span>,  <span class="hljs-number">3.5000</span>]]])
</code></pre>
<h3 id="constantpad2d">ConstantPad2d</h3>
<pre><code class="lang-py">class torch.nn.ConstantPad2d(padding, value)
</code></pre>
<p>&#x901A;&#x8FC7;&#x5728;&#x5404;&#x8FB9;&#x4E0A;&#x586B;&#x5145;&#x56FA;&#x5B9A;&#x6570;&#x5B57;&#x7684;&#x65B9;&#x5F0F;&#x5BF9;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x8FDB;&#x884C;&#x586B;&#x5145;&#x64CD;&#x4F5C;</p>
<p>&#x5BF9;&#x4E8E;<code>N</code>&#x7EF4;&#x7684;&#x586B;&#x5145;&#x64CD;&#x4F5C;&#xFF0C;&#x8C03;&#x7528;<a href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x8981;&#x586B;&#x5145;&#x7684;&#x8303;&#x56F4;&#x5927;&#x5C0F;&#x3002;&#x5982;&#x679C;&#x8F93;&#x5165;&#x6570;&#x636E;&#x662F;&#x4E00;&#x4E2A;<code>int</code>, &#x90A3;&#x5404;&#x4E2A;&#x8FB9;&#x754C;&#x4E0A;&#x90FD;&#x4F1A;&#x586B;&#x5145;&#x540C;&#x6837;&#x5927;&#x5C0F;&#x7684;&#x6570;&#x636E;&#x3002;&#x5982;&#x679C;&#x662F;&#x4E00;&#x4E2A;&#x56DB;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5143;&#x7EC4;&#xFF0C;&#x90A3;&#x4E48;&#x6309;&#x7167;(<img src="img/08b2cac9ee37dde4cec3d372ebbfa0bd.jpg" alt="">, <img src="img/9f56071a00e2baa50d7fa9bde997852d.jpg" alt="">, <img src="img/65f6ce26141c225acd502a7bef164f66.jpg" alt="">, <img src="img/9a98061e27ba6ed06e846767b9c77c3a.jpg" alt="">)&#x7684;&#x5927;&#x5C0F;&#x8BBE;&#x5B9A;&#x6765;&#x5728;&#x5404;&#x8FB9;&#x4E0A;&#x586B;&#x5145;&#x3002;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/ff71b16eb10237262566c6907acaaf1f.jpg" alt=""></li>
<li>&#x8F93;&#x51FA;: <img src="img/a0ef05f779873fc4dcbf020b1ea14754.jpg" alt=""> &#x5176;&#x4E2D; <img src="img/75aa7c4a6c84e0ccb8aa91592cf6a077.jpg" alt=""> <img src="img/e2294a717e6d12035072d23c45273863.jpg" alt=""></li>
</ul>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ConstantPad2d(<span class="hljs-number">2</span>, <span class="hljs-number">3.5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input
tensor([[[ <span class="hljs-number">1.6585</span>,  <span class="hljs-number">0.4320</span>],
 [<span class="hljs-number">-0.8701</span>, <span class="hljs-number">-0.4649</span>]]])
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>],
 [ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>],
 [ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">1.6585</span>,  <span class="hljs-number">0.4320</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>],
 [ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>, <span class="hljs-number">-0.8701</span>, <span class="hljs-number">-0.4649</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>],
 [ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>],
 [ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>]]])
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>],
 [ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>],
 [ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">1.6585</span>,  <span class="hljs-number">0.4320</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>],
 [ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>, <span class="hljs-number">-0.8701</span>, <span class="hljs-number">-0.4649</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>],
 [ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>],
 [ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>]]])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># using different paddings for different sides</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ConstantPad2d((<span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>), <span class="hljs-number">3.5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>],
 [ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>],
 [ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">1.6585</span>,  <span class="hljs-number">0.4320</span>],
 [ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>, <span class="hljs-number">-0.8701</span>, <span class="hljs-number">-0.4649</span>],
 [ <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>,  <span class="hljs-number">3.5000</span>]]])
</code></pre>
<h3 id="constantpad3d">ConstantPad3d</h3>
<pre><code class="lang-py">class torch.nn.ConstantPad3d(padding, value)
</code></pre>
<p>&#x901A;&#x8FC7;&#x5728;&#x5404;&#x8FB9;&#x4E0A;&#x586B;&#x5145;&#x56FA;&#x5B9A;&#x6570;&#x5B57;&#x7684;&#x65B9;&#x5F0F;&#x5BF9;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x8FDB;&#x884C;&#x586B;&#x5145;&#x64CD;&#x4F5C;</p>
<p>&#x5BF9;&#x4E8E;<code>N</code>&#x7EF4;&#x7684;&#x586B;&#x5145;&#x64CD;&#x4F5C;&#xFF0C;&#x8C03;&#x7528;<a href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>&#x3002;</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a>) &#x2013; &#x8981;&#x586B;&#x5145;&#x7684;&#x8303;&#x56F4;&#x5927;&#x5C0F;&#x3002;&#x5982;&#x679C;&#x8F93;&#x5165;&#x6570;&#x636E;&#x662F;&#x4E00;&#x4E2A;<code>int</code>, &#x90A3;&#x5404;&#x4E2A;&#x8FB9;&#x754C;&#x4E0A;&#x90FD;&#x4F1A;&#x586B;&#x5145;&#x540C;&#x6837;&#x5927;&#x5C0F;&#x7684;&#x6570;&#x636E;&#x3002;&#x5982;&#x679C;&#x662F;&#x4E00;&#x4E2A;&#x516D;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5143;&#x7EC4;&#xFF0C;&#x90A3;&#x4E48;&#x6309;&#x7167;(<img src="img/08b2cac9ee37dde4cec3d372ebbfa0bd.jpg" alt="">, <img src="img/9f56071a00e2baa50d7fa9bde997852d.jpg" alt="">, <img src="img/65f6ce26141c225acd502a7bef164f66.jpg" alt="">, <img src="img/9a98061e27ba6ed06e846767b9c77c3a.jpg" alt="">, <img src="img/bffb266183e8fa640240e16a45076c34.jpg" alt="">, <img src="img/9138ac0ee6f6e96dfe795ead91ec0003.jpg" alt="">)&#x7684;&#x5927;&#x5C0F;&#x8BBE;&#x5B9A;&#x6765;&#x5728;&#x5404;&#x8FB9;&#x4E0A;&#x586B;&#x5145;&#x3002;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/c187d190013d0785320e3412fe8cd669.jpg" alt=""></li>
<li>&#x8F93;&#x51FA;: <img src="img/41ca4c8d4c65c979d2d643c6f62ea280.jpg" alt=""> &#x5176;&#x4E2D; <img src="img/006114d9c80210ede5da92f2f3a44bb7.jpg" alt=""> <img src="img/75aa7c4a6c84e0ccb8aa91592cf6a077.jpg" alt=""> <img src="img/e2294a717e6d12035072d23c45273863.jpg" alt=""></li>
</ul>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ConstantPad3d(<span class="hljs-number">3</span>, <span class="hljs-number">3.5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">16</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># using different paddings for different sides</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ConstantPad3d((<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>), <span class="hljs-number">3.5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h2 id="&#x975E;&#x7EBF;&#x6027;&#x6FC0;&#x6D3B;&#x52A0;&#x6743;&#x6C42;&#x548C;&#xFF0C;&#x975E;&#x7EBF;&#x6027;--non-linear-activations-weighted-sum-nonlinearity-">&#x975E;&#x7EBF;&#x6027;&#x6FC0;&#x6D3B;(&#x52A0;&#x6743;&#x6C42;&#x548C;&#xFF0C;&#x975E;&#x7EBF;&#x6027;) ( Non-linear activations (weighted sum, nonlinearity) )</h2>
<h3 id="elu">ELU</h3>
<pre><code class="lang-py">class torch.nn.ELU(alpha=1.0, inplace=False)
</code></pre>
<p>&#x5C06;&#x4E0B;&#x9762;&#x7684;&#x5143;&#x7D20;&#x7EA7;&#x51FD;&#x6570;&#x5E94;&#x7528;&#x5230;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0A;&#xFF1A;</p>
<p><img src="img/1285687f031aec0751f4e0481f97b6b0.jpg" alt=""></p>
<p>Parameters: </p>
<ul>
<li><strong>alpha</strong> &#x2013; ELU&#x64CD;&#x4F5C;&#x7684;<img src="img/82005cc2e0087e2a52c7e43df4a19a00.jpg" alt="">&#x503C;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; 1.0</li>
<li><strong>inplace</strong> &#x2013; &#x662F;&#x5426;&#x8FDB;&#x884C;&#x539F;&#x4F4D;&#x64CD;&#x4F5C;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; <code>False</code></li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> &#x5176;&#x4E2D; <code>*</code> &#x4EE3;&#x8868;&#x652F;&#x6301;&#x4EFB;&#x610F;&#x5927;&#x5C0F;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, &#x4E0E;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4FDD;&#x6301;&#x4E00;&#x6837;&#x7684;&#x5F62;&#x72B6;&#x5927;&#x5C0F;</li>
</ul>
<p><img src="img/5d789140032850b13d5c00493bf62412.jpg" alt="https://pytorch.org/docs/stable/_images//ELU.png"></p>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ELU()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="hardshrink">Hardshrink</h3>
<pre><code class="lang-py">class torch.nn.Hardshrink(lambd=0.5)
</code></pre>
<p>&#x5C06;&#x4E0B;&#x9762;&#x7684;&#x5143;&#x7D20;&#x7EA7;hard shrinkage&#x51FD;&#x6570;&#x5E94;&#x7528;&#x5230;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0A;&#xFF1A;</p>
<p><img src="img/f934a1a7fa1553c38403f2e010708ed9.jpg" alt=""></p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>lambd</strong> &#x2013; Hardshrink&#x8FD0;&#x7B97;&#x4E2D;&#x7684; <img src="img/5e8df2ba7e47a784c714d176ed8bbb7a.jpg" alt=""> &#x503C;&#x3002; &#x9ED8;&#x8BA4;: 0.5</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> &#x5176;&#x4E2D; <code>*</code> &#x4EE3;&#x8868;&#x652F;&#x6301;&#x4EFB;&#x610F;&#x5927;&#x5C0F;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, &#x4E0E;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4FDD;&#x6301;&#x4E00;&#x6837;&#x7684;&#x5F62;&#x72B6;&#x5927;&#x5C0F;</li>
</ul>
<p><img src="img/45889773f687ed0d33a3ef9b66b0da32.jpg" alt="https://pytorch.org/docs/stable/_images//Hardshrink.png"></p>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Hardshrink()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="hardtanh">Hardtanh</h3>
<pre><code class="lang-py">class torch.nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False, min_value=None, max_value=None)
</code></pre>
<p>&#x5C06;&#x4E0B;&#x9762;&#x7684;&#x5143;&#x7D20;&#x7EA7;HardTanh&#x51FD;&#x6570;&#x5E94;&#x7528;&#x5230;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0A;&#x3002;</p>
<p>HardTanh&#x51FD;&#x6570;&#x5B9A;&#x4E49;&#x5982;&#x4E0B;:</p>
<p><img src="img/988cd664634d88b1e654ea5e8fe27d9a.jpg" alt=""></p>
<p>&#x7EBF;&#x6027;&#x533A;&#x57DF;<img src="img/f30fa7744d61427a11bf0e75b1557a16.jpg" alt=""> &#x7684;&#x5927;&#x5C0F;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x8BBE;&#x7F6E;<code>min_val</code> &#x53C2;&#x6570; <code>max_val</code>&#x6765;&#x8FDB;&#x884C;&#x8C03;&#x6574;&#x3002;</p>
<p><img src="img/b22ab176e5aca7ca2b17e84fe525620e.jpg" alt="https://pytorch.org/docs/stable/_images//Hardtanh.png"></p>
<p>&#x53C2;&#x6570;: </p>
<ul>
<li><strong>min_val</strong> &#x2013; &#x7EBF;&#x6027;&#x533A;&#x57DF;&#x7684;&#x4E0B;&#x9650;. &#x9ED8;&#x8BA4;: -1</li>
<li><strong>max_val</strong> &#x2013; &#x7EBF;&#x6027;&#x533A;&#x57DF;&#x7684;&#x4E0A;&#x9650;. &#x9ED8;&#x8BA4;: 1</li>
<li><strong>inplace</strong> &#x2013; &#x662F;&#x5426;&#x8FDB;&#x884C;&#x539F;&#x4F4D;&#x64CD;&#x4F5C;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; <code>False</code></li>
</ul>
<p>&#x4E4B;&#x524D;&#x7248;&#x672C;&#x7684;<code>min_value</code> &#x548C; <code>max_value</code> &#x53C2;&#x6570;&#x5DF2;&#x7ECF;&#x88AB;&#x5E9F;&#x5F03;&#x6389;&#x4E86;&#xFF0C;&#x6539;&#x4E3A;<code>min_val</code> &#x548C; <code>max_val</code>&#x53C2;&#x6570;&#x3002;</p>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> &#x5176;&#x4E2D; <code>*</code> &#x4EE3;&#x8868;&#x652F;&#x6301;&#x4EFB;&#x610F;&#x5927;&#x5C0F;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, &#x4E0E;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4FDD;&#x6301;&#x4E00;&#x6837;&#x7684;&#x5F62;&#x72B6;&#x5927;&#x5C0F;</li>
</ul>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Hardtanh(<span class="hljs-number">-2</span>, <span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="leakyrelu">LeakyReLU</h3>
<pre><code class="lang-py">class torch.nn.LeakyReLU(negative_slope=0.01, inplace=False)
</code></pre>
<p>&#x5C06;&#x4E0B;&#x9762;&#x7684;&#x5143;&#x7D20;&#x7EA7;&#x51FD;&#x6570;&#x5E94;&#x7528;&#x5230;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0A;&#xFF1A;</p>
<p><img src="img/92ce1c3da3211f30ef5273403da71c7a.jpg" alt=""></p>
<p>&#x6216;</p>
<p><img src="img/377c237cda65f4c68e3138efcc2bfef4.jpg" alt=""></p>
<p>Parameters: </p>
<ul>
<li><strong>negative_slope</strong> &#x2013; &#x63A7;&#x5236;&#x8D1F;&#x6570;&#x8303;&#x56F4;&#x51FD;&#x6570;&#x7684;&#x659C;&#x7387;&#x3002; &#x9ED8;&#x8BA4;: 1e-2</li>
<li><strong>inplace</strong> &#x2013; &#x662F;&#x5426;&#x8FDB;&#x884C;&#x539F;&#x4F4D;&#x64CD;&#x4F5C;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; <code>False</code></li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> &#x5176;&#x4E2D; <code>*</code> &#x4EE3;&#x8868;&#x652F;&#x6301;&#x4EFB;&#x610F;&#x5927;&#x5C0F;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, &#x4E0E;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4FDD;&#x6301;&#x4E00;&#x6837;&#x7684;&#x5F62;&#x72B6;&#x5927;&#x5C0F;</li>
</ul>
<p><img src="img/7df3c1e498d7d00e9e32ce7716e15fc3.jpg" alt="https://pytorch.org/docs/stable/_images//LeakyReLU.png"></p>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.LeakyReLU(<span class="hljs-number">0.1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="logsigmoid">LogSigmoid</h3>
<pre><code class="lang-py">class torch.nn.LogSigmoid
</code></pre>
<p>&#x5C06;&#x4E0B;&#x9762;&#x7684;&#x5143;&#x7D20;&#x7EA7;&#x51FD;&#x6570;LogSigmoid&#x5E94;&#x7528;&#x5230;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0A;&#xFF1A;(&#x6B64;&#x5904;&#x6F0F;&#x4E86;&#x4E00;&#x5F20;&#x56FE;&#xFF0C;&#x540E;&#x671F;&#x8865;&#x4E00;&#x4E0B;)</p>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> &#x5176;&#x4E2D; <code>*</code> &#x4EE3;&#x8868;&#x652F;&#x6301;&#x4EFB;&#x610F;&#x5927;&#x5C0F;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, &#x4E0E;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4FDD;&#x6301;&#x4E00;&#x6837;&#x7684;&#x5F62;&#x72B6;&#x5927;&#x5C0F;</li>
</ul>
<p><img src="img/f03b653b702dcd536fbb404c6461b399.jpg" alt="https://pytorch.org/docs/stable/_images//LogSigmoid.png"></p>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.LogSigmoid()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="prelu">PReLU</h3>
<pre><code class="lang-py">class torch.nn.PReLU(num_parameters=1, init=0.25)
</code></pre>
<p>&#x5C06;&#x4E0B;&#x9762;&#x7684;&#x5143;&#x7D20;&#x7EA7;&#x51FD;&#x6570;&#x5E94;&#x7528;&#x5230;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0A;&#xFF1A;</p>
<p><img src="img/96fb709d31ba330ca192080e660d4cf1.jpg" alt=""></p>
<p>&#x6216;</p>
<p><img src="img/f460dc15bedfa96b8a320033b3f4fd6f.jpg" alt=""></p>
<p>&#x6B64;&#x5904; <img src="img/070b1af5eca3a5c5d72884b536090f17.jpg" alt=""> &#x662F;&#x4E00;&#x4E2A;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x53C2;&#x6570;&#x3002; &#x5982;&#x679C;&#x5728;&#x8C03;&#x7528;<code>nn.PReLU()</code>&#x51FD;&#x6570;&#x7684;&#x65F6;&#x5019;&#x6CA1;&#x6709;&#x4F20;&#x5165;&#x53C2;&#x6570;&#xFF0C;&#x90A3;&#x4E48;&#x4F1A;&#x9ED8;&#x8BA4;&#x5728;&#x6240;&#x6709;&#x7684;&#x8F93;&#x5165;&#x901A;&#x9053;&#x4E0A;&#x5E94;&#x7528;&#x540C;&#x4E00;&#x4E2A;<img src="img/070b1af5eca3a5c5d72884b536090f17.jpg" alt=""> &#x53C2;&#x6570;&#x3002; &#x5982;&#x679C;&#x4EE5;<code>nn.PReLU(nChannels)</code>&#x8FD9;&#x79CD;&#x65B9;&#x5F0F;&#x8C03;&#x7528;&#xFF0C; &#x6BCF;&#x4E2A;&#x8F93;&#x5165;&#x901A;&#x9053;&#x90FD;&#x4F1A;&#x6709;&#x4E00;&#x4E2A;&#x5355;&#x72EC;&#x7684;<img src="img/070b1af5eca3a5c5d72884b536090f17.jpg" alt=""> &#x53C2;&#x6570;&#x3002;</p>
<p>Note</p>
<p>&#x60F3;&#x8981;&#x5B66;&#x4E00;&#x4E2A;&#x597D;&#x7684;<img src="img/070b1af5eca3a5c5d72884b536090f17.jpg" alt="">&#x53C2;&#x6570;&#xFF0C;&#x6700;&#x597D;&#x4E0D;&#x8981;&#x7528;weight decay&#x3002;</p>
<p>Note</p>
<p>&#x901A;&#x9053;&#x7EF4;&#x5EA6;&#x662F;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x7B2C;&#x4E8C;&#x4E2A;&#x7EF4;&#x5EA6;&#x3002;&#x5F53;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x7EF4;&#x5EA6;&#x6570; &lt; 2&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x90A3;&#x6B64;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x5C31;&#x6CA1;&#x6709;&#x901A;&#x9053;&#x7EF4;&#x5EA6;&#xFF0C;&#x800C;&#x6B64;&#x65F6;&#x5176;&#x901A;&#x9053;&#x6570;&#x88AB;&#x8BA4;&#x4E3A;&#x662F;1&#x3002;</p>
<p>Parameters: </p>
<ul>
<li><strong>num_parameters</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; &#x8981;&#x8FDB;&#x884C;&#x8BAD;&#x7EC3;&#x5B66;&#x4E60;&#x7684; <img src="img/070b1af5eca3a5c5d72884b536090f17.jpg" alt=""> &#x53C2;&#x6570;&#x7684;&#x6570;&#x91CF;&#x3002;&#x5C3D;&#x7BA1;&#x6B64;&#x51FD;&#x6570;&#x7684;&#x8F93;&#x5165;&#x662F;&#x4E00;&#x4E2A;&#x6574;&#x5F62;&#xFF0C;&#x4F46;&#x6B64;&#x51FD;&#x6570;&#x8981;&#x6C42;&#x8F93;&#x5165;&#x7684;&#x6574;&#x5F62;&#x53EA;&#x80FD;&#x4E3A;&#x4E24;&#x4E2A;&#x503C;&#xFF0C;1&#x6216;&#x8005;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x901A;&#x9053;&#x6570;&#x3002;&#x9ED8;&#x8BA4;&#xFF1A;1 </li>
<li><strong>init</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a>) &#x2013; <img src="img/070b1af5eca3a5c5d72884b536090f17.jpg" alt="">&#x7684;&#x521D;&#x59CB;&#x503C;&#xFF0C;&#x9ED8;&#x8BA4;: 0.25</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> &#x5176;&#x4E2D; <code>*</code> &#x4EE3;&#x8868;&#x652F;&#x6301;&#x4EFB;&#x610F;&#x5927;&#x5C0F;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, &#x4E0E;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4FDD;&#x6301;&#x4E00;&#x6837;&#x7684;&#x5F62;&#x72B6;&#x5927;&#x5C0F;</li>
</ul>
<table>
<thead>
<tr>
<th>Variables:</th>
<th><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; &#x5927;&#x5C0F;&#x4E3A;<code>num_parameters</code>&#x7684;&#x53EF;&#x5B66;&#x4E60;&#x53C2;&#x6570;&#x3002;The attr:<code>dtype</code> is default to&#xFF08;&#x8FD9;&#x53E5;&#x8BDD;&#x6709;&#x70B9;&#x95EE;&#x9898;&#xFF0C; to&#x540E;&#x9762;&#x6F0F;&#x6389;&#x4E86;&#xFF09;</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><img src="img/59baba7257ac05b747455a25a3457baf.jpg" alt="https://pytorch.org/docs/stable/_images//PReLU.png"></p>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.PReLU()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="relu">ReLU</h3>
<pre><code class="lang-py">class torch.nn.ReLU(inplace=False)
</code></pre>
<p>&#x5C06;&#x5143;&#x7D20;&#x7EA7;&#x7EBF;&#x6027;&#x6574;&#x6D41;&#x51FD;&#x6570;&#x51FD;&#x6570;&#x5E94;&#x7528;&#x5230;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0A;<img src="img/f859c48107afb47986b3297459048c80.jpg" alt="">
Applies the rectified linear unit function element-wise </p>
<p><img src="img/6bfe295d2f51e4e33648ffb4273723a6.jpg" alt="https://pytorch.org/docs/stable/_images//ReLU.png"></p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>inplace</strong> &#x2013; &#x662F;&#x5426;&#x8FDB;&#x884C;&#x539F;&#x4F4D;&#x64CD;&#x4F5C;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; <code>False</code></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> &#x5176;&#x4E2D; <code>*</code> &#x4EE3;&#x8868;&#x652F;&#x6301;&#x4EFB;&#x610F;&#x5927;&#x5C0F;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, &#x4E0E;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4FDD;&#x6301;&#x4E00;&#x6837;&#x7684;&#x5F62;&#x72B6;&#x5927;&#x5C0F;</li>
</ul>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ReLU()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="relu6">ReLU6</h3>
<pre><code class="lang-py">class torch.nn.ReLU6(inplace=False)
</code></pre>
<p>&#x5C06;&#x4E0B;&#x9762;&#x7684;&#x5143;&#x7D20;&#x7EA7;&#x51FD;&#x6570;&#x5E94;&#x7528;&#x5230;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0A;:</p>
<p><img src="img/38c45c0cb00fa6f9a372816012b26b01.jpg" alt=""></p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>inplace</strong> &#x2013; &#x662F;&#x5426;&#x8FDB;&#x884C;&#x539F;&#x4F4D;&#x64CD;&#x4F5C;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; <code>False</code></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> &#x5176;&#x4E2D; <code>*</code> &#x4EE3;&#x8868;&#x652F;&#x6301;&#x4EFB;&#x610F;&#x5927;&#x5C0F;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, &#x4E0E;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4FDD;&#x6301;&#x4E00;&#x6837;&#x7684;&#x5F62;&#x72B6;&#x5927;&#x5C0F;</li>
</ul>
<p><img src="img/3a82f9216f0db7d59f6c0f1c169156b0.jpg" alt="https://pytorch.org/docs/stable/_images//ReLU6.png"></p>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ReLU6()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="rrelu">RReLU</h3>
<pre><code class="lang-py">class torch.nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=False)
</code></pre>
<p>&#x5C06;&#x5143;&#x7D20;&#x7EA7;&#x968F;&#x673A;&#x7EBF;&#x6027;&#x6574;&#x6D41;&#x51FD;&#x6570;&#x51FD;&#x6570;&#x5E94;&#x7528;&#x5230;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0A;&#xFF0C;&#x8BE6;&#x60C5;&#x6B64;&#x6587;&#x7AE0;&#xFF1A;</p>
<p><a href="https://arxiv.org/abs/1505.00853" target="_blank">Empirical Evaluation of Rectified Activations in Convolutional Network</a>.</p>
<p>&#x6B64;&#x51FD;&#x6570;&#x5B9A;&#x4E49;&#x5982;&#x4E0B;:</p>
<p><img src="img/69aa6828f2f0bcd0ee1e173223ff4640.jpg" alt=""></p>
<p>&#x5176;&#x4E2D; <img src="img/070b1af5eca3a5c5d72884b536090f17.jpg" alt=""> &#x662F;&#x4ECE;&#x6B64;&#x5747;&#x5300;&#x5206;&#x5E03;&#x4E2D;&#x91C7;&#x6837;&#x800C;&#x6765;&#xFF1A;<img src="img/7323b93ed925c9e5b0ce10c8a6c99daf.jpg" alt="">.</p>
<blockquote>
<p>&#x8BE6;&#x89C1;: <a href="https://arxiv.org/pdf/1505.00853.pdf" target="_blank">https://arxiv.org/pdf/1505.00853.pdf</a></p>
</blockquote>
<p>Parameters: </p>
<ul>
<li><strong>lower</strong> &#x2013; &#x5747;&#x5300;&#x5206;&#x5E03;&#x4E0B;&#x9650;&#xFF0C; &#x9ED8;&#x8BA4;: <img src="img/444fc0427eb64f0bd2c9c16edf680d4f.jpg" alt=""></li>
<li><strong>upper</strong> &#x2013; &#x5747;&#x5300;&#x5206;&#x5E03;&#x4E0A;&#x9650;&#xFF0C;&#x9ED8;&#x8BA4;: <img src="img/a90c89c913a1fe1e9462d60d8668936b.jpg" alt=""></li>
<li><strong>inplace</strong> &#x2013; &#x662F;&#x5426;&#x8FDB;&#x884C;&#x539F;&#x4F4D;&#x64CD;&#x4F5C;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; <code>False</code></li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> &#x5176;&#x4E2D; <code>*</code> &#x4EE3;&#x8868;&#x652F;&#x6301;&#x4EFB;&#x610F;&#x5927;&#x5C0F;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, &#x4E0E;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4FDD;&#x6301;&#x4E00;&#x6837;&#x7684;&#x5F62;&#x72B6;&#x5927;&#x5C0F;</li>
</ul>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.RReLU(<span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="selu">SELU</h3>
<pre><code class="lang-py">class torch.nn.SELU(inplace=False)
</code></pre>
<p>&#x5C06;&#x4E0B;&#x9762;&#x7684;&#x5143;&#x7D20;&#x7EA7;&#x51FD;&#x6570;&#x5E94;&#x7528;&#x5230;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0A;:</p>
<p><img src="img/c6753a504d14886a424af779f5906dc5.jpg" alt=""></p>
<p>&#x5176;&#x4E2D; <img src="img/e97a0b3de4bafa3464e17a8d8f66fd9d.jpg" alt=""> &#x800C;&#x4E14; <img src="img/aa01199f9b814d719de1e728e4a44ac3.jpg" alt="">.</p>
<p><img src="img/10123138310ae40f4a78f55cefe37008.jpg" alt="https://pytorch.org/docs/stable/_images//SELU.png"></p>
<p>More details can be found in the paper <a href="https://arxiv.org/abs/1706.02515" target="_blank">Self-Normalizing Neural Networks</a> .</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>inplace</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; &#x662F;&#x5426;&#x8FDB;&#x884C;&#x539F;&#x4F4D;&#x64CD;&#x4F5C;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; <code>False</code></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> &#x5176;&#x4E2D; <code>*</code> &#x4EE3;&#x8868;&#x652F;&#x6301;&#x4EFB;&#x610F;&#x5927;&#x5C0F;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, &#x4E0E;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4FDD;&#x6301;&#x4E00;&#x6837;&#x7684;&#x5F62;&#x72B6;&#x5927;&#x5C0F;</li>
</ul>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.SELU()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="celu">CELU</h3>
<pre><code class="lang-py">class torch.nn.CELU(alpha=1.0, inplace=False)
</code></pre>
<p>&#x5C06;&#x4E0B;&#x9762;&#x7684;&#x5143;&#x7D20;&#x7EA7;&#x51FD;&#x6570;&#x5E94;&#x7528;&#x5230;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0A;&#xFF1A;</p>
<p><img src="img/86d69b42683362b7781f1a5809c0d0d1.jpg" alt=""></p>
<p>&#x66F4;&#x591A;&#x7EC6;&#x8282;&#x8BF7;&#x89C1;paper: <a href="https://arxiv.org/abs/1704.07483" target="_blank">Continuously Differentiable Exponential Linear Units</a> .</p>
<p>Parameters: </p>
<ul>
<li><strong>alpha</strong> &#x2013; CELU&#x64CD;&#x4F5C;&#x4E2D;&#x7684; <img src="img/82005cc2e0087e2a52c7e43df4a19a00.jpg" alt=""> &#x503C;&#xFF0C;&#x9ED8;&#x8BA4;: 1.0</li>
<li><strong>inplace</strong> &#x2013; &#x662F;&#x5426;&#x8FDB;&#x884C;&#x539F;&#x4F4D;&#x64CD;&#x4F5C;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; <code>False</code></li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> &#x5176;&#x4E2D; <code>*</code> &#x4EE3;&#x8868;&#x652F;&#x6301;&#x4EFB;&#x610F;&#x5927;&#x5C0F;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, &#x4E0E;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4FDD;&#x6301;&#x4E00;&#x6837;&#x7684;&#x5F62;&#x72B6;&#x5927;&#x5C0F;</li>
</ul>
<p><img src="img/8d5fd8f893fb491c170f9a38af6edef9.jpg" alt="https://pytorch.org/docs/stable/_images//CELU.png"></p>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.CELU()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="sigmoid">Sigmoid</h3>
<pre><code class="lang-py">class torch.nn.Sigmoid
</code></pre>
<p>&#x5C06;&#x4E0B;&#x9762;&#x7684;&#x5143;&#x7D20;&#x7EA7;&#x51FD;&#x6570;&#x5E94;&#x7528;&#x5230;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0A;&#xFF1A;</p>
<p><img src="img/8bf3a718397550598124548beb8c6b23.jpg" alt=""></p>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> &#x5176;&#x4E2D; <code>*</code> &#x4EE3;&#x8868;&#x652F;&#x6301;&#x4EFB;&#x610F;&#x5927;&#x5C0F;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, &#x4E0E;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4FDD;&#x6301;&#x4E00;&#x6837;&#x7684;&#x5F62;&#x72B6;&#x5927;&#x5C0F;</li>
</ul>
<p><img src="img/961caeb46e669eb70392afd515f9bde7.jpg" alt="https://pytorch.org/docs/stable/_images//Sigmoid.png"></p>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Sigmoid()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="softplus">Softplus</h3>
<pre><code class="lang-py">class torch.nn.Softplus(beta=1, threshold=20)
</code></pre>
<p>&#x5C06;&#x4E0B;&#x9762;&#x7684;&#x5143;&#x7D20;&#x7EA7;&#x51FD;&#x6570;&#x5E94;&#x7528;&#x5230;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0A;:</p>
<p><img src="img/a0855ca512a1ba09192648efd45082ad.jpg" alt=""></p>
<p>SoftPlus&#x662F;&#x4E00;&#x4E2A;&#x5E73;&#x6ED1;&#x7684;&#x7C7B;ReLU&#x51FD;&#x6570;&#xFF0C;&#x53EF;&#x4EE5;&#x7528;&#x4E8E;&#x5C06;&#x8F93;&#x51FA;&#x7ED3;&#x679C;&#x89C4;&#x8303;&#x5230;&#x5168;&#x6B63;&#x3002;</p>
<p>&#x4E3A;&#x4E86;&#x6570;&#x503C;&#x7A33;&#x5B9A;&#x6027;&#xFF0C;&#x5728;&#x5B9E;&#x73B0;&#x6B64;&#x51FD;&#x6570;&#x7684;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x5F53; <code>x</code> &#x8D85;&#x8FC7;&#x67D0;&#x4E2A;&#x7279;&#x5B9A;&#x503C;&#x4E4B;&#x540E;&#xFF0C;&#x6211;&#x4EEC;&#x4F1A;&#x5C06;&#x6B64;&#x51FD;&#x6570;&#x8F6C;&#x5316;&#x4E3A;&#x4E00;&#x4E2A;&#x7EBF;&#x6027;&#x51FD;&#x6570;&#x3002;</p>
<p>Parameters: </p>
<ul>
<li><strong>beta</strong> &#x2013; Softplus&#x64CD;&#x4F5C;&#x7684; <img src="img/50705df736e9a7919e768cf8c4e4f794.jpg" alt=""> &#x503C;&#xFF0C;&#x9ED8;&#x8BA4;: 1</li>
<li><strong>threshold</strong> &#x2013; &#x5C06;&#x51FD;&#x6570;&#x8F6C;&#x5316;&#x4E3A;&#x7EBF;&#x6027;&#x51FD;&#x6570;&#x7684;&#x9608;&#x503C;&#xFF0C; &#x9ED8;&#x8BA4;: 20</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> &#x5176;&#x4E2D; <code>*</code> &#x4EE3;&#x8868;&#x652F;&#x6301;&#x4EFB;&#x610F;&#x5927;&#x5C0F;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, &#x4E0E;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4FDD;&#x6301;&#x4E00;&#x6837;&#x7684;&#x5F62;&#x72B6;&#x5927;&#x5C0F;</li>
</ul>
<p><img src="img/af06304134e1f56d7abc15570fa5adb9.jpg" alt="https://pytorch.org/docs/stable/_images//Softplus.png"></p>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Softplus()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="softshrink">Softshrink</h3>
<pre><code class="lang-py">class torch.nn.Softshrink(lambd=0.5)
</code></pre>
<p>&#x5C06;&#x4E0B;&#x9762;&#x7684;&#x5143;&#x7D20;&#x7EA7;&#x8F6F;&#x6536;&#x7F29;&#x51FD;&#x6570;&#x5E94;&#x7528;&#x5230;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0A;:</p>
<p><img src="img/5baf58b3007cf434725f41bf2dfae2ce.jpg" alt=""></p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>lambd</strong> &#x2013; &#x8F6F;&#x6536;&#x7F29;&#x8FD0;&#x7B97;&#x7684;<img src="img/5e8df2ba7e47a784c714d176ed8bbb7a.jpg" alt="">&#x503C;&#xFF0C;&#x9ED8;&#x8BA4;: 0.5</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> &#x5176;&#x4E2D; <code>*</code> &#x4EE3;&#x8868;&#x652F;&#x6301;&#x4EFB;&#x610F;&#x5927;&#x5C0F;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, &#x4E0E;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4FDD;&#x6301;&#x4E00;&#x6837;&#x7684;&#x5F62;&#x72B6;&#x5927;&#x5C0F;</li>
</ul>
<p><img src="img/cec198ab680657d41c1d2ac2176e5664.jpg" alt="https://pytorch.org/docs/stable/_images//Softshrink.png"></p>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Softshrink()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="softsign">Softsign</h3>
<pre><code class="lang-py">class torch.nn.Softsign
</code></pre>
<p>&#x5C06;&#x4E0B;&#x9762;&#x7684;&#x5143;&#x7D20;&#x7EA7;&#x51FD;&#x6570;&#x5E94;&#x7528;&#x5230;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0A;:</p>
<p><img src="img/39ba3e3786920ceb12bc26b08b00de1c.jpg" alt=""></p>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> &#x5176;&#x4E2D; <code>*</code> &#x4EE3;&#x8868;&#x652F;&#x6301;&#x4EFB;&#x610F;&#x5927;&#x5C0F;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, &#x4E0E;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4FDD;&#x6301;&#x4E00;&#x6837;&#x7684;&#x5F62;&#x72B6;&#x5927;&#x5C0F;</li>
</ul>
<p><img src="img/b92a09469ce9fc0abfbe8c9af4228391.jpg" alt="https://pytorch.org/docs/stable/_images//Softsign.png"></p>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Softsign()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="tanh">Tanh</h3>
<pre><code class="lang-py">class torch.nn.Tanh
</code></pre>
<p>&#x5C06;&#x4E0B;&#x9762;&#x7684;&#x5143;&#x7D20;&#x7EA7;&#x51FD;&#x6570;&#x5E94;&#x7528;&#x5230;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0A;:</p>
<p><img src="img/e3f58d9a8cbc89b247dd8de1c28bf7ce.jpg" alt=""></p>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> &#x5176;&#x4E2D; <code>*</code> &#x4EE3;&#x8868;&#x652F;&#x6301;&#x4EFB;&#x610F;&#x5927;&#x5C0F;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, &#x4E0E;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4FDD;&#x6301;&#x4E00;&#x6837;&#x7684;&#x5F62;&#x72B6;&#x5927;&#x5C0F;</li>
</ul>
<p><img src="img/5605304fc1fec06669e17cc872d47580.jpg" alt="https://pytorch.org/docs/stable/_images//Tanh.png"></p>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Tanh()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="tanhshrink">Tanhshrink</h3>
<pre><code class="lang-py">class torch.nn.Tanhshrink
</code></pre>
<p>&#x5C06;&#x4E0B;&#x9762;&#x7684;&#x5143;&#x7D20;&#x7EA7;&#x51FD;&#x6570;&#x5E94;&#x7528;&#x5230;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0A;:</p>
<p><img src="img/136fda10bacf7ae9068ca487ba861805.jpg" alt=""></p>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> &#x5176;&#x4E2D; <code>*</code> &#x4EE3;&#x8868;&#x652F;&#x6301;&#x4EFB;&#x610F;&#x5927;&#x5C0F;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, &#x4E0E;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4FDD;&#x6301;&#x4E00;&#x6837;&#x7684;&#x5F62;&#x72B6;&#x5927;&#x5C0F;</li>
</ul>
<p><img src="img/8aea39259742ccdb14701a8f3c351b56.jpg" alt="https://pytorch.org/docs/stable/_images//Tanhshrink.png"></p>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Tanhshrink()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="threshold">Threshold</h3>
<pre><code class="lang-py">class torch.nn.Threshold(threshold, value, inplace=False)
</code></pre>
<p>&#x4F7F;&#x7528;&#x9608;&#x503C;&#x8FC7;&#x6EE4;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;</p>
<p>&#x9608;&#x503C;&#x88AB;&#x5B9A;&#x4E49;&#x5982;&#x4E0B;&#xFF1A;</p>
<p><img src="img/3b32031caba73686c02a117e8e307c6f.jpg" alt=""></p>
<p>Parameters: </p>
<ul>
<li><strong>threshold</strong> &#x2013; &#x9608;&#x503C;&#x5927;&#x5C0F; </li>
<li><strong>value</strong> &#x2013; &#x5C0F;&#x4E8E;&#x9608;&#x503C;&#x7684;&#x5143;&#x7D20;&#x7684;&#x66FF;&#x6362;&#x503C;</li>
<li><strong>inplace</strong> &#x2013; &#x662F;&#x5426;&#x8FDB;&#x884C;&#x539F;&#x4F4D;&#x64CD;&#x4F5C;&#x3002; &#x9ED8;&#x8BA4;&#xFF1A; <code>False</code></li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>&#x8F93;&#x5165;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> &#x5176;&#x4E2D; <code>*</code> &#x4EE3;&#x8868;&#x652F;&#x6301;&#x4EFB;&#x610F;&#x5927;&#x5C0F;&#x7684;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, &#x4E0E;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4FDD;&#x6301;&#x4E00;&#x6837;&#x7684;&#x5F62;&#x72B6;&#x5927;&#x5C0F;</li>
</ul>
<p>&#x793A;&#x4F8B;:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Threshold(<span class="hljs-number">0.1</span>, <span class="hljs-number">20</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h2 id="non-linear-activations-other">Non-linear activations (other)</h2>
<h3 id="softmin">Softmin</h3>
<pre><code class="lang-py">class torch.nn.Softmin(dim=None)
</code></pre>
<p>Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range <code>(0, 1)</code> and sum to 1</p>
<p><img src="img/440f43280457a9287bbddf28553f8f70.jpg" alt=""></p>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: any shape</li>
<li>Output: same as input</li>
</ul>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; A dimension along which Softmin will be computed (so every slice along dim will sum to 1).</th>
</tr>
</thead>
<tbody>
<tr>
<td>Returns:</td>
<td>a Tensor of the same dimension and shape as the input, with values in the range [0, 1]</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Softmin()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="softmax">Softmax</h3>
<pre><code class="lang-py">class torch.nn.Softmax(dim=None)
</code></pre>
<p>Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range (0,1) and sum to 1</p>
<p>Softmax is defined as:</p>
<p><img src="img/cbfd37534eccdda606d4f8494c31d2c0.jpg" alt=""></p>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: any shape</li>
<li>Output: same as input</li>
</ul>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>a Tensor of the same dimension and shape as the input with values in the range [0, 1]</th>
</tr>
</thead>
<tbody>
<tr>
<td>Parameters:</td>
<td><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; A dimension along which Softmax will be computed (so every slice along dim will sum to 1).</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<p>Note</p>
<p>This module doesn&#x2019;t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use <code>LogSoftmax</code> instead (it&#x2019;s faster and has better numerical properties).</p>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Softmax()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="softmax2d">Softmax2d</h3>
<pre><code class="lang-py">class torch.nn.Softmax2d
</code></pre>
<p>Applies SoftMax over features to each spatial location.</p>
<p>When given an image of <code>Channels x Height x Width</code>, it will apply <code>Softmax</code> to each location <img src="img/b16a2a186bda385e5f0016f5fe5a5c36.jpg" alt=""></p>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/23f8772594b27bd387be708fe9c085e1.jpg" alt=""></li>
<li>Output: <img src="img/23f8772594b27bd387be708fe9c085e1.jpg" alt=""> (same shape as input)</li>
</ul>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>a Tensor of the same dimension and shape as the input with values in the range [0, 1]</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Softmax2d()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># you softmax over the 2nd dimension</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="logsoftmax">LogSoftmax</h3>
<pre><code class="lang-py">class torch.nn.LogSoftmax(dim=None)
</code></pre>
<p>Applies the <img src="img/db163ba416e1349a426e6a137e082ae2.jpg" alt=""> function to an n-dimensional input Tensor. The LogSoftmax formulation can be simplified as:</p>
<p><img src="img/397c4cfa2f291306d481811192d2d5d9.jpg" alt=""></p>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: any shape</li>
<li>Output: same as input</li>
</ul>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; A dimension along which Softmax will be computed (so every slice along dim will sum to 1).</th>
</tr>
</thead>
<tbody>
<tr>
<td>Returns:</td>
<td>a Tensor of the same dimension and shape as the input with values in the range [-inf, 0)</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.LogSoftmax()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="adaptivelogsoftmaxwithloss">AdaptiveLogSoftmaxWithLoss</h3>
<pre><code class="lang-py">class torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, div_value=4.0, head_bias=False)
</code></pre>
<p>Efficient softmax approximation as described in <a href="https://arxiv.org/abs/1609.04309" target="_blank">Efficient softmax approximation for GPUs</a> by Edouard Grave, Armand Joulin, Moustapha Ciss&#xE9;, David Grangier, and Herv&#xE9; J&#xE9;gou.</p>
<p>Adaptive softmax is an approximate strategy for training models with large output spaces. It is most effective when the label distribution is highly imbalanced, for example in natural language modelling, where the word frequency distribution approximately follows the <a href="https://en.wikipedia.org/wiki/Zipf%27s_law" target="_blank">Zipf&#x2019;s law</a>.</p>
<p>Adaptive softmax partitions the labels into several clusters, according to their frequency. These clusters may contain different number of targets each. Additionally, clusters containing less frequent labels assign lower dimensional embeddings to those labels, which speeds up the computation. For each minibatch, only clusters for which at least one target is present are evaluated.</p>
<p>The idea is that the clusters which are accessed frequently (like the first one, containing most frequent labels), should also be cheap to compute &#x2013; that is, contain a small number of assigned labels.</p>
<p>We highly recommend taking a look at the original paper for more details.</p>
<ul>
<li><code>cutoffs</code> should be an ordered Sequence of integers sorted in the increasing order. It controls number of clusters and the partitioning of targets into clusters. For example setting <code>cutoffs = [10, 100, 1000]</code> means that first <code>10</code> targets will be assigned to the &#x2018;head&#x2019; of the adaptive softmax, targets <code>11, 12, &#x2026;, 100</code> will be assigned to the first cluster, and targets <code>101, 102, &#x2026;, 1000</code> will be assigned to the second cluster, while targets <code>1001, 1002, &#x2026;, n_classes - 1</code> will be assigned to the last, third cluster</li>
<li><code>div_value</code> is used to compute the size of each additional cluster, which is given as <img src="img/32071c42affaaf731df4e3398b16de10.jpg" alt="">, where <img src="img/5df73ea97de3a7712b50ce2fecfea1a7.jpg" alt=""> is the cluster index (with clusters for less frequent words having larger indices, and indices starting from <img src="img/a3ea24a1f2a3549d3e5b0cacf3ecb7c7.jpg" alt="">).</li>
<li><code>head_bias</code> if set to True, adds a bias term to the &#x2018;head&#x2019; of the adaptive softmax. See paper for details. Set to False in the official implementation.</li>
</ul>
<p>Warning</p>
<p>Labels passed as inputs to this module should be sorted accoridng to their frequency. This means that the most frequent label should be represented by the index <code>0</code>, and the least frequent label should be represented by the index <code>n_classes - 1</code>.</p>
<p>Note</p>
<p>This module returns a <code>NamedTuple</code> with <code>output</code> and <code>loss</code> fields. See further documentation for details.</p>
<p>Note</p>
<p>To compute log-probabilities for all classes, the <code>log_prob</code> method can be used.</p>
<p>Parameters: </p>
<ul>
<li><strong>in_features</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; Number of features in the input tensor</li>
<li><strong>n_classes</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; Number of classes in the dataset.</li>
<li><strong>cutoffs</strong> (<em>Sequence</em>) &#x2013; Cutoffs used to assign targets to their buckets.</li>
<li><strong>div_value</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a><em>,</em> <em>optional</em>) &#x2013; value used as an exponent to compute sizes of the clusters. Default: 4.0</li>
</ul>
<p>| Returns: | </p>
<ul>
<li><strong>output</strong> is a Tensor of size <code>N</code> containing computed target log probabilities for each example</li>
<li><strong>loss</strong> is a Scalar representing the computed negative log likelihood loss</li>
</ul>
<table>
<thead>
<tr>
<th>Return type:</th>
<th><code>NamedTuple</code> with <code>output</code> and <code>loss</code> fields</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>input: <img src="img/768be7688f5f58f4766106ddb821b007.jpg" alt=""></li>
<li>target: <img src="img/2a3e2b832e04fe8d66596083b23da518.jpg" alt=""> where each value satisfies <img src="img/fe1e80e9faca308456bc49d4e79013e0.jpg" alt=""></li>
<li>output: <img src="img/2a3e2b832e04fe8d66596083b23da518.jpg" alt=""></li>
<li>loss: <code>Scalar</code></li>
</ul>
<pre><code class="lang-py">log_prob(input)
</code></pre>
<p>Computes log probabilities for all <img src="img/b285cfddea560d447d391e9d7ba660ba.jpg" alt=""></p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; a minibatch of examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Returns:</td>
<td>log-probabilities of for each class <img src="img/32624581da7de65d68eb11d4201f9bef.jpg" alt=""> in range <img src="img/4e4201057f969a42a8d2de89e5f7c728.jpg" alt="">, where <img src="img/b285cfddea560d447d391e9d7ba660ba.jpg" alt=""> is a parameter passed to <code>AdaptiveLogSoftmaxWithLoss</code> constructor.</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/768be7688f5f58f4766106ddb821b007.jpg" alt=""></li>
<li>Output: <img src="img/f3bcbc1689556ad810d1c658d44bd970.jpg" alt=""></li>
</ul>
<pre><code class="lang-py">predict(input)
</code></pre>
<p>This is equivalent to <code>self.log_pob(input).argmax(dim=1)</code>, but is more efficient in some cases.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; a minibatch of examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Returns:</td>
<td>a class with the highest probability for each example</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
<tr>
<td>Return type:</td>
<td>output (<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/768be7688f5f58f4766106ddb821b007.jpg" alt=""></li>
<li>Output: <img src="img/2a3e2b832e04fe8d66596083b23da518.jpg" alt=""></li>
</ul>
<h2 id="normalization-layers">Normalization layers</h2>
<h3 id="batchnorm1d">BatchNorm1d</h3>
<pre><code class="lang-py">class torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
</code></pre>
<p>Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper <a href="https://arxiv.org/abs/1502.03167" target="_blank">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p>
<p><img src="img/beea63da4eceb7d4c8971e826bafbb1a.jpg" alt=""></p>
<p>The mean and standard-deviation are calculated per-dimension over the mini-batches and <img src="img/cdab9437b701fd21fb3294cfba7c4bc2.jpg" alt=""> and <img src="img/50705df736e9a7919e768cf8c4e4f794.jpg" alt=""> are learnable parameter vectors of size <code>C</code> (where <code>C</code> is the input size). By default, the elements of <img src="img/cdab9437b701fd21fb3294cfba7c4bc2.jpg" alt=""> are sampled from <img src="img/7ad9c99a642c915c6d560cbca6352454.jpg" alt=""> and the elements of <img src="img/50705df736e9a7919e768cf8c4e4f794.jpg" alt=""> are set to 0.</p>
<p>Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p>
<p>If <code>track_running_stats</code> is set to <code>False</code>, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.</p>
<p>Note</p>
<p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is <img src="img/05beed2a6202dfed2f2c4d1ddf9f445f.jpg" alt="">, where <img src="img/9d834e987d38585c39d150fe8f46bc74.jpg" alt=""> is the estimated statistic and <img src="img/22c5ed7653e3fae804006a00210327fc.jpg" alt=""> is the new observed value.</p>
<p>Because the Batch Normalization is done over the <code>C</code> dimension, computing statistics on <code>(N, L)</code> slices, it&#x2019;s common terminology to call this Temporal Batch Normalization.</p>
<p>Parameters: </p>
<ul>
<li><strong>num_features</strong> &#x2013; <img src="img/6c8feca3b2da3d6cf371417edff4be4f.jpg" alt=""> from an expected input of size <img src="img/5816e96aa78b7425cf792435bba8bc29.jpg" alt=""> or <img src="img/db4a9fef02111450bf98261889de550c.jpg" alt=""> from input of size <img src="img/b6d0ccc6531c5d648e750c417c5cc72d.jpg" alt=""></li>
<li><strong>eps</strong> &#x2013; a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> &#x2013; the value used for the running_mean and running_var computation. Can be set to <code>None</code> for cumulative moving average (i.e. simple average). Default: 0.1</li>
<li><strong>affine</strong> &#x2013; a boolean value that when set to <code>True</code>, this module has learnable affine parameters. Default: <code>True</code></li>
<li><strong>track_running_stats</strong> &#x2013; a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>True</code></li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/9b9aebaa467ad07dca05b5086bd21ca2.jpg" alt=""> or <img src="img/5816e96aa78b7425cf792435bba8bc29.jpg" alt=""></li>
<li>Output: <img src="img/9b9aebaa467ad07dca05b5086bd21ca2.jpg" alt=""> or <img src="img/5816e96aa78b7425cf792435bba8bc29.jpg" alt=""> (same shape as input)</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.BatchNorm1d(<span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Without Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.BatchNorm1d(<span class="hljs-number">100</span>, affine=<span class="hljs-keyword">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="batchnorm2d">BatchNorm2d</h3>
<pre><code class="lang-py">class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
</code></pre>
<p>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper <a href="https://arxiv.org/abs/1502.03167" target="_blank">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p>
<p><img src="img/63ee6938c8dea3b7cc66a2a245b15cfc.jpg" alt=""></p>
<p>The mean and standard-deviation are calculated per-dimension over the mini-batches and <img src="img/cdab9437b701fd21fb3294cfba7c4bc2.jpg" alt=""> and <img src="img/50705df736e9a7919e768cf8c4e4f794.jpg" alt=""> are learnable parameter vectors of size <code>C</code> (where <code>C</code> is the input size). By default, the elements of <img src="img/cdab9437b701fd21fb3294cfba7c4bc2.jpg" alt=""> are sampled from <img src="img/7ad9c99a642c915c6d560cbca6352454.jpg" alt=""> and the elements of <img src="img/50705df736e9a7919e768cf8c4e4f794.jpg" alt=""> are set to 0.</p>
<p>Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p>
<p>If <code>track_running_stats</code> is set to <code>False</code>, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.</p>
<p>Note</p>
<p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is <img src="img/05beed2a6202dfed2f2c4d1ddf9f445f.jpg" alt="">, where <img src="img/9d834e987d38585c39d150fe8f46bc74.jpg" alt=""> is the estimated statistic and <img src="img/22c5ed7653e3fae804006a00210327fc.jpg" alt=""> is the new observed value.</p>
<p>Because the Batch Normalization is done over the <code>C</code> dimension, computing statistics on <code>(N, H, W)</code> slices, it&#x2019;s common terminology to call this Spatial Batch Normalization.</p>
<p>Parameters: </p>
<ul>
<li><strong>num_features</strong> &#x2013; <img src="img/6c8feca3b2da3d6cf371417edff4be4f.jpg" alt=""> from an expected input of size <img src="img/23f8772594b27bd387be708fe9c085e1.jpg" alt=""></li>
<li><strong>eps</strong> &#x2013; a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> &#x2013; the value used for the running_mean and running_var computation. Can be set to <code>None</code> for cumulative moving average (i.e. simple average). Default: 0.1</li>
<li><strong>affine</strong> &#x2013; a boolean value that when set to <code>True</code>, this module has learnable affine parameters. Default: <code>True</code></li>
<li><strong>track_running_stats</strong> &#x2013; a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>True</code></li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/23f8772594b27bd387be708fe9c085e1.jpg" alt=""></li>
<li>Output: <img src="img/23f8772594b27bd387be708fe9c085e1.jpg" alt=""> (same shape as input)</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.BatchNorm2d(<span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Without Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.BatchNorm2d(<span class="hljs-number">100</span>, affine=<span class="hljs-keyword">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">100</span>, <span class="hljs-number">35</span>, <span class="hljs-number">45</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="batchnorm3d">BatchNorm3d</h3>
<pre><code class="lang-py">class torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
</code></pre>
<p>Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper <a href="https://arxiv.org/abs/1502.03167" target="_blank">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p>
<p><img src="img/63ee6938c8dea3b7cc66a2a245b15cfc.jpg" alt=""></p>
<p>The mean and standard-deviation are calculated per-dimension over the mini-batches and <img src="img/cdab9437b701fd21fb3294cfba7c4bc2.jpg" alt=""> and <img src="img/50705df736e9a7919e768cf8c4e4f794.jpg" alt=""> are learnable parameter vectors of size <code>C</code> (where <code>C</code> is the input size). By default, the elements of <img src="img/cdab9437b701fd21fb3294cfba7c4bc2.jpg" alt=""> are sampled from <img src="img/7ad9c99a642c915c6d560cbca6352454.jpg" alt=""> and the elements of <img src="img/50705df736e9a7919e768cf8c4e4f794.jpg" alt=""> are set to 0.</p>
<p>Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p>
<p>If <code>track_running_stats</code> is set to <code>False</code>, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.</p>
<p>Note</p>
<p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is <img src="img/05beed2a6202dfed2f2c4d1ddf9f445f.jpg" alt="">, where <img src="img/9d834e987d38585c39d150fe8f46bc74.jpg" alt=""> is the estimated statistic and <img src="img/22c5ed7653e3fae804006a00210327fc.jpg" alt=""> is the new observed value.</p>
<p>Because the Batch Normalization is done over the <code>C</code> dimension, computing statistics on <code>(N, D, H, W)</code> slices, it&#x2019;s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.</p>
<p>Parameters: </p>
<ul>
<li><strong>num_features</strong> &#x2013; <img src="img/6c8feca3b2da3d6cf371417edff4be4f.jpg" alt=""> from an expected input of size <img src="img/f5a45f7b445db562b21cfcb525637aab.jpg" alt=""></li>
<li><strong>eps</strong> &#x2013; a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> &#x2013; the value used for the running_mean and running_var computation. Can be set to <code>None</code> for cumulative moving average (i.e. simple average). Default: 0.1</li>
<li><strong>affine</strong> &#x2013; a boolean value that when set to <code>True</code>, this module has learnable affine parameters. Default: <code>True</code></li>
<li><strong>track_running_stats</strong> &#x2013; a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>True</code></li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/f5a45f7b445db562b21cfcb525637aab.jpg" alt=""></li>
<li>Output: <img src="img/f5a45f7b445db562b21cfcb525637aab.jpg" alt=""> (same shape as input)</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.BatchNorm3d(<span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Without Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.BatchNorm3d(<span class="hljs-number">100</span>, affine=<span class="hljs-keyword">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">100</span>, <span class="hljs-number">35</span>, <span class="hljs-number">45</span>, <span class="hljs-number">10</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="groupnorm">GroupNorm</h3>
<pre><code class="lang-py">class torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True)
</code></pre>
<p>Applies Group Normalization over a mini-batch of inputs as described in the paper <a href="https://arxiv.org/abs/1803.08494" target="_blank">Group Normalization</a> .</p>
<p><img src="img/2fee766f06767b7b87b3531029d92e1d.jpg" alt=""></p>
<p>The input channels are separated into <code>num_groups</code> groups, each containing <code>num_channels / num_groups</code> channels. The mean and standard-deviation are calculated separately over the each group. <img src="img/cdab9437b701fd21fb3294cfba7c4bc2.jpg" alt=""> and <img src="img/50705df736e9a7919e768cf8c4e4f794.jpg" alt=""> are learnable per-channel affine transform parameter vectorss of size <code>num_channels</code> if <code>affine</code> is <code>True</code>.</p>
<p>This layer uses statistics computed from input data in both training and evaluation modes.</p>
<p>Parameters: </p>
<ul>
<li><strong>num_groups</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; number of groups to separate the channels into</li>
<li><strong>num_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; number of channels expected in input</li>
<li><strong>eps</strong> &#x2013; a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>affine</strong> &#x2013; a boolean value that when set to <code>True</code>, this module has learnable per-channel affine parameters initialized to ones (for weights) and zeros (for biases). Default: <code>True</code>.</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/f5be296779e9f325e5c8f0c2284bc073.jpg" alt=""></li>
<li>Output: <img src="img/f5be296779e9f325e5c8f0c2284bc073.jpg" alt=""> (same shape as input)</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">6</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Separate 6 channels into 3 groups</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.GroupNorm(<span class="hljs-number">3</span>, <span class="hljs-number">6</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Separate 6 channels into 6 groups (equivalent with InstanceNorm)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.GroupNorm(<span class="hljs-number">6</span>, <span class="hljs-number">6</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Put all 6 channels into a single group (equivalent with LayerNorm)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.GroupNorm(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Activating the module</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="instancenorm1d">InstanceNorm1d</h3>
<pre><code class="lang-py">class torch.nn.InstanceNorm1d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
</code></pre>
<p>Applies Instance Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper <a href="https://arxiv.org/abs/1607.08022" target="_blank">Instance Normalization: The Missing Ingredient for Fast Stylization</a> .</p>
<p><img src="img/63ee6938c8dea3b7cc66a2a245b15cfc.jpg" alt=""></p>
<p>The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. <img src="img/cdab9437b701fd21fb3294cfba7c4bc2.jpg" alt=""> and <img src="img/50705df736e9a7919e768cf8c4e4f794.jpg" alt=""> are learnable parameter vectors of size <code>C</code> (where <code>C</code> is the input size) if <code>affine</code> is <code>True</code>.</p>
<p>By default, this layer uses instance statistics computed from input data in both training and evaluation modes.</p>
<p>If <code>track_running_stats</code> is set to <code>True</code>, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p>
<p>Note</p>
<p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is <img src="img/05beed2a6202dfed2f2c4d1ddf9f445f.jpg" alt="">, where <img src="img/9d834e987d38585c39d150fe8f46bc74.jpg" alt=""> is the estimated statistic and <img src="img/22c5ed7653e3fae804006a00210327fc.jpg" alt=""> is the new observed value.</p>
<p>Note</p>
<p><a href="#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code>InstanceNorm1d</code></a> and <a href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a> are very similar, but have some subtle differences. <a href="#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code>InstanceNorm1d</code></a> is applied on each channel of channeled data like multidimensional time series, but <a href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a> is usually applied on entire sample and often in NLP tasks. Additionaly, <a href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a> applies elementwise affine transform, while <a href="#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code>InstanceNorm1d</code></a> usually don&#x2019;t apply affine transform.</p>
<p>Parameters: </p>
<ul>
<li><strong>num_features</strong> &#x2013; <img src="img/6c8feca3b2da3d6cf371417edff4be4f.jpg" alt=""> from an expected input of size <img src="img/5816e96aa78b7425cf792435bba8bc29.jpg" alt=""> or <img src="img/db4a9fef02111450bf98261889de550c.jpg" alt=""> from input of size <img src="img/b6d0ccc6531c5d648e750c417c5cc72d.jpg" alt=""></li>
<li><strong>eps</strong> &#x2013; a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> &#x2013; the value used for the running_mean and running_var computation. Default: 0.1</li>
<li><strong>affine</strong> &#x2013; a boolean value that when set to <code>True</code>, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: <code>False</code>.</li>
<li><strong>track_running_stats</strong> &#x2013; a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>False</code></li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/5816e96aa78b7425cf792435bba8bc29.jpg" alt=""></li>
<li>Output: <img src="img/5816e96aa78b7425cf792435bba8bc29.jpg" alt=""> (same shape as input)</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Without Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.InstanceNorm1d(<span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.InstanceNorm1d(<span class="hljs-number">100</span>, affine=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">100</span>, <span class="hljs-number">40</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="instancenorm2d">InstanceNorm2d</h3>
<pre><code class="lang-py">class torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
</code></pre>
<p>Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper <a href="https://arxiv.org/abs/1607.08022" target="_blank">Instance Normalization: The Missing Ingredient for Fast Stylization</a> .</p>
<p><img src="img/63ee6938c8dea3b7cc66a2a245b15cfc.jpg" alt=""></p>
<p>The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. <img src="img/cdab9437b701fd21fb3294cfba7c4bc2.jpg" alt=""> and <img src="img/50705df736e9a7919e768cf8c4e4f794.jpg" alt=""> are learnable parameter vectors of size <code>C</code> (where <code>C</code> is the input size) if <code>affine</code> is <code>True</code>.</p>
<p>By default, this layer uses instance statistics computed from input data in both training and evaluation modes.</p>
<p>If <code>track_running_stats</code> is set to <code>True</code>, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p>
<p>Note</p>
<p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is <img src="img/05beed2a6202dfed2f2c4d1ddf9f445f.jpg" alt="">, where <img src="img/9d834e987d38585c39d150fe8f46bc74.jpg" alt=""> is the estimated statistic and <img src="img/22c5ed7653e3fae804006a00210327fc.jpg" alt=""> is the new observed value.</p>
<p>Note</p>
<p><a href="#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code>InstanceNorm2d</code></a> and <a href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a> are very similar, but have some subtle differences. <a href="#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code>InstanceNorm2d</code></a> is applied on each channel of channeled data like RGB images, but <a href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a> is usually applied on entire sample and often in NLP tasks. Additionaly, <a href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a> applies elementwise affine transform, while <a href="#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code>InstanceNorm2d</code></a> usually don&#x2019;t apply affine transform.</p>
<p>Parameters: </p>
<ul>
<li><strong>num_features</strong> &#x2013; <img src="img/6c8feca3b2da3d6cf371417edff4be4f.jpg" alt=""> from an expected input of size <img src="img/23f8772594b27bd387be708fe9c085e1.jpg" alt=""></li>
<li><strong>eps</strong> &#x2013; a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> &#x2013; the value used for the running_mean and running_var computation. Default: 0.1</li>
<li><strong>affine</strong> &#x2013; a boolean value that when set to <code>True</code>, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: <code>False</code>.</li>
<li><strong>track_running_stats</strong> &#x2013; a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>False</code></li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/23f8772594b27bd387be708fe9c085e1.jpg" alt=""></li>
<li>Output: <img src="img/23f8772594b27bd387be708fe9c085e1.jpg" alt=""> (same shape as input)</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Without Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.InstanceNorm2d(<span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.InstanceNorm2d(<span class="hljs-number">100</span>, affine=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">100</span>, <span class="hljs-number">35</span>, <span class="hljs-number">45</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="instancenorm3d">InstanceNorm3d</h3>
<pre><code class="lang-py">class torch.nn.InstanceNorm3d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
</code></pre>
<p>Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper <a href="https://arxiv.org/abs/1607.08022" target="_blank">Instance Normalization: The Missing Ingredient for Fast Stylization</a> .</p>
<p><img src="img/63ee6938c8dea3b7cc66a2a245b15cfc.jpg" alt=""></p>
<p>The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. <img src="img/cdab9437b701fd21fb3294cfba7c4bc2.jpg" alt=""> and <img src="img/50705df736e9a7919e768cf8c4e4f794.jpg" alt=""> are learnable parameter vectors of size C (where C is the input size) if <code>affine</code> is <code>True</code>.</p>
<p>By default, this layer uses instance statistics computed from input data in both training and evaluation modes.</p>
<p>If <code>track_running_stats</code> is set to <code>True</code>, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p>
<p>Note</p>
<p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is <img src="img/05beed2a6202dfed2f2c4d1ddf9f445f.jpg" alt="">, where <img src="img/9d834e987d38585c39d150fe8f46bc74.jpg" alt=""> is the estimated statistic and <img src="img/22c5ed7653e3fae804006a00210327fc.jpg" alt=""> is the new observed value.</p>
<p>Note</p>
<p><a href="#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code>InstanceNorm3d</code></a> and <a href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a> are very similar, but have some subtle differences. <a href="#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code>InstanceNorm3d</code></a> is applied on each channel of channeled data like 3D models with RGB color, but <a href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a> is usually applied on entire sample and often in NLP tasks. Additionaly, <a href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a> applies elementwise affine transform, while <a href="#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code>InstanceNorm3d</code></a> usually don&#x2019;t apply affine transform.</p>
<p>Parameters: </p>
<ul>
<li><strong>num_features</strong> &#x2013; <img src="img/6c8feca3b2da3d6cf371417edff4be4f.jpg" alt=""> from an expected input of size <img src="img/f5a45f7b445db562b21cfcb525637aab.jpg" alt=""></li>
<li><strong>eps</strong> &#x2013; a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> &#x2013; the value used for the running_mean and running_var computation. Default: 0.1</li>
<li><strong>affine</strong> &#x2013; a boolean value that when set to <code>True</code>, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: <code>False</code>.</li>
<li><strong>track_running_stats</strong> &#x2013; a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>False</code></li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/f5a45f7b445db562b21cfcb525637aab.jpg" alt=""></li>
<li>Output: <img src="img/f5a45f7b445db562b21cfcb525637aab.jpg" alt=""> (same shape as input)</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Without Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.InstanceNorm3d(<span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.InstanceNorm3d(<span class="hljs-number">100</span>, affine=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">100</span>, <span class="hljs-number">35</span>, <span class="hljs-number">45</span>, <span class="hljs-number">10</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="layernorm">LayerNorm</h3>
<pre><code class="lang-py">class torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True)
</code></pre>
<p>Applies Layer Normalization over a mini-batch of inputs as described in the paper <a href="https://arxiv.org/abs/1607.06450" target="_blank">Layer Normalization</a> .</p>
<p><img src="img/2fee766f06767b7b87b3531029d92e1d.jpg" alt=""></p>
<p>The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by <code>normalized_shape</code>. <img src="img/cdab9437b701fd21fb3294cfba7c4bc2.jpg" alt=""> and <img src="img/50705df736e9a7919e768cf8c4e4f794.jpg" alt=""> are learnable affine transform parameters of <code>normalized_shape</code> if <code>elementwise_affine</code> is <code>True</code>.</p>
<p>Note</p>
<p>Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the <code>affine</code> option, Layer Normalization applies per-element scale and bias with <code>elementwise_affine</code>.</p>
<p>This layer uses statistics computed from input data in both training and evaluation modes.</p>
<p>Parameters: </p>
<ul>
<li><p><strong>normalized_shape</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)" target="_blank"><em>list</em></a> <em>or</em> <em>torch.Size</em>) &#x2013;</p>
<p>input shape from an expected input of size</p>
<p><img src="img/7058ab5ae52adb329c22fa5456ad910f.jpg" alt=""></p>
<p>If a single integer is used, it is treated as a singleton list, and this module will normalize over the last dimension which is expected to be of that specific size.</p>
</li>
<li><p><strong>eps</strong> &#x2013; a value added to the denominator for numerical stability. Default: 1e-5</p>
</li>
<li><strong>elementwise_affine</strong> &#x2013; a boolean value that when set to <code>True</code>, this module has learnable per-element affine parameters initialized to ones (for weights) and zeros (for biases). Default: <code>True</code>.</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""></li>
<li>Output: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> (same shape as input)</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.LayerNorm(input.size()[<span class="hljs-number">1</span>:])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Without Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.LayerNorm(input.size()[<span class="hljs-number">1</span>:], elementwise_affine=<span class="hljs-keyword">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Normalize over last two dimensions</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.LayerNorm([<span class="hljs-number">10</span>, <span class="hljs-number">10</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Normalize over last dimension of size 10</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.LayerNorm(<span class="hljs-number">10</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Activating the module</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="localresponsenorm">LocalResponseNorm</h3>
<pre><code class="lang-py">class torch.nn.LocalResponseNorm(size, alpha=0.0001, beta=0.75, k=1.0)
</code></pre>
<p>Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.</p>
<p><img src="img/5522547c6e594dc7c5ffe998f57ad26b.jpg" alt=""></p>
<p>Parameters: </p>
<ul>
<li><strong>size</strong> &#x2013; amount of neighbouring channels used for normalization</li>
<li><strong>alpha</strong> &#x2013; multiplicative factor. Default: 0.0001</li>
<li><strong>beta</strong> &#x2013; exponent. Default: 0.75</li>
<li><strong>k</strong> &#x2013; additive factor. Default: 1</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/0113f670591e6e2a1a50722e1affdce5.jpg" alt=""></li>
<li>Output: <img src="img/0113f670591e6e2a1a50722e1affdce5.jpg" alt=""> (same shape as input)</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>lrn = nn.LocalResponseNorm(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>signal_2d = torch.randn(<span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">24</span>, <span class="hljs-number">24</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>signal_4d = torch.randn(<span class="hljs-number">16</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output_2d = lrn(signal_2d)
<span class="hljs-meta">&gt;&gt;&gt; </span>output_4d = lrn(signal_4d)
</code></pre>
<h2 id="recurrent-layers">Recurrent layers</h2>
<h3 id="rnn">RNN</h3>
<pre><code class="lang-py">class torch.nn.RNN(*args, **kwargs)
</code></pre>
<p>Applies a multi-layer Elman RNN with <img src="img/73b754b4f63e76c0f0327be51d4b263c.jpg" alt=""> or <img src="img/86a6387f3ec09e33de3faaa24f784bca.jpg" alt=""> non-linearity to an input sequence.</p>
<p>For each element in the input sequence, each layer computes the following function:</p>
<p><img src="img/1d1bd72124738a26685d33ce01c89beb.jpg" alt=""></p>
<p>where <img src="img/a048a5bfcc0242b6427d15ed11ef7e23.jpg" alt=""> is the hidden state at time <code>t</code>, <img src="img/22c5ed7653e3fae804006a00210327fc.jpg" alt=""> is the input at time <code>t</code>, and <img src="img/722edd552cee200694a3bfccd4f755df.jpg" alt=""> is the hidden state of the previous layer at time <code>t-1</code> or the initial hidden state at time <code>0</code>. If <code>nonlinearity</code> is <code>&#x2018;relu&#x2019;</code>, then <code>ReLU</code> is used instead of <code>tanh</code>.</p>
<p>Parameters: </p>
<ul>
<li><strong>input_size</strong> &#x2013; The number of expected features in the input <code>x</code></li>
<li><strong>hidden_size</strong> &#x2013; The number of features in the hidden state <code>h</code></li>
<li><strong>num_layers</strong> &#x2013; Number of recurrent layers. E.g., setting <code>num_layers=2</code> would mean stacking two RNNs together to form a <code>stacked RNN</code>, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1</li>
<li><strong>nonlinearity</strong> &#x2013; The non-linearity to use. Can be either &#x2018;tanh&#x2019; or &#x2018;relu&#x2019;. Default: &#x2018;tanh&#x2019;</li>
<li><strong>bias</strong> &#x2013; If <code>False</code>, then the layer does not use bias weights <code>b_ih</code> and <code>b_hh</code>. Default: <code>True</code></li>
<li><strong>batch_first</strong> &#x2013; If <code>True</code>, then the input and output tensors are provided as <code>(batch, seq, feature)</code>. Default: <code>False</code></li>
<li><strong>dropout</strong> &#x2013; If non-zero, introduces a <code>Dropout</code> layer on the outputs of each RNN layer except the last layer, with dropout probability equal to <code>dropout</code>. Default: 0</li>
<li><strong>bidirectional</strong> &#x2013; If <code>True</code>, becomes a bidirectional RNN. Default: <code>False</code></li>
</ul>
<pre><code class="lang-py">Inputs: input, h_0
</code></pre>
<ul>
<li><strong>input</strong> of shape <code>(seq_len, batch, input_size)</code>: tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See <a href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code>torch.nn.utils.rnn.pack_padded_sequence()</code></a> or <a href="#torch.nn.utils.rnn.pack_sequence" title="torch.nn.utils.rnn.pack_sequence"><code>torch.nn.utils.rnn.pack_sequence()</code></a> for details.</li>
<li><strong>h_0</strong> of shape <code>(num_layers * num_directions, batch, hidden_size)</code>: tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.</li>
</ul>
<pre><code class="lang-py">Outputs: output, h_n
</code></pre>
<ul>
<li><p><strong>output</strong> of shape <code>(seq_len, batch, num_directions * hidden_size)</code>: tensor containing the output features (<code>h_k</code>) from the last layer of the RNN, for each <code>k</code>. If a <a href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code>torch.nn.utils.rnn.PackedSequence</code></a> has been given as the input, the output will also be a packed sequence.</p>
<p>For the unpacked case, the directions can be separated using <code>output.view(seq_len, batch, num_directions, hidden_size)</code>, with forward and backward being direction <code>0</code> and <code>1</code> respectively. Similarly, the directions can be separated in the packed case.</p>
</li>
<li><p><strong>h_n</strong> (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for <code>k = seq_len</code>.</p>
<p>Like <em>output</em>, the layers can be separated using <code>h_n.view(num_layers, num_directions, batch, hidden_size)</code>.</p>
</li>
</ul>
<p>| Variables: | </p>
<ul>
<li><strong>weight_ih_l[k]</strong> &#x2013; the learnable input-hidden weights of the k-th layer, of shape <code>(hidden_size * input_size)</code> for <code>k = 0</code>. Otherwise, the shape is <code>(hidden_size * hidden_size)</code></li>
<li><strong>weight_hh_l[k]</strong> &#x2013; the learnable hidden-hidden weights of the k-th layer, of shape <code>(hidden_size * hidden_size)</code></li>
<li><strong>bias_ih_l[k]</strong> &#x2013; the learnable input-hidden bias of the k-th layer, of shape <code>(hidden_size)</code></li>
<li><strong>bias_hh_l[k]</strong> &#x2013; the learnable hidden-hidden bias of the k-th layer, of shape <code>(hidden_size)</code></li>
</ul>
<p>Note</p>
<p>All the weights and biases are initialized from <img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt=""> where <img src="img/cb80fd45c1b2dc2b84b2e80eb48d111e.jpg" alt=""></p>
<p>Note</p>
<p>If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype <code>torch.float16</code> 4) V100 GPU is used, 5) input data is not in <code>PackedSequence</code> format persistent algorithm can be selected to improve performance.</p>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>rnn = nn.RNN(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>h0 = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">20</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output, hn = rnn(input, h0)
</code></pre>
<h3 id="lstm">LSTM</h3>
<pre><code class="lang-py">class torch.nn.LSTM(*args, **kwargs)
</code></pre>
<p>Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.</p>
<p>For each element in the input sequence, each layer computes the following function:</p>
<p><img src="img/e45b4c4446dc36020077ab726cee248f.jpg" alt=""></p>
<p>where <img src="img/a048a5bfcc0242b6427d15ed11ef7e23.jpg" alt=""> is the hidden state at time <code>t</code>, <img src="img/a96fd1792ebb964c44e6a4802fe73a45.jpg" alt=""> is the cell state at time <code>t</code>, <img src="img/22c5ed7653e3fae804006a00210327fc.jpg" alt=""> is the input at time <code>t</code>, <img src="img/722edd552cee200694a3bfccd4f755df.jpg" alt=""> is the hidden state of the layer at time <code>t-1</code> or the initial hidden state at time <code>0</code>, and <img src="img/0c33b098890c73bacbf2dbe5476b8ea0.jpg" alt="">, <img src="img/39e0c3cfa9742216d02b21de5ed57650.jpg" alt="">, <img src="img/2fe3fba6f09d597fd2b3cd6a1e0b4547.jpg" alt="">, <img src="img/8b4c3e8be7da971e832789294ddd61d4.jpg" alt=""> are the input, forget, cell, and output gates, respectively. <img src="img/2469b2bd2a1ab19ebfcee223dcb52bb1.jpg" alt=""> is the sigmoid function.</p>
<p>In a multilayer LSTM, the input <img src="img/3aef28832238eb9de1c3d226cc4f026e.jpg" alt=""> of the <img src="img/4c55f62a52ee5572ab96494e9e0a2876.jpg" alt=""> -th layer (<img src="img/c2c7ccc0042019ca7a1bb7d536da8a87.jpg" alt="">) is the hidden state <img src="img/aa2a9f5361143e6f3a32d54920079b52.jpg" alt=""> of the previous layer multiplied by dropout <img src="img/5e5fffda0db50ff1fedeef29921cdf85.jpg" alt=""> where each <img src="img/5b70351b42153bea8ab63d8e783cc0ac.jpg" alt=""> is a Bernoulli random variable which is <img src="img/28256dd5af833c877d63bfabfaa7b301.jpg" alt=""> with probability <code>dropout</code>.</p>
<p>Parameters: </p>
<ul>
<li><strong>input_size</strong> &#x2013; The number of expected features in the input <code>x</code></li>
<li><strong>hidden_size</strong> &#x2013; The number of features in the hidden state <code>h</code></li>
<li><strong>num_layers</strong> &#x2013; Number of recurrent layers. E.g., setting <code>num_layers=2</code> would mean stacking two LSTMs together to form a <code>stacked LSTM</code>, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1</li>
<li><strong>bias</strong> &#x2013; If <code>False</code>, then the layer does not use bias weights <code>b_ih</code> and <code>b_hh</code>. Default: <code>True</code></li>
<li><strong>batch_first</strong> &#x2013; If <code>True</code>, then the input and output tensors are provided as (batch, seq, feature). Default: <code>False</code></li>
<li><strong>dropout</strong> &#x2013; If non-zero, introduces a <code>Dropout</code> layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to <code>dropout</code>. Default: 0</li>
<li><strong>bidirectional</strong> &#x2013; If <code>True</code>, becomes a bidirectional LSTM. Default: <code>False</code></li>
</ul>
<pre><code class="lang-py">Inputs: input, (h_0, c_0)
</code></pre>
<ul>
<li><p><strong>input</strong> of shape <code>(seq_len, batch, input_size)</code>: tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See <a href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code>torch.nn.utils.rnn.pack_padded_sequence()</code></a> or <a href="#torch.nn.utils.rnn.pack_sequence" title="torch.nn.utils.rnn.pack_sequence"><code>torch.nn.utils.rnn.pack_sequence()</code></a> for details.</p>
</li>
<li><p><strong>h_0</strong> of shape <code>(num_layers * num_directions, batch, hidden_size)</code>: tensor containing the initial hidden state for each element in the batch. If the RNN is bidirectional, num_directions should be 2, else it should be 1.</p>
</li>
<li><p><strong>c_0</strong> of shape <code>(num_layers * num_directions, batch, hidden_size)</code>: tensor containing the initial cell state for each element in the batch.</p>
<p>If <code>(h_0, c_0)</code> is not provided, both <strong>h_0</strong> and <strong>c_0</strong> default to zero.</p>
</li>
</ul>
<pre><code class="lang-py">Outputs: output, (h_n, c_n)
</code></pre>
<ul>
<li><p><strong>output</strong> of shape <code>(seq_len, batch, num_directions * hidden_size)</code>: tensor containing the output features <code>(h_t)</code> from the last layer of the LSTM, for each t. If a <a href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code>torch.nn.utils.rnn.PackedSequence</code></a> has been given as the input, the output will also be a packed sequence.</p>
<p>For the unpacked case, the directions can be separated using <code>output.view(seq_len, batch, num_directions, hidden_size)</code>, with forward and backward being direction <code>0</code> and <code>1</code> respectively. Similarly, the directions can be separated in the packed case.</p>
</li>
<li><p><strong>h_n</strong> of shape <code>(num_layers * num_directions, batch, hidden_size)</code>: tensor containing the hidden state for <code>t = seq_len</code>.</p>
<p>Like <em>output</em>, the layers can be separated using <code>h_n.view(num_layers, num_directions, batch, hidden_size)</code> and similarly for <em>c_n</em>.</p>
</li>
<li><p><strong>c_n</strong> (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for <code>t = seq_len</code></p>
</li>
</ul>
<p>| Variables: | </p>
<ul>
<li><strong>weight_ih_l[k]</strong> &#x2013; the learnable input-hidden weights of the <img src="img/3daedae8ea4977a42453935c04c06ad0.jpg" alt=""> layer <code>(W_ii&amp;#124;W_if&amp;#124;W_ig&amp;#124;W_io)</code>, of shape <code>(4*hidden_size x input_size)</code></li>
<li><strong>weight_hh_l[k]</strong> &#x2013; the learnable hidden-hidden weights of the <img src="img/3daedae8ea4977a42453935c04c06ad0.jpg" alt=""> layer <code>(W_hi&amp;#124;W_hf&amp;#124;W_hg&amp;#124;W_ho)</code>, of shape <code>(4*hidden_size x hidden_size)</code></li>
<li><strong>bias_ih_l[k]</strong> &#x2013; the learnable input-hidden bias of the <img src="img/3daedae8ea4977a42453935c04c06ad0.jpg" alt=""> layer <code>(b_ii&amp;#124;b_if&amp;#124;b_ig&amp;#124;b_io)</code>, of shape <code>(4*hidden_size)</code></li>
<li><strong>bias_hh_l[k]</strong> &#x2013; the learnable hidden-hidden bias of the <img src="img/3daedae8ea4977a42453935c04c06ad0.jpg" alt=""> layer <code>(b_hi&amp;#124;b_hf&amp;#124;b_hg&amp;#124;b_ho)</code>, of shape <code>(4*hidden_size)</code></li>
</ul>
<p>Note</p>
<p>All the weights and biases are initialized from <img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt=""> where <img src="img/cb80fd45c1b2dc2b84b2e80eb48d111e.jpg" alt=""></p>
<p>Note</p>
<p>If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype <code>torch.float16</code> 4) V100 GPU is used, 5) input data is not in <code>PackedSequence</code> format persistent algorithm can be selected to improve performance.</p>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>rnn = nn.LSTM(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>h0 = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">20</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>c0 = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">20</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output, (hn, cn) = rnn(input, (h0, c0))
</code></pre>
<h3 id="gru">GRU</h3>
<pre><code class="lang-py">class torch.nn.GRU(*args, **kwargs)
</code></pre>
<p>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</p>
<p>For each element in the input sequence, each layer computes the following function:</p>
<p><img src="img/76771dd2e48bad7097dc9524356200ef.jpg" alt=""></p>
<p>where <img src="img/a048a5bfcc0242b6427d15ed11ef7e23.jpg" alt=""> is the hidden state at time <code>t</code>, <img src="img/22c5ed7653e3fae804006a00210327fc.jpg" alt=""> is the input at time <code>t</code>, <img src="img/722edd552cee200694a3bfccd4f755df.jpg" alt=""> is the hidden state of the layer at time <code>t-1</code> or the initial hidden state at time <code>0</code>, and <img src="img/33becaceee3dd4f30f106b6a8605226f.jpg" alt="">, <img src="img/98ba4bd98c899c9f15a00fe76fe782b2.jpg" alt="">, <img src="img/4d63033e7717e68b17fc937ffcbcde4b.jpg" alt=""> are the reset, update, and new gates, respectively. <img src="img/2469b2bd2a1ab19ebfcee223dcb52bb1.jpg" alt=""> is the sigmoid function.</p>
<p>In a multilayer GRU, the input <img src="img/3aef28832238eb9de1c3d226cc4f026e.jpg" alt=""> of the <img src="img/4c55f62a52ee5572ab96494e9e0a2876.jpg" alt=""> -th layer (<img src="img/c2c7ccc0042019ca7a1bb7d536da8a87.jpg" alt="">) is the hidden state <img src="img/aa2a9f5361143e6f3a32d54920079b52.jpg" alt=""> of the previous layer multiplied by dropout <img src="img/5e5fffda0db50ff1fedeef29921cdf85.jpg" alt=""> where each <img src="img/5b70351b42153bea8ab63d8e783cc0ac.jpg" alt=""> is a Bernoulli random variable which is <img src="img/28256dd5af833c877d63bfabfaa7b301.jpg" alt=""> with probability <code>dropout</code>.</p>
<p>Parameters: </p>
<ul>
<li><strong>input_size</strong> &#x2013; The number of expected features in the input <code>x</code></li>
<li><strong>hidden_size</strong> &#x2013; The number of features in the hidden state <code>h</code></li>
<li><strong>num_layers</strong> &#x2013; Number of recurrent layers. E.g., setting <code>num_layers=2</code> would mean stacking two GRUs together to form a <code>stacked GRU</code>, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1</li>
<li><strong>bias</strong> &#x2013; If <code>False</code>, then the layer does not use bias weights <code>b_ih</code> and <code>b_hh</code>. Default: <code>True</code></li>
<li><strong>batch_first</strong> &#x2013; If <code>True</code>, then the input and output tensors are provided as (batch, seq, feature). Default: <code>False</code></li>
<li><strong>dropout</strong> &#x2013; If non-zero, introduces a <code>Dropout</code> layer on the outputs of each GRU layer except the last layer, with dropout probability equal to <code>dropout</code>. Default: 0</li>
<li><strong>bidirectional</strong> &#x2013; If <code>True</code>, becomes a bidirectional GRU. Default: <code>False</code></li>
</ul>
<pre><code class="lang-py">Inputs: input, h_0
</code></pre>
<ul>
<li><strong>input</strong> of shape <code>(seq_len, batch, input_size)</code>: tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See <a href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code>torch.nn.utils.rnn.pack_padded_sequence()</code></a> for details.</li>
<li><strong>h_0</strong> of shape <code>(num_layers * num_directions, batch, hidden_size)</code>: tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.</li>
</ul>
<pre><code class="lang-py">Outputs: output, h_n
</code></pre>
<ul>
<li><p><strong>output</strong> of shape <code>(seq_len, batch, num_directions * hidden_size)</code>: tensor containing the output features h_t from the last layer of the GRU, for each t. If a <a href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code>torch.nn.utils.rnn.PackedSequence</code></a> has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using <code>output.view(seq_len, batch, num_directions, hidden_size)</code>, with forward and backward being direction <code>0</code> and <code>1</code> respectively.</p>
<p>Similarly, the directions can be separated in the packed case.</p>
</li>
<li><p><strong>h_n</strong> of shape <code>(num_layers * num_directions, batch, hidden_size)</code>: tensor containing the hidden state for <code>t = seq_len</code></p>
<p>Like <em>output</em>, the layers can be separated using <code>h_n.view(num_layers, num_directions, batch, hidden_size)</code>.</p>
</li>
</ul>
<p>| Variables: | </p>
<ul>
<li><strong>weight_ih_l[k]</strong> &#x2013; the learnable input-hidden weights of the <img src="img/3daedae8ea4977a42453935c04c06ad0.jpg" alt=""> layer (W_ir|W_iz|W_in), of shape <code>(3*hidden_size x input_size)</code></li>
<li><strong>weight_hh_l[k]</strong> &#x2013; the learnable hidden-hidden weights of the <img src="img/3daedae8ea4977a42453935c04c06ad0.jpg" alt=""> layer (W_hr|W_hz|W_hn), of shape <code>(3*hidden_size x hidden_size)</code></li>
<li><strong>bias_ih_l[k]</strong> &#x2013; the learnable input-hidden bias of the <img src="img/3daedae8ea4977a42453935c04c06ad0.jpg" alt=""> layer (b_ir|b_iz|b_in), of shape <code>(3*hidden_size)</code></li>
<li><strong>bias_hh_l[k]</strong> &#x2013; the learnable hidden-hidden bias of the <img src="img/3daedae8ea4977a42453935c04c06ad0.jpg" alt=""> layer (b_hr|b_hz|b_hn), of shape <code>(3*hidden_size)</code></li>
</ul>
<p>Note</p>
<p>All the weights and biases are initialized from <img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt=""> where <img src="img/cb80fd45c1b2dc2b84b2e80eb48d111e.jpg" alt=""></p>
<p>Note</p>
<p>If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype <code>torch.float16</code> 4) V100 GPU is used, 5) input data is not in <code>PackedSequence</code> format persistent algorithm can be selected to improve performance.</p>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>rnn = nn.GRU(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>h0 = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">20</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output, hn = rnn(input, h0)
</code></pre>
<h3 id="rnncell">RNNCell</h3>
<pre><code class="lang-py">class torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity=&apos;tanh&apos;)
</code></pre>
<p>An Elman RNN cell with tanh or ReLU non-linearity.</p>
<p><img src="img/e748fe354d996221dbfa5f8e3412451e.jpg" alt=""></p>
<p>If <code>nonlinearity</code> is <code>&#x2018;relu&#x2019;</code>, then ReLU is used in place of tanh.</p>
<p>Parameters: </p>
<ul>
<li><strong>input_size</strong> &#x2013; The number of expected features in the input <code>x</code></li>
<li><strong>hidden_size</strong> &#x2013; The number of features in the hidden state <code>h</code></li>
<li><strong>bias</strong> &#x2013; If <code>False</code>, then the layer does not use bias weights <code>b_ih</code> and <code>b_hh</code>. Default: <code>True</code></li>
<li><strong>nonlinearity</strong> &#x2013; The non-linearity to use. Can be either &#x2018;tanh&#x2019; or &#x2018;relu&#x2019;. Default: &#x2018;tanh&#x2019;</li>
</ul>
<pre><code class="lang-py">Inputs: input, hidden
</code></pre>
<ul>
<li><strong>input</strong> of shape <code>(batch, input_size)</code>: tensor containing input features</li>
<li><strong>hidden</strong> of shape <code>(batch, hidden_size)</code>: tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.</li>
</ul>
<pre><code class="lang-py">Outputs: h&#x2019;
</code></pre>
<ul>
<li><strong>h&#x2019;</strong> of shape <code>(batch, hidden_size)</code>: tensor containing the next hidden state for each element in the batch</li>
</ul>
<p>| Variables: | </p>
<ul>
<li><strong>weight_ih</strong> &#x2013; the learnable input-hidden weights, of shape <code>(hidden_size x input_size)</code></li>
<li><strong>weight_hh</strong> &#x2013; the learnable hidden-hidden weights, of shape <code>(hidden_size x hidden_size)</code></li>
<li><strong>bias_ih</strong> &#x2013; the learnable input-hidden bias, of shape <code>(hidden_size)</code></li>
<li><strong>bias_hh</strong> &#x2013; the learnable hidden-hidden bias, of shape <code>(hidden_size)</code></li>
</ul>
<p>Note</p>
<p>All the weights and biases are initialized from <img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt=""> where <img src="img/cb80fd45c1b2dc2b84b2e80eb48d111e.jpg" alt=""></p>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>rnn = nn.RNNCell(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>hx = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">20</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = []
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">6</span>):
 hx = rnn(input[i], hx)
 output.append(hx)
</code></pre>
<h3 id="lstmcell">LSTMCell</h3>
<pre><code class="lang-py">class torch.nn.LSTMCell(input_size, hidden_size, bias=True)
</code></pre>
<p>A long short-term memory (LSTM) cell.</p>
<p><img src="img/fb4d6ec81b25bb8201fbedd23b71b45f.jpg" alt=""></p>
<p>where <img src="img/2469b2bd2a1ab19ebfcee223dcb52bb1.jpg" alt=""> is the sigmoid function.</p>
<p>Parameters: </p>
<ul>
<li><strong>input_size</strong> &#x2013; The number of expected features in the input <code>x</code></li>
<li><strong>hidden_size</strong> &#x2013; The number of features in the hidden state <code>h</code></li>
<li><strong>bias</strong> &#x2013; If <code>False</code>, then the layer does not use bias weights <code>b_ih</code> and <code>b_hh</code>. Default: <code>True</code></li>
</ul>
<pre><code class="lang-py">Inputs: input, (h_0, c_0)
</code></pre>
<ul>
<li><p><strong>input</strong> of shape <code>(batch, input_size)</code>: tensor containing input features</p>
</li>
<li><p><strong>h_0</strong> of shape <code>(batch, hidden_size)</code>: tensor containing the initial hidden state for each element in the batch.</p>
</li>
<li><p><strong>c_0</strong> of shape <code>(batch, hidden_size)</code>: tensor containing the initial cell state for each element in the batch.</p>
<p>If <code>(h_0, c_0)</code> is not provided, both <strong>h_0</strong> and <strong>c_0</strong> default to zero.</p>
</li>
</ul>
<pre><code class="lang-py">Outputs: h_1, c_1
</code></pre>
<ul>
<li><strong>h_1</strong> of shape <code>(batch, hidden_size)</code>: tensor containing the next hidden state for each element in the batch</li>
<li><strong>c_1</strong> of shape <code>(batch, hidden_size)</code>: tensor containing the next cell state for each element in the batch</li>
</ul>
<p>| Variables: | </p>
<ul>
<li><strong>weight_ih</strong> &#x2013; the learnable input-hidden weights, of shape <code>(4*hidden_size x input_size)</code></li>
<li><strong>weight_hh</strong> &#x2013; the learnable hidden-hidden weights, of shape <code>(4*hidden_size x hidden_size)</code></li>
<li><strong>bias_ih</strong> &#x2013; the learnable input-hidden bias, of shape <code>(4*hidden_size)</code></li>
<li><strong>bias_hh</strong> &#x2013; the learnable hidden-hidden bias, of shape <code>(4*hidden_size)</code></li>
</ul>
<p>Note</p>
<p>All the weights and biases are initialized from <img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt=""> where <img src="img/cb80fd45c1b2dc2b84b2e80eb48d111e.jpg" alt=""></p>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>rnn = nn.LSTMCell(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>hx = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">20</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>cx = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">20</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = []
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">6</span>):
 hx, cx = rnn(input[i], (hx, cx))
 output.append(hx)
</code></pre>
<h3 id="grucell">GRUCell</h3>
<pre><code class="lang-py">class torch.nn.GRUCell(input_size, hidden_size, bias=True)
</code></pre>
<p>A gated recurrent unit (GRU) cell</p>
<p><img src="img/a557e5b089bda248c2e25791d88d4b2a.jpg" alt=""></p>
<p>where <img src="img/2469b2bd2a1ab19ebfcee223dcb52bb1.jpg" alt=""> is the sigmoid function.</p>
<p>Parameters: </p>
<ul>
<li><strong>input_size</strong> &#x2013; The number of expected features in the input <code>x</code></li>
<li><strong>hidden_size</strong> &#x2013; The number of features in the hidden state <code>h</code></li>
<li><strong>bias</strong> &#x2013; If <code>False</code>, then the layer does not use bias weights <code>b_ih</code> and <code>b_hh</code>. Default: <code>True</code></li>
</ul>
<pre><code class="lang-py">Inputs: input, hidden
</code></pre>
<ul>
<li><strong>input</strong> of shape <code>(batch, input_size)</code>: tensor containing input features</li>
<li><strong>hidden</strong> of shape <code>(batch, hidden_size)</code>: tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.</li>
</ul>
<pre><code class="lang-py">Outputs: h&#x2019;
</code></pre>
<ul>
<li><strong>h&#x2019;</strong> of shape <code>(batch, hidden_size)</code>: tensor containing the next hidden state for each element in the batch</li>
</ul>
<p>| Variables: | </p>
<ul>
<li><strong>weight_ih</strong> &#x2013; the learnable input-hidden weights, of shape <code>(3*hidden_size x input_size)</code></li>
<li><strong>weight_hh</strong> &#x2013; the learnable hidden-hidden weights, of shape <code>(3*hidden_size x hidden_size)</code></li>
<li><strong>bias_ih</strong> &#x2013; the learnable input-hidden bias, of shape <code>(3*hidden_size)</code></li>
<li><strong>bias_hh</strong> &#x2013; the learnable hidden-hidden bias, of shape <code>(3*hidden_size)</code></li>
</ul>
<p>Note</p>
<p>All the weights and biases are initialized from <img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt=""> where <img src="img/cb80fd45c1b2dc2b84b2e80eb48d111e.jpg" alt=""></p>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>rnn = nn.GRUCell(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>hx = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">20</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = []
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">6</span>):
 hx = rnn(input[i], hx)
 output.append(hx)
</code></pre>
<h2 id="linear-layers">Linear layers</h2>
<h3 id="linear">Linear</h3>
<pre><code class="lang-py">class torch.nn.Linear(in_features, out_features, bias=True)
</code></pre>
<p>Applies a linear transformation to the incoming data: <img src="img/8c4834b7cb4b9c7a795bf354412e8dd3.jpg" alt=""></p>
<p>Parameters: </p>
<ul>
<li><strong>in_features</strong> &#x2013; size of each input sample</li>
<li><strong>out_features</strong> &#x2013; size of each output sample</li>
<li><strong>bias</strong> &#x2013; If set to False, the layer will not learn an additive bias. Default: <code>True</code></li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/37251f14c8c7345b66309c1ce6181e4d.jpg" alt=""> where <img src="img/28ec51e742166ea3400be6e7343bbfa5.jpg" alt=""> means any number of additional dimensions</li>
<li>Output: <img src="img/052531b4914630967eb9a6ed4f143697.jpg" alt=""> where all but the last dimension are the same shape as the input.</li>
</ul>
<p>| Variables: | </p>
<ul>
<li><strong>weight</strong> &#x2013; the learnable weights of the module of shape <img src="img/7bdd499093e2167451c56eb5c4480786.jpg" alt="">. The values are initialized from <img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt="">, where <img src="img/9d1dd979275f32a1bcc00f4e3885e68c.jpg" alt=""></li>
<li><strong>bias</strong> &#x2013; the learnable bias of the module of shape <img src="img/27eac2d9aa3b57eabe07fcce145717d2.jpg" alt="">. If <code>bias</code> is <code>True</code>, the values are initialized from <img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt=""> where <img src="img/9d1dd979275f32a1bcc00f4e3885e68c.jpg" alt=""></li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">128</span>, <span class="hljs-number">20</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(output.size())
torch.Size([<span class="hljs-number">128</span>, <span class="hljs-number">30</span>])
</code></pre>
<h3 id="bilinear">Bilinear</h3>
<pre><code class="lang-py">class torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True)
</code></pre>
<p>Applies a bilinear transformation to the incoming data: <img src="img/a0d89d1240ed669c322d042acea66b2c.jpg" alt=""></p>
<p>Parameters: </p>
<ul>
<li><strong>in1_features</strong> &#x2013; size of each first input sample</li>
<li><strong>in2_features</strong> &#x2013; size of each second input sample</li>
<li><strong>out_features</strong> &#x2013; size of each output sample</li>
<li><strong>bias</strong> &#x2013; If set to False, the layer will not learn an additive bias. Default: <code>True</code></li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/9cf39fb88b1a94018532514fcb3e125c.jpg" alt="">, <img src="img/5b3219dff177846f3a5aebdb36ae5d30.jpg" alt=""> where <img src="img/28ec51e742166ea3400be6e7343bbfa5.jpg" alt=""> means any number of additional dimensions. All but the last dimension of the inputs should be the same.</li>
<li>Output: <img src="img/052531b4914630967eb9a6ed4f143697.jpg" alt=""> where all but the last dimension are the same shape as the input.</li>
</ul>
<p>| Variables: | </p>
<ul>
<li><strong>weight</strong> &#x2013; the learnable weights of the module of shape <img src="img/3f2a47921a3568c8f0f8c45847fd2ad3.jpg" alt="">. The values are initialized from <img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt="">, where <img src="img/76606e23079bb41cbaeb5fa9ddc71c86.jpg" alt=""></li>
<li><strong>bias</strong> &#x2013; the learnable bias of the module of shape <img src="img/27eac2d9aa3b57eabe07fcce145717d2.jpg" alt=""> If <code>bias</code> is <code>True</code>, the values are initialized from <img src="img/3d305f1c240ff844b6cb2c1c6660e0af.jpg" alt="">, where <img src="img/76606e23079bb41cbaeb5fa9ddc71c86.jpg" alt=""></li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Bilinear(<span class="hljs-number">20</span>, <span class="hljs-number">30</span>, <span class="hljs-number">40</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input1 = torch.randn(<span class="hljs-number">128</span>, <span class="hljs-number">20</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input2 = torch.randn(<span class="hljs-number">128</span>, <span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input1, input2)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(output.size())
torch.Size([<span class="hljs-number">128</span>, <span class="hljs-number">40</span>])
</code></pre>
<h2 id="dropout-layers">Dropout layers</h2>
<h3 id="dropout">Dropout</h3>
<pre><code class="lang-py">class torch.nn.Dropout(p=0.5, inplace=False)
</code></pre>
<p>During training, randomly zeroes some of the elements of the input tensor with probability <code>p</code> using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.</p>
<p>This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper <a href="https://arxiv.org/abs/1207.0580" target="_blank">Improving neural networks by preventing co-adaptation of feature detectors</a> .</p>
<p>Furthermore, the outputs are scaled by a factor of <img src="img/37052da8591fea742432c58ac3a4dc59.jpg" alt=""> during training. This means that during evaluation the module simply computes an identity function.</p>
<p>Parameters: </p>
<ul>
<li><strong>p</strong> &#x2013; probability of an element to be zeroed. Default: 0.5</li>
<li><strong>inplace</strong> &#x2013; If set to <code>True</code>, will do this operation in-place. Default: <code>False</code></li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <code>Any</code>. Input can be of any shape</li>
<li>Output: <code>Same</code>. Output is of the same shape as input</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Dropout(p=<span class="hljs-number">0.2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="dropout2d">Dropout2d</h3>
<pre><code class="lang-py">class torch.nn.Dropout2d(p=0.5, inplace=False)
</code></pre>
<p>Randomly zero out entire channels (a channel is a 2D feature map, e.g., the <img src="img/d8fdd0e28cfb03738fc5227885ee035a.jpg" alt="">-th channel of the <img src="img/31df9c730e19ca29b59dce64b99d98c1.jpg" alt="">-th sample in the batched input is a 2D tensor <img src="img/5bc8fbe2fea3359e55846184c5eb123a.jpg" alt="">) of the input tensor). Each channel will be zeroed out independently on every forward call. with probability <code>p</code> using samples from a Bernoulli distribution.</p>
<p>Usually the input comes from <code>nn.Conv2d</code> modules.</p>
<p>As described in the paper <a href="http://arxiv.org/abs/1411.4280" target="_blank">Efficient Object Localization Using Convolutional Networks</a> , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease.</p>
<p>In this case, <code>nn.Dropout2d()</code> will help promote independence between feature maps and should be used instead.</p>
<p>Parameters: </p>
<ul>
<li><strong>p</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a><em>,</em> <em>optional</em>) &#x2013; probability of an element to be zero-ed.</li>
<li><strong>inplace</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; If set to <code>True</code>, will do this operation in-place</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/23f8772594b27bd387be708fe9c085e1.jpg" alt=""></li>
<li>Output: <img src="img/23f8772594b27bd387be708fe9c085e1.jpg" alt=""> (same shape as input)</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Dropout2d(p=<span class="hljs-number">0.2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="dropout3d">Dropout3d</h3>
<pre><code class="lang-py">class torch.nn.Dropout3d(p=0.5, inplace=False)
</code></pre>
<p>Randomly zero out entire channels (a channel is a 3D feature map, e.g., the <img src="img/d8fdd0e28cfb03738fc5227885ee035a.jpg" alt="">-th channel of the <img src="img/31df9c730e19ca29b59dce64b99d98c1.jpg" alt="">-th sample in the batched input is a 3D tensor <img src="img/5bc8fbe2fea3359e55846184c5eb123a.jpg" alt="">) of the input tensor). Each channel will be zeroed out independently on every forward call. with probability <code>p</code> using samples from a Bernoulli distribution.</p>
<p>Usually the input comes from <code>nn.Conv3d</code> modules.</p>
<p>As described in the paper <a href="http://arxiv.org/abs/1411.4280" target="_blank">Efficient Object Localization Using Convolutional Networks</a> , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease.</p>
<p>In this case, <code>nn.Dropout3d()</code> will help promote independence between feature maps and should be used instead.</p>
<p>Parameters: </p>
<ul>
<li><strong>p</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a><em>,</em> <em>optional</em>) &#x2013; probability of an element to be zeroed.</li>
<li><strong>inplace</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; If set to <code>True</code>, will do this operation in-place</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/f5a45f7b445db562b21cfcb525637aab.jpg" alt=""></li>
<li>Output: <img src="img/f5a45f7b445db562b21cfcb525637aab.jpg" alt=""> (same shape as input)</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Dropout3d(p=<span class="hljs-number">0.2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">4</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="alphadropout">AlphaDropout</h3>
<pre><code class="lang-py">class torch.nn.AlphaDropout(p=0.5, inplace=False)
</code></pre>
<p>Applies Alpha Dropout over the input.</p>
<p>Alpha Dropout is a type of Dropout that maintains the self-normalizing property. For an input with zero mean and unit standard deviation, the output of Alpha Dropout maintains the original mean and standard deviation of the input. Alpha Dropout goes hand-in-hand with SELU activation function, which ensures that the outputs have zero mean and unit standard deviation.</p>
<p>During training, it randomly masks some of the elements of the input tensor with probability <em>p</em> using samples from a bernoulli distribution. The elements to masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit standard deviation.</p>
<p>During evaluation the module simply computes an identity function.</p>
<p>More details can be found in the paper <a href="https://arxiv.org/abs/1706.02515" target="_blank">Self-Normalizing Neural Networks</a> .</p>
<p>Parameters: </p>
<ul>
<li><strong>p</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a>) &#x2013; probability of an element to be dropped. Default: 0.5</li>
<li><strong>inplace</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; If set to <code>True</code>, will do this operation in-place</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <code>Any</code>. Input can be of any shape</li>
<li>Output: <code>Same</code>. Output is of the same shape as input</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AlphaDropout(p=<span class="hljs-number">0.2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h2 id="sparse-layers">Sparse layers</h2>
<h3 id="embedding">Embedding</h3>
<pre><code class="lang-py">class torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None)
</code></pre>
<p>A simple lookup table that stores embeddings of a fixed dictionary and size.</p>
<p>This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.</p>
<p>Parameters: </p>
<ul>
<li><strong>num_embeddings</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; size of the dictionary of embeddings</li>
<li><strong>embedding_dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; the size of each embedding vector</li>
<li><strong>padding_idx</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; If given, pads the output with the embedding vector at <code>padding_idx</code> (initialized to zeros) whenever it encounters the index.</li>
<li><strong>max_norm</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a><em>,</em> <em>optional</em>) &#x2013; If given, each embedding vector with norm larger than <code>max_norm</code> is renormalized to have norm <code>max_norm</code>.</li>
<li><strong>norm_type</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a><em>,</em> <em>optional</em>) &#x2013; The p of the p-norm to compute for the <code>max_norm</code> option. Default <code>2</code>.</li>
<li><strong>scale_grad_by_freq</strong> (<em>boolean__,</em> <em>optional</em>) &#x2013; If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default <code>False</code>.</li>
<li><strong>sparse</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; If <code>True</code>, gradient w.r.t. <code>weight</code> matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.</li>
</ul>
<table>
<thead>
<tr>
<th>Variables:</th>
<th><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from <img src="img/dd84ddbf2f8040d87fb315eeeba51f6d.jpg" alt=""></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>Shape:</p>
<blockquote>
<ul>
<li>Input: LongTensor of arbitrary shape containing the indices to extract</li>
<li>Output: <code>(*, embedding_dim)</code>, where <code>*</code> is the input shape</li>
</ul>
</blockquote>
<p>Note</p>
<p>Keep in mind that only a limited number of optimizers support sparse gradients: currently it&#x2019;s <code>optim.SGD</code> (<code>CUDA</code> and <code>CPU</code>), <code>optim.SparseAdam</code> (<code>CUDA</code> and <code>CPU</code>) and <code>optim.Adagrad</code> (<code>CPU</code>)</p>
<p>Note</p>
<p>With <code>padding_idx</code> set, the embedding vector at <code>padding_idx</code> is initialized to all zeros. However, note that this vector can be modified afterwards, e.g., using a customized initialization method, and thus changing the vector used to pad the output. The gradient for this vector from <a href="#torch.nn.Embedding" title="torch.nn.Embedding"><code>Embedding</code></a> is always zero.</p>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># an Embedding module containing 10 tensors of size 3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>embedding = nn.Embedding(<span class="hljs-number">10</span>, <span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># a batch of 2 samples of 4 indices each</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.LongTensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">9</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>embedding(input)
tensor([[[<span class="hljs-number">-0.0251</span>, <span class="hljs-number">-1.6902</span>,  <span class="hljs-number">0.7172</span>],
 [<span class="hljs-number">-0.6431</span>,  <span class="hljs-number">0.0748</span>,  <span class="hljs-number">0.6969</span>],
 [ <span class="hljs-number">1.4970</span>,  <span class="hljs-number">1.3448</span>, <span class="hljs-number">-0.9685</span>],
 [<span class="hljs-number">-0.3677</span>, <span class="hljs-number">-2.7265</span>, <span class="hljs-number">-0.1685</span>]],

 [[ <span class="hljs-number">1.4970</span>,  <span class="hljs-number">1.3448</span>, <span class="hljs-number">-0.9685</span>],
 [ <span class="hljs-number">0.4362</span>, <span class="hljs-number">-0.4004</span>,  <span class="hljs-number">0.9400</span>],
 [<span class="hljs-number">-0.6431</span>,  <span class="hljs-number">0.0748</span>,  <span class="hljs-number">0.6969</span>],
 [ <span class="hljs-number">0.9124</span>, <span class="hljs-number">-2.3616</span>,  <span class="hljs-number">1.1151</span>]]])

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># example with padding_idx</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>embedding = nn.Embedding(<span class="hljs-number">10</span>, <span class="hljs-number">3</span>, padding_idx=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.LongTensor([[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>,<span class="hljs-number">5</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>embedding(input)
tensor([[[ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">0.1535</span>, <span class="hljs-number">-2.0309</span>,  <span class="hljs-number">0.9315</span>],
 [ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>],
 [<span class="hljs-number">-0.1655</span>,  <span class="hljs-number">0.9897</span>,  <span class="hljs-number">0.0635</span>]]])
</code></pre>
<pre><code class="lang-py">classmethod from_pretrained(embeddings, freeze=<span class="hljs-keyword">True</span>, sparse=<span class="hljs-keyword">False</span>)
</code></pre>
<p>Creates Embedding instance from given 2-dimensional FloatTensor.</p>
<p>Parameters: </p>
<ul>
<li><strong>embeddings</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; FloatTensor containing weights for the Embedding. First dimension is being passed to Embedding as &#x2018;num_embeddings&#x2019;, second as &#x2018;embedding_dim&#x2019;.</li>
<li><strong>freeze</strong> (<em>boolean__,</em> <em>optional</em>) &#x2013; If <code>True</code>, the tensor does not get updated in the learning process. Equivalent to <code>embedding.weight.requires_grad = False</code>. Default: <code>True</code></li>
<li><strong>sparse</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; if <code>True</code>, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># FloatTensor containing pretrained weights</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>weight = torch.FloatTensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2.3</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5.1</span>, <span class="hljs-number">6.3</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>embedding = nn.Embedding.from_pretrained(weight)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get embeddings for index 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.LongTensor([<span class="hljs-number">1</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>embedding(input)
tensor([[ <span class="hljs-number">4.0000</span>,  <span class="hljs-number">5.1000</span>,  <span class="hljs-number">6.3000</span>]])
</code></pre>
<h3 id="embeddingbag">EmbeddingBag</h3>
<pre><code class="lang-py">class torch.nn.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, mode=&apos;mean&apos;, sparse=False)
</code></pre>
<p>Computes sums or means of &#x2018;bags&#x2019; of embeddings, without instantiating the intermediate embeddings.</p>
<p>For bags of constant length, this class</p>
<blockquote>
<ul>
<li>with <code>mode=&quot;sum&quot;</code> is equivalent to <a href="#torch.nn.Embedding" title="torch.nn.Embedding"><code>Embedding</code></a> followed by <code>torch.sum(dim=1)</code>,</li>
<li>with <code>mode=&quot;mean&quot;</code> is equivalent to <a href="#torch.nn.Embedding" title="torch.nn.Embedding"><code>Embedding</code></a> followed by <code>torch.mean(dim=1)</code>,</li>
<li>with <code>mode=&quot;max&quot;</code> is equivalent to <a href="#torch.nn.Embedding" title="torch.nn.Embedding"><code>Embedding</code></a> followed by <code>torch.max(dim=1)</code>.</li>
</ul>
</blockquote>
<p>However, <a href="#torch.nn.EmbeddingBag" title="torch.nn.EmbeddingBag"><code>EmbeddingBag</code></a> is much more time and memory efficient than using a chain of these operations.</p>
<p>Parameters: </p>
<ul>
<li><strong>num_embeddings</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; size of the dictionary of embeddings</li>
<li><strong>embedding_dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; the size of each embedding vector</li>
<li><strong>max_norm</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a><em>,</em> <em>optional</em>) &#x2013; If given, each embedding vector with norm larger than <code>max_norm</code> is renormalized to have norm <code>max_norm</code>.</li>
<li><strong>norm_type</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a><em>,</em> <em>optional</em>) &#x2013; The p of the p-norm to compute for the <code>max_norm</code> option. Default <code>2</code>.</li>
<li><strong>scale_grad_by_freq</strong> (<em>boolean__,</em> <em>optional</em>) &#x2013; if given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default <code>False</code>. Note: this option is not supported when <code>mode=&quot;max&quot;</code>.</li>
<li><strong>mode</strong> (<em>string__,</em> <em>optional</em>) &#x2013; <code>&quot;sum&quot;</code>, <code>&quot;mean&quot;</code> or <code>&quot;max&quot;</code>. Specifies the way to reduce the bag. Default: <code>&quot;mean&quot;</code></li>
<li><strong>sparse</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; if <code>True</code>, gradient w.r.t. <code>weight</code> matrix will be a sparse tensor. See Notes for more details regarding sparse gradients. Note: this option is not supported when <code>mode=&quot;max&quot;</code>.</li>
</ul>
<table>
<thead>
<tr>
<th>Variables:</th>
<th><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the learnable weights of the module of shape <code>(num_embeddings x embedding_dim)</code> initialized from <img src="img/dd84ddbf2f8040d87fb315eeeba51f6d.jpg" alt="">.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>Inputs: <code>input</code> (LongTensor) and <code>offsets</code> (LongTensor, optional)</p>
<blockquote>
<ul>
<li>If <code>input</code> is 2D of shape <code>B x N</code>,</li>
</ul>
<pre><code>it will be treated as `B` bags (sequences) each of fixed length `N`, and this will return `B` values aggregated in a way depending on the `mode`. `offsets` is ignored and required to be `None` in this case.
</code></pre><ul>
<li>If <code>input</code> is 1D of shape <code>N</code>,</li>
</ul>
<pre><code>it will be treated as a concatenation of multiple bags (sequences). `offsets` is required to be a 1D tensor containing the starting index positions of each bag in `input`. Therefore, for `offsets` of shape `B`, `input` will be viewed as having `B` bags. Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.
</code></pre></blockquote>
<p>Output shape: <code>B x embedding_dim</code></p>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># an Embedding module containing 10 tensors of size 3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>embedding_sum = nn.EmbeddingBag(<span class="hljs-number">10</span>, <span class="hljs-number">3</span>, mode=<span class="hljs-string">&apos;sum&apos;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># a batch of 2 samples of 4 indices each</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.LongTensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">4</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">9</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>offsets = torch.LongTensor([<span class="hljs-number">0</span>,<span class="hljs-number">4</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>embedding_sum(input, offsets)
tensor([[<span class="hljs-number">-0.8861</span>, <span class="hljs-number">-5.4350</span>, <span class="hljs-number">-0.0523</span>],
 [ <span class="hljs-number">1.1306</span>, <span class="hljs-number">-2.5798</span>, <span class="hljs-number">-1.0044</span>]])
</code></pre>
<h2 id="distance-functions">Distance functions</h2>
<h3 id="cosinesimilarity">CosineSimilarity</h3>
<pre><code class="lang-py">class torch.nn.CosineSimilarity(dim=1, eps=1e-08)
</code></pre>
<p>Returns cosine similarity between <img src="img/abdadb44ea35aecb39004dd7f55d9543.jpg" alt=""> and <img src="img/88fdc6eeb68ef4aacf7cd6bd43fa176e.jpg" alt="">, computed along dim.</p>
<p><img src="img/93f92bee7ec6c9e48618f7c929ab51e3.jpg" alt=""></p>
<p>Parameters: </p>
<ul>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; Dimension where cosine similarity is computed. Default: 1</li>
<li><strong>eps</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a><em>,</em> <em>optional</em>) &#x2013; Small value to avoid division by zero. Default: 1e-8</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input1: <img src="img/c2101d997ef86641ad9f92513b080e8a.jpg" alt=""> where D is at position <code>dim</code></li>
<li>Input2: <img src="img/c2101d997ef86641ad9f92513b080e8a.jpg" alt="">, same shape as the Input1</li>
<li>Output: <img src="img/999c5fb65c1a9ba017a0d60c030400c5.jpg" alt=""></li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>input1 = torch.randn(<span class="hljs-number">100</span>, <span class="hljs-number">128</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input2 = torch.randn(<span class="hljs-number">100</span>, <span class="hljs-number">128</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>cos = nn.CosineSimilarity(dim=<span class="hljs-number">1</span>, eps=<span class="hljs-number">1e-6</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = cos(input1, input2)
</code></pre>
<h3 id="pairwisedistance">PairwiseDistance</h3>
<pre><code class="lang-py">class torch.nn.PairwiseDistance(p=2.0, eps=1e-06, keepdim=False)
</code></pre>
<p>Computes the batchwise pairwise distance between vectors <img src="img/2f0f406e2d42300da1a9891d89381576.jpg" alt="">, <img src="img/60787776d6fd54b3adfd3762b910bd3f.jpg" alt=""> using the p-norm:</p>
<p><img src="img/4206b08d53423c6d6f77c51751d33cae.jpg" alt=""></p>
<p>Parameters: </p>
<ul>
<li><strong>p</strong> (<em>real</em>) &#x2013; the norm degree. Default: 2</li>
<li><strong>eps</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a><em>,</em> <em>optional</em>) &#x2013; Small value to avoid division by zero. Default: 1e-6</li>
<li><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Determines whether or not to keep the batch dimension. Default: False</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input1: <img src="img/3dc464d2e10c731f17264e33e497c1a8.jpg" alt=""> where <code>D = vector dimension</code></li>
<li>Input2: <img src="img/3dc464d2e10c731f17264e33e497c1a8.jpg" alt="">, same shape as the Input1</li>
<li>Output: <img src="img/2a3e2b832e04fe8d66596083b23da518.jpg" alt="">. If <code>keepdim</code> is <code>False</code>, then <img src="img/f1b7cdb5b976f1adde1e8b2850a1c127.jpg" alt="">.</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>pdist = nn.PairwiseDistance(p=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input1 = torch.randn(<span class="hljs-number">100</span>, <span class="hljs-number">128</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input2 = torch.randn(<span class="hljs-number">100</span>, <span class="hljs-number">128</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = pdist(input1, input2)
</code></pre>
<h2 id="loss-functions">Loss functions</h2>
<h3 id="l1loss">L1Loss</h3>
<pre><code class="lang-py">class torch.nn.L1Loss(size_average=None, reduce=None, reduction=&apos;mean&apos;)
</code></pre>
<p>Creates a criterion that measures the mean absolute error (MAE) between each element in the input <code>x</code> and target <code>y</code>.</p>
<p>The loss can be described as:</p>
<p><img src="img/415564bfa6c89ba182a02fe2a3d0ca49.jpg" alt=""></p>
<p>where <img src="img/9341d9048ac485106d2b2ee8de14876f.jpg" alt=""> is the batch size. If reduce is <code>True</code>, then:</p>
<p><img src="img/dd1952e377a9b618cc6538b18165a417.jpg" alt=""></p>
<p><code>x</code> and <code>y</code> are tensors of arbitrary shapes with a total of <code>n</code> elements each.</p>
<p>The sum operation still operates over all the elements, and divides by <code>n</code>.</p>
<p>The division by <code>n</code> can be avoided if one sets the constructor argument <code>size_average=False</code>.</p>
<p>Parameters: </p>
<ul>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li>
<li><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) &#x2013; Specifies the reduction to apply to the output: &#x2018;none&#x2019; | &#x2018;mean&#x2019; | &#x2018;sum&#x2019;. &#x2018;none&#x2019;: no reduction will be applied, &#x2018;mean&#x2019;: the sum of the output will be divided by the number of elements in the output, &#x2018;sum&#x2019;: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: &#x2018;mean&#x2019;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> where <code>*</code> means, any number of additional dimensions</li>
<li>Target: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, same shape as the input</li>
<li>Output: scalar. If reduce is <code>False</code>, then <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, same shape as the input</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>loss = nn.L1Loss()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, requires_grad=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>target = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = loss(input, target)
<span class="hljs-meta">&gt;&gt;&gt; </span>output.backward()
</code></pre>
<h3 id="mseloss">MSELoss</h3>
<pre><code class="lang-py">class torch.nn.MSELoss(size_average=None, reduce=None, reduction=&apos;mean&apos;)
</code></pre>
<p>Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input <code>x</code> and target <code>y</code>.</p>
<p>The loss can be described as:</p>
<p><img src="img/e67b64ef5017709a433d1214a681717e.jpg" alt=""></p>
<p>where <img src="img/9341d9048ac485106d2b2ee8de14876f.jpg" alt=""> is the batch size. If reduce is <code>True</code>, then:</p>
<p><img src="img/f3a00c026a75843dd3a04c64f9cecb47.jpg" alt=""></p>
<p>The sum operation still operates over all the elements, and divides by <code>n</code>.</p>
<p>The division by <code>n</code> can be avoided if one sets <code>size_average</code> to <code>False</code>.</p>
<p>To get a batch of losses, a loss per batch element, set <code>reduce</code> to <code>False</code>. These losses are not averaged and are not affected by <code>size_average</code>.</p>
<p>Parameters: </p>
<ul>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li>
<li><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) &#x2013; Specifies the reduction to apply to the output: &#x2018;none&#x2019; | &#x2018;mean&#x2019; | &#x2018;sum&#x2019;. &#x2018;none&#x2019;: no reduction will be applied, &#x2018;mean&#x2019;: the sum of the output will be divided by the number of elements in the output, &#x2018;sum&#x2019;: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: &#x2018;mean&#x2019;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> where <code>*</code> means, any number of additional dimensions</li>
<li>Target: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, same shape as the input</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>loss = nn.MSELoss()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, requires_grad=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>target = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = loss(input, target)
<span class="hljs-meta">&gt;&gt;&gt; </span>output.backward()
</code></pre>
<h3 id="crossentropyloss">CrossEntropyLoss</h3>
<pre><code class="lang-py">class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&apos;mean&apos;)
</code></pre>
<p>This criterion combines <code>nn.LogSoftmax()</code> and <code>nn.NLLLoss()</code> in one single class.</p>
<p>It is useful when training a classification problem with <code>C</code> classes. If provided, the optional argument <code>weight</code> should be a 1D <code>Tensor</code> assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.</p>
<p>The <code>input</code> is expected to contain scores for each class.</p>
<p><code>input</code> has to be a Tensor of size either <img src="img/cca0c8da541b81bec031e4e52161d2c7.jpg" alt=""> or <img src="img/f30f7531b252dc52c6bb945ebb508cc4.jpg" alt=""> with <img src="img/6573879e7d2cf58e8dfdbf8baa9f7a1a.jpg" alt=""> for the <code>K</code>-dimensional case (described later).</p>
<p>This criterion expects a class index (0 to <code>C-1</code>) as the <code>target</code> for each value of a 1D tensor of size <code>minibatch</code></p>
<p>The loss can be described as:</p>
<p><img src="img/29028e6a28821a298d2a456d6bb175f9.jpg" alt=""></p>
<p>or in the case of the <code>weight</code> argument being specified:</p>
<p><img src="img/bc344720164c2bc94ebd3f405b898216.jpg" alt=""></p>
<p>The losses are averaged across observations for each minibatch.</p>
<p>Can also be used for higher dimension inputs, such as 2D images, by providing an input of size <img src="img/f30f7531b252dc52c6bb945ebb508cc4.jpg" alt=""> with <img src="img/6573879e7d2cf58e8dfdbf8baa9f7a1a.jpg" alt="">, where <img src="img/a5db490cd70a38a0bb9f3de58c51589f.jpg" alt=""> is the number of dimensions, and a target of appropriate shape (see below).</p>
<p>Parameters: </p>
<ul>
<li><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; a manual rescaling weight given to each class. If given, has to be a Tensor of size <code>C</code></li>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li>
<li><strong>ignore_index</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; Specifies a target value that is ignored and does not contribute to the input gradient. When <code>size_average</code> is <code>True</code>, the loss is averaged over non-ignored targets.</li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li>
<li><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) &#x2013; Specifies the reduction to apply to the output: &#x2018;none&#x2019; | &#x2018;mean&#x2019; | &#x2018;sum&#x2019;. &#x2018;none&#x2019;: no reduction will be applied, &#x2018;mean&#x2019;: the sum of the output will be divided by the number of elements in the output, &#x2018;sum&#x2019;: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: &#x2018;mean&#x2019;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li><pre><code class="lang-py">Input: \((N, C)\) where C = number of classes, <span class="hljs-keyword">or</span>
</code></pre>
<p><img src="img/ddeb501040934760370435d1c223e6b6.jpg" alt=""> with <img src="img/6573879e7d2cf58e8dfdbf8baa9f7a1a.jpg" alt=""> in the case of <code>K</code>-dimensional loss.</p>
</li>
<li><pre><code class="lang-py">Target: \((N)\) where each value <span class="hljs-keyword">is</span> \(<span class="hljs-number">0</span> \leq \text{targets}[i] \leq C<span class="hljs-number">-1</span>\), <span class="hljs-keyword">or</span>
</code></pre>
<p><img src="img/5981db74a9cd434c7580e6ba530e21b6.jpg" alt=""> with <img src="img/6573879e7d2cf58e8dfdbf8baa9f7a1a.jpg" alt=""> in the case of K-dimensional loss.</p>
</li>
<li><pre><code class="lang-py">Output: scalar. If reduce <span class="hljs-keyword">is</span> <span class="hljs-keyword">False</span>, then the same size
</code></pre>
<p>as the target: <img src="img/2a3e2b832e04fe8d66596083b23da518.jpg" alt="">, or <img src="img/5981db74a9cd434c7580e6ba530e21b6.jpg" alt=""> with <img src="img/6573879e7d2cf58e8dfdbf8baa9f7a1a.jpg" alt=""> in the case of K-dimensional loss.</p>
</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>loss = nn.CrossEntropyLoss()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, requires_grad=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>target = torch.empty(<span class="hljs-number">3</span>, dtype=torch.long).random_(<span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = loss(input, target)
<span class="hljs-meta">&gt;&gt;&gt; </span>output.backward()
</code></pre>
<h3 id="ctcloss">CTCLoss</h3>
<pre><code class="lang-py">class torch.nn.CTCLoss(blank=0, reduction=&apos;mean&apos;)
</code></pre>
<p>The Connectionist Temporal Classification loss.</p>
<p>Parameters: </p>
<ul>
<li><strong>blank</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; blank label. Default <img src="img/28256dd5af833c877d63bfabfaa7b301.jpg" alt="">.</li>
<li><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) &#x2013; Specifies the reduction to apply to the output: &#x2018;none&#x2019; | &#x2018;mean&#x2019; | &#x2018;sum&#x2019;. &#x2018;none&#x2019;: no reduction will be applied, &#x2018;mean&#x2019;: the output losses will be divided by the target lengths and then the mean over the batch is taken. Default: &#x2018;mean&#x2019;</li>
</ul>
<pre><code class="lang-py">Inputs:
</code></pre>
<pre><code class="lang-py">log_probs: Tensor of size \((T, N, C)\) where C = number of characters <span class="hljs-keyword">in</span> alphabet including blank,
</code></pre>
<p><cite>T = input length</cite>, and <cite>N = batch size</cite>. The logarithmized probabilities of the outputs (e.g. obtained with <a href="#torch.nn.functional.log_softmax" title="torch.nn.functional.log_softmax"><code>torch.nn.functional.log_softmax()</code></a>).</p>
<pre><code class="lang-py">targets: Tensor of size \((N, S)\) <span class="hljs-keyword">or</span> (sum(target_lengths)).
</code></pre>
<p>Targets (cannot be blank). In the second form, the targets are assumed to be concatenated.</p>
<pre><code class="lang-py">input_lengths: Tuple <span class="hljs-keyword">or</span> tensor of size \((N)\).
</code></pre>
<p>Lengths of the inputs (must each be <img src="img/0063f9a7d145aadc1082a0c4c8712a62.jpg" alt="">)</p>
<pre><code class="lang-py">target_lengths: Tuple <span class="hljs-keyword">or</span> tensor of size  \((N)\).
</code></pre>
<p>Lengths of the targets</p>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>ctc_loss = nn.CTCLoss()
<span class="hljs-meta">&gt;&gt;&gt; </span>log_probs = torch.randn(<span class="hljs-number">50</span>, <span class="hljs-number">16</span>, <span class="hljs-number">20</span>).log_softmax(<span class="hljs-number">2</span>).detach().requires_grad_()
<span class="hljs-meta">&gt;&gt;&gt; </span>targets = torch.randint(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>, (<span class="hljs-number">16</span>, <span class="hljs-number">30</span>), dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_lengths = torch.full((<span class="hljs-number">16</span>,), <span class="hljs-number">50</span>, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>target_lengths = torch.randint(<span class="hljs-number">10</span>,<span class="hljs-number">30</span>,(<span class="hljs-number">16</span>,), dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss.backward()
</code></pre>
<pre><code class="lang-py">Reference:
</code></pre>
<p>A. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks: <a href="https://www.cs.toronto.edu/~graves/icml_2006.pdf" target="_blank">https://www.cs.toronto.edu/~graves/icml_2006.pdf</a></p>
<p>Note</p>
<p>In order to use CuDNN, the following must be satisfied: <code>targets</code> must be in concatenated format, all <code>input_lengths</code> must be <code>T</code>. <img src="img/e465f6009cd227a31d00f005c2cb1c5b.jpg" alt="">, <code>target_lengths</code> <img src="img/3a2535723ad2261fbfc71d099a993883.jpg" alt="">, the integer arguments must be of dtype <code>torch.int32</code>.</p>
<p>The regular implementation uses the (more common in PyTorch) <code>torch.long</code> dtype.</p>
<p>Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. Please see the notes on <a href="notes/randomness.html">Reproducibility</a> for background.</p>
<h3 id="nllloss">NLLLoss</h3>
<pre><code class="lang-py">class torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&apos;mean&apos;)
</code></pre>
<p>The negative log likelihood loss. It is useful to train a classification problem with <code>C</code> classes.</p>
<p>If provided, the optional argument <code>weight</code> should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.</p>
<p>The input given through a forward call is expected to contain log-probabilities of each class. <code>input</code> has to be a Tensor of size either <img src="img/cca0c8da541b81bec031e4e52161d2c7.jpg" alt=""> or <img src="img/f30f7531b252dc52c6bb945ebb508cc4.jpg" alt=""> with <img src="img/6573879e7d2cf58e8dfdbf8baa9f7a1a.jpg" alt=""> for the <code>K</code>-dimensional case (described later).</p>
<p>Obtaining log-probabilities in a neural network is easily achieved by adding a <code>LogSoftmax</code> layer in the last layer of your network. You may use <code>CrossEntropyLoss</code> instead, if you prefer not to add an extra layer.</p>
<p>The target that this loss expects is a class index <code>(0 to C-1, where C = number of classes)</code></p>
<p>If <code>reduce</code> is <code>False</code>, the loss can be described as:</p>
<p><img src="img/edf1079de0e5df9633d0de83b68250f2.jpg" alt=""></p>
<p>where <img src="img/9341d9048ac485106d2b2ee8de14876f.jpg" alt=""> is the batch size. If <code>reduce</code> is <code>True</code> (default), then</p>
<p><img src="img/6eeb4eee2867f6565cb78f3d2e8503f2.jpg" alt=""></p>
<p>Can also be used for higher dimension inputs, such as 2D images, by providing an input of size <img src="img/f30f7531b252dc52c6bb945ebb508cc4.jpg" alt=""> with <img src="img/6573879e7d2cf58e8dfdbf8baa9f7a1a.jpg" alt="">, where <img src="img/a5db490cd70a38a0bb9f3de58c51589f.jpg" alt=""> is the number of dimensions, and a target of appropriate shape (see below). In the case of images, it computes NLL loss per-pixel.</p>
<p>Parameters: </p>
<ul>
<li><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; a manual rescaling weight given to each class. If given, it has to be a Tensor of size <code>C</code>. Otherwise, it is treated as if having all ones.</li>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li>
<li><strong>ignore_index</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; Specifies a target value that is ignored and does not contribute to the input gradient. When <code>size_average</code> is <code>True</code>, the loss is averaged over non-ignored targets.</li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li>
<li><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) &#x2013; Specifies the reduction to apply to the output: &#x2018;none&#x2019; | &#x2018;mean&#x2019; | &#x2018;sum&#x2019;. &#x2018;none&#x2019;: no reduction will be applied, &#x2018;mean&#x2019;: the sum of the output will be divided by the number of elements in the output, &#x2018;sum&#x2019;: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: &#x2018;mean&#x2019;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li><pre><code class="lang-py">Input: \((N, C)\) where C = number of classes, <span class="hljs-keyword">or</span>
</code></pre>
<p><img src="img/ddeb501040934760370435d1c223e6b6.jpg" alt=""> with <img src="img/6573879e7d2cf58e8dfdbf8baa9f7a1a.jpg" alt=""> in the case of <code>K</code>-dimensional loss.</p>
</li>
<li><pre><code class="lang-py">Target: \((N)\) where each value <span class="hljs-keyword">is</span> \(<span class="hljs-number">0</span> \leq \text{targets}[i] \leq C<span class="hljs-number">-1</span>\), <span class="hljs-keyword">or</span>
</code></pre>
<p><img src="img/5981db74a9cd434c7580e6ba530e21b6.jpg" alt=""> with <img src="img/6573879e7d2cf58e8dfdbf8baa9f7a1a.jpg" alt=""> in the case of K-dimensional loss.</p>
</li>
<li><pre><code class="lang-py">Output: scalar. If reduce <span class="hljs-keyword">is</span> <span class="hljs-keyword">False</span>, then the same size
</code></pre>
<p>as the target: <img src="img/2a3e2b832e04fe8d66596083b23da518.jpg" alt="">, or <img src="img/5981db74a9cd434c7580e6ba530e21b6.jpg" alt=""> with <img src="img/6573879e7d2cf58e8dfdbf8baa9f7a1a.jpg" alt=""> in the case of K-dimensional loss.</p>
</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.LogSoftmax()
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = nn.NLLLoss()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># input is of size N x C = 3 x 5</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, requires_grad=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># each element in target has to have 0 &lt;= value &lt; C</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>output = loss(m(input), target)
<span class="hljs-meta">&gt;&gt;&gt; </span>output.backward()
&gt;&gt;&gt;
&gt;&gt;&gt;
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 2D loss example (used, for example, with image inputs)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>N, C = <span class="hljs-number">5</span>, <span class="hljs-number">4</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = nn.NLLLoss()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># input is of size N x C x height x width</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data = torch.randn(N, <span class="hljs-number">16</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>conv = nn.Conv2d(<span class="hljs-number">16</span>, C, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.LogSoftmax()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># each element in target has to have 0 &lt;= value &lt; C</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target = torch.empty(N, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>, dtype=torch.long).random_(<span class="hljs-number">0</span>, C)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = loss(m(conv(data)), target)
<span class="hljs-meta">&gt;&gt;&gt; </span>output.backward()
</code></pre>
<h3 id="poissonnllloss">PoissonNLLLoss</h3>
<pre><code class="lang-py">class torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction=&apos;mean&apos;)
</code></pre>
<p>Negative log likelihood loss with Poisson distribution of target.</p>
<p>The loss can be described as:</p>
<p><img src="img/f50aaec015c8f6f2ef26d16a60023ea1.jpg" alt=""></p>
<p>The last term can be omitted or approximated with Stirling formula. The approximation is used for target values more than 1. For targets less or equal to 1 zeros are added to the loss.</p>
<p>Parameters: </p>
<ul>
<li><strong>log_input</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; if <code>True</code> the loss is computed as <img src="img/18729f59c6d4705e1945b3d7b3e09e32.jpg" alt="">, if <code>False</code> the loss is <img src="img/036591dc90f1dafaa138920518e2b05b.jpg" alt="">.</li>
<li><p><strong>full</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013;</p>
<p>whether to compute full loss, i. e. to add the Stirling approximation term</p>
<p><img src="img/b063f11c809ea98839d91fc34d0b4bf0.jpg" alt=""></p>
</li>
<li><p><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li><strong>eps</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a><em>,</em> <em>optional</em>) &#x2013; Small value to avoid evaluation of <img src="img/f6dcd4f69520c309a6d71002bd330cb8.jpg" alt=""> when <code>log_input == False</code>. Default: 1e-8</li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li>
<li><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) &#x2013; Specifies the reduction to apply to the output: &#x2018;none&#x2019; | &#x2018;mean&#x2019; | &#x2018;sum&#x2019;. &#x2018;none&#x2019;: no reduction will be applied, &#x2018;mean&#x2019;: the sum of the output will be divided by the number of elements in the output, &#x2018;sum&#x2019;: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: &#x2018;mean&#x2019;</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>loss = nn.PoissonNLLLoss()
<span class="hljs-meta">&gt;&gt;&gt; </span>log_input = torch.randn(<span class="hljs-number">5</span>, <span class="hljs-number">2</span>, requires_grad=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>target = torch.randn(<span class="hljs-number">5</span>, <span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = loss(log_input, target)
<span class="hljs-meta">&gt;&gt;&gt; </span>output.backward()
</code></pre>
<h3 id="kldivloss">KLDivLoss</h3>
<pre><code class="lang-py">class torch.nn.KLDivLoss(size_average=None, reduce=None, reduction=&apos;mean&apos;)
</code></pre>
<p>The <a href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence" target="_blank">Kullback-Leibler divergence</a> Loss</p>
<p>KL divergence is a useful distance measure for continuous distributions and is often useful when performing direct regression over the space of (discretely sampled) continuous output distributions.</p>
<p>As with <a href="#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><code>NLLLoss</code></a>, the <code>input</code> given is expected to contain <em>log-probabilities</em>. However, unlike <a href="#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><code>NLLLoss</code></a>, <code>input</code> is not restricted to a 2D Tensor. The targets are given as <em>probabilities</em> (i.e. without taking the logarithm).</p>
<p>This criterion expects a <code>target</code> <code>Tensor</code> of the same size as the <code>input</code> <code>Tensor</code>.</p>
<p>The unreduced (i.e. with <code>reduce</code> set to <code>False</code>) loss can be described as:</p>
<p><img src="img/eba993f61a08816ebd5f577851d521f2.jpg" alt=""></p>
<p>where the index <img src="img/9341d9048ac485106d2b2ee8de14876f.jpg" alt=""> spans all dimensions of <code>input</code> and <img src="img/db4a9fef02111450bf98261889de550c.jpg" alt=""> has the same shape as <code>input</code>. If <code>reduce</code> is <code>True</code> (the default), then:</p>
<p><img src="img/d88944e162eff94bddc1b9a94bcaa3a6.jpg" alt=""></p>
<p>In default reduction mode &#x2018;mean&#x2019;, the losses are averaged for each minibatch over observations <strong>as well as</strong> over dimensions. &#x2018;batchmean&#x2019; mode gives the correct KL divergence where losses are averaged over batch dimension only. &#x2018;mean&#x2019; mode&#x2019;s behavior will be changed to the same as &#x2018;batchmean&#x2019; in the next major release.</p>
<p>Parameters: </p>
<ul>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li>
<li><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) &#x2013; Specifies the reduction to apply to the output: &#x2018;none&#x2019; | &#x2018;batchmean&#x2019; | &#x2018;sum&#x2019; | &#x2018;mean&#x2019;. &#x2018;none&#x2019;: no reduction will be applied. &#x2018;batchmean&#x2019;: the sum of the output will be divided by batchsize. &#x2018;sum&#x2019;: the output will be summed. &#x2018;mean&#x2019;: the output will be divided by the number of elements in the output. Default: &#x2018;mean&#x2019;</li>
</ul>
<p>:param .. note:: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated,: and in the meantime, specifying either of those two args will override <code>reduction</code>. :param .. note:: <code>reduction=&#x2019;mean&#x2019;</code> doesn&#x2019;t return the true kl divergence value, please use: <code>reduction=&#x2019;batchmean&#x2019;</code> which aligns with KL math definition.</p>
<blockquote>
<p>In the next major release, &#x2018;mean&#x2019; will be changed to be the same as &#x2018;batchmean&#x2019;.</p>
</blockquote>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>input: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> where <code>*</code> means, any number of additional dimensions</li>
<li>target: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, same shape as the input</li>
<li><pre><code class="lang-py">output: scalar by default. If reduce <span class="hljs-keyword">is</span> <span class="hljs-keyword">False</span>, then \((N, *)\),
</code></pre>
<p>the same shape as the input</p>
</li>
</ul>
<h3 id="bceloss">BCELoss</h3>
<pre><code class="lang-py">class torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction=&apos;mean&apos;)
</code></pre>
<p>Creates a criterion that measures the Binary Cross Entropy between the target and the output:</p>
<p>The loss can be described as:</p>
<p><img src="img/f233882012c0c24fcad1869a163b5b7c.jpg" alt=""></p>
<p>where <img src="img/9341d9048ac485106d2b2ee8de14876f.jpg" alt=""> is the batch size. If reduce is <code>True</code>, then</p>
<p><img src="img/d88944e162eff94bddc1b9a94bcaa3a6.jpg" alt=""></p>
<p>This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets <code>y</code> should be numbers between 0 and 1.</p>
<p>Parameters: </p>
<ul>
<li><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size &#x201C;nbatch&#x201D;.</li>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li>
<li><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) &#x2013; Specifies the reduction to apply to the output: &#x2018;none&#x2019; | &#x2018;mean&#x2019; | &#x2018;sum&#x2019;. &#x2018;none&#x2019;: no reduction will be applied, &#x2018;mean&#x2019;: the sum of the output will be divided by the number of elements in the output, &#x2018;sum&#x2019;: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: &#x2018;mean&#x2019;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> where <code>*</code> means, any number of additional dimensions</li>
<li>Target: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, same shape as the input</li>
<li>Output: scalar. If <code>reduce</code> is False, then <code>(N, *)</code>, same shape as input.</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Sigmoid()
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = nn.BCELoss()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">3</span>, requires_grad=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>target = torch.empty(<span class="hljs-number">3</span>).random_(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = loss(m(input), target)
<span class="hljs-meta">&gt;&gt;&gt; </span>output.backward()
</code></pre>
<h3 id="bcewithlogitsloss">BCEWithLogitsLoss</h3>
<pre><code class="lang-py">class torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction=&apos;mean&apos;, pos_weight=None)
</code></pre>
<p>This loss combines a <code>Sigmoid</code> layer and the <code>BCELoss</code> in one single class. This version is more numerically stable than using a plain <code>Sigmoid</code> followed by a <code>BCELoss</code> as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.</p>
<p>The loss can be described as:</p>
<p><img src="img/0c49aad6f81ec7936e313096f7a53f97.jpg" alt=""></p>
<p>where <img src="img/9341d9048ac485106d2b2ee8de14876f.jpg" alt=""> is the batch size. If reduce is <code>True</code>, then</p>
<p><img src="img/0a16102d9320f70f18d7c8b152000489.jpg" alt=""></p>
<p>This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets <code>t[i]</code> should be numbers between 0 and 1.</p>
<p>It&#x2019;s possible to trade off recall and precision by adding weights to positive examples. In this case the loss can be described as:</p>
<p><img src="img/d5ca42e0ee1490d1dea4d5f38cc120d7.jpg" alt=""></p>
<p>where <img src="img/76dc369e067e5fa42a4b32b6afd5e570.jpg" alt=""> is the positive weight of class <img src="img/493731e423d5db62086d0b8705dda0c8.jpg" alt="">. <img src="img/65abc7465f8ac5056f8562962f0ae02e.jpg" alt=""> increases the recall, <img src="img/989afd86a6407cf24295ae8d52ff0080.jpg" alt=""> increases the precision.</p>
<p>For example, if a dataset contains 100 positive and 300 negative examples of a single class, then <code>pos_weight</code> for the class should be equal to <img src="img/6a003751ded2d5e5198d93ee7db1ba5d.jpg" alt="">. The loss would act as if the dataset contains <img src="img/fb2ad75ea1ac3ba3ae507a3c8a34db12.jpg" alt=""> positive examples.</p>
<p>Parameters: </p>
<ul>
<li><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size &#x201C;nbatch&#x201D;.</li>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li>
<li><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) &#x2013; Specifies the reduction to apply to the output: &#x2018;none&#x2019; | &#x2018;mean&#x2019; | &#x2018;sum&#x2019;. &#x2018;none&#x2019;: no reduction will be applied, &#x2018;mean&#x2019;: the sum of the output will be divided by the number of elements in the output, &#x2018;sum&#x2019;: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: &#x2018;mean&#x2019;</li>
<li><strong>pos_weight</strong> &#x2013; a weight of positive examples. Must be a vector with length equal to the number of classes.</li>
</ul>
<h3 id="marginrankingloss">MarginRankingLoss</h3>
<pre><code class="lang-py">class torch.nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction=&apos;mean&apos;)
</code></pre>
<p>Creates a criterion that measures the loss given inputs <code>x1</code>, <code>x2</code>, two 1D mini-batch <code>Tensor</code>s, and a label 1D mini-batch tensor <code>y</code> with values (<code>1</code> or <code>-1</code>).</p>
<p>If <code>y == 1</code> then it assumed the first input should be ranked higher (have a larger value) than the second input, and vice-versa for <code>y == -1</code>.</p>
<p>The loss function for each sample in the mini-batch is:</p>
<p><img src="img/1664f71bed4b6591f02c8bbb10f2d389.jpg" alt=""></p>
<p>Parameters: </p>
<ul>
<li><strong>margin</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a><em>,</em> <em>optional</em>) &#x2013; Has a default value of <code>0</code>.</li>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li>
<li><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) &#x2013; Specifies the reduction to apply to the output: &#x2018;none&#x2019; | &#x2018;mean&#x2019; | &#x2018;sum&#x2019;. &#x2018;none&#x2019;: no reduction will be applied, &#x2018;mean&#x2019;: the sum of the output will be divided by the number of elements in the output, &#x2018;sum&#x2019;: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: &#x2018;mean&#x2019;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/3dc464d2e10c731f17264e33e497c1a8.jpg" alt=""> where <code>N</code> is the batch size and <code>D</code> is the size of a sample.</li>
<li>Target: <img src="img/2a3e2b832e04fe8d66596083b23da518.jpg" alt=""></li>
<li>Output: scalar. If <code>reduce</code> is False, then <code>(N)</code>.</li>
</ul>
<h3 id="hingeembeddingloss">HingeEmbeddingLoss</h3>
<pre><code class="lang-py">class torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction=&apos;mean&apos;)
</code></pre>
<p>Measures the loss given an input tensor <code>x</code> and a labels tensor <code>y</code> containing values (<code>1</code> or <code>-1</code>). This is usually used for measuring whether two inputs are similar or dissimilar, e.g. using the L1 pairwise distance as <code>x</code>, and is typically used for learning nonlinear embeddings or semi-supervised learning.</p>
<p>The loss function for <img src="img/493731e423d5db62086d0b8705dda0c8.jpg" alt="">-th sample in the mini-batch is</p>
<p><img src="img/1e176ed632f1cbc86eb8db4bf6034f24.jpg" alt=""></p>
<p>and the total loss functions is</p>
<p><img src="img/0a16102d9320f70f18d7c8b152000489.jpg" alt=""></p>
<p>where <img src="img/97b4908568a8d2f8549d90e683a8efa2.jpg" alt="">.</p>
<p>Parameters: </p>
<ul>
<li><strong>margin</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a><em>,</em> <em>optional</em>) &#x2013; Has a default value of <code>1</code>.</li>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li>
<li><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) &#x2013; Specifies the reduction to apply to the output: &#x2018;none&#x2019; | &#x2018;mean&#x2019; | &#x2018;sum&#x2019;. &#x2018;none&#x2019;: no reduction will be applied, &#x2018;mean&#x2019;: the sum of the output will be divided by the number of elements in the output, &#x2018;sum&#x2019;: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: &#x2018;mean&#x2019;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: Tensor of arbitrary shape. The sum operation operates over all the elements.</li>
<li>Target: Same shape as input.</li>
<li>Output: scalar. If reduce is <code>False</code>, then same shape as the input</li>
</ul>
<h3 id="multilabelmarginloss">MultiLabelMarginLoss</h3>
<pre><code class="lang-py">class torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction=&apos;mean&apos;)
</code></pre>
<p>Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input <code>x</code> (a 2D mini-batch <code>Tensor</code>) and output <code>y</code> (which is a 2D <code>Tensor</code> of target class indices). For each sample in the mini-batch:</p>
<p><img src="img/f4d7e37a53b15d27b3a25c9dc586cd00.jpg" alt=""></p>
<p>where <img src="img/b75aa938b45ed55c0aa471218a7224ce.jpg" alt=""> to <img src="img/c915dd214c2340e40ac8e79013465783.jpg" alt="">, <img src="img/4da95e4f78bfe10987f5549ced63a7e6.jpg" alt=""> to <img src="img/ec0f8a278c4b73ccb95d3a4c1d129697.jpg" alt="">, <img src="img/8058aac8e03495c71f75b04169d5baca.jpg" alt="">, and <img src="img/27b5f368a27d1e15b3656796a28a4411.jpg" alt=""> for all <img src="img/31df9c730e19ca29b59dce64b99d98c1.jpg" alt=""> and <img src="img/d8fdd0e28cfb03738fc5227885ee035a.jpg" alt="">.</p>
<p><code>y</code> and <code>x</code> must have the same size.</p>
<p>The criterion only considers a contiguous block of non-negative targets that starts at the front.</p>
<p>This allows for different samples to have variable amounts of target classes</p>
<p>Parameters: </p>
<ul>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li>
<li><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) &#x2013; Specifies the reduction to apply to the output: &#x2018;none&#x2019; | &#x2018;mean&#x2019; | &#x2018;sum&#x2019;. &#x2018;none&#x2019;: no reduction will be applied, &#x2018;mean&#x2019;: the sum of the output will be divided by the number of elements in the output, &#x2018;sum&#x2019;: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: &#x2018;mean&#x2019;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/861a7d7a604a97f5620afad259a4c26d.jpg" alt=""> or <img src="img/9b9aebaa467ad07dca05b5086bd21ca2.jpg" alt=""> where <code>N</code> is the batch size and <code>C</code> is the number of classes.</li>
<li>Target: <img src="img/861a7d7a604a97f5620afad259a4c26d.jpg" alt=""> or <img src="img/9b9aebaa467ad07dca05b5086bd21ca2.jpg" alt="">, same shape as the input.</li>
<li>Output: scalar. If <code>reduce</code> is False, then <code>(N)</code>.</li>
</ul>
<h3 id="smoothl1loss">SmoothL1Loss</h3>
<pre><code class="lang-py">class torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction=&apos;mean&apos;)
</code></pre>
<p>Creates a criterion that uses a squared term if the absolute element-wise error falls below 1 and an L1 term otherwise. It is less sensitive to outliers than the <code>MSELoss</code> and in some cases prevents exploding gradients (e.g. see &#x201C;Fast R-CNN&#x201D; paper by Ross Girshick). Also known as the Huber loss:</p>
<p><img src="img/cd503c18d22f0e18a5109f3f13d028b2.jpg" alt=""></p>
<p>where <img src="img/bbfcb7c1428a33547e15f8853dbe6e4f.jpg" alt=""> is given by:</p>
<p><img src="img/621fa336f1f8b6169430fa6b42a00b6d.jpg" alt=""></p>
<p><code>x</code> and <code>y</code> arbitrary shapes with a total of <code>n</code> elements each the sum operation still operates over all the elements, and divides by <code>n</code>.</p>
<p>The division by <code>n</code> can be avoided if one sets <code>size_average</code> to <code>False</code></p>
<p>Parameters: </p>
<ul>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li>
<li><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) &#x2013; Specifies the reduction to apply to the output: &#x2018;none&#x2019; | &#x2018;mean&#x2019; | &#x2018;sum&#x2019;. &#x2018;none&#x2019;: no reduction will be applied, &#x2018;mean&#x2019;: the sum of the output will be divided by the number of elements in the output, &#x2018;sum&#x2019;: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: &#x2018;mean&#x2019;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt=""> where <code>*</code> means, any number of additional dimensions</li>
<li>Target: <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, same shape as the input</li>
<li>Output: scalar. If reduce is <code>False</code>, then <img src="img/eb7a3f5bc15cc379e78f768e821eb094.jpg" alt="">, same shape as the input</li>
</ul>
<h3 id="softmarginloss">SoftMarginLoss</h3>
<pre><code class="lang-py">class torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction=&apos;mean&apos;)
</code></pre>
<p>Creates a criterion that optimizes a two-class classification logistic loss between input tensor <code>x</code> and target tensor <code>y</code> (containing 1 or -1).</p>
<p><img src="img/811f3185227a964c048126484987ef1c.jpg" alt=""></p>
<p>Parameters: </p>
<ul>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li>
<li><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) &#x2013; Specifies the reduction to apply to the output: &#x2018;none&#x2019; | &#x2018;mean&#x2019; | &#x2018;sum&#x2019;. &#x2018;none&#x2019;: no reduction will be applied, &#x2018;mean&#x2019;: the sum of the output will be divided by the number of elements in the output, &#x2018;sum&#x2019;: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: &#x2018;mean&#x2019;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: Tensor of arbitrary shape.</li>
<li>Target: Same shape as input.</li>
<li>Output: scalar. If reduce is <code>False</code>, then same shape as the input</li>
</ul>
<h3 id="multilabelsoftmarginloss">MultiLabelSoftMarginLoss</h3>
<pre><code class="lang-py">class torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=None, reduce=None, reduction=&apos;mean&apos;)
</code></pre>
<p>Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input <code>x</code> and target <code>y</code> of size <code>(N, C)</code>. For each sample in the minibatch:</p>
<p><img src="img/342414ff43adf5d0fa0b62fcde9538a2.jpg" alt=""></p>
<p>where <code>i == 0</code> to <code>x.nElement()-1</code>, <code>y[i] in {0,1}</code>.</p>
<p>Parameters: </p>
<ul>
<li><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; a manual rescaling weight given to each class. If given, it has to be a Tensor of size <code>C</code>. Otherwise, it is treated as if having all ones.</li>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li>
<li><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) &#x2013; Specifies the reduction to apply to the output: &#x2018;none&#x2019; | &#x2018;mean&#x2019; | &#x2018;sum&#x2019;. &#x2018;none&#x2019;: no reduction will be applied, &#x2018;mean&#x2019;: the sum of the output will be divided by the number of elements in the output, &#x2018;sum&#x2019;: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: &#x2018;mean&#x2019;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/9b9aebaa467ad07dca05b5086bd21ca2.jpg" alt=""> where <code>N</code> is the batch size and <code>C</code> is the number of classes.</li>
<li>Target: <img src="img/9b9aebaa467ad07dca05b5086bd21ca2.jpg" alt="">, same shape as the input.</li>
<li>Output: scalar. If <code>reduce</code> is False, then <code>(N)</code>.</li>
</ul>
<h3 id="cosineembeddingloss">CosineEmbeddingLoss</h3>
<pre><code class="lang-py">class torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction=&apos;mean&apos;)
</code></pre>
<p>Creates a criterion that measures the loss given input tensors <img src="img/abdadb44ea35aecb39004dd7f55d9543.jpg" alt="">, <img src="img/88fdc6eeb68ef4aacf7cd6bd43fa176e.jpg" alt=""> and a <code>Tensor</code> label <code>y</code> with values 1 or -1. This is used for measuring whether two inputs are similar or dissimilar, using the cosine distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.</p>
<p>The loss function for each sample is:</p>
<p><img src="img/f894a052d4408e0269216f8b803d074a.jpg" alt=""></p>
<p>Parameters: </p>
<ul>
<li><strong>margin</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a><em>,</em> <em>optional</em>) &#x2013; Should be a number from <code>-1</code> to <code>1</code>, <code>0</code> to <code>0.5</code> is suggested. If <code>margin</code> is missing, the default value is <code>0</code>.</li>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li>
<li><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) &#x2013; Specifies the reduction to apply to the output: &#x2018;none&#x2019; | &#x2018;mean&#x2019; | &#x2018;sum&#x2019;. &#x2018;none&#x2019;: no reduction will be applied, &#x2018;mean&#x2019;: the sum of the output will be divided by the number of elements in the output, &#x2018;sum&#x2019;: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: &#x2018;mean&#x2019;</li>
</ul>
<h3 id="multimarginloss">MultiMarginLoss</h3>
<pre><code class="lang-py">class torch.nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction=&apos;mean&apos;)
</code></pre>
<p>Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input <code>x</code> (a 2D mini-batch <code>Tensor</code>) and output <code>y</code> (which is a 1D tensor of target class indices, <img src="img/fffe5e09046ebb236f89daa5091946f6.jpg" alt="">):</p>
<p>For each mini-batch sample, the loss in terms of the 1D input <code>x</code> and scalar output <code>y</code> is:</p>
<p><img src="img/0f825c52299de2e574d5903469e1af9c.jpg" alt=""></p>
<p>where <code>i == 0</code> to <code>x.size(0)</code> and <img src="img/99e4beebf24a180393aa15ec3740cf3a.jpg" alt="">.</p>
<p>Optionally, you can give non-equal weighting on the classes by passing a 1D <code>weight</code> tensor into the constructor.</p>
<p>The loss function then becomes:</p>
<p><img src="img/03b7ccf64bc071a7c2abbeab89f12d08.jpg" alt=""></p>
<p>Parameters: </p>
<ul>
<li><strong>p</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; Has a default value of <code>1</code>. <code>1</code> and <code>2</code> are the only supported values</li>
<li><strong>margin</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a><em>,</em> <em>optional</em>) &#x2013; Has a default value of <code>1</code>.</li>
<li><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; a manual rescaling weight given to each class. If given, it has to be a Tensor of size <code>C</code>. Otherwise, it is treated as if having all ones.</li>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li>
<li><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) &#x2013; Specifies the reduction to apply to the output: &#x2018;none&#x2019; | &#x2018;mean&#x2019; | &#x2018;sum&#x2019;. &#x2018;none&#x2019;: no reduction will be applied, &#x2018;mean&#x2019;: the sum of the output will be divided by the number of elements in the output, &#x2018;sum&#x2019;: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: &#x2018;mean&#x2019;</li>
</ul>
<h3 id="tripletmarginloss">TripletMarginLoss</h3>
<pre><code class="lang-py">class torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction=&apos;mean&apos;)
</code></pre>
<p>Creates a criterion that measures the triplet loss given an input tensors x1, x2, x3 and a margin with a value greater than 0. This is used for measuring a relative similarity between samples. A triplet is composed by <code>a</code>, <code>p</code> and <code>n</code>: anchor, positive examples and negative example respectively. The shapes of all input tensors should be <img src="img/3dc464d2e10c731f17264e33e497c1a8.jpg" alt="">.</p>
<p>The distance swap is described in detail in the paper <a href="http://www.iis.ee.ic.ac.uk/%7Evbalnt/shallow_descr/TFeat_paper.pdf" target="_blank">Learning shallow convolutional feature descriptors with triplet losses</a> by V. Balntas, E. Riba et al.</p>
<p>The loss function for each sample in the mini-batch is:</p>
<p><img src="img/a2c4faa5dd95a547388c1b7f69bbc4db.jpg" alt=""></p>
<p>where</p>
<p><img src="img/bac339e9cf6ad679fa9ce3ce33c431ab.jpg" alt=""></p>
<p>Parameters: </p>
<ul>
<li><strong>margin</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a><em>,</em> <em>optional</em>) &#x2013; Default: <code>1</code>.</li>
<li><strong>p</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; The norm degree for pairwise distance. Default: <code>2</code>.</li>
<li><strong>swap</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a><em>,</em> <em>optional</em>) &#x2013; The distance swap is described in detail in the paper <code>Learning shallow convolutional feature descriptors with triplet losses</code> by V. Balntas, E. Riba et al. Default: <code>False</code>.</li>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li>
<li><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) &#x2013; Specifies the reduction to apply to the output: &#x2018;none&#x2019; | &#x2018;mean&#x2019; | &#x2018;sum&#x2019;. &#x2018;none&#x2019;: no reduction will be applied, &#x2018;mean&#x2019;: the sum of the output will be divided by the number of elements in the output, &#x2018;sum&#x2019;: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: &#x2018;mean&#x2019;</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/3dc464d2e10c731f17264e33e497c1a8.jpg" alt=""> where <code>D</code> is the vector dimension.</li>
<li>Output: scalar. If <code>reduce</code> is False, then <code>(N)</code>.</li>
</ul>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>triplet_loss = nn.TripletMarginLoss(margin=<span class="hljs-number">1.0</span>, p=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input1 = torch.randn(<span class="hljs-number">100</span>, <span class="hljs-number">128</span>, requires_grad=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input2 = torch.randn(<span class="hljs-number">100</span>, <span class="hljs-number">128</span>, requires_grad=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input3 = torch.randn(<span class="hljs-number">100</span>, <span class="hljs-number">128</span>, requires_grad=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = triplet_loss(input1, input2, input3)
<span class="hljs-meta">&gt;&gt;&gt; </span>output.backward()
</code></pre>
<h2 id="vision-layers">Vision layers</h2>
<h3 id="pixelshuffle">PixelShuffle</h3>
<pre><code class="lang-py">class torch.nn.PixelShuffle(upscale_factor)
</code></pre>
<p>Rearranges elements in a tensor of shape <img src="img/1bc8a113de558f2e7d966e72ae39cb95.jpg" alt=""> to a tensor of shape <img src="img/d4e6de257f72abc5a96af64211b7f909.jpg" alt="">.</p>
<p>This is useful for implementing efficient sub-pixel convolution with a stride of <img src="img/71c5422a7f21b7096aa6d904d5a4f78d.jpg" alt="">.</p>
<p>Look at the paper: <a href="https://arxiv.org/abs/1609.05158" target="_blank">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</a> by Shi et. al (2016) for more details.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>upscale_factor</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; factor to increase spatial resolution by</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/d6770cccc0ae9886c3b91d55efa20b28.jpg" alt=""></li>
<li>Output: <img src="img/a162cf8e9185f67b3f5b084d1031dc7e.jpg" alt=""></li>
</ul>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>pixel_shuffle = nn.PixelShuffle(<span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">9</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = pixel_shuffle(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(output.size())
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])
</code></pre>
<h3 id="upsample">Upsample</h3>
<pre><code class="lang-py">class torch.nn.Upsample(size=None, scale_factor=None, mode=&apos;nearest&apos;, align_corners=None)
</code></pre>
<p>Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.</p>
<p>The input data is assumed to be of the form <code>minibatch x channels x [optional depth] x [optional height] x width</code>. Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.</p>
<p>The algorithms available for upsampling are nearest neighbor and linear, bilinear and trilinear for 3D, 4D and 5D input Tensor, respectively.</p>
<p>One can either give a <code>scale_factor</code> or the target output <code>size</code> to calculate the output size. (You cannot give both, as it is ambiguous)</p>
<p>Parameters: </p>
<ul>
<li><strong>size</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; a tuple of ints <code>([optional D_out], [optional H_out], W_out)</code> output sizes</li>
<li><strong>scale_factor</strong> (<em>int / tuple of python:ints__,</em> <em>optional</em>) &#x2013; the multiplier for the image height / width / depth</li>
<li><strong>mode</strong> (<em>string__,</em> <em>optional</em>) &#x2013; the upsampling algorithm: one of <code>nearest</code>, <code>linear</code>, <code>bilinear</code> and <code>trilinear</code>. Default: <code>nearest</code></li>
<li><strong>align_corners</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; if True, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when <code>mode</code> is <code>linear</code>, <code>bilinear</code>, or <code>trilinear</code>. Default: False</li>
</ul>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/964aa6df63e83f4468aa090441f01972.jpg" alt="">, <img src="img/ff71b16eb10237262566c6907acaaf1f.jpg" alt=""> or <img src="img/c187d190013d0785320e3412fe8cd669.jpg" alt=""></li>
<li>Output: <img src="img/ac2661719f40fc422e2b1590a1e7b4a4.jpg" alt="">, <img src="img/a0ef05f779873fc4dcbf020b1ea14754.jpg" alt=""> or <img src="img/41ca4c8d4c65c979d2d643c6f62ea280.jpg" alt="">, where</li>
</ul>
<p><img src="img/da11a1265058248a851d6d0331110214.jpg" alt=""></p>
<p><img src="img/828543b18440713aad6ad023732327ec.jpg" alt=""></p>
<p><img src="img/5a7c5c22409d4ab3c83641508bf72cb6.jpg" alt=""></p>
<p>Warning</p>
<p>With <code>align_corners = True</code>, the linearly interpolating modes (<code>linear</code>, <code>bilinear</code>, and <code>trilinear</code>) don&#x2019;t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is <code>align_corners = False</code>. See below for concrete examples on how this affects the outputs.</p>
<p>Note</p>
<p>If you want downsampling/general resizing, you should use <code>interpolate()</code>.</p>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>).view(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>).float()
<span class="hljs-meta">&gt;&gt;&gt; </span>input
tensor([[[[ <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>],
 [ <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>]]]])

<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Upsample(scale_factor=<span class="hljs-number">2</span>, mode=<span class="hljs-string">&apos;nearest&apos;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[[ <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">2.</span>],
 [ <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">2.</span>],
 [ <span class="hljs-number">3.</span>,  <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">4.</span>],
 [ <span class="hljs-number">3.</span>,  <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">4.</span>]]]])

<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Upsample(scale_factor=<span class="hljs-number">2</span>, mode=<span class="hljs-string">&apos;bilinear&apos;</span>)  <span class="hljs-comment"># align_corners=False</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[[ <span class="hljs-number">1.0000</span>,  <span class="hljs-number">1.2500</span>,  <span class="hljs-number">1.7500</span>,  <span class="hljs-number">2.0000</span>],
 [ <span class="hljs-number">1.5000</span>,  <span class="hljs-number">1.7500</span>,  <span class="hljs-number">2.2500</span>,  <span class="hljs-number">2.5000</span>],
 [ <span class="hljs-number">2.5000</span>,  <span class="hljs-number">2.7500</span>,  <span class="hljs-number">3.2500</span>,  <span class="hljs-number">3.5000</span>],
 [ <span class="hljs-number">3.0000</span>,  <span class="hljs-number">3.2500</span>,  <span class="hljs-number">3.7500</span>,  <span class="hljs-number">4.0000</span>]]]])

<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Upsample(scale_factor=<span class="hljs-number">2</span>, mode=<span class="hljs-string">&apos;bilinear&apos;</span>, align_corners=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[[ <span class="hljs-number">1.0000</span>,  <span class="hljs-number">1.3333</span>,  <span class="hljs-number">1.6667</span>,  <span class="hljs-number">2.0000</span>],
 [ <span class="hljs-number">1.6667</span>,  <span class="hljs-number">2.0000</span>,  <span class="hljs-number">2.3333</span>,  <span class="hljs-number">2.6667</span>],
 [ <span class="hljs-number">2.3333</span>,  <span class="hljs-number">2.6667</span>,  <span class="hljs-number">3.0000</span>,  <span class="hljs-number">3.3333</span>],
 [ <span class="hljs-number">3.0000</span>,  <span class="hljs-number">3.3333</span>,  <span class="hljs-number">3.6667</span>,  <span class="hljs-number">4.0000</span>]]]])

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Try scaling the same data in a larger tensor</span>
&gt;&gt;&gt;
<span class="hljs-meta">&gt;&gt;&gt; </span>input_3x3 = torch.zeros(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>).view(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_3x3[:, :, :<span class="hljs-number">2</span>, :<span class="hljs-number">2</span>].copy_(input)
tensor([[[[ <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>],
 [ <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>]]]])
<span class="hljs-meta">&gt;&gt;&gt; </span>input_3x3
tensor([[[[ <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">0.</span>],
 [ <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">0.</span>],
 [ <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>]]]])

<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Upsample(scale_factor=<span class="hljs-number">2</span>, mode=<span class="hljs-string">&apos;bilinear&apos;</span>)  <span class="hljs-comment"># align_corners=False</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Notice that values in top left corner are the same with the small input (except at boundary)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input_3x3)
tensor([[[[ <span class="hljs-number">1.0000</span>,  <span class="hljs-number">1.2500</span>,  <span class="hljs-number">1.7500</span>,  <span class="hljs-number">1.5000</span>,  <span class="hljs-number">0.5000</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">1.5000</span>,  <span class="hljs-number">1.7500</span>,  <span class="hljs-number">2.2500</span>,  <span class="hljs-number">1.8750</span>,  <span class="hljs-number">0.6250</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">2.5000</span>,  <span class="hljs-number">2.7500</span>,  <span class="hljs-number">3.2500</span>,  <span class="hljs-number">2.6250</span>,  <span class="hljs-number">0.8750</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">2.2500</span>,  <span class="hljs-number">2.4375</span>,  <span class="hljs-number">2.8125</span>,  <span class="hljs-number">2.2500</span>,  <span class="hljs-number">0.7500</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">0.7500</span>,  <span class="hljs-number">0.8125</span>,  <span class="hljs-number">0.9375</span>,  <span class="hljs-number">0.7500</span>,  <span class="hljs-number">0.2500</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>]]]])

<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Upsample(scale_factor=<span class="hljs-number">2</span>, mode=<span class="hljs-string">&apos;bilinear&apos;</span>, align_corners=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Notice that values in top left corner are now changed</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input_3x3)
tensor([[[[ <span class="hljs-number">1.0000</span>,  <span class="hljs-number">1.4000</span>,  <span class="hljs-number">1.8000</span>,  <span class="hljs-number">1.6000</span>,  <span class="hljs-number">0.8000</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">1.8000</span>,  <span class="hljs-number">2.2000</span>,  <span class="hljs-number">2.6000</span>,  <span class="hljs-number">2.2400</span>,  <span class="hljs-number">1.1200</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">2.6000</span>,  <span class="hljs-number">3.0000</span>,  <span class="hljs-number">3.4000</span>,  <span class="hljs-number">2.8800</span>,  <span class="hljs-number">1.4400</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">2.4000</span>,  <span class="hljs-number">2.7200</span>,  <span class="hljs-number">3.0400</span>,  <span class="hljs-number">2.5600</span>,  <span class="hljs-number">1.2800</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">1.2000</span>,  <span class="hljs-number">1.3600</span>,  <span class="hljs-number">1.5200</span>,  <span class="hljs-number">1.2800</span>,  <span class="hljs-number">0.6400</span>,  <span class="hljs-number">0.0000</span>],
 [ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>]]]])
</code></pre>
<h3 id="upsamplingnearest2d">UpsamplingNearest2d</h3>
<pre><code class="lang-py">class torch.nn.UpsamplingNearest2d(size=None, scale_factor=None)
</code></pre>
<p>Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.</p>
<p>To specify the scale, it takes either the <code>size</code> or the <code>scale_factor</code> as it&#x2019;s constructor argument.</p>
<p>When <code>size</code> is given, it is the output size of the image <code>(h, w)</code>.</p>
<p>Parameters: </p>
<ul>
<li><strong>size</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; a tuple of ints <code>(H_out, W_out)</code> output sizes</li>
<li><strong>scale_factor</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; the multiplier for the image height or width</li>
</ul>
<p>Warning</p>
<p>This class is deprecated in favor of <code>interpolate()</code>.</p>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/ff71b16eb10237262566c6907acaaf1f.jpg" alt=""></li>
<li>Output: <img src="img/a0ef05f779873fc4dcbf020b1ea14754.jpg" alt=""> where</li>
</ul>
<p><img src="img/682de298a3561bebd964280ba0d59633.jpg" alt=""></p>
<p><img src="img/2a53007c25abe7f8f65f1a2e958fa146.jpg" alt=""></p>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>).view(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input
tensor([[[[ <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>],
 [ <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>]]]])

<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.UpsamplingNearest2d(scale_factor=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[[ <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">2.</span>],
 [ <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">2.</span>],
 [ <span class="hljs-number">3.</span>,  <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">4.</span>],
 [ <span class="hljs-number">3.</span>,  <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">4.</span>]]]])
</code></pre>
<h3 id="upsamplingbilinear2d">UpsamplingBilinear2d</h3>
<pre><code class="lang-py">class torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None)
</code></pre>
<p>Applies a 2D bilinear upsampling to an input signal composed of several input channels.</p>
<p>To specify the scale, it takes either the <code>size</code> or the <code>scale_factor</code> as it&#x2019;s constructor argument.</p>
<p>When <code>size</code> is given, it is the output size of the image <code>(h, w)</code>.</p>
<p>Parameters: </p>
<ul>
<li><strong>size</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)" target="_blank"><em>tuple</em></a><em>,</em> <em>optional</em>) &#x2013; a tuple of ints <code>(H_out, W_out)</code> output sizes</li>
<li><strong>scale_factor</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; the multiplier for the image height or width</li>
</ul>
<p>Warning</p>
<p>This class is deprecated in favor of <code>interpolate()</code>. It is equivalent to <code>nn.functional.interpolate(..., mode=&apos;bilinear&apos;, align_corners=True)</code>.</p>
<pre><code class="lang-py">Shape:
</code></pre>
<ul>
<li>Input: <img src="img/ff71b16eb10237262566c6907acaaf1f.jpg" alt=""></li>
<li>Output: <img src="img/a0ef05f779873fc4dcbf020b1ea14754.jpg" alt=""> where</li>
</ul>
<p><img src="img/682de298a3561bebd964280ba0d59633.jpg" alt=""></p>
<p><img src="img/2a53007c25abe7f8f65f1a2e958fa146.jpg" alt=""></p>
<p>Examples:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>input = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>).view(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input
tensor([[[[ <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>],
 [ <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>]]]])

<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.UpsamplingBilinear2d(scale_factor=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>m(input)
tensor([[[[ <span class="hljs-number">1.0000</span>,  <span class="hljs-number">1.3333</span>,  <span class="hljs-number">1.6667</span>,  <span class="hljs-number">2.0000</span>],
 [ <span class="hljs-number">1.6667</span>,  <span class="hljs-number">2.0000</span>,  <span class="hljs-number">2.3333</span>,  <span class="hljs-number">2.6667</span>],
 [ <span class="hljs-number">2.3333</span>,  <span class="hljs-number">2.6667</span>,  <span class="hljs-number">3.0000</span>,  <span class="hljs-number">3.3333</span>],
 [ <span class="hljs-number">3.0000</span>,  <span class="hljs-number">3.3333</span>,  <span class="hljs-number">3.6667</span>,  <span class="hljs-number">4.0000</span>]]]])
</code></pre>
<h2 id="dataparallel-layers-multi-gpu-distributed">DataParallel layers (multi-GPU, distributed)</h2>
<h3 id="dataparallel">DataParallel</h3>
<pre><code class="lang-py">class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)
</code></pre>
<p>Implements data parallelism at the module level.</p>
<p>This container parallelizes the application of the given <code>module</code> by splitting the input across the specified devices by chunking in the batch dimension (other objects will be copied once per device). In the forward pass, the module is replicated on each device, and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module.</p>
<p>The batch size should be larger than the number of GPUs used.</p>
<p>See also: <a href="notes/cuda.html#cuda-nn-dataparallel-instead">Use nn.DataParallel instead of multiprocessing</a></p>
<p>Arbitrary positional and keyword inputs are allowed to be passed into DataParallel EXCEPT Tensors. All tensors will be scattered on dim specified (default 0). Primitive types will be broadcasted, but all other types will be a shallow copy and can be corrupted if written to in the model&#x2019;s forward pass.</p>
<p>The parallelized <code>module</code> must have its parameters and buffers on <code>device_ids[0]</code> before running this <a href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code>DataParallel</code></a> module.</p>
<p>Warning</p>
<p>In each forward, <code>module</code> is <strong>replicated</strong> on each device, so any updates to the runing module in <code>forward</code> will be lost. For example, if <code>module</code> has a counter attribute that is incremented in each <code>forward</code>, it will always stay at the initial value becasue the update is done on the replicas which are destroyed after <code>forward</code>. However, <a href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code>DataParallel</code></a> guarantees that the replica on <code>device[0]</code> will have its parameters and buffers sharing storage with the base parallelized <code>module</code>. So <strong>in-place</strong> updates to the parameters or buffers on <code>device[0]</code> will be recorded. E.g., <a href="#torch.nn.BatchNorm2d" title="torch.nn.BatchNorm2d"><code>BatchNorm2d</code></a> and <a href="#torch.nn.utils.spectral_norm" title="torch.nn.utils.spectral_norm"><code>spectral_norm()</code></a> rely on this behavior to update the buffers.</p>
<p>Warning</p>
<p>Forward and backward hooks defined on <code>module</code> and its submodules will be invoked <code>len(device_ids)</code> times, each with inputs located on a particular device. Particularly, the hooks are only guaranteed to be executed in correct order with respect to operations on corresponding devices. For example, it is not guaranteed that hooks set via <a href="#torch.nn.Module.register_forward_pre_hook" title="torch.nn.Module.register_forward_pre_hook"><code>register_forward_pre_hook()</code></a> be executed before <code>all</code> <code>len(device_ids)</code> <a href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code>forward()</code></a> calls, but that each such hook be executed before the corresponding <a href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code>forward()</code></a> call of that device.</p>
<p>Warning</p>
<p>When <code>module</code> returns a scalar (i.e., 0-dimensional tensor) in <code>forward()</code>, this wrapper will return a vector of length equal to number of devices used in data parallelism, containing the result from each device.</p>
<p>Note</p>
<p>There is a subtlety in using the <code>pack sequence -&amp;gt; recurrent network -&amp;gt; unpack sequence</code> pattern in a <a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> wrapped in <a href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code>DataParallel</code></a>. See <a href="notes/faq.html#pack-rnn-unpack-with-data-parallelism">My recurrent network doesn&#x2019;t work with data parallelism</a> section in FAQ for details.</p>
<p>Parameters: </p>
<ul>
<li><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) &#x2013; module to be parallelized</li>
<li><strong>device_ids</strong> (<em>list of python:int</em> <em>or</em> <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) &#x2013; CUDA devices (default: all devices)</li>
<li><strong>output_device</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) &#x2013; device location of output (default: device_ids[0])</li>
</ul>
<table>
<thead>
<tr>
<th>Variables:</th>
<th><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) &#x2013; the module to be parallelized</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>net = torch.nn.DataParallel(model, device_ids=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>output = net(input_var)
</code></pre>
<h3 id="distributeddataparallel">DistributedDataParallel</h3>
<pre><code class="lang-py">class torch.nn.parallel.DistributedDataParallel(module, device_ids=None, output_device=None, dim=0, broadcast_buffers=True, process_group=None, bucket_cap_mb=25, check_reduction=False)
</code></pre>
<p>Implements distributed data parallelism that is based on torch.distributed package at the module level.</p>
<p>This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine and each device, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged.</p>
<p>The batch size should be larger than the number of GPUs used locally. It should also be an integer multiple of the number of GPUs so that each chunk is the same size (so that each GPU processes the same number of samples).</p>
<p>See also: <a href="distributed.html#distributed-basics">Basics</a> and <a href="notes/cuda.html#cuda-nn-dataparallel-instead">Use nn.DataParallel instead of multiprocessing</a>. The same constraints on input as in <a href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code>torch.nn.DataParallel</code></a> apply.</p>
<p>Creation of this class requires that <code>torch.distributed</code> to be already initialized, by calling <a href="distributed.html#torch.distributed.init_process_group" title="torch.distributed.init_process_group"><code>torch.distributed.init_process_group()</code></a></p>
<p><code>DistributedDataParallel</code> can be used in the following two ways:</p>
<ol>
<li>Single-Process Multi-GPU</li>
</ol>
<p>In this case, a single process will be spawned on each host/node and each process will operate on all the GPUs of the node where it&#x2019;s running. To use <code>DistributedDataParallel</code> in this way, you can simply construct the model as the following:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.distributed.init_process_group(backend=<span class="hljs-string">&quot;nccl&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistributedDataParallel(model) <span class="hljs-comment"># device_ids will include all GPU devices be default</span>
</code></pre>
<ol>
<li>Multi-Process Single-GPU</li>
</ol>
<p>This is the highly recommended way to use <code>DistributedDataParallel</code>, with multiple processes, each of which operates on a single GPU. This is currently the fastest approach to do data parallel training using PyTorch and applies to both single-node(multi-GPU) and multi-node data parallel training. It is proven to be significantly faster than <a href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code>torch.nn.DataParallel</code></a> for single-node multi-GPU data parallel training.</p>
<p>Here is how to use it: on each host with N GPUs, you should spawn up N processes, while ensuring that each process invidually works on a single GPU from 0 to N-1. Therefore, it is your job to ensure that your training script operates on a single given GPU by calling:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.cuda.set_device(i)
</code></pre>
<p>where i is from 0 to N-1. In each process, you should refer the following to construct this module:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.distributed.init_process_group(backend=<span class="hljs-string">&apos;nccl&apos;</span>, world_size=<span class="hljs-number">4</span>, init_method=<span class="hljs-string">&apos;...&apos;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistributedDataParallel(model, device_ids=[i], output_device=i)
</code></pre>
<p>In order to spawn up multiple processes per node, you can use either <code>torch.distributed.launch</code> or <code>torch.multiprocessing.spawn</code></p>
<p>Note</p>
<p><code>nccl</code> backend is currently the fastest and highly recommended backend to be used with Multi-Process Single-GPU distributed training and this applies to both single-node and multi-node distributed training</p>
<p>Warning</p>
<p>This module works only with the <code>gloo</code> and <code>nccl</code> backends.</p>
<p>Warning</p>
<p>Constructor, forward method, and differentiation of the output (or a function of the output of this module) is a distributed synchronization point. Take that into account in case different processes might be executing different code.</p>
<p>Warning</p>
<p>This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Same applies to buffers.</p>
<p>Warning</p>
<p>This module assumes all parameters are registered in the model of each distributed processes are in the same order. The module itself will conduct gradient all-reduction following the reverse order of the registered parameters of the model. In other wise, it is users&#x2019; responsibility to ensure that each distributed process has the exact same model and thus the exact parameter registeration order.</p>
<p>Warning</p>
<p>This module assumes all buffers and gradients are dense.</p>
<p>Warning</p>
<p>This module doesn&#x2019;t work with <a href="autograd.html#torch.autograd.grad" title="torch.autograd.grad"><code>torch.autograd.grad()</code></a> (i.e. it will only work if gradients are to be accumulated in <code>.grad</code> attributes of parameters).</p>
<p>Warning</p>
<p>If you plan on using this module with a <code>nccl</code> backend or a <code>gloo</code> backend (that uses Infiniband), together with a DataLoader that uses multiple workers, please change the multiprocessing start method to <code>forkserver</code> (Python 3 only) or <code>spawn</code>. Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will likely experience deadlocks if you don&#x2019;t change this setting.</p>
<p>Warning</p>
<p>Forward and backward hooks defined on <code>module</code> and its submodules won&#x2019;t be invoked anymore, unless the hooks are initialized in the <code>forward()</code> method.</p>
<p>Warning</p>
<p>You should never try to change your model&#x2019;s parameters after wrapping up your model with DistributedDataParallel. In other words, when wrapping up your model with DistributedDataParallel, the constructor of DistributedDataParallel will register the additional gradient reduction functions on all the parameters of the model itself at the time of construction. If you change the model&#x2019;s parameters after the DistributedDataParallel construction, this is not supported and unexpected behaviors can happen, since some parameters&#x2019; gradient reduction functions might not get called.</p>
<p>Note</p>
<p>Parameters are never broadcast between processes. The module performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all processes in the same way. Buffers (e.g. BatchNorm stats) are broadcast from the module in process of rank 0, to all other replicas in the system in every iteration.</p>
<p>Parameters: </p>
<ul>
<li><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) &#x2013; module to be parallelized</li>
<li><strong>device_ids</strong> (<em>list of python:int</em> <em>or</em> <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) &#x2013; CUDA devices (default: all devices)</li>
<li><strong>output_device</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a> <em>or</em> <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) &#x2013; device location of output (default: device_ids[0])</li>
<li><strong>broadcast_buffers</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a>) &#x2013; flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function. (default: True)</li>
<li><strong>process_group</strong> &#x2013; the process group to be used for distributed data all-reduction. If None, the default process group, which is created by <code>torch.distributed.init_process_group</code>, will be used. (default: None)</li>
<li><strong>bucket_cap_mb</strong> &#x2013; DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB) (default: 25)</li>
<li><strong>check_reduction</strong> &#x2013; when setting to True, it enables DistributedDataParallel to automatically check if the previous iteration&#x2019;s backward reductions were successfully issued at the beginning of every iteration&#x2019;s forward function. You normally don&#x2019;t need this option enabled unless you are observing weird behaviors such as different ranks are getting different gradients, which should not happen if DistributedDataParallel is corrected used. (default: False)</li>
</ul>
<table>
<thead>
<tr>
<th>Variables:</th>
<th><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) &#x2013; the module to be parallelized</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="lang-py">Example::
</code></pre>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.distributed.init_process_group(backend=<span class="hljs-string">&apos;nccl&apos;</span>, world_size=<span class="hljs-number">4</span>, init_method=<span class="hljs-string">&apos;...&apos;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>net = torch.nn.DistributedDataParallel(model, pg)
</code></pre>
<h3 id="distributeddataparallelcpu">DistributedDataParallelCPU</h3>
<pre><code class="lang-py">class torch.nn.parallel.DistributedDataParallelCPU(module)
</code></pre>
<p>Implements distributed data parallelism for CPU at the module level.</p>
<p>This module supports the <code>mpi</code> and <code>gloo</code> backends.</p>
<p>This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged.</p>
<p>This module could be used in conjunction with the DistributedSampler, (see :class <code>torch.utils.data.distributed.DistributedSampler</code>) which will load a subset of the original datset for each node with the same batch size. So strong scaling should be configured like this:</p>
<p>n = 1, batch size = 12</p>
<p>n = 2, batch size = 64</p>
<p>n = 4, batch size = 32</p>
<p>n = 8, batch size = 16</p>
<p>Creation of this class requires the distributed package to be already initialized in the process group mode (see <a href="distributed.html#torch.distributed.init_process_group" title="torch.distributed.init_process_group"><code>torch.distributed.init_process_group()</code></a>).</p>
<p>Warning</p>
<p>Constructor, forward method, and differentiation of the output (or a function of the output of this module) is a distributed synchronization point. Take that into account in case different node might be executing different code.</p>
<p>Warning</p>
<p>This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later.</p>
<p>Warning</p>
<p>This module assumes all gradients are dense.</p>
<p>Warning</p>
<p>This module doesn&#x2019;t work with <a href="autograd.html#torch.autograd.grad" title="torch.autograd.grad"><code>torch.autograd.grad()</code></a> (i.e. it will only work if gradients are to be accumulated in <code>.grad</code> attributes of parameters).</p>
<p>Warning</p>
<p>Forward and backward hooks defined on <code>module</code> and its submodules won&#x2019;t be invoked anymore, unless the hooks are initialized in the <code>forward()</code> method.</p>
<p>Note</p>
<p>Parameters are broadcast between nodes in the <strong>init</strong>() function. The module performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all nodes in the same way.</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>module</strong> &#x2013; module to be parallelized</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.distributed.init_process_group(world_size=<span class="hljs-number">4</span>, init_method=<span class="hljs-string">&apos;...&apos;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>net = torch.nn.DistributedDataParallelCPU(model)
</code></pre>
<h2 id="utilities">Utilities</h2>
<h3 id="clipgradnorm">clip<em>grad_norm</em></h3>
<pre><code class="lang-py">torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=<span class="hljs-number">2</span>)
</code></pre>
<p>Clips gradient norm of an iterable of parameters.</p>
<p>The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.</p>
<p>Parameters: </p>
<ul>
<li><strong>parameters</strong> (<em>Iterable__[</em><a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>] or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; an iterable of Tensors or a single Tensor that will have gradients normalized</li>
<li><strong>max_norm</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a> <em>or</em> <a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; max norm of the gradients</li>
<li><strong>norm_type</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a> <em>or</em> <a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; type of the used p-norm. Can be <code>&apos;inf&apos;</code> for infinity norm.</li>
</ul>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>Total norm of the parameters (viewed as a single vector).</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<h3 id="clipgradvalue">clip<em>grad_value</em></h3>
<pre><code class="lang-py">torch.nn.utils.clip_grad_value_(parameters, clip_value)
</code></pre>
<p>Clips gradient of an iterable of parameters at specified value.</p>
<p>Gradients are modified in-place.</p>
<p>Parameters: </p>
<ul>
<li><strong>parameters</strong> (<em>Iterable__[</em><a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>] or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; an iterable of Tensors or a single Tensor that will have gradients normalized</li>
<li><strong>clip_value</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a> <em>or</em> <a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a>) &#x2013; maximum allowed value of the gradients The gradients are clipped in the range [-clip_value, clip_value]</li>
</ul>
<h3 id="parameterstovector">parameters_to_vector</h3>
<pre><code class="lang-py">torch.nn.utils.parameters_to_vector(parameters)
</code></pre>
<p>Convert parameters to one vector</p>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>parameters</strong> (<em>Iterable__[</em><a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) &#x2013; an iterator of Tensors that are the parameters of a model.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Returns:</td>
<td>The parameters represented by a single vector</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<h3 id="vectortoparameters">vector_to_parameters</h3>
<pre><code class="lang-py">torch.nn.utils.vector_to_parameters(vec, parameters)
</code></pre>
<p>Convert one vector to the parameters</p>
<p>Parameters: </p>
<ul>
<li><strong>vec</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; a single vector represents the parameters of a model.</li>
<li><strong>parameters</strong> (<em>Iterable__[</em><a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) &#x2013; an iterator of Tensors that are the parameters of a model.</li>
</ul>
<h3 id="weightnorm">weight_norm</h3>
<pre><code class="lang-py">torch.nn.utils.weight_norm(module, name=<span class="hljs-string">&apos;weight&apos;</span>, dim=<span class="hljs-number">0</span>)
</code></pre>
<p>Applies weight normalization to a parameter in the given module.</p>
<p><img src="img/06160be4a838f9d6d20cabc64f32670e.jpg" alt=""></p>
<p>Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This replaces the parameter specified by <code>name</code> (e.g. &#x201C;weight&#x201D;) with two parameters: one specifying the magnitude (e.g. &#x201C;weight_g&#x201D;) and one specifying the direction (e.g. &#x201C;weight_v&#x201D;). Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every <code>forward()</code> call.</p>
<p>By default, with <code>dim=0</code>, the norm is computed independently per output channel/plane. To compute a norm over the entire weight tensor, use <code>dim=None</code>.</p>
<p>See <a href="https://arxiv.org/abs/1602.07868" target="_blank">https://arxiv.org/abs/1602.07868</a></p>
<p>Parameters: </p>
<ul>
<li><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) &#x2013; containing module</li>
<li><strong>name</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)" target="_blank"><em>str</em></a><em>,</em> <em>optional</em>) &#x2013; name of weight parameter</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; dimension over which to compute the norm</li>
</ul>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>The original module with the weight norm hook</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre><code class="lang-py">&gt;&gt;&gt; m = weight_norm(nn.Linear(20, 40), name=&apos;weight&apos;)
Linear (20 -&gt; 40)
&gt;&gt;&gt; m.weight_g.size()
torch.Size([40, 1])
&gt;&gt;&gt; m.weight_v.size()
torch.Size([40, 20])
</code></pre>
<h3 id="removeweightnorm">remove_weight_norm</h3>
<pre><code class="lang-py">torch.nn.utils.remove_weight_norm(module, name=<span class="hljs-string">&apos;weight&apos;</span>)
</code></pre>
<p>Removes the weight normalization reparameterization from a module.</p>
<p>Parameters: </p>
<ul>
<li><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) &#x2013; containing module</li>
<li><strong>name</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)" target="_blank"><em>str</em></a><em>,</em> <em>optional</em>) &#x2013; name of weight parameter</li>
</ul>
<p>Example</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = weight_norm(nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">40</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>remove_weight_norm(m)
</code></pre>
<h3 id="spectralnorm">spectral_norm</h3>
<pre><code class="lang-py">torch.nn.utils.spectral_norm(module, name=<span class="hljs-string">&apos;weight&apos;</span>, n_power_iterations=<span class="hljs-number">1</span>, eps=<span class="hljs-number">1e-12</span>, dim=<span class="hljs-keyword">None</span>)
</code></pre>
<p>Applies spectral normalization to a parameter in the given module.</p>
<p><img src="img/1ca46cc2506aac38bf00645f64b1a3e3.jpg" alt=""></p>
<p>Spectral normalization stabilizes the training of discriminators (critics) in Generaive Adversarial Networks (GANs) by rescaling the weight tensor with spectral norm <img src="img/2469b2bd2a1ab19ebfcee223dcb52bb1.jpg" alt=""> of the weight matrix calculated using power iteration method. If the dimension of the weight tensor is greater than 2, it is reshaped to 2D in power iteration method to get spectral norm. This is implemented via a hook that calculates spectral norm and rescales weight before every <code>forward()</code> call.</p>
<p>See <a href="https://arxiv.org/abs/1802.05957" target="_blank">Spectral Normalization for Generative Adversarial Networks</a> .</p>
<p>Parameters: </p>
<ul>
<li><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) &#x2013; containing module</li>
<li><strong>name</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)" target="_blank"><em>str</em></a><em>,</em> <em>optional</em>) &#x2013; name of weight parameter</li>
<li><strong>n_power_iterations</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; number of power iterations to calculate spectal norm</li>
<li><strong>eps</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a><em>,</em> <em>optional</em>) &#x2013; epsilon for numerical stability in calculating norms</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; dimension corresponding to number of outputs, the default is 0, except for modules that are instances of ConvTranspose1/2/3d, when it is 1</li>
</ul>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>The original module with the spectal norm hook</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<pre><code class="lang-py">&gt;&gt;&gt; m = spectral_norm(nn.Linear(20, 40))
Linear (20 -&gt; 40)
&gt;&gt;&gt; m.weight_u.size()
torch.Size([20])
</code></pre>
<h3 id="removespectralnorm">remove_spectral_norm</h3>
<pre><code class="lang-py">torch.nn.utils.remove_spectral_norm(module, name=<span class="hljs-string">&apos;weight&apos;</span>)
</code></pre>
<p>Removes the spectral normalization reparameterization from a module.</p>
<p>Parameters: </p>
<ul>
<li><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) &#x2013; containing module</li>
<li><strong>name</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)" target="_blank"><em>str</em></a><em>,</em> <em>optional</em>) &#x2013; name of weight parameter</li>
</ul>
<p>Example</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>m = spectral_norm(nn.Linear(<span class="hljs-number">40</span>, <span class="hljs-number">10</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>remove_spectral_norm(m)
</code></pre>
<h3 id="packedsequence">PackedSequence</h3>
<pre><code class="lang-py">torch.nn.utils.rnn.PackedSequence(data, batch_sizes=<span class="hljs-keyword">None</span>)
</code></pre>
<p>Holds the data and list of <code>batch_sizes</code> of a packed sequence.</p>
<p>All RNN modules accept packed sequences as inputs.</p>
<p>Note</p>
<p>Instances of this class should never be created manually. They are meant to be instantiated by functions like <a href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code>pack_padded_sequence()</code></a>.</p>
<p>Batch sizes represent the number elements at each sequence step in the batch, not the varying sequence lengths passed to <a href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code>pack_padded_sequence()</code></a>. For instance, given data <code>abc</code> and <code>x</code> the <a href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code>PackedSequence</code></a> would contain data <code>axbc</code> with <code>batch_sizes=[2,1,1]</code>.</p>
<p>| Variables: | </p>
<ul>
<li><strong>data</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; Tensor containing packed sequence</li>
<li><strong>batch_sizes</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; Tensor of integers holding information about the batch size at each sequence step</li>
</ul>
<h3 id="packpaddedsequence">pack_padded_sequence</h3>
<pre><code class="lang-py">torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=<span class="hljs-keyword">False</span>)
</code></pre>
<p>Packs a Tensor containing padded sequences of variable length.</p>
<p>Input can be of size <code>T x B x *</code> where <code>T</code> is the length of the longest sequence (equal to <code>lengths[0]</code>), <code>B</code> is the batch size, and <code>*</code> is any number of dimensions (including 0). If <code>batch_first</code> is True <code>B x T x *</code> inputs are expected.</p>
<p>The sequences should be sorted by length in a decreasing order, i.e. <code>input[:,0]</code> should be the longest sequence, and <code>input[:,B-1]</code> the shortest one.</p>
<p>Note</p>
<p>This function accepts any input that has at least two dimensions. You can apply it to pack the labels, and use the output of the RNN with them to compute the loss directly. A Tensor can be retrieved from a <a href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code>PackedSequence</code></a> object by accessing its <code>.data</code> attribute.</p>
<p>Parameters: </p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; padded batch of variable length sequences.</li>
<li><strong>lengths</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; list of sequences lengths of each batch element.</li>
<li><strong>batch_first</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; if <code>True</code>, the input is expected in <code>B x T x *</code> format.</li>
</ul>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>a <a href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code>PackedSequence</code></a> object</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<h3 id="padpackedsequence">pad_packed_sequence</h3>
<pre><code class="lang-py">torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=<span class="hljs-keyword">False</span>, padding_value=<span class="hljs-number">0.0</span>, total_length=<span class="hljs-keyword">None</span>)
</code></pre>
<p>Pads a packed batch of variable length sequences.</p>
<p>It is an inverse operation to <a href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code>pack_padded_sequence()</code></a>.</p>
<p>The returned Tensor&#x2019;s data will be of size <code>T x B x *</code>, where <code>T</code> is the length of the longest sequence and <code>B</code> is the batch size. If <code>batch_first</code> is True, the data will be transposed into <code>B x T x *</code> format.</p>
<p>Batch elements will be ordered decreasingly by their length.</p>
<p>Note</p>
<p><code>total_length</code> is useful to implement the <code>pack sequence -&amp;gt; recurrent network -&amp;gt; unpack sequence</code> pattern in a <a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> wrapped in <a href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code>DataParallel</code></a>. See <a href="notes/faq.html#pack-rnn-unpack-with-data-parallelism">this FAQ section</a> for details.</p>
<p>Parameters: </p>
<ul>
<li><strong>sequence</strong> (<em>PackedSequence</em>) &#x2013; batch to pad</li>
<li><strong>batch_first</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; if <code>True</code>, the output will be in <code>B x T x *</code> format.</li>
<li><strong>padding_value</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a><em>,</em> <em>optional</em>) &#x2013; values for padded elements.</li>
<li><strong>total_length</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank"><em>int</em></a><em>,</em> <em>optional</em>) &#x2013; if not <code>None</code>, the output will be padded to have length <code>total_length</code>. This method will throw <a href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.7)" target="_blank"><code>ValueError</code></a> if <code>total_length</code> is less than the max sequence length in <code>sequence</code>.</li>
</ul>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>Tuple of Tensor containing the padded sequence, and a Tensor containing the list of lengths of each sequence in the batch.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<h3 id="padsequence">pad_sequence</h3>
<pre><code class="lang-py">torch.nn.utils.rnn.pad_sequence(sequences, batch_first=<span class="hljs-keyword">False</span>, padding_value=<span class="hljs-number">0</span>)
</code></pre>
<p>Pad a list of variable length Tensors with zero</p>
<p><code>pad_sequence</code> stacks a list of Tensors along a new dimension, and pads them to equal length. For example, if the input is list of sequences with size <code>L x *</code> and if batch_first is False, and <code>T x B x *</code> otherwise.</p>
<p><code>B</code> is batch size. It is equal to the number of elements in <code>sequences</code>. <code>T</code> is length of the longest sequence. <code>L</code> is length of the sequence. <code>*</code> is any number of trailing dimensions, including none.</p>
<p>Example</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.nn.utils.rnn <span class="hljs-keyword">import</span> pad_sequence
<span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.ones(<span class="hljs-number">25</span>, <span class="hljs-number">300</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>b = torch.ones(<span class="hljs-number">22</span>, <span class="hljs-number">300</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>c = torch.ones(<span class="hljs-number">15</span>, <span class="hljs-number">300</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pad_sequence([a, b, c]).size()
torch.Size([<span class="hljs-number">25</span>, <span class="hljs-number">3</span>, <span class="hljs-number">300</span>])
</code></pre>
<p>Note</p>
<p>This function returns a Tensor of size <code>T x B x *</code> or <code>B x T x *</code> where <code>T</code> is the length of the longest sequence. This function assumes trailing dimensions and type of all the Tensors in sequences are same.</p>
<p>Parameters: </p>
<ul>
<li><strong>sequences</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)" target="_blank"><em>list</em></a><em>[</em><a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) &#x2013; list of variable length sequences.</li>
<li><strong>batch_first</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank"><em>bool</em></a><em>,</em> <em>optional</em>) &#x2013; output will be in <code>B x T x *</code> if True, or in <code>T x B x *</code> otherwise</li>
<li><strong>padding_value</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)" target="_blank"><em>float</em></a><em>,</em> <em>optional</em>) &#x2013; value for padded elements. Default: 0.</li>
</ul>
<table>
<thead>
<tr>
<th>Returns:</th>
<th>Tensor of size <code>T x B x *</code> if <code>batch_first</code> is <code>False</code>. Tensor of size <code>B x T x *</code> otherwise</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<h3 id="packsequence">pack_sequence</h3>
<pre><code class="lang-py">torch.nn.utils.rnn.pack_sequence(sequences)
</code></pre>
<p>Packs a list of variable length Tensors</p>
<p><code>sequences</code> should be a list of Tensors of size <code>L x *</code>, where <code>L</code> is the length of a sequence and <code>*</code> is any number of trailing dimensions, including zero. They should be sorted in the order of decreasing length.</p>
<p>Example</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.nn.utils.rnn <span class="hljs-keyword">import</span> pack_sequence
<span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>b = torch.tensor([<span class="hljs-number">4</span>,<span class="hljs-number">5</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>c = torch.tensor([<span class="hljs-number">6</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>pack_sequence([a, b, c])
PackedSequence(data=tensor([ <span class="hljs-number">1</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">3</span>]), batch_sizes=tensor([ <span class="hljs-number">3</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">1</span>]))
</code></pre>
<table>
<thead>
<tr>
<th>Parameters:</th>
<th><strong>sequences</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)" target="_blank"><em>list</em></a><em>[</em><a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) &#x2013; A list of sequences of decreasing length.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Returns:</td>
<td>a <a href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code>PackedSequence</code></a> object</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<p><hr></p>
<div align="center">
    <p><a href="http://www.apachecn.org" target="_blank"><font face="KaiTi" size="6" color="red">&#x6211;&#x4EEC;&#x4E00;&#x76F4;&#x5728;&#x52AA;&#x529B;</font></a></p>
    <p><a href="https://github.com/apachecn/pytorch-doc-zh/" target="_blank">apachecn/pytorch-doc-zh</a></p>
    <p><iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=watch&amp;count=true&amp;v=2" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=star&amp;count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=fork&amp;count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <a target="_blank" href="shang.qq.com/wpa/qunwpa"><img border="0" src="http://data.apachecn.org/img/logo/ApacheCN-group.png" alt="ML | ApacheCN" title="ML | ApacheCN"></a></p>
</div>
 <div style="text-align:center;margin:0 0 10.5px;">
     <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
     <ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-3565452474788507" data-ad-slot="2543897000">
     </ins>
     <script>(adsbygoogle = window.adsbygoogle || []).push({});</script>

    <script>
      var _hmt = _hmt || [];
      (function() {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91";
        var s = document.getElementsByTagName("script")[0]; 
        s.parentNode.insertBefore(hm, s);
      })();
    </script>

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-102475051-10"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-102475051-10');
    </script>
</div>

<p><meta name="google-site-verification" content="pyo9N70ZWyh8JB43bIu633mhxesJ1IcwWCZlM3jUfFo"></p>
<p><iframe src="https://www.bilibili.com/read/cv2710377" style="display:none"></iframe>
<img src="http://t.cn/AiCoDHwb" hidden="hidden"></p>
<div>
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
    <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script>
    <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
    <div id="gitalk-container"></div>
    <script type="text/javascript">
        const gitalk = new Gitalk({
        clientID: '2e62dee5b9896e2eede6',
        clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53',
        repo: 'pytorch-doc-zh',
        owner: 'apachecn',
        admin: ['jiangzhonglian', 'wizardforcel'],
        id: md5(location.pathname),
        distractionFreeMode: false
        })
        gitalk.render('gitalk-container')
    </script>
</div>

<footer class="page-footer"><span class="copyright">Copyright &#xA9; ibooker.org.cn 2019 all right reserved&#xFF0C;&#x7531; ApacheCN &#x56E2;&#x961F;&#x63D0;&#x4F9B;&#x652F;&#x6301;</span><span class="footer-modification">&#x8BE5;&#x6587;&#x4EF6;&#x4FEE;&#x8BA2;&#x65F6;&#x95F4;&#xFF1A; 
2019-12-27 08:05:23
</span></footer>
<script>console.log("plugin-popup....");document.onclick = function(e){ e.target.tagName === "IMG" && window.open(e.target.src,e.target.src)}</script><style>img{cursor:pointer}</style>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="storage.html" class="navigation navigation-prev " aria-label="Previous page: torch.Storage">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="nn_functional.html" class="navigation navigation-next " aria-label="Next page: torch.nn.functional">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"torch.nn","level":"1.3.2.8","depth":3,"next":{"title":"torch.nn.functional","level":"1.3.2.9","depth":3,"path":"nn_functional.md","ref":"nn_functional.md","articles":[]},"previous":{"title":"torch.Storage","level":"1.3.2.7","depth":3,"path":"storage.md","ref":"storage.md","articles":[]},"dir":"ltr"},"config":{"plugins":["github","github-buttons","-sharing","insert-logo","sharing-plus","back-to-top-button","code","copy-code-button","mathjax","pageview-count","edit-link","emphasize","alerts","auto-scroll-table","popup","hide-element","page-toc-button","tbfed-pagefooter","sitemap","advanced-emoji","expandable-chapters","splitter","search-pro"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright &copy ibooker.org.cn 2019","modify_label":"该文件修订时间： ","modify_format":"YYYY-MM-DD HH:mm:ss"},"emphasize":{},"github":{"url":"https://github.com/apachecn/pytorch-doc-zh"},"splitter":{},"search-pro":{},"search":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"auto-scroll-table":{},"popup":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"code":{"copyButtons":true},"hide-element":{"elements":[".gitbook-link"]},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"sitemap":{"hostname":"http://pytorch.apachecn.org"},"page-toc-button":{"maxTocDepth":4,"minTocSize":4},"back-to-top-button":{},"pageview-count":{},"alerts":{},"github-buttons":{"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"copy-code-button":{},"advanced-emoji":{"embedEmojis":false},"sharing":{"qq":false,"all":["qq","douban","facebook","google","linkedin","twitter","weibo","whatsapp"],"douban":false,"facebook":false,"weibo":true,"whatsapp":false,"twitter":false,"line":false,"google":false,"qzone":true},"edit-link":{"label":"编辑本页","base":"https://github.com/apachecn/pytorch-doc-zh/blob/master/docs/1.0"},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"insert-logo":{"style":"background: none; max-height: 150px; min-height: 150px","url":"http://data.apachecn.org/img/logo.jpg"},"expandable-chapters":{}},"my_links":{"sidebar":{"Home":"https://www.baidu.com"}},"theme":"default","author":"ApacheCN","my_plugins":["donate","todo","-lunr","-search","expandable-chapters-small","chapter-fold","expandable-chapters","expandable-chapters-small","back-to-top-button","ga","baidu","sitemap","tbfed-pagefooter","advanced-emoji","sectionx","page-treeview","simple-page-toc","ancre-navigation","theme-apachecn@git+https://github.com/apachecn/theme-apachecn#HEAD","pagefooter-apachecn@git+https://github.com/apachecn/gitbook-plugin-pagefooter-apachecn#HEAD"],"my_pluginsConfig":{"page-treeview":{"copyright":"Copyright &#169; aleen42","minHeaderCount":"2","minHeaderDeep":"2"},"ignores":["node_modules"],"simple-page-toc":{"maxDepth":3,"skipFirstH1":true},"page-copyright":{"wisdom":"Designer, Frontend Developer & overall web enthusiast","noPowered":false,"copyright":"Copyright &#169; 你的名字","style":"normal","timeColor":"#666","utcOffset":"8","format":"YYYY-MM-dd hh:mm:ss","signature":"你的签名","copyrightColor":"#666","description":"modified at"},"donate":{"wechat":"微信收款的二维码URL","alipay":"支付宝收款的二维码URL","title":"","button":"赏","alipayText":"支付宝打赏","wechatText":"微信打赏"},"page-toc-button":{"maxTocDepth":2,"minTocSize":2},"github-buttons":{"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"},{"user":"apachecn","width":"160","type":"follow","count":true,"size":"small"}]},"ga":{"token":"UA-102475051-10"},"baidu":{"token":"75439e2cbd22bdd813226000e9dcc12f"},"pagefooter-apachecn":{"copyright":"Copyright &copy ibooker.org.cn 2019","modify_label":"该文件修订时间： ","modify_format":"YYYY-MM-DD HH:mm:ss"}},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"Pytorch 中文文档","language":"zh-hans","gitbook":"*","description":"Pytorch 中文文档: 教程和文档"},"file":{"path":"nn.md","mtime":"2019-12-27T08:05:23.759Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2019-12-27T08:07:43.252Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-insert-logo/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-code/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-copy-code-button/toggle.js"></script>
        
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-edit-link/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-alerts/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-auto-scroll-table/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-hide-element/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-page-toc-button/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search-pro/jquery.mark.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search-pro/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

