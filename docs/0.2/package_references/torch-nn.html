
<!DOCTYPE HTML>
<html lang="zh-hans" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>torch.nn · Pytorch 中文文档</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        <meta name="author" content="ApacheCN">
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-insert-logo/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-code/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-pageview-count/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-emphasize/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-alerts/style.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-auto-scroll-table/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-page-toc-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-tbfed-pagefooter/footer.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-advanced-emoji/emoji-website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search-pro/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="functional.html" />
    
    
    <link rel="prev" href="Storage.html" />
    

    <style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
    <script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"}]};
    </script>

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="输入并搜索" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    PyTorch 0.2 中文文档
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" >
            
                <span>
            
                    
                    说明
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="../notes/autograd.html">
            
                <a href="../notes/autograd.html">
            
                    
                    自动求导机制
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="../notes/cuda.html">
            
                <a href="../notes/cuda.html">
            
                    
                    CUDA语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="../notes/extending.html">
            
                <a href="../notes/extending.html">
            
                    
                    扩展PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="../notes/multiprocessing.html">
            
                <a href="../notes/multiprocessing.html">
            
                    
                    多进程最佳实践
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="../notes/serialization.html">
            
                <a href="../notes/serialization.html">
            
                    
                    序列化语义
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" >
            
                <span>
            
                    
                    PACKAGE参考
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="torch.html">
            
                <a href="torch.html">
            
                    
                    torch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="Tensor.html">
            
                <a href="Tensor.html">
            
                    
                    torch.Tensor
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="Storage.html">
            
                <a href="Storage.html">
            
                    
                    torch.Storage
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.3.4" data-path="torch-nn.html">
            
                <a href="torch-nn.html">
            
                    
                    torch.nn
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.5" data-path="functional.html">
            
                <a href="functional.html">
            
                    
                    torch.nn.functional
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.6" data-path="torch-autograd.html">
            
                <a href="torch-autograd.html">
            
                    
                    torch.autograd
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.7" data-path="torch-optim.html">
            
                <a href="torch-optim.html">
            
                    
                    torch.optim
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.8" data-path="nn_init.html">
            
                <a href="nn_init.html">
            
                    
                    torch.nn.init
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.9" data-path="torch-multiprocessing.html">
            
                <a href="torch-multiprocessing.html">
            
                    
                    torch.multiprocessing
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.10" data-path="legacy.html">
            
                <a href="legacy.html">
            
                    
                    torch.legacy
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.11" data-path="torch-cuda.html">
            
                <a href="torch-cuda.html">
            
                    
                    torch.cuda
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.12" data-path="ffi.html">
            
                <a href="ffi.html">
            
                    
                    torch.utils.ffi
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.13" data-path="data.html">
            
                <a href="data.html">
            
                    
                    torch.utils.data
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.14" data-path="model_zoo.html">
            
                <a href="model_zoo.html">
            
                    
                    torch.utils.model_zoo
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4" >
            
                <span>
            
                    
                    TORCHVISION参考
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" data-path="../torchvision/torchvision.html">
            
                <a href="../torchvision/torchvision.html">
            
                    
                    torchvision
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2" data-path="../torchvision/torchvision-datasets.html">
            
                <a href="../torchvision/torchvision-datasets.html">
            
                    
                    torchvision.datasets
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.3" data-path="../torchvision/torchvision-models.html">
            
                <a href="../torchvision/torchvision-models.html">
            
                    
                    torchvision.models
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4" data-path="../torchvision/torchvision-transform.html">
            
                <a href="../torchvision/torchvision-transform.html">
            
                    
                    torchvision.transforms
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.5" data-path="../torchvision/torchvision-utils.html">
            
                <a href="../torchvision/torchvision-utils.html">
            
                    
                    torchvision.utils
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../acknowledgement.html">
            
                <a href="../acknowledgement.html">
            
                    
                    致谢
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本书使用 GitBook 发布
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >torch.nn</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="torchnn">torch.nn</h1>
<h2 id="parameters">Parameters</h2>
<h3 id="class-torchnnparameter">class torch.nn.Parameter()</h3>
<p><code>Variable</code>&#x7684;&#x4E00;&#x79CD;&#xFF0C;&#x5E38;&#x88AB;&#x7528;&#x4E8E;&#x6A21;&#x5757;&#x53C2;&#x6570;(<code>module parameter</code>)&#x3002;</p>
<p><code>Parameters</code> &#x662F; <code>Variable</code> &#x7684;&#x5B50;&#x7C7B;&#x3002;<code>Paramenters</code>&#x548C;<code>Modules</code>&#x4E00;&#x8D77;&#x4F7F;&#x7528;&#x7684;&#x65F6;&#x5019;&#x4F1A;&#x6709;&#x4E00;&#x4E9B;&#x7279;&#x6B8A;&#x7684;&#x5C5E;&#x6027;&#xFF0C;&#x5373;&#xFF1A;&#x5F53;<code>Paramenters</code>&#x8D4B;&#x503C;&#x7ED9;<code>Module</code>&#x7684;&#x5C5E;&#x6027;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4ED6;&#x4F1A;&#x81EA;&#x52A8;&#x7684;&#x88AB;&#x52A0;&#x5230; <code>Module</code>&#x7684; &#x53C2;&#x6570;&#x5217;&#x8868;&#x4E2D;(&#x5373;&#xFF1A;&#x4F1A;&#x51FA;&#x73B0;&#x5728; <code>parameters() &#x8FED;&#x4EE3;&#x5668;&#x4E2D;</code>)&#x3002;&#x5C06;<code>Varibale</code>&#x8D4B;&#x503C;&#x7ED9;<code>Module</code>&#x5C5E;&#x6027;&#x5219;&#x4E0D;&#x4F1A;&#x6709;&#x8FD9;&#x6837;&#x7684;&#x5F71;&#x54CD;&#x3002;
&#x8FD9;&#x6837;&#x505A;&#x7684;&#x539F;&#x56E0;&#x662F;&#xFF1A;&#x6211;&#x4EEC;&#x6709;&#x65F6;&#x5019;&#x4F1A;&#x9700;&#x8981;&#x7F13;&#x5B58;&#x4E00;&#x4E9B;&#x4E34;&#x65F6;&#x7684;&#x72B6;&#x6001;(<code>state</code>), &#x6BD4;&#x5982;&#xFF1A;&#x6A21;&#x578B;&#x4E2D;<code>RNN</code>&#x7684;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x9690;&#x72B6;&#x6001;&#x3002;&#x5982;&#x679C;&#x6CA1;&#x6709;<code>Parameter</code>&#x8FD9;&#x4E2A;&#x7C7B;&#x7684;&#x8BDD;&#xFF0C;&#x90A3;&#x4E48;&#x8FD9;&#x4E9B;&#x4E34;&#x65F6;&#x53D8;&#x91CF;&#x4E5F;&#x4F1A;&#x6CE8;&#x518C;&#x6210;&#x4E3A;&#x6A21;&#x578B;&#x53D8;&#x91CF;&#x3002;</p>
<p><code>Variable</code> &#x4E0E; <code>Parameter</code>&#x7684;&#x53E6;&#x4E00;&#x4E2A;&#x4E0D;&#x540C;&#x4E4B;&#x5904;&#x5728;&#x4E8E;&#xFF0C;<code>Parameter</code>&#x4E0D;&#x80FD;&#x88AB; <code>volatile</code>(&#x5373;&#xFF1A;&#x65E0;&#x6CD5;&#x8BBE;&#x7F6E;<code>volatile=True</code>)&#x800C;&#x4E14;&#x9ED8;&#x8BA4;<code>requires_grad=True</code>&#x3002;<code>Variable</code>&#x9ED8;&#x8BA4;<code>requires_grad=False</code>&#x3002;</p>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;:</p>
<ul>
<li><p>data (Tensor) &#x2013; parameter tensor.</p>
</li>
<li><p>requires_grad (bool, optional) &#x2013; &#x9ED8;&#x8BA4;&#x4E3A;<code>True</code>&#xFF0C;&#x5728;<code>BP</code>&#x7684;&#x8FC7;&#x7A0B;&#x4E2D;&#x4F1A;&#x5BF9;&#x5176;&#x6C42;&#x5FAE;&#x5206;&#x3002;</p>
</li>
</ul>
<h2 id="containers&#xFF08;&#x5BB9;&#x5668;&#xFF09;&#xFF1A;">Containers&#xFF08;&#x5BB9;&#x5668;&#xFF09;&#xFF1A;</h2>
<h3 id="class-torchnnmodule">class torch.nn.Module</h3>
<p>&#x6240;&#x6709;&#x7F51;&#x7EDC;&#x7684;&#x57FA;&#x7C7B;&#x3002;</p>
<p>&#x4F60;&#x7684;&#x6A21;&#x578B;&#x4E5F;&#x5E94;&#x8BE5;&#x7EE7;&#x627F;&#x8FD9;&#x4E2A;&#x7C7B;&#x3002;</p>
<p><code>Modules</code>&#x4E5F;&#x53EF;&#x4EE5;&#x5305;&#x542B;&#x5176;&#x5B83;<code>Modules</code>,&#x5141;&#x8BB8;&#x4F7F;&#x7528;&#x6811;&#x7ED3;&#x6784;&#x5D4C;&#x5165;&#x4ED6;&#x4EEC;&#x3002;&#x4F60;&#x53EF;&#x4EE5;&#x5C06;&#x5B50;&#x6A21;&#x5757;&#x8D4B;&#x503C;&#x7ED9;&#x6A21;&#x578B;&#x5C5E;&#x6027;&#x3002;</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Model</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)<span class="hljs-comment"># submodule: Conv2d</span>
        self.conv2 = nn.Conv2d(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
       x = F.relu(self.conv1(x))
       <span class="hljs-keyword">return</span> F.relu(self.conv2(x))
</code></pre>
<p>&#x901A;&#x8FC7;&#x4E0A;&#x9762;&#x65B9;&#x5F0F;&#x8D4B;&#x503C;&#x7684;<code>submodule</code>&#x4F1A;&#x88AB;&#x6CE8;&#x518C;&#x3002;&#x5F53;&#x8C03;&#x7528; <code>.cuda()</code> &#x7684;&#x65F6;&#x5019;&#xFF0C;<code>submodule</code>&#x7684;&#x53C2;&#x6570;&#x4E5F;&#x4F1A;&#x8F6C;&#x6362;&#x4E3A;<code>cuda Tensor</code>&#x3002;</p>
<h4 id="addmodulename-module">add_module(name, module)</h4>
<p>&#x5C06;&#x4E00;&#x4E2A; <code>child module</code> &#x6DFB;&#x52A0;&#x5230;&#x5F53;&#x524D; <code>modle</code>&#x3002;
&#x88AB;&#x6DFB;&#x52A0;&#x7684;<code>module</code>&#x53EF;&#x4EE5;&#x901A;&#x8FC7; <code>name</code>&#x5C5E;&#x6027;&#x6765;&#x83B7;&#x53D6;&#x3002;
&#x4F8B;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Model</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(Model, self).__init__()
        self.add_module(<span class="hljs-string">&quot;conv&quot;</span>, nn.Conv2d(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">4</span>))
        <span class="hljs-comment">#self.conv = nn.Conv2d(10, 20, 4) &#x548C;&#x4E0A;&#x9762;&#x8FD9;&#x4E2A;&#x589E;&#x52A0;module&#x7684;&#x65B9;&#x5F0F;&#x7B49;&#x4EF7;</span>
model = Model()
print(model.conv)
</code></pre>
<p>&#x8F93;&#x51FA;&#xFF1A;</p>
<pre><code>Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
</code></pre><h4 id="children">children()</h4>
<p>Returns an iterator over immediate children modules.
&#x8FD4;&#x56DE;&#x5F53;&#x524D;&#x6A21;&#x578B; &#x5B50;&#x6A21;&#x5757;&#x7684;&#x8FED;&#x4EE3;&#x5668;&#x3002;</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Model</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(Model, self).__init__()
        self.add_module(<span class="hljs-string">&quot;conv&quot;</span>, nn.Conv2d(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">4</span>))
        self.add_module(<span class="hljs-string">&quot;conv1&quot;</span>, nn.Conv2d(<span class="hljs-number">20</span> ,<span class="hljs-number">10</span>, <span class="hljs-number">4</span>))
model = Model()

<span class="hljs-keyword">for</span> sub_module <span class="hljs-keyword">in</span> model.children():
    print(sub_module)
</code></pre>
<pre><code>Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
Conv2d(20, 10, kernel_size=(4, 4), stride=(1, 1))
</code></pre><h4 id="cpudeviceidnone">cpu(device_id=None)</h4>
<p>&#x5C06;&#x6240;&#x6709;&#x7684;&#x6A21;&#x578B;&#x53C2;&#x6570;(<code>parameters</code>)&#x548C;<code>buffers</code>&#x590D;&#x5236;&#x5230;<code>CPU</code></p>
<p><code>NOTE</code>&#xFF1A;&#x5B98;&#x65B9;&#x6587;&#x6863;&#x7528;&#x7684;move&#xFF0C;&#x4F46;&#x6211;&#x89C9;&#x7740;<code>copy</code>&#x66F4;&#x5408;&#x7406;&#x3002;</p>
<h4 id="cudadeviceidnone">cuda(device_id=None)</h4>
<p>&#x5C06;&#x6240;&#x6709;&#x7684;&#x6A21;&#x578B;&#x53C2;&#x6570;(<code>parameters</code>)&#x548C;<code>buffers</code>&#x8D4B;&#x503C;<code>GPU</code></p>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;:</p>
<ul>
<li>device_id (int, optional) &#x2013; &#x5982;&#x679C;&#x6307;&#x5B9A;&#x7684;&#x8BDD;&#xFF0C;&#x6240;&#x6709;&#x7684;&#x6A21;&#x578B;&#x53C2;&#x6570;&#x90FD;&#x4F1A;&#x590D;&#x5236;&#x5230;&#x6307;&#x5B9A;&#x7684;&#x8BBE;&#x5907;&#x4E0A;&#x3002;</li>
</ul>
<h4 id="double">double()</h4>
<p>&#x5C06;<code>parameters</code>&#x548C;<code>buffers</code>&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x8F6C;&#x6362;&#x6210;<code>double</code>&#x3002;</p>
<h4 id="eval">eval()</h4>
<p>&#x5C06;&#x6A21;&#x578B;&#x8BBE;&#x7F6E;&#x6210;<code>evaluation</code>&#x6A21;&#x5F0F;</p>
<p>&#x4EC5;&#x4EC5;&#x5F53;&#x6A21;&#x578B;&#x4E2D;&#x6709;<code>Dropout</code>&#x548C;<code>BatchNorm</code>&#x662F;&#x624D;&#x4F1A;&#x6709;&#x5F71;&#x54CD;&#x3002;</p>
<h4 id="float">float()</h4>
<p>&#x5C06;<code>parameters</code>&#x548C;<code>buffers</code>&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x8F6C;&#x6362;&#x6210;<code>float</code>&#x3002;</p>
<h4 id="forward-input">forward(* input)</h4>
<p>&#x5B9A;&#x4E49;&#x4E86;&#x6BCF;&#x6B21;&#x6267;&#x884C;&#x7684; &#x8BA1;&#x7B97;&#x6B65;&#x9AA4;&#x3002;
&#x5728;&#x6240;&#x6709;&#x7684;&#x5B50;&#x7C7B;&#x4E2D;&#x90FD;&#x9700;&#x8981;&#x91CD;&#x5199;&#x8FD9;&#x4E2A;&#x51FD;&#x6570;&#x3002;</p>
<h4 id="half">half()</h4>
<p>&#x5C06;<code>parameters</code>&#x548C;<code>buffers</code>&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x8F6C;&#x6362;&#x6210;<code>half</code>&#x3002;</p>
<h4 id="loadstatedictstatedict">load_state_dict(state_dict)</h4>
<p>&#x5C06;<code>state_dict</code>&#x4E2D;&#x7684;<code>parameters</code>&#x548C;<code>buffers</code>&#x590D;&#x5236;&#x5230;&#x6B64;<code>module</code>&#x548C;&#x5B83;&#x7684;&#x540E;&#x4EE3;&#x4E2D;&#x3002;<code>state_dict</code>&#x4E2D;&#x7684;<code>key</code>&#x5FC5;&#x987B;&#x548C; <code>model.state_dict()</code>&#x8FD4;&#x56DE;&#x7684;<code>key</code>&#x4E00;&#x81F4;&#x3002;
<code>NOTE</code>&#xFF1A;&#x7528;&#x6765;&#x52A0;&#x8F7D;&#x6A21;&#x578B;&#x53C2;&#x6570;&#x3002;</p>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;:</p>
<ul>
<li>state_dict (dict) &#x2013; &#x4FDD;&#x5B58;<code>parameters</code>&#x548C;<code>persistent buffers</code>&#x7684;&#x5B57;&#x5178;&#x3002;</li>
</ul>
<h4 id="modules">modules()</h4>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5305;&#x542B; &#x5F53;&#x524D;&#x6A21;&#x578B; &#x6240;&#x6709;&#x6A21;&#x5757;&#x7684;&#x8FED;&#x4EE3;&#x5668;&#x3002;</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Model</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(Model, self).__init__()
        self.add_module(<span class="hljs-string">&quot;conv&quot;</span>, nn.Conv2d(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">4</span>))
        self.add_module(<span class="hljs-string">&quot;conv1&quot;</span>, nn.Conv2d(<span class="hljs-number">20</span> ,<span class="hljs-number">10</span>, <span class="hljs-number">4</span>))
model = Model()

<span class="hljs-keyword">for</span> module <span class="hljs-keyword">in</span> model.modules():
    print(module)
</code></pre>
<pre><code>Model (
  (conv): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
  (conv1): Conv2d(20, 10, kernel_size=(4, 4), stride=(1, 1))
)
Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
Conv2d(20, 10, kernel_size=(4, 4), stride=(1, 1))
</code></pre><p>&#x53EF;&#x4EE5;&#x770B;&#x51FA;&#xFF0C;<code>modules()</code>&#x8FD4;&#x56DE;&#x7684;<code>iterator</code>&#x4E0D;&#x6B62;&#x5305;&#x542B; &#x5B50;&#x6A21;&#x5757;&#x3002;&#x8FD9;&#x662F;&#x548C;<code>children()</code>&#x7684;&#x4E0D;&#x540C;&#x3002;</p>
<p><strong><code>NOTE&#xFF1A;</code></strong>
&#x91CD;&#x590D;&#x7684;&#x6A21;&#x5757;&#x53EA;&#x88AB;&#x8FD4;&#x56DE;&#x4E00;&#x6B21;(<code>children()&#x4E5F;&#x662F;</code>)&#x3002; &#x5728;&#x4E0B;&#x9762;&#x7684;&#x4F8B;&#x5B50;&#x4E2D;, <code>submodule</code> &#x53EA;&#x4F1A;&#x88AB;&#x8FD4;&#x56DE;&#x4E00;&#x6B21;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Model</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(Model, self).__init__()
        submodule = nn.Conv2d(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">4</span>)
        self.add_module(<span class="hljs-string">&quot;conv&quot;</span>, submodule)
        self.add_module(<span class="hljs-string">&quot;conv1&quot;</span>, submodule)
model = Model()

<span class="hljs-keyword">for</span> module <span class="hljs-keyword">in</span> model.modules():
    print(module)
</code></pre>
<pre><code>Model (
  (conv): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
  (conv1): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
)
Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
</code></pre><h4 id="namedchildren">named_children()</h4>
<p>&#x8FD4;&#x56DE; &#x5305;&#x542B; &#x6A21;&#x578B;&#x5F53;&#x524D;&#x5B50;&#x6A21;&#x5757; &#x7684;&#x8FED;&#x4EE3;&#x5668;&#xFF0C;<code>yield</code> &#x6A21;&#x5757;&#x540D;&#x5B57;&#x548C;&#x6A21;&#x5757;&#x672C;&#x8EAB;&#x3002;</p>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-keyword">for</span> name, module <span class="hljs-keyword">in</span> model.named_children():
    <span class="hljs-keyword">if</span> name <span class="hljs-keyword">in</span> [<span class="hljs-string">&apos;conv4&apos;</span>, <span class="hljs-string">&apos;conv5&apos;</span>]:
        print(module)
</code></pre>
<h4 id="namedmodulesmemonone-prefixsource">named_modules(memo=None, prefix=&apos;&apos;)[source]</h4>
<p>&#x8FD4;&#x56DE;&#x5305;&#x542B;&#x7F51;&#x7EDC;&#x4E2D;&#x6240;&#x6709;&#x6A21;&#x5757;&#x7684;&#x8FED;&#x4EE3;&#x5668;, <code>yielding</code>  &#x6A21;&#x5757;&#x540D;&#x548C;&#x6A21;&#x5757;&#x672C;&#x8EAB;&#x3002;</p>
<p><strong><code>&#x6CE8;&#x610F;&#xFF1A;</code></strong></p>
<p>&#x91CD;&#x590D;&#x7684;&#x6A21;&#x5757;&#x53EA;&#x88AB;&#x8FD4;&#x56DE;&#x4E00;&#x6B21;(<code>children()&#x4E5F;&#x662F;</code>)&#x3002; &#x5728;&#x4E0B;&#x9762;&#x7684;&#x4F8B;&#x5B50;&#x4E2D;, <code>submodule</code> &#x53EA;&#x4F1A;&#x88AB;&#x8FD4;&#x56DE;&#x4E00;&#x6B21;&#x3002;</p>
<h4 id="parametersmemonone">parameters(memo=None)</h4>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A; &#x5305;&#x542B;&#x6A21;&#x578B;&#x6240;&#x6709;&#x53C2;&#x6570; &#x7684;&#x8FED;&#x4EE3;&#x5668;&#x3002;</p>
<p>&#x4E00;&#x822C;&#x7528;&#x6765;&#x5F53;&#x4F5C;<code>optimizer</code>&#x7684;&#x53C2;&#x6570;&#x3002;</p>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python">for param in model.parameters():
    print(type(param.data), param.size())

&lt;class &apos;torch.FloatTensor&apos;&gt; (20L,)
&lt;class &apos;torch.FloatTensor&apos;&gt; (20L, 1L, 5L, 5L)
</code></pre>
<h4 id="registerbackwardhookhook">register_backward_hook(hook)</h4>
<p>&#x5728;<code>module</code>&#x4E0A;&#x6CE8;&#x518C;&#x4E00;&#x4E2A;<code>bachward hook</code>&#x3002;</p>
<p>&#x6BCF;&#x6B21;&#x8BA1;&#x7B97;<code>module</code>&#x7684;<code>inputs</code>&#x7684;&#x68AF;&#x5EA6;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x8FD9;&#x4E2A;<code>hook</code>&#x4F1A;&#x88AB;&#x8C03;&#x7528;&#x3002;<code>hook</code>&#x5E94;&#x8BE5;&#x62E5;&#x6709;&#x4E0B;&#x9762;&#x7684;<code>signature</code>&#x3002;</p>
<p><code>hook(module, grad_input, grad_output) -&gt; Variable or None</code></p>
<p>&#x5982;&#x679C;<code>module</code>&#x6709;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x7684;&#x8BDD;&#xFF0C;&#x90A3;&#x4E48;<code>grad_input</code> <code>grad_output</code>&#x5C06;&#x4F1A;&#x662F;&#x4E2A;<code>tuple</code>&#x3002;
<code>hook</code>&#x4E0D;&#x5E94;&#x8BE5;&#x4FEE;&#x6539;&#x5B83;&#x7684;<code>arguments</code>&#xFF0C;&#x4F46;&#x662F;&#x5B83;&#x53EF;&#x4EE5;&#x9009;&#x62E9;&#x6027;&#x7684;&#x8FD4;&#x56DE;&#x5173;&#x4E8E;&#x8F93;&#x5165;&#x7684;&#x68AF;&#x5EA6;&#xFF0C;&#x8FD9;&#x4E2A;&#x8FD4;&#x56DE;&#x7684;&#x68AF;&#x5EA6;&#x5728;&#x540E;&#x7EED;&#x7684;&#x8BA1;&#x7B97;&#x4E2D;&#x4F1A;&#x66FF;&#x4EE3;<code>grad_input</code>&#x3002;</p>
<p>&#x8FD9;&#x4E2A;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A; &#x53E5;&#x67C4;(<code>handle</code>)&#x3002;&#x5B83;&#x6709;&#x4E00;&#x4E2A;&#x65B9;&#x6CD5; <code>handle.remove()</code>&#xFF0C;&#x53EF;&#x4EE5;&#x7528;&#x8FD9;&#x4E2A;&#x65B9;&#x6CD5;&#x5C06;<code>hook</code>&#x4ECE;<code>module</code>&#x79FB;&#x9664;&#x3002;</p>
<h4 id="registerbuffername-tensor">register_buffer(name, tensor)</h4>
<p>&#x7ED9;<code>module</code>&#x6DFB;&#x52A0;&#x4E00;&#x4E2A;<code>persistent buffer</code>&#x3002;</p>
<p><code>persistent buffer</code>&#x901A;&#x5E38;&#x88AB;&#x7528;&#x5728;&#x8FD9;&#x4E48;&#x4E00;&#x79CD;&#x60C5;&#x51B5;&#xFF1A;&#x6211;&#x4EEC;&#x9700;&#x8981;&#x4FDD;&#x5B58;&#x4E00;&#x4E2A;&#x72B6;&#x6001;&#xFF0C;&#x4F46;&#x662F;&#x8FD9;&#x4E2A;&#x72B6;&#x6001;&#x4E0D;&#x80FD;&#x770B;&#x4F5C;&#x6210;&#x4E3A;&#x6A21;&#x578B;&#x53C2;&#x6570;&#x3002;
&#x4F8B;&#x5982;&#xFF1A;, <code>BatchNorm&#x2019;s</code> running_mean &#x4E0D;&#x662F;&#x4E00;&#x4E2A; <code>parameter</code>, &#x4F46;&#x662F;&#x5B83;&#x4E5F;&#x662F;&#x9700;&#x8981;&#x4FDD;&#x5B58;&#x7684;&#x72B6;&#x6001;&#x4E4B;&#x4E00;&#x3002;</p>
<p><code>Buffers</code>&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x6CE8;&#x518C;&#x65F6;&#x5019;&#x7684;<code>name</code>&#x83B7;&#x53D6;&#x3002;</p>
<p><strong><code>NOTE</code>:&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x7528; buffer &#x4FDD;&#x5B58; <code>moving average</code></strong></p>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python">self.register_buffer(<span class="hljs-string">&apos;running_mean&apos;</span>, torch.zeros(num_features))

self.running_mean
</code></pre>
<h4 id="registerforwardhookhook">register_forward_hook(hook)</h4>
<p>&#x5728;<code>module</code>&#x4E0A;&#x6CE8;&#x518C;&#x4E00;&#x4E2A;<code>forward hook</code>&#x3002;
&#x6BCF;&#x6B21;&#x8C03;&#x7528;<code>forward()</code>&#x8BA1;&#x7B97;&#x8F93;&#x51FA;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x8FD9;&#x4E2A;<code>hook</code>&#x5C31;&#x4F1A;&#x88AB;&#x8C03;&#x7528;&#x3002;&#x5B83;&#x5E94;&#x8BE5;&#x62E5;&#x6709;&#x4EE5;&#x4E0B;&#x7B7E;&#x540D;&#xFF1A;</p>
<p><code>hook(module, input, output) -&gt; None</code></p>
<p><code>hook</code>&#x4E0D;&#x5E94;&#x8BE5;&#x4FEE;&#x6539; <code>input</code>&#x548C;<code>output</code>&#x7684;&#x503C;&#x3002; &#x8FD9;&#x4E2A;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A; &#x53E5;&#x67C4;(<code>handle</code>)&#x3002;&#x5B83;&#x6709;&#x4E00;&#x4E2A;&#x65B9;&#x6CD5; <code>handle.remove()</code>&#xFF0C;&#x53EF;&#x4EE5;&#x7528;&#x8FD9;&#x4E2A;&#x65B9;&#x6CD5;&#x5C06;<code>hook</code>&#x4ECE;<code>module</code>&#x79FB;&#x9664;&#x3002;</p>
<h4 id="registerparametername-param">register_parameter(name, param)</h4>
<p>&#x5411;<code>module</code>&#x6DFB;&#x52A0; <code>parameter</code></p>
<p><code>parameter</code>&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x6CE8;&#x518C;&#x65F6;&#x5019;&#x7684;<code>name</code>&#x83B7;&#x53D6;&#x3002;</p>
<h4 id="statedictdestinationnone-prefixsource">state_dict(destination=None, prefix=&apos;&apos;)[source]</h4>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5B57;&#x5178;&#xFF0C;&#x4FDD;&#x5B58;&#x7740;<code>module</code>&#x7684;&#x6240;&#x6709;&#x72B6;&#x6001;&#xFF08;<code>state</code>&#xFF09;&#x3002;</p>
<p><code>parameters</code>&#x548C;<code>persistent buffers</code>&#x90FD;&#x4F1A;&#x5305;&#x542B;&#x5728;&#x5B57;&#x5178;&#x4E2D;&#xFF0C;&#x5B57;&#x5178;&#x7684;<code>key</code>&#x5C31;&#x662F;<code>parameter</code>&#x548C;<code>buffer</code>&#x7684; <code>names</code>&#x3002;</p>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch.autograd <span class="hljs-keyword">import</span> Variable
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Model</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(Model, self).__init__()
        self.conv2 = nn.Linear(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)
        self.vari = Variable(torch.rand([<span class="hljs-number">1</span>]))
        self.par = nn.Parameter(torch.rand([<span class="hljs-number">1</span>]))
        self.register_buffer(<span class="hljs-string">&quot;buffer&quot;</span>, torch.randn([<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]))

model = Model()
print(model.state_dict().keys())
</code></pre>
<pre><code>odict_keys([&apos;par&apos;, &apos;buffer&apos;, &apos;conv2.weight&apos;, &apos;conv2.bias&apos;])
</code></pre><h4 id="trainmodetrue">train(mode=True)</h4>
<p>&#x5C06;<code>module</code>&#x8BBE;&#x7F6E;&#x4E3A; <code>training mode</code>&#x3002;</p>
<p>&#x4EC5;&#x4EC5;&#x5F53;&#x6A21;&#x578B;&#x4E2D;&#x6709;<code>Dropout</code>&#x548C;<code>BatchNorm</code>&#x662F;&#x624D;&#x4F1A;&#x6709;&#x5F71;&#x54CD;&#x3002;</p>
<h4 id="zerograd">zero_grad()</h4>
<p>&#x5C06;<code>module</code>&#x4E2D;&#x7684;&#x6240;&#x6709;&#x6A21;&#x578B;&#x53C2;&#x6570;&#x7684;&#x68AF;&#x5EA6;&#x8BBE;&#x7F6E;&#x4E3A;0.</p>
<h3 id="class-torchnnsequential-args">class torch.nn.Sequential(* args)</h3>
<p>&#x4E00;&#x4E2A;&#x65F6;&#x5E8F;&#x5BB9;&#x5668;&#x3002;<code>Modules</code> &#x4F1A;&#x4EE5;&#x4ED6;&#x4EEC;&#x4F20;&#x5165;&#x7684;&#x987A;&#x5E8F;&#x88AB;&#x6DFB;&#x52A0;&#x5230;&#x5BB9;&#x5668;&#x4E2D;&#x3002;&#x5F53;&#x7136;&#xFF0C;&#x4E5F;&#x53EF;&#x4EE5;&#x4F20;&#x5165;&#x4E00;&#x4E2A;<code>OrderedDict</code>&#x3002;</p>
<p>&#x4E3A;&#x4E86;&#x66F4;&#x5BB9;&#x6613;&#x7684;&#x7406;&#x89E3;&#x5982;&#x4F55;&#x4F7F;&#x7528;<code>Sequential</code>, &#x4E0B;&#x9762;&#x7ED9;&#x51FA;&#x4E86;&#x4E00;&#x4E2A;&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-python"><span class="hljs-comment"># Example of using Sequential</span>

model = nn.Sequential(
          nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>,<span class="hljs-number">5</span>),
          nn.ReLU(),
          nn.Conv2d(<span class="hljs-number">20</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>),
          nn.ReLU()
        )
<span class="hljs-comment"># Example of using Sequential with OrderedDict</span>
model = nn.Sequential(OrderedDict([
          (<span class="hljs-string">&apos;conv1&apos;</span>, nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>,<span class="hljs-number">5</span>)),
          (<span class="hljs-string">&apos;relu1&apos;</span>, nn.ReLU()),
          (<span class="hljs-string">&apos;conv2&apos;</span>, nn.Conv2d(<span class="hljs-number">20</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>)),
          (<span class="hljs-string">&apos;relu2&apos;</span>, nn.ReLU())
        ]))
</code></pre>
<h3 id="class-torchnnmodulelistmodulesnonesource">class torch.nn.ModuleList(modules=None)[source]</h3>
<p>&#x5C06;<code>submodules</code>&#x4FDD;&#x5B58;&#x5728;&#x4E00;&#x4E2A;<code>list</code>&#x4E2D;&#x3002;</p>
<p><code>ModuleList</code>&#x53EF;&#x4EE5;&#x50CF;&#x4E00;&#x822C;&#x7684;<code>Python list</code>&#x4E00;&#x6837;&#x88AB;<code>&#x7D22;&#x5F15;</code>&#x3002;&#x800C;&#x4E14;<code>ModuleList</code>&#x4E2D;&#x5305;&#x542B;&#x7684;<code>modules</code>&#x5DF2;&#x7ECF;&#x88AB;&#x6B63;&#x786E;&#x7684;&#x6CE8;&#x518C;&#xFF0C;&#x5BF9;&#x6240;&#x6709;&#x7684;<code>module method</code>&#x53EF;&#x89C1;&#x3002;</p>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;:</p>
<ul>
<li>modules (list, optional) &#x2013; &#x5C06;&#x8981;&#x88AB;&#x6DFB;&#x52A0;&#x5230;<code>MuduleList</code>&#x4E2D;&#x7684; <code>modules</code> &#x5217;&#x8868;</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModule</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(MyModule, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>)])

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        <span class="hljs-comment"># ModuleList can act as an iterable, or be indexed         using ints</span>
        <span class="hljs-keyword">for</span> i, l <span class="hljs-keyword">in</span> enumerate(self.linears):
            x = self.linears[i // <span class="hljs-number">2</span>](x) + l(x)
        <span class="hljs-keyword">return</span> x
</code></pre>
<h4 id="appendmodulesource">append(module)[source]</h4>
<p>&#x7B49;&#x4EF7;&#x4E8E; list &#x7684; <code>append()</code></p>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;:</p>
<ul>
<li>module (nn.Module) &#x2013; &#x8981; append &#x7684;<code>module</code><h4 id="extendmodulessource">extend(modules)[source]</h4>
&#x7B49;&#x4EF7;&#x4E8E; <code>list</code> &#x7684; <code>extend()</code> &#x65B9;&#x6CD5;</li>
</ul>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;:</p>
<ul>
<li>modules (list) &#x2013; list of modules to append</li>
</ul>
<h3 id="class-torchnnparameterlistparametersnone">class torch.nn.ParameterList(parameters=None)</h3>
<p>&#x5C06;<code>submodules</code>&#x4FDD;&#x5B58;&#x5728;&#x4E00;&#x4E2A;<code>list</code>&#x4E2D;&#x3002;</p>
<p><code>ParameterList</code>&#x53EF;&#x4EE5;&#x50CF;&#x4E00;&#x822C;&#x7684;<code>Python list</code>&#x4E00;&#x6837;&#x88AB;<code>&#x7D22;&#x5F15;</code>&#x3002;&#x800C;&#x4E14;<code>ParameterList</code>&#x4E2D;&#x5305;&#x542B;&#x7684;<code>parameters</code>&#x5DF2;&#x7ECF;&#x88AB;&#x6B63;&#x786E;&#x7684;&#x6CE8;&#x518C;&#xFF0C;&#x5BF9;&#x6240;&#x6709;&#x7684;<code>module method</code>&#x53EF;&#x89C1;&#x3002;</p>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;:</p>
<ul>
<li>modules (list, optional) &#x2013; a list of nn.Parameter</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModule</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(MyModule, self).__init__()
        self.params = nn.ParameterList([nn.Parameter(torch.randn(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>)])

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        <span class="hljs-comment"># ModuleList can act as an iterable, or be indexed using ints</span>
        <span class="hljs-keyword">for</span> i, p <span class="hljs-keyword">in</span> enumerate(self.params):
            x = self.params[i // <span class="hljs-number">2</span>].mm(x) + p.mm(x)
        <span class="hljs-keyword">return</span> x
</code></pre>
<h4 id="appendparametersource">append(parameter)[source]</h4>
<p>&#x7B49;&#x4EF7;&#x4E8E;<code>python list</code> &#x7684; <code>append</code> &#x65B9;&#x6CD5;&#x3002;</p>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;:</p>
<ul>
<li>parameter (nn.Parameter) &#x2013; parameter to append<h4 id="extendparameterssource">extend(parameters)[source]</h4>
&#x7B49;&#x4EF7;&#x4E8E;<code>python list</code> &#x7684; <code>extend</code> &#x65B9;&#x6CD5;&#x3002;</li>
</ul>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;:</p>
<ul>
<li>parameters (list) &#x2013; list of parameters to append</li>
</ul>
<h2 id="&#x5377;&#x79EF;&#x5C42;">&#x5377;&#x79EF;&#x5C42;</h2>
<h3 id="class-torchnnconv1dinchannels-outchannels-kernelsize-stride1-padding0-dilation1-groups1-biastrue">class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)</h3>
<p>&#x4E00;&#x7EF4;&#x5377;&#x79EF;&#x5C42;&#xFF0C;&#x8F93;&#x5165;&#x7684;&#x5C3A;&#x5EA6;&#x662F;(N, C_in,L)&#xFF0C;&#x8F93;&#x51FA;&#x5C3A;&#x5EA6;&#xFF08; N,C_out,L_out&#xFF09;&#x7684;&#x8BA1;&#x7B97;&#x65B9;&#x5F0F;&#xFF1A;  </p>
<p><script type="math/tex; mode=display">
out(N_i, C_{out_j})=bias(C _{out_j})+\sum^{C_{in}-1}_{k=0}weight(C_{out_j},k)\bigotimes input(N_i,k)
</script></p>
<p><strong>&#x8BF4;&#x660E;</strong>   </p>
<p><code>bigotimes</code>: &#x8868;&#x793A;&#x76F8;&#x5173;&#x7CFB;&#x6570;&#x8BA1;&#x7B97;<br><code>stride</code>: &#x63A7;&#x5236;&#x76F8;&#x5173;&#x7CFB;&#x6570;&#x7684;&#x8BA1;&#x7B97;&#x6B65;&#x957F;<br><code>dilation</code>: &#x7528;&#x4E8E;&#x63A7;&#x5236;&#x5185;&#x6838;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#xFF0C;&#x8BE6;&#x7EC6;&#x63CF;&#x8FF0;&#x5728;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">&#x8FD9;&#x91CC;</a><br><code>groups</code>: &#x63A7;&#x5236;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x4E4B;&#x95F4;&#x7684;&#x8FDE;&#x63A5;&#xFF0C;    <code>group=1</code>&#xFF0C;&#x8F93;&#x51FA;&#x662F;&#x6240;&#x6709;&#x7684;&#x8F93;&#x5165;&#x7684;&#x5377;&#x79EF;&#xFF1B;<code>group=2</code>&#xFF0C;&#x6B64;&#x65F6;&#x76F8;&#x5F53;&#x4E8E;&#x6709;&#x5E76;&#x6392;&#x7684;&#x4E24;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#xFF0C;&#x6BCF;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x8BA1;&#x7B97;&#x8F93;&#x5165;&#x901A;&#x9053;&#x7684;&#x4E00;&#x534A;&#xFF0C;&#x5E76;&#x4E14;&#x4EA7;&#x751F;&#x7684;&#x8F93;&#x51FA;&#x662F;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x7684;&#x4E00;&#x534A;&#xFF0C;&#x968F;&#x540E;&#x5C06;&#x8FD9;&#x4E24;&#x4E2A;&#x8F93;&#x51FA;&#x8FDE;&#x63A5;&#x8D77;&#x6765;&#x3002;</p>
<p><strong>Parameters&#xFF1A;</strong>  </p>
<ul>
<li>in_channels(<code>int</code>) &#x2013; &#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x901A;&#x9053;</li>
<li>out_channels(<code>int</code>) &#x2013; &#x5377;&#x79EF;&#x4EA7;&#x751F;&#x7684;&#x901A;&#x9053;</li>
<li>kerner_size(<code>int</code> or <code>tuple</code>) - &#x5377;&#x79EF;&#x6838;&#x7684;&#x5C3A;&#x5BF8;</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x5377;&#x79EF;&#x6B65;&#x957F;</li>
<li>padding (<code>int</code> or <code>tuple</code>, <code>optional</code>)- &#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x8FB9;&#x8865;&#x5145;0&#x7684;&#x5C42;&#x6570;   </li>
<li>dilation(<code>int</code> or <code>tuple</code>, `optional``) &#x2013; &#x5377;&#x79EF;&#x6838;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x95F4;&#x8DDD;</li>
<li>groups(<code>int</code>, <code>optional</code>) &#x2013; &#x4ECE;&#x8F93;&#x5165;&#x901A;&#x9053;&#x5230;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x7684;&#x963B;&#x585E;&#x8FDE;&#x63A5;&#x6570;</li>
<li>bias(<code>bool</code>, <code>optional</code>) - &#x5982;&#x679C;<code>bias=True</code>&#xFF0C;&#x6DFB;&#x52A0;&#x504F;&#x7F6E;</li>
</ul>
<p><strong>shape:</strong><br>&#x8F93;&#x5165;: (N,C_in,L_in)<br>&#x8F93;&#x51FA;: (N,C_out,L_out)<br>&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x7684;&#x8BA1;&#x7B97;&#x65B9;&#x5F0F;&#xFF1A;<br><script type="math/tex; ">L_{out}=floor((L_{in}+2*padding-dilation*(kernerl\_size-1)-1)/stride+1)</script></p>
<p><strong>&#x53D8;&#x91CF;:</strong><br>weight(<code>tensor</code>) - &#x5377;&#x79EF;&#x7684;&#x6743;&#x91CD;&#xFF0C;&#x5927;&#x5C0F;&#x662F;(<code>out_channels</code>, <code>in_channels</code>, <code>kernel_size</code>)<br>bias(<code>tensor</code>) - &#x5377;&#x79EF;&#x7684;&#x504F;&#x7F6E;&#x7CFB;&#x6570;&#xFF0C;&#x5927;&#x5C0F;&#x662F;&#xFF08;<code>out_channel</code>&#xFF09;  </p>
<p><strong>example:</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Conv1d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="class-torchnnconv2dinchannels-outchannels-kernelsize-stride1-padding0-dilation1-groups1-biastrue">class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)</h3>
<p>&#x4E8C;&#x7EF4;&#x5377;&#x79EF;&#x5C42;, &#x8F93;&#x5165;&#x7684;&#x5C3A;&#x5EA6;&#x662F;(N, C_in,H,W)&#xFF0C;&#x8F93;&#x51FA;&#x5C3A;&#x5EA6;&#xFF08;N,C_out,H_out,W_out&#xFF09;&#x7684;&#x8BA1;&#x7B97;&#x65B9;&#x5F0F;&#xFF1A;  </p>
<p><script type="math/tex; ">out(N_i, C_{out_j})=bias(C_{out_j})+\sum^{C_{in}-1}_{k=0}weight(C_{out_j},k)\bigotimes input(N_i,k)</script>  </p>
<p><strong>&#x8BF4;&#x660E;</strong><br><code>bigotimes</code>: &#x8868;&#x793A;&#x4E8C;&#x7EF4;&#x7684;&#x76F8;&#x5173;&#x7CFB;&#x6570;&#x8BA1;&#x7B97;
<code>stride</code>: &#x63A7;&#x5236;&#x76F8;&#x5173;&#x7CFB;&#x6570;&#x7684;&#x8BA1;&#x7B97;&#x6B65;&#x957F;<br><code>dilation</code>: &#x7528;&#x4E8E;&#x63A7;&#x5236;&#x5185;&#x6838;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#xFF0C;&#x8BE6;&#x7EC6;&#x63CF;&#x8FF0;&#x5728;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">&#x8FD9;&#x91CC;</a><br><code>groups</code>: &#x63A7;&#x5236;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x4E4B;&#x95F4;&#x7684;&#x8FDE;&#x63A5;&#xFF1A; <code>group=1</code>&#xFF0C;&#x8F93;&#x51FA;&#x662F;&#x6240;&#x6709;&#x7684;&#x8F93;&#x5165;&#x7684;&#x5377;&#x79EF;&#xFF1B;<code>group=2</code>&#xFF0C;&#x6B64;&#x65F6;&#x76F8;&#x5F53;&#x4E8E;&#x6709;&#x5E76;&#x6392;&#x7684;&#x4E24;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#xFF0C;&#x6BCF;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x8BA1;&#x7B97;&#x8F93;&#x5165;&#x901A;&#x9053;&#x7684;&#x4E00;&#x534A;&#xFF0C;&#x5E76;&#x4E14;&#x4EA7;&#x751F;&#x7684;&#x8F93;&#x51FA;&#x662F;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x7684;&#x4E00;&#x534A;&#xFF0C;&#x968F;&#x540E;&#x5C06;&#x8FD9;&#x4E24;&#x4E2A;&#x8F93;&#x51FA;&#x8FDE;&#x63A5;&#x8D77;&#x6765;&#x3002;      </p>
<p>&#x53C2;&#x6570;<code>kernel_size</code>&#xFF0C;<code>stride,padding</code>&#xFF0C;<code>dilation</code>&#x4E5F;&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;<code>int</code>&#x7684;&#x6570;&#x636E;&#xFF0C;&#x6B64;&#x65F6;&#x5377;&#x79EF;height&#x548C;width&#x503C;&#x76F8;&#x540C;;&#x4E5F;&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;<code>tuple</code>&#x6570;&#x7EC4;&#xFF0C;<code>tuple</code>&#x7684;&#x7B2C;&#x4E00;&#x7EF4;&#x5EA6;&#x8868;&#x793A;height&#x7684;&#x6570;&#x503C;&#xFF0C;tuple&#x7684;&#x7B2C;&#x4E8C;&#x7EF4;&#x5EA6;&#x8868;&#x793A;width&#x7684;&#x6570;&#x503C;</p>
<p><strong>Parameters&#xFF1A;</strong>  </p>
<ul>
<li>in_channels(<code>int</code>) &#x2013; &#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x901A;&#x9053;</li>
<li>out_channels(<code>int</code>) &#x2013; &#x5377;&#x79EF;&#x4EA7;&#x751F;&#x7684;&#x901A;&#x9053;</li>
<li>kerner_size(<code>int</code> or <code>tuple</code>) - &#x5377;&#x79EF;&#x6838;&#x7684;&#x5C3A;&#x5BF8;</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x5377;&#x79EF;&#x6B65;&#x957F;</li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x8FB9;&#x8865;&#x5145;0&#x7684;&#x5C42;&#x6570;</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) &#x2013; &#x5377;&#x79EF;&#x6838;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x95F4;&#x8DDD;</li>
<li>groups(<code>int</code>, <code>optional</code>) &#x2013; &#x4ECE;&#x8F93;&#x5165;&#x901A;&#x9053;&#x5230;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x7684;&#x963B;&#x585E;&#x8FDE;&#x63A5;&#x6570;</li>
<li>bias(<code>bool</code>, <code>optional</code>) - &#x5982;&#x679C;<code>bias=True</code>&#xFF0C;&#x6DFB;&#x52A0;&#x504F;&#x7F6E;</li>
</ul>
<p><strong>shape:</strong><br>input: (N,C_in,H_in,W_in)<br>output: (N,C_out,H_out,W_out)<br><script type="math/tex; ">H_{out}=floor((H_{in}+2*padding[0]-dilation[0]*(kernerl\_size[0]-1)-1)/stride[0]+1)</script>    </p>
<p><script type="math/tex; ">W_{out}=floor((W_{in}+2*padding[1]-dilation[1]*(kernerl\_size[1]-1)-1)/stride[1]+1)</script>  </p>
<p><strong>&#x53D8;&#x91CF;:</strong><br>weight(<code>tensor</code>) - &#x5377;&#x79EF;&#x7684;&#x6743;&#x91CD;&#xFF0C;&#x5927;&#x5C0F;&#x662F;(<code>out_channels</code>, <code>in_channels</code>,<code>kernel_size</code>)<br>bias(<code>tensor</code>) - &#x5377;&#x79EF;&#x7684;&#x504F;&#x7F6E;&#x7CFB;&#x6570;&#xFF0C;&#x5927;&#x5C0F;&#x662F;&#xFF08;<code>out_channel</code>&#xFF09;  </p>
<p><strong>example:</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With square kernels and equal stride</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># non-square kernels and unequal stride and with padding</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">5</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">4</span>, <span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># non-square kernels and unequal stride and with padding and dilation</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">5</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">4</span>, <span class="hljs-number">2</span>), dilation=(<span class="hljs-number">3</span>, <span class="hljs-number">1</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>, <span class="hljs-number">100</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="class-torchnnconv3dinchannels-outchannels-kernelsize-stride1-padding0-dilation1-groups1-biastrue">class torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)</h3>
<p>&#x4E09;&#x7EF4;&#x5377;&#x79EF;&#x5C42;, &#x8F93;&#x5165;&#x7684;&#x5C3A;&#x5EA6;&#x662F;(N, C_in,D,H,W)&#xFF0C;&#x8F93;&#x51FA;&#x5C3A;&#x5EA6;&#xFF08;N,C_out,D_out,H_out,W_out&#xFF09;&#x7684;&#x8BA1;&#x7B97;&#x65B9;&#x5F0F;&#xFF1A;<br><script type="math/tex; ">out(N_i, C_{out_j})=bias(C_{out_j})+\sum^{C_{in}-1}_{k=0}weight(C_{out_j},k)\bigotimes input(N_i,k)</script></p>
<p><strong>&#x8BF4;&#x660E;</strong><br><code>bigotimes</code>: &#x8868;&#x793A;&#x4E8C;&#x7EF4;&#x7684;&#x76F8;&#x5173;&#x7CFB;&#x6570;&#x8BA1;&#x7B97;
<code>stride</code>: &#x63A7;&#x5236;&#x76F8;&#x5173;&#x7CFB;&#x6570;&#x7684;&#x8BA1;&#x7B97;&#x6B65;&#x957F;<br><code>dilation</code>: &#x7528;&#x4E8E;&#x63A7;&#x5236;&#x5185;&#x6838;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#xFF0C;&#x8BE6;&#x7EC6;&#x63CF;&#x8FF0;&#x5728;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">&#x8FD9;&#x91CC;</a><br><code>groups</code>: &#x63A7;&#x5236;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x4E4B;&#x95F4;&#x7684;&#x8FDE;&#x63A5;&#xFF1A; <code>group=1</code>&#xFF0C;&#x8F93;&#x51FA;&#x662F;&#x6240;&#x6709;&#x7684;&#x8F93;&#x5165;&#x7684;&#x5377;&#x79EF;&#xFF1B;<code>group=2</code>&#xFF0C;&#x6B64;&#x65F6;&#x76F8;&#x5F53;&#x4E8E;&#x6709;&#x5E76;&#x6392;&#x7684;&#x4E24;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#xFF0C;&#x6BCF;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x8BA1;&#x7B97;&#x8F93;&#x5165;&#x901A;&#x9053;&#x7684;&#x4E00;&#x534A;&#xFF0C;&#x5E76;&#x4E14;&#x4EA7;&#x751F;&#x7684;&#x8F93;&#x51FA;&#x662F;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x7684;&#x4E00;&#x534A;&#xFF0C;&#x968F;&#x540E;&#x5C06;&#x8FD9;&#x4E24;&#x4E2A;&#x8F93;&#x51FA;&#x8FDE;&#x63A5;&#x8D77;&#x6765;&#x3002;<br>&#x53C2;&#x6570;<code>kernel_size</code>&#xFF0C;<code>stride</code>&#xFF0C;<code>padding</code>&#xFF0C;<code>dilation</code>&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;<code>int</code>&#x7684;&#x6570;&#x636E; - &#x5377;&#x79EF;height&#x548C;width&#x503C;&#x76F8;&#x540C;&#xFF0C;&#x4E5F;&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;&#x6709;&#x4E09;&#x4E2A;<code>int</code>&#x6570;&#x636E;&#x7684;<code>tuple</code>&#x6570;&#x7EC4;&#xFF0C;<code>tuple</code>&#x7684;&#x7B2C;&#x4E00;&#x7EF4;&#x5EA6;&#x8868;&#x793A;depth&#x7684;&#x6570;&#x503C;&#xFF0C;<code>tuple</code>&#x7684;&#x7B2C;&#x4E8C;&#x7EF4;&#x5EA6;&#x8868;&#x793A;height&#x7684;&#x6570;&#x503C;&#xFF0C;<code>tuple</code>&#x7684;&#x7B2C;&#x4E09;&#x7EF4;&#x5EA6;&#x8868;&#x793A;width&#x7684;&#x6570;&#x503C;</p>
<p><strong>Parameters&#xFF1A;</strong>  </p>
<ul>
<li>in_channels(<code>int</code>) &#x2013; &#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x901A;&#x9053;</li>
<li>out_channels(<code>int</code>) &#x2013; &#x5377;&#x79EF;&#x4EA7;&#x751F;&#x7684;&#x901A;&#x9053;</li>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - &#x5377;&#x79EF;&#x6838;&#x7684;&#x5C3A;&#x5BF8;</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x5377;&#x79EF;&#x6B65;&#x957F;</li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x8FB9;&#x8865;&#x5145;0&#x7684;&#x5C42;&#x6570;</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) &#x2013; &#x5377;&#x79EF;&#x6838;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x95F4;&#x8DDD;</li>
<li>groups(<code>int</code>, <code>optional</code>) &#x2013; &#x4ECE;&#x8F93;&#x5165;&#x901A;&#x9053;&#x5230;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x7684;&#x963B;&#x585E;&#x8FDE;&#x63A5;&#x6570;</li>
<li>bias(<code>bool</code>, <code>optional</code>) - &#x5982;&#x679C;<code>bias=True</code>&#xFF0C;&#x6DFB;&#x52A0;&#x504F;&#x7F6E;</li>
</ul>
<p><strong>shape:</strong><br><code>input</code>: (N,C_in,D_in,H_in,W_in)<br><code>output</code>: (N,C_out,D_out,H_out,W_out)<br><script type="math/tex; ">D_{out}=floor((D_{in}+2*padding[0]-dilation[0]*(kernerl\_size[0]-1)-1)/stride[0]+1)</script>      </p>
<p><script type="math/tex; ">H_{out}=floor((H_{in}+2*padding[1]-dilation[2]*(kernerl\_size[1]-1)-1)/stride[1]+1)</script>    </p>
<p><script type="math/tex; ">W_{out}=floor((W_{in}+2*padding[2]-dilation[2]*(kernerl\_size[2]-1)-1)/stride[2]+1)</script>  </p>
<p><strong>&#x53D8;&#x91CF;:</strong>  </p>
<ul>
<li>weight(<code>tensor</code>) - &#x5377;&#x79EF;&#x7684;&#x6743;&#x91CD;&#xFF0C;shape&#x662F;(<code>out_channels</code>, <code>in_channels</code>,<code>kernel_size</code>)`</li>
<li>bias(<code>tensor</code>) - &#x5377;&#x79EF;&#x7684;&#x504F;&#x7F6E;&#x7CFB;&#x6570;&#xFF0C;shape&#x662F;&#xFF08;<code>out_channel</code>&#xFF09;  </li>
</ul>
<p><strong>example:</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With square kernels and equal stride</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Conv3d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># non-square kernels and unequal stride and with padding</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Conv3d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">4</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>, <span class="hljs-number">100</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="class-torchnnconvtranspose1dinchannels-outchannels-kernelsize-stride1-padding0-outputpadding0-groups1-biastrue">class torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True)</h3>
<p>1&#x7EF4;&#x7684;&#x89E3;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#xFF08;<code>transposed convolution operator</code>&#xFF0C;&#x6CE8;&#x610F;&#x6539;&#x89C6;&#x4F5C;&#x64CD;&#x4F5C;&#x53EF;&#x89C6;&#x4F5C;&#x89E3;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#xFF0C;&#x4F46;&#x5E76;&#x4E0D;&#x662F;&#x771F;&#x6B63;&#x7684;&#x89E3;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#xFF09;
&#x8BE5;&#x6A21;&#x5757;&#x53EF;&#x4EE5;&#x770B;&#x4F5C;&#x662F;<code>Conv1d</code>&#x76F8;&#x5BF9;&#x4E8E;&#x5176;&#x8F93;&#x5165;&#x7684;&#x68AF;&#x5EA6;&#xFF0C;&#x6709;&#x65F6;&#xFF08;&#x4F46;&#x4E0D;&#x6B63;&#x786E;&#x5730;&#xFF09;&#x88AB;&#x79F0;&#x4E3A;&#x89E3;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x3002;</p>
<p><strong>&#x6CE8;&#x610F;</strong><br>&#x7531;&#x4E8E;&#x5185;&#x6838;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x8F93;&#x5165;&#x7684;&#x6700;&#x540E;&#x7684;&#x4E00;&#x4E9B;&#x5217;&#x7684;&#x6570;&#x636E;&#x53EF;&#x80FD;&#x4F1A;&#x4E22;&#x5931;&#x3002;&#x56E0;&#x4E3A;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x662F;&#x4E0D;&#x662F;&#x5B8C;&#x5168;&#x7684;&#x4E92;&#x76F8;&#x5173;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x7528;&#x6237;&#x53EF;&#x4EE5;&#x8FDB;&#x884C;&#x9002;&#x5F53;&#x7684;&#x586B;&#x5145;&#xFF08;padding&#x64CD;&#x4F5C;&#xFF09;&#x3002;  </p>
<p><strong>&#x53C2;&#x6570;</strong></p>
<ul>
<li>in_channels(<code>int</code>) &#x2013; &#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x901A;&#x9053;&#x6570;</li>
<li>out_channels(<code>int</code>) &#x2013; &#x5377;&#x79EF;&#x4EA7;&#x751F;&#x7684;&#x901A;&#x9053;</li>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - &#x5377;&#x79EF;&#x6838;&#x7684;&#x5927;&#x5C0F;</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x5377;&#x79EF;&#x6B65;&#x957F;</li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x8FB9;&#x8865;&#x5145;0&#x7684;&#x5C42;&#x6570;</li>
<li>output_padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x8F93;&#x51FA;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x8FB9;&#x8865;&#x5145;0&#x7684;&#x5C42;&#x6570;</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) &#x2013; &#x5377;&#x79EF;&#x6838;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x95F4;&#x8DDD;</li>
<li>groups(<code>int</code>, <code>optional</code>) &#x2013; &#x4ECE;&#x8F93;&#x5165;&#x901A;&#x9053;&#x5230;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x7684;&#x963B;&#x585E;&#x8FDE;&#x63A5;&#x6570;</li>
<li>bias(<code>bool</code>, <code>optional</code>) - &#x5982;&#x679C;<code>bias=True</code>&#xFF0C;&#x6DFB;&#x52A0;&#x504F;&#x7F6E;</li>
</ul>
<p><strong>shape:</strong><br>&#x8F93;&#x5165;: (N,C_in,L_in)<br>&#x8F93;&#x51FA;: (N,C_out,L_out)<br><script type="math/tex; ">L_{out}=(L_{in}-1)*stride-2*padding+kernel\_size+output\_padding</script>  </p>
<p><strong>&#x53D8;&#x91CF;:</strong>  </p>
<ul>
<li>weight(<code>tensor</code>) - &#x5377;&#x79EF;&#x7684;&#x6743;&#x91CD;&#xFF0C;&#x5927;&#x5C0F;&#x662F;(<code>in_channels</code>, <code>in_channels</code>,<code>kernel_size</code>)  </li>
<li>bias(<code>tensor</code>) - &#x5377;&#x79EF;&#x7684;&#x504F;&#x7F6E;&#x7CFB;&#x6570;&#xFF0C;&#x5927;&#x5C0F;&#x662F;(<code>out_channel</code>)</li>
</ul>
<h3 id="class-torchnnconvtranspose2dinchannels-outchannels-kernelsize-stride1-padding0-outputpadding0-groups1-biastrue">class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True)</h3>
<p>2&#x7EF4;&#x7684;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#xFF08;<code>transposed convolution operator</code>&#xFF0C;&#x6CE8;&#x610F;&#x6539;&#x89C6;&#x4F5C;&#x64CD;&#x4F5C;&#x53EF;&#x89C6;&#x4F5C;&#x89E3;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#xFF0C;&#x4F46;&#x5E76;&#x4E0D;&#x662F;&#x771F;&#x6B63;&#x7684;&#x89E3;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#xFF09;
&#x8BE5;&#x6A21;&#x5757;&#x53EF;&#x4EE5;&#x770B;&#x4F5C;&#x662F;<code>Conv2d</code>&#x76F8;&#x5BF9;&#x4E8E;&#x5176;&#x8F93;&#x5165;&#x7684;&#x68AF;&#x5EA6;&#xFF0C;&#x6709;&#x65F6;&#xFF08;&#x4F46;&#x4E0D;&#x6B63;&#x786E;&#x5730;&#xFF09;&#x88AB;&#x79F0;&#x4E3A;&#x89E3;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x3002;</p>
<p><strong>&#x8BF4;&#x660E;</strong></p>
<p><code>stride</code>: &#x63A7;&#x5236;&#x76F8;&#x5173;&#x7CFB;&#x6570;&#x7684;&#x8BA1;&#x7B97;&#x6B65;&#x957F;<br><code>dilation</code>: &#x7528;&#x4E8E;&#x63A7;&#x5236;&#x5185;&#x6838;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#xFF0C;&#x8BE6;&#x7EC6;&#x63CF;&#x8FF0;&#x5728;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">&#x8FD9;&#x91CC;</a><br><code>groups</code>: &#x63A7;&#x5236;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x4E4B;&#x95F4;&#x7684;&#x8FDE;&#x63A5;&#xFF1A; <code>group=1</code>&#xFF0C;&#x8F93;&#x51FA;&#x662F;&#x6240;&#x6709;&#x7684;&#x8F93;&#x5165;&#x7684;&#x5377;&#x79EF;&#xFF1B;<code>group=2</code>&#xFF0C;&#x6B64;&#x65F6;&#x76F8;&#x5F53;&#x4E8E;&#x6709;&#x5E76;&#x6392;&#x7684;&#x4E24;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#xFF0C;&#x6BCF;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x8BA1;&#x7B97;&#x8F93;&#x5165;&#x901A;&#x9053;&#x7684;&#x4E00;&#x534A;&#xFF0C;&#x5E76;&#x4E14;&#x4EA7;&#x751F;&#x7684;&#x8F93;&#x51FA;&#x662F;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x7684;&#x4E00;&#x534A;&#xFF0C;&#x968F;&#x540E;&#x5C06;&#x8FD9;&#x4E24;&#x4E2A;&#x8F93;&#x51FA;&#x8FDE;&#x63A5;&#x8D77;&#x6765;&#x3002;     </p>
<p>&#x53C2;&#x6570;<code>kernel_size</code>&#xFF0C;<code>stride</code>&#xFF0C;<code>padding</code>&#xFF0C;<code>dilation</code>&#x6570;&#x636E;&#x7C7B;&#x578B;&#xFF1A;
&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;<code>int</code>&#x7C7B;&#x578B;&#x7684;&#x6570;&#x636E;&#xFF0C;&#x6B64;&#x65F6;&#x5377;&#x79EF;height&#x548C;width&#x503C;&#x76F8;&#x540C;;
&#x4E5F;&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;<code>tuple</code>&#x6570;&#x7EC4;&#xFF08;&#x5305;&#x542B;&#x6765;&#x4E24;&#x4E2A;<code>int</code>&#x7C7B;&#x578B;&#x7684;&#x6570;&#x636E;&#xFF09;&#xFF0C;&#x7B2C;&#x4E00;&#x4E2A;<code>int</code>&#x6570;&#x636E;&#x8868;&#x793A;<code>height</code>&#x7684;&#x6570;&#x503C;&#xFF0C;&#x7B2C;&#x4E8C;&#x4E2A;<code>int</code>&#x7C7B;&#x578B;&#x7684;&#x6570;&#x636E;&#x8868;&#x793A;width&#x7684;&#x6570;&#x503C;</p>
<p><strong>&#x6CE8;&#x610F;</strong><br>&#x7531;&#x4E8E;&#x5185;&#x6838;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x8F93;&#x5165;&#x7684;&#x6700;&#x540E;&#x7684;&#x4E00;&#x4E9B;&#x5217;&#x7684;&#x6570;&#x636E;&#x53EF;&#x80FD;&#x4F1A;&#x4E22;&#x5931;&#x3002;&#x56E0;&#x4E3A;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x662F;&#x4E0D;&#x662F;&#x5B8C;&#x5168;&#x7684;&#x4E92;&#x76F8;&#x5173;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x7528;&#x6237;&#x53EF;&#x4EE5;&#x8FDB;&#x884C;&#x9002;&#x5F53;&#x7684;&#x586B;&#x5145;&#xFF08;<code>padding</code>&#x64CD;&#x4F5C;&#xFF09;&#x3002;</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li>in_channels(<code>int</code>) &#x2013; &#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x901A;&#x9053;&#x6570;</li>
<li>out_channels(<code>int</code>) &#x2013; &#x5377;&#x79EF;&#x4EA7;&#x751F;&#x7684;&#x901A;&#x9053;&#x6570;</li>
<li>kerner_size(<code>int</code> or <code>tuple</code>) - &#x5377;&#x79EF;&#x6838;&#x7684;&#x5927;&#x5C0F;</li>
<li>stride(<code>int</code> or <code>tuple</code>,<code>optional</code>) - &#x5377;&#x79EF;&#x6B65;&#x957F;</li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x8FB9;&#x8865;&#x5145;0&#x7684;&#x5C42;&#x6570;</li>
<li>output_padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x8F93;&#x51FA;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x8FB9;&#x8865;&#x5145;0&#x7684;&#x5C42;&#x6570;</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) &#x2013; &#x5377;&#x79EF;&#x6838;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x95F4;&#x8DDD;</li>
<li>groups(<code>int</code>, <code>optional</code>) &#x2013; &#x4ECE;&#x8F93;&#x5165;&#x901A;&#x9053;&#x5230;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x7684;&#x963B;&#x585E;&#x8FDE;&#x63A5;&#x6570;</li>
<li>bias(<code>bool</code>, <code>optional</code>) - &#x5982;&#x679C;<code>bias=True</code>&#xFF0C;&#x6DFB;&#x52A0;&#x504F;&#x7F6E;</li>
</ul>
<p><strong>shape:</strong><br>&#x8F93;&#x5165;: (N,C_in,H_in&#xFF0C;W_in)<br>&#x8F93;&#x51FA;: (N,C_out,H_out,W_out)<br><script type="math/tex; ">H_{out}=(H_{in}-1)*stride[0]-2*padding[0]+kernel\_size[0]+output\_padding[0]</script>    </p>
<p><script type="math/tex; ">W_{out}=(W_{in}-1)*stride[1]-2*padding[1]+kernel\_size[1]+output\_padding[1]</script></p>
<p><strong>&#x53D8;&#x91CF;:</strong>     </p>
<ul>
<li>weight(<code>tensor</code>) - &#x5377;&#x79EF;&#x7684;&#x6743;&#x91CD;&#xFF0C;&#x5927;&#x5C0F;&#x662F;(<code>in_channels</code>, <code>in_channels</code>,<code>kernel_size</code>)    </li>
<li>bias(<code>tensor</code>) - &#x5377;&#x79EF;&#x7684;&#x504F;&#x7F6E;&#x7CFB;&#x6570;&#xFF0C;&#x5927;&#x5C0F;&#x662F;&#xFF08;<code>out_channel</code>&#xFF09;  </li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With square kernels and equal stride</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ConvTranspose2d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># non-square kernels and unequal stride and with padding</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ConvTranspose2d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">5</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">4</span>, <span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>, <span class="hljs-number">100</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># exact output size can be also specified as an argument</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>downsample = nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>upsample = nn.ConvTranspose2d(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>h = downsample(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>h.size()
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>output = upsample(h, output_size=input.size())
<span class="hljs-meta">&gt;&gt;&gt; </span>output.size()
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])
</code></pre>
<h3 id="torchnnconvtranspose3dinchannels-outchannels-kernelsize-stride1-padding0-outputpadding0-groups1-biastrue">torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True)</h3>
<p>3&#x7EF4;&#x7684;&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#xFF08;<code>transposed convolution operator</code>&#xFF0C;&#x6CE8;&#x610F;&#x6539;&#x89C6;&#x4F5C;&#x64CD;&#x4F5C;&#x53EF;&#x89C6;&#x4F5C;&#x89E3;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#xFF0C;&#x4F46;&#x5E76;&#x4E0D;&#x662F;&#x771F;&#x6B63;&#x7684;&#x89E3;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#xFF09;
&#x8F6C;&#x7F6E;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x5C06;&#x6BCF;&#x4E2A;&#x8F93;&#x5165;&#x503C;&#x548C;&#x4E00;&#x4E2A;&#x53EF;&#x5B66;&#x4E60;&#x6743;&#x91CD;&#x7684;&#x5377;&#x79EF;&#x6838;&#x76F8;&#x4E58;&#xFF0C;&#x8F93;&#x51FA;&#x6240;&#x6709;&#x8F93;&#x5165;&#x901A;&#x9053;&#x7684;&#x6C42;&#x548C;</p>
<p>&#x8BE5;&#x6A21;&#x5757;&#x53EF;&#x4EE5;&#x770B;&#x4F5C;&#x662F;<code>Conv3d</code>&#x76F8;&#x5BF9;&#x4E8E;&#x5176;&#x8F93;&#x5165;&#x7684;&#x68AF;&#x5EA6;&#xFF0C;&#x6709;&#x65F6;&#xFF08;&#x4F46;&#x4E0D;&#x6B63;&#x786E;&#x5730;&#xFF09;&#x88AB;&#x79F0;&#x4E3A;&#x89E3;&#x5377;&#x79EF;&#x64CD;&#x4F5C;&#x3002;</p>
<p><strong>&#x8BF4;&#x660E;</strong></p>
<p><code>stride</code>: &#x63A7;&#x5236;&#x76F8;&#x5173;&#x7CFB;&#x6570;&#x7684;&#x8BA1;&#x7B97;&#x6B65;&#x957F;<br><code>dilation</code>: &#x7528;&#x4E8E;&#x63A7;&#x5236;&#x5185;&#x6838;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#xFF0C;&#x8BE6;&#x7EC6;&#x63CF;&#x8FF0;&#x5728;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">&#x8FD9;&#x91CC;</a><br><code>groups</code>: &#x63A7;&#x5236;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x4E4B;&#x95F4;&#x7684;&#x8FDE;&#x63A5;&#xFF1A; <code>group=1</code>&#xFF0C;&#x8F93;&#x51FA;&#x662F;&#x6240;&#x6709;&#x7684;&#x8F93;&#x5165;&#x7684;&#x5377;&#x79EF;&#xFF1B;<code>group=2</code>&#xFF0C;&#x6B64;&#x65F6;&#x76F8;&#x5F53;&#x4E8E;&#x6709;&#x5E76;&#x6392;&#x7684;&#x4E24;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#xFF0C;&#x6BCF;&#x4E2A;&#x5377;&#x79EF;&#x5C42;&#x8BA1;&#x7B97;&#x8F93;&#x5165;&#x901A;&#x9053;&#x7684;&#x4E00;&#x534A;&#xFF0C;&#x5E76;&#x4E14;&#x4EA7;&#x751F;&#x7684;&#x8F93;&#x51FA;&#x662F;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x7684;&#x4E00;&#x534A;&#xFF0C;&#x968F;&#x540E;&#x5C06;&#x8FD9;&#x4E24;&#x4E2A;&#x8F93;&#x51FA;&#x8FDE;&#x63A5;&#x8D77;&#x6765;&#x3002;     </p>
<p>&#x53C2;&#x6570;<code>kernel\_size</code>&#xFF0C;<code>stride</code>, <code>padding</code>&#xFF0C;<code>dilation</code>&#x6570;&#x636E;&#x7C7B;&#x578B;&#xFF1A;
&#x4E00;&#x4E2A;<code>int</code>&#x7C7B;&#x578B;&#x7684;&#x6570;&#x636E;&#xFF0C;&#x6B64;&#x65F6;&#x5377;&#x79EF;height&#x548C;width&#x503C;&#x76F8;&#x540C;;
&#x4E5F;&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;<code>tuple</code>&#x6570;&#x7EC4;&#xFF08;&#x5305;&#x542B;&#x6765;&#x4E24;&#x4E2A;<code>int</code>&#x7C7B;&#x578B;&#x7684;&#x6570;&#x636E;&#xFF09;&#xFF0C;&#x7B2C;&#x4E00;&#x4E2A;<code>int</code>&#x6570;&#x636E;&#x8868;&#x793A;height&#x7684;&#x6570;&#x503C;&#xFF0C;tuple&#x7684;&#x7B2C;&#x4E8C;&#x4E2A;int&#x7C7B;&#x578B;&#x7684;&#x6570;&#x636E;&#x8868;&#x793A;width&#x7684;&#x6570;&#x503C;</p>
<p><strong>&#x6CE8;&#x610F;</strong><br>&#x7531;&#x4E8E;&#x5185;&#x6838;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x8F93;&#x5165;&#x7684;&#x6700;&#x540E;&#x7684;&#x4E00;&#x4E9B;&#x5217;&#x7684;&#x6570;&#x636E;&#x53EF;&#x80FD;&#x4F1A;&#x4E22;&#x5931;&#x3002;&#x56E0;&#x4E3A;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x662F;&#x4E0D;&#x662F;&#x5B8C;&#x5168;&#x7684;&#x4E92;&#x76F8;&#x5173;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x7528;&#x6237;&#x53EF;&#x4EE5;&#x8FDB;&#x884C;&#x9002;&#x5F53;&#x7684;&#x586B;&#x5145;&#xFF08;padding&#x64CD;&#x4F5C;&#xFF09;&#x3002;</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li>in_channels(<code>int</code>) &#x2013; &#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x901A;&#x9053;&#x6570;</li>
<li>out_channels(<code>int</code>) &#x2013; &#x5377;&#x79EF;&#x4EA7;&#x751F;&#x7684;&#x901A;&#x9053;&#x6570;</li>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - &#x5377;&#x79EF;&#x6838;&#x7684;&#x5927;&#x5C0F;</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x5377;&#x79EF;&#x6B65;&#x957F;</li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x8FB9;&#x8865;&#x5145;0&#x7684;&#x5C42;&#x6570;</li>
<li>output_padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x8F93;&#x51FA;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x8FB9;&#x8865;&#x5145;0&#x7684;&#x5C42;&#x6570;</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) &#x2013; &#x5377;&#x79EF;&#x6838;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x7684;&#x95F4;&#x8DDD;</li>
<li>groups(<code>int</code>, <code>optional</code>) &#x2013; &#x4ECE;&#x8F93;&#x5165;&#x901A;&#x9053;&#x5230;&#x8F93;&#x51FA;&#x901A;&#x9053;&#x7684;&#x963B;&#x585E;&#x8FDE;&#x63A5;&#x6570;</li>
<li>bias(<code>bool</code>, <code>optional</code>) - &#x5982;&#x679C;<code>bias=True</code>&#xFF0C;&#x6DFB;&#x52A0;&#x504F;&#x7F6E;</li>
</ul>
<p><strong>shape:</strong><br>&#x8F93;&#x5165;: (N,C_in,H_in&#xFF0C;W_in)<br>&#x8F93;&#x51FA;: (N,C_out,H_out,W_out)<br><script type="math/tex; ">D_{out}=(D_{in}-1)*stride[0]-2*padding[0]+kernel\_size[0]+output\_padding[0]</script>  </p>
<p><script type="math/tex; ">H_{out}=(H_{in}-1)*stride[1]-2*padding[1]+kernel\_size[1]+output\_padding[0]</script>     </p>
<p><script type="math/tex; ">W_{out}=(W_{in}-1)*stride[2]-2*padding[2]+kernel\_size[2]+output\_padding[2]</script></p>
<p><strong>&#x53D8;&#x91CF;:</strong>  </p>
<ul>
<li>weight(<code>tensor</code>) - &#x5377;&#x79EF;&#x7684;&#x6743;&#x91CD;&#xFF0C;&#x5927;&#x5C0F;&#x662F;(<code>in_channels</code>, <code>in_channels</code>,<code>kernel_size</code>)  </li>
<li>bias(<code>tensor</code>) - &#x5377;&#x79EF;&#x7684;&#x504F;&#x7F6E;&#x7CFB;&#x6570;&#xFF0C;&#x5927;&#x5C0F;&#x662F;&#xFF08;<code>out_channel</code>&#xFF09;</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With square kernels and equal stride</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ConvTranspose3d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># non-square kernels and unequal stride and with padding</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Conv3d(<span class="hljs-number">16</span>, <span class="hljs-number">33</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>, <span class="hljs-number">100</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h2 id="&#x6C60;&#x5316;&#x5C42;">&#x6C60;&#x5316;&#x5C42;</h2>
<h3 id="class-torchnnmaxpool1dkernelsize-stridenone-padding0-dilation1-returnindicesfalse-ceilmodefalse">class torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</h3>
<p>&#x5BF9;&#x4E8E;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x8F93;&#x5165;&#x901A;&#x9053;&#xFF0C;&#x63D0;&#x4F9B;1&#x7EF4;&#x6700;&#x5927;&#x6C60;&#x5316;&#xFF08;<code>max pooling</code>&#xFF09;&#x64CD;&#x4F5C;</p>
<p>&#x5982;&#x679C;&#x8F93;&#x5165;&#x7684;&#x5927;&#x5C0F;&#x662F;(N,C,L)&#xFF0C;&#x90A3;&#x4E48;&#x8F93;&#x51FA;&#x7684;&#x5927;&#x5C0F;&#x662F;(N,C,L_out)&#x7684;&#x8BA1;&#x7B97;&#x65B9;&#x5F0F;&#x662F;&#xFF1A;<br><script type="math/tex; ">out(N_i, C_j,k)=max^{kernel\_size-1}_{m=0}input(N_{i},C_j,stride*k+m)</script>    </p>
<p>&#x5982;&#x679C;<code>padding</code>&#x4E0D;&#x662F;0&#xFF0C;&#x4F1A;&#x5728;&#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x8FB9;&#x6DFB;&#x52A0;&#x76F8;&#x5E94;&#x6570;&#x76EE;0<br><code>dilation</code>&#x7528;&#x4E8E;&#x63A7;&#x5236;&#x5185;&#x6838;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#xFF0C;&#x8BE6;&#x7EC6;&#x63CF;&#x8FF0;&#x5728;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">&#x8FD9;&#x91CC;</a>    </p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - max pooling&#x7684;&#x7A97;&#x53E3;&#x5927;&#x5C0F;</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - max pooling&#x7684;&#x7A97;&#x53E3;&#x79FB;&#x52A8;&#x7684;&#x6B65;&#x957F;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#x662F;<code>kernel_size</code></li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x8FB9;&#x8865;&#x5145;0&#x7684;&#x5C42;&#x6570;</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) &#x2013; &#x4E00;&#x4E2A;&#x63A7;&#x5236;&#x7A97;&#x53E3;&#x4E2D;&#x5143;&#x7D20;&#x6B65;&#x5E45;&#x7684;&#x53C2;&#x6570;</li>
<li>return_indices - &#x5982;&#x679C;&#x7B49;&#x4E8E;<code>True</code>&#xFF0C;&#x4F1A;&#x8FD4;&#x56DE;&#x8F93;&#x51FA;&#x6700;&#x5927;&#x503C;&#x7684;&#x5E8F;&#x53F7;&#xFF0C;&#x5BF9;&#x4E8E;&#x4E0A;&#x91C7;&#x6837;&#x64CD;&#x4F5C;&#x4F1A;&#x6709;&#x5E2E;&#x52A9;</li>
<li>ceil_mode - &#x5982;&#x679C;&#x7B49;&#x4E8E;<code>True</code>&#xFF0C;&#x8BA1;&#x7B97;&#x8F93;&#x51FA;&#x4FE1;&#x53F7;&#x5927;&#x5C0F;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4F1A;&#x4F7F;&#x7528;&#x5411;&#x4E0A;&#x53D6;&#x6574;&#xFF0C;&#x4EE3;&#x66FF;&#x9ED8;&#x8BA4;&#x7684;&#x5411;&#x4E0B;&#x53D6;&#x6574;&#x7684;&#x64CD;&#x4F5C;</li>
</ul>
<p><strong>shape:</strong><br>&#x8F93;&#x5165;: (N,C_in,L_in)<br>&#x8F93;&#x51FA;: (N,C_out,L_out)<br><script type="math/tex; ">L_{out}=floor((L_{in} + 2*padding - dilation*(kernel\_size - 1) - 1)/stride + 1</script>  </p>
<p><strong>example:</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of size=3, stride=2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.MaxPool1d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="class-torchnnmaxpool2dkernelsize-stridenone-padding0-dilation1-returnindicesfalse-ceilmodefalse">class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</h3>
<p>&#x5BF9;&#x4E8E;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x8F93;&#x5165;&#x901A;&#x9053;&#xFF0C;&#x63D0;&#x4F9B;2&#x7EF4;&#x6700;&#x5927;&#x6C60;&#x5316;&#xFF08;<code>max pooling</code>&#xFF09;&#x64CD;&#x4F5C;</p>
<p>&#x5982;&#x679C;&#x8F93;&#x5165;&#x7684;&#x5927;&#x5C0F;&#x662F;(N,C,H,W)&#xFF0C;&#x90A3;&#x4E48;&#x8F93;&#x51FA;&#x7684;&#x5927;&#x5C0F;&#x662F;(N,C,H_out,W_out)&#x548C;&#x6C60;&#x5316;&#x7A97;&#x53E3;&#x5927;&#x5C0F;(kH,kW)&#x7684;&#x5173;&#x7CFB;&#x662F;&#xFF1A;<br><script type="math/tex; ">out(N_i, C_j,k)=max^{kH-1}_{m=0}max^{kW-1}_{m=0}input(N_{i},C_j,stride[0]*h+m,stride[1]*w+n)</script>   </p>
<p>&#x5982;&#x679C;<code>padding</code>&#x4E0D;&#x662F;0&#xFF0C;&#x4F1A;&#x5728;&#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x8FB9;&#x6DFB;&#x52A0;&#x76F8;&#x5E94;&#x6570;&#x76EE;0<br><code>dilation</code>&#x7528;&#x4E8E;&#x63A7;&#x5236;&#x5185;&#x6838;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#xFF0C;&#x8BE6;&#x7EC6;&#x63CF;&#x8FF0;&#x5728;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">&#x8FD9;&#x91CC;</a>    </p>
<p>&#x53C2;&#x6570;<code>kernel_size</code>&#xFF0C;<code>stride</code>, <code>padding</code>&#xFF0C;<code>dilation</code>&#x6570;&#x636E;&#x7C7B;&#x578B;&#xFF1A;
&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;<code>int</code>&#x7C7B;&#x578B;&#x7684;&#x6570;&#x636E;&#xFF0C;&#x6B64;&#x65F6;&#x5377;&#x79EF;height&#x548C;width&#x503C;&#x76F8;&#x540C;;
&#x4E5F;&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;<code>tuple</code>&#x6570;&#x7EC4;&#xFF08;&#x5305;&#x542B;&#x6765;&#x4E24;&#x4E2A;int&#x7C7B;&#x578B;&#x7684;&#x6570;&#x636E;&#xFF09;&#xFF0C;&#x7B2C;&#x4E00;&#x4E2A;<code>int</code>&#x6570;&#x636E;&#x8868;&#x793A;height&#x7684;&#x6570;&#x503C;&#xFF0C;<code>tuple</code>&#x7684;&#x7B2C;&#x4E8C;&#x4E2A;int&#x7C7B;&#x578B;&#x7684;&#x6570;&#x636E;&#x8868;&#x793A;width&#x7684;&#x6570;&#x503C;</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - max pooling&#x7684;&#x7A97;&#x53E3;&#x5927;&#x5C0F;</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - max pooling&#x7684;&#x7A97;&#x53E3;&#x79FB;&#x52A8;&#x7684;&#x6B65;&#x957F;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#x662F;<code>kernel_size</code></li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x8FB9;&#x8865;&#x5145;0&#x7684;&#x5C42;&#x6570;</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) &#x2013; &#x4E00;&#x4E2A;&#x63A7;&#x5236;&#x7A97;&#x53E3;&#x4E2D;&#x5143;&#x7D20;&#x6B65;&#x5E45;&#x7684;&#x53C2;&#x6570;</li>
<li>return_indices - &#x5982;&#x679C;&#x7B49;&#x4E8E;<code>True</code>&#xFF0C;&#x4F1A;&#x8FD4;&#x56DE;&#x8F93;&#x51FA;&#x6700;&#x5927;&#x503C;&#x7684;&#x5E8F;&#x53F7;&#xFF0C;&#x5BF9;&#x4E8E;&#x4E0A;&#x91C7;&#x6837;&#x64CD;&#x4F5C;&#x4F1A;&#x6709;&#x5E2E;&#x52A9;</li>
<li>ceil_mode - &#x5982;&#x679C;&#x7B49;&#x4E8E;<code>True</code>&#xFF0C;&#x8BA1;&#x7B97;&#x8F93;&#x51FA;&#x4FE1;&#x53F7;&#x5927;&#x5C0F;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4F1A;&#x4F7F;&#x7528;&#x5411;&#x4E0A;&#x53D6;&#x6574;&#xFF0C;&#x4EE3;&#x66FF;&#x9ED8;&#x8BA4;&#x7684;&#x5411;&#x4E0B;&#x53D6;&#x6574;&#x7684;&#x64CD;&#x4F5C;</li>
</ul>
<p><strong>shape:</strong><br>&#x8F93;&#x5165;: (N,C,H_{in},W_in)<br>&#x8F93;&#x51FA;: (N,C,H_out,W_out)<br><script type="math/tex; ">H_{out}=floor((H_{in} + 2*padding[0] - dilation[0]*(kernel\_size[0] - 1) - 1)/stride[0] + 1</script>   </p>
<p><script type="math/tex; ">W_{out}=floor((W_{in} + 2*padding[1] - dilation[1]*(kernel\_size[1] - 1) - 1)/stride[1] + 1</script>      </p>
<p><strong>example:</strong>  </p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of square window of size=3, stride=2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of non-square window</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.MaxPool2d((<span class="hljs-number">3</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>, <span class="hljs-number">32</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="class-torchnnmaxpool3dkernelsize-stridenone-padding0-dilation1-returnindicesfalse-ceilmodefalse">class torch.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</h3>
<p>&#x5BF9;&#x4E8E;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x8F93;&#x5165;&#x901A;&#x9053;&#xFF0C;&#x63D0;&#x4F9B;3&#x7EF4;&#x6700;&#x5927;&#x6C60;&#x5316;&#xFF08;max pooling&#xFF09;&#x64CD;&#x4F5C;</p>
<p>&#x5982;&#x679C;&#x8F93;&#x5165;&#x7684;&#x5927;&#x5C0F;&#x662F;(N,C,D,H,W)&#xFF0C;&#x90A3;&#x4E48;&#x8F93;&#x51FA;&#x7684;&#x5927;&#x5C0F;&#x662F;(N,C,D,H_out,W_out)&#x548C;&#x6C60;&#x5316;&#x7A97;&#x53E3;&#x5927;&#x5C0F;(kD,kH,kW)&#x7684;&#x5173;&#x7CFB;&#x662F;&#xFF1A;<br><script type="math/tex; ">out(N_i,C_j,d,h,w)=max^{kD-1}_{m=0}max^{kH-1}_{m=0}max^{kW-1}_{m=0}</script>   </p>
<p><script type="math/tex; ">input(N_{i},C_j,stride[0]*k+d,stride[1]*h+m,stride[2]*w+n)</script>  </p>
<p>&#x5982;&#x679C;<code>padding</code>&#x4E0D;&#x662F;0&#xFF0C;&#x4F1A;&#x5728;&#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x8FB9;&#x6DFB;&#x52A0;&#x76F8;&#x5E94;&#x6570;&#x76EE;0<br><code>dilation</code>&#x7528;&#x4E8E;&#x63A7;&#x5236;&#x5185;&#x6838;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#xFF0C;&#x8BE6;&#x7EC6;&#x63CF;&#x8FF0;&#x5728;<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank">&#x8FD9;&#x91CC;</a>      </p>
<p>&#x53C2;&#x6570;<code>kernel_size</code>&#xFF0C;<code>stride</code>, <code>padding</code>&#xFF0C;<code>dilation</code>&#x6570;&#x636E;&#x7C7B;&#x578B;&#xFF1A;
&#x53EF;&#x4EE5;&#x662F;<code>int</code>&#x7C7B;&#x578B;&#x7684;&#x6570;&#x636E;&#xFF0C;&#x6B64;&#x65F6;&#x5377;&#x79EF;height&#x548C;width&#x503C;&#x76F8;&#x540C;;
&#x4E5F;&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;<code>tuple</code>&#x6570;&#x7EC4;&#xFF08;&#x5305;&#x542B;&#x6765;&#x4E24;&#x4E2A;<code>int</code>&#x7C7B;&#x578B;&#x7684;&#x6570;&#x636E;&#xFF09;&#xFF0C;&#x7B2C;&#x4E00;&#x4E2A;<code>int</code>&#x6570;&#x636E;&#x8868;&#x793A;height&#x7684;&#x6570;&#x503C;&#xFF0C;<code>tuple</code>&#x7684;&#x7B2C;&#x4E8C;&#x4E2A;<code>int</code>&#x7C7B;&#x578B;&#x7684;&#x6570;&#x636E;&#x8868;&#x793A;width&#x7684;&#x6570;&#x503C;</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - max pooling&#x7684;&#x7A97;&#x53E3;&#x5927;&#x5C0F;</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - max pooling&#x7684;&#x7A97;&#x53E3;&#x79FB;&#x52A8;&#x7684;&#x6B65;&#x957F;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#x662F;kernel_size</li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x8FB9;&#x8865;&#x5145;0&#x7684;&#x5C42;&#x6570;</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) &#x2013; &#x4E00;&#x4E2A;&#x63A7;&#x5236;&#x7A97;&#x53E3;&#x4E2D;&#x5143;&#x7D20;&#x6B65;&#x5E45;&#x7684;&#x53C2;&#x6570;</li>
<li>return_indices - &#x5982;&#x679C;&#x7B49;&#x4E8E;<code>True</code>&#xFF0C;&#x4F1A;&#x8FD4;&#x56DE;&#x8F93;&#x51FA;&#x6700;&#x5927;&#x503C;&#x7684;&#x5E8F;&#x53F7;&#xFF0C;&#x5BF9;&#x4E8E;&#x4E0A;&#x91C7;&#x6837;&#x64CD;&#x4F5C;&#x4F1A;&#x6709;&#x5E2E;&#x52A9;</li>
<li>ceil_mode - &#x5982;&#x679C;&#x7B49;&#x4E8E;<code>True</code>&#xFF0C;&#x8BA1;&#x7B97;&#x8F93;&#x51FA;&#x4FE1;&#x53F7;&#x5927;&#x5C0F;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4F1A;&#x4F7F;&#x7528;&#x5411;&#x4E0A;&#x53D6;&#x6574;&#xFF0C;&#x4EE3;&#x66FF;&#x9ED8;&#x8BA4;&#x7684;&#x5411;&#x4E0B;&#x53D6;&#x6574;&#x7684;&#x64CD;&#x4F5C;</li>
</ul>
<p><strong>shape:</strong><br>&#x8F93;&#x5165;: (N,C,H_in,W_in)<br>&#x8F93;&#x51FA;: (N,C,H_out,W_out)<br><script type="math/tex; ">D_{out}=floor((D_{in} + 2*padding[0] - dilation[0]*(kernel\_size[0] - 1) - 1)/stride[0] + 1)</script>   </p>
<p><script type="math/tex; ">H_{out}=floor((H_{in} + 2*padding[1] - dilation[1]*(kernel\_size[0] - 1) - 1)/stride[1] + 1)</script></p>
<p><script type="math/tex; ">W_{out}=floor((W_{in} + 2*padding[2] - dilation[2]*(kernel\_size[2] - 1) - 1)/stride[2] + 1)</script>  </p>
<p><strong>example:</strong>  </p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of square window of size=3, stride=2</span>
&gt;&gt;&gt;m = nn.MaxPool3d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of non-square window</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.MaxPool3d((<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>,<span class="hljs-number">44</span>, <span class="hljs-number">31</span>))  
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h4 id="class-torchnnmaxunpool1dkernelsize-stridenone-padding0">class torch.nn.MaxUnpool1d(kernel_size, stride=None, padding=0)</h4>
<p><code>Maxpool1d</code>&#x7684;&#x9006;&#x8FC7;&#x7A0B;&#xFF0C;&#x4E0D;&#x8FC7;&#x5E76;&#x4E0D;&#x662F;&#x5B8C;&#x5168;&#x7684;&#x9006;&#x8FC7;&#x7A0B;&#xFF0C;&#x56E0;&#x4E3A;&#x5728;<code>maxpool1d</code>&#x7684;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x4E00;&#x4E9B;&#x6700;&#x5927;&#x503C;&#x7684;&#x5DF2;&#x7ECF;&#x4E22;&#x5931;&#x3002;
<code>MaxUnpool1d</code>&#x8F93;&#x5165;<code>MaxPool1d</code>&#x7684;&#x8F93;&#x51FA;&#xFF0C;&#x5305;&#x62EC;&#x6700;&#x5927;&#x503C;&#x7684;&#x7D22;&#x5F15;&#xFF0C;&#x5E76;&#x8BA1;&#x7B97;&#x6240;&#x6709;<code>maxpool1d</code>&#x8FC7;&#x7A0B;&#x4E2D;&#x975E;&#x6700;&#x5927;&#x503C;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;&#x96F6;&#x7684;&#x90E8;&#x5206;&#x7684;&#x53CD;&#x5411;&#x3002;</p>
<p><strong>&#x6CE8;&#x610F;&#xFF1A;</strong><br><code>MaxPool1d</code>&#x53EF;&#x4EE5;&#x5C06;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x6620;&#x5C04;&#x5230;&#x76F8;&#x540C;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x53CD;&#x6F14;&#x8FC7;&#x7A0B;&#x53EF;&#x80FD;&#x4F1A;&#x53D8;&#x5F97;&#x6A21;&#x68F1;&#x4E24;&#x53EF;&#x3002; &#x4E3A;&#x4E86;&#x9002;&#x5E94;&#x8FD9;&#x4E00;&#x70B9;&#xFF0C;&#x53EF;&#x4EE5;&#x5728;&#x8C03;&#x7528;&#x4E2D;&#x5C06;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#xFF08;<code>output_size</code>&#xFF09;&#x4F5C;&#x4E3A;&#x989D;&#x5916;&#x7684;&#x53C2;&#x6570;&#x4F20;&#x5165;&#x3002; &#x5177;&#x4F53;&#x7528;&#x6CD5;&#xFF0C;&#x8BF7;&#x53C2;&#x9605;&#x4E0B;&#x9762;&#x7684;&#x8F93;&#x5165;&#x548C;&#x793A;&#x4F8B;</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - max pooling&#x7684;&#x7A97;&#x53E3;&#x5927;&#x5C0F;</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - max pooling&#x7684;&#x7A97;&#x53E3;&#x79FB;&#x52A8;&#x7684;&#x6B65;&#x957F;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#x662F;<code>kernel_size</code></li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x8FB9;&#x8865;&#x5145;0&#x7684;&#x5C42;&#x6570;</li>
</ul>
<p><strong>&#x8F93;&#x5165;&#xFF1A;</strong><br><code>input</code>:&#x9700;&#x8981;&#x8F6C;&#x6362;&#x7684;<code>tensor</code>
<code>indices</code>&#xFF1A;Maxpool1d&#x7684;&#x7D22;&#x5F15;&#x53F7;
<code>output_size</code>:&#x4E00;&#x4E2A;&#x6307;&#x5B9A;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x7684;<code>torch.Size</code></p>
<p><strong>shape:</strong><br><code>input</code>: (N,C,H_in)<br><code>output</code>:(N,C,H_out)<br><script type="math/tex; ">H_{out}=(H_{in}-1)*stride[0]-2*padding[0]+kernel\_size[0]</script><br>&#x4E5F;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;<code>output_size</code>&#x6307;&#x5B9A;&#x8F93;&#x51FA;&#x7684;&#x5927;&#x5C0F;</p>
<p><strong>Example&#xFF1A;</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>pool = nn.MaxPool1d(<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, return_indices=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>unpool = nn.MaxUnpool1d(<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = Variable(torch.Tensor([[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>]]]))
<span class="hljs-meta">&gt;&gt;&gt; </span>output, indices = pool(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>unpool(output, indices)
    Variable containing:
    (<span class="hljs-number">0</span> ,.,.) =
       <span class="hljs-number">0</span>   <span class="hljs-number">2</span>   <span class="hljs-number">0</span>   <span class="hljs-number">4</span>   <span class="hljs-number">0</span>   <span class="hljs-number">6</span>   <span class="hljs-number">0</span>   <span class="hljs-number">8</span>
    [torch.FloatTensor of size <span class="hljs-number">1</span>x1x8]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Example showcasing the use of output_size</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input = Variable(torch.Tensor([[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]]]))
<span class="hljs-meta">&gt;&gt;&gt; </span>output, indices = pool(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>unpool(output, indices, output_size=input.size())
    Variable containing:
    (<span class="hljs-number">0</span> ,.,.) =
       <span class="hljs-number">0</span>   <span class="hljs-number">2</span>   <span class="hljs-number">0</span>   <span class="hljs-number">4</span>   <span class="hljs-number">0</span>   <span class="hljs-number">6</span>   <span class="hljs-number">0</span>   <span class="hljs-number">8</span>   <span class="hljs-number">0</span>
    [torch.FloatTensor of size <span class="hljs-number">1</span>x1x9]
<span class="hljs-meta">&gt;&gt;&gt; </span>unpool(output, indices)
    Variable containing:
    (<span class="hljs-number">0</span> ,.,.) =
       <span class="hljs-number">0</span>   <span class="hljs-number">2</span>   <span class="hljs-number">0</span>   <span class="hljs-number">4</span>   <span class="hljs-number">0</span>   <span class="hljs-number">6</span>   <span class="hljs-number">0</span>   <span class="hljs-number">8</span>
    [torch.FloatTensor of size <span class="hljs-number">1</span>x1x8]
</code></pre>
<h4 id="class-torchnnmaxunpool2dkernelsize-stridenone-padding0">class torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0)</h4>
<p><code>Maxpool2d</code>&#x7684;&#x9006;&#x8FC7;&#x7A0B;&#xFF0C;&#x4E0D;&#x8FC7;&#x5E76;&#x4E0D;&#x662F;&#x5B8C;&#x5168;&#x7684;&#x9006;&#x8FC7;&#x7A0B;&#xFF0C;&#x56E0;&#x4E3A;&#x5728;maxpool2d&#x7684;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x4E00;&#x4E9B;&#x6700;&#x5927;&#x503C;&#x7684;&#x5DF2;&#x7ECF;&#x4E22;&#x5931;&#x3002;
<code>MaxUnpool2d</code>&#x7684;&#x8F93;&#x5165;&#x662F;<code>MaxPool2d</code>&#x7684;&#x8F93;&#x51FA;&#xFF0C;&#x5305;&#x62EC;&#x6700;&#x5927;&#x503C;&#x7684;&#x7D22;&#x5F15;&#xFF0C;&#x5E76;&#x8BA1;&#x7B97;&#x6240;&#x6709;<code>maxpool2d</code>&#x8FC7;&#x7A0B;&#x4E2D;&#x975E;&#x6700;&#x5927;&#x503C;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;&#x96F6;&#x7684;&#x90E8;&#x5206;&#x7684;&#x53CD;&#x5411;&#x3002;</p>
<p><strong>&#x6CE8;&#x610F;&#xFF1A;</strong><br><code>MaxPool2d</code>&#x53EF;&#x4EE5;&#x5C06;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x6620;&#x5C04;&#x5230;&#x76F8;&#x540C;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x53CD;&#x6F14;&#x8FC7;&#x7A0B;&#x53EF;&#x80FD;&#x4F1A;&#x53D8;&#x5F97;&#x6A21;&#x68F1;&#x4E24;&#x53EF;&#x3002; &#x4E3A;&#x4E86;&#x9002;&#x5E94;&#x8FD9;&#x4E00;&#x70B9;&#xFF0C;&#x53EF;&#x4EE5;&#x5728;&#x8C03;&#x7528;&#x4E2D;&#x5C06;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#xFF08;<code>output_size</code>&#xFF09;&#x4F5C;&#x4E3A;&#x989D;&#x5916;&#x7684;&#x53C2;&#x6570;&#x4F20;&#x5165;&#x3002;&#x5177;&#x4F53;&#x7528;&#x6CD5;&#xFF0C;&#x8BF7;&#x53C2;&#x9605;&#x4E0B;&#x9762;&#x793A;&#x4F8B;</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - max pooling&#x7684;&#x7A97;&#x53E3;&#x5927;&#x5C0F;</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - max pooling&#x7684;&#x7A97;&#x53E3;&#x79FB;&#x52A8;&#x7684;&#x6B65;&#x957F;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#x662F;<code>kernel_size</code></li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x8FB9;&#x8865;&#x5145;0&#x7684;&#x5C42;&#x6570;</li>
</ul>
<p><strong>&#x8F93;&#x5165;&#xFF1A;</strong><br><code>input</code>:&#x9700;&#x8981;&#x8F6C;&#x6362;&#x7684;<code>tensor</code><br><code>indices</code>&#xFF1A;Maxpool1d&#x7684;&#x7D22;&#x5F15;&#x53F7;<br><code>output_size</code>:&#x4E00;&#x4E2A;&#x6307;&#x5B9A;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x7684;<code>torch.Size</code></p>
<p><strong>&#x5927;&#x5C0F;&#xFF1A;</strong><br><code>input</code>: (N,C,H_in,W_in)<br><code>output</code>:(N,C,H_out,W_out)        </p>
<p><script type="math/tex; ">H_{out}=(H_{in}-1)*stride[0]-2*padding[0]+kernel\_size[0]</script>    </p>
<p><script type="math/tex; ">W_{out}=(W_{in}-1)*stride[1]-2*padding[1]+kernel\_size[1]</script>        </p>
<p>&#x4E5F;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;<code>output_size</code>&#x6307;&#x5B9A;&#x8F93;&#x51FA;&#x7684;&#x5927;&#x5C0F;</p>
<p><strong>Example&#xFF1A;</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>pool = nn.MaxPool2d(<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, return_indices=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>unpool = nn.MaxUnpool2d(<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = Variable(torch.Tensor([[[[ <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>],
    ...                                  [ <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>],
    ...                                  [ <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>],
    ...                                  [<span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>, <span class="hljs-number">16</span>]]]]))
<span class="hljs-meta">&gt;&gt;&gt; </span>output, indices = pool(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>unpool(output, indices)
    Variable containing:
    (<span class="hljs-number">0</span> ,<span class="hljs-number">0</span> ,.,.) =
       <span class="hljs-number">0</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>
       <span class="hljs-number">0</span>   <span class="hljs-number">6</span>   <span class="hljs-number">0</span>   <span class="hljs-number">8</span>
       <span class="hljs-number">0</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>
       <span class="hljs-number">0</span>  <span class="hljs-number">14</span>   <span class="hljs-number">0</span>  <span class="hljs-number">16</span>
    [torch.FloatTensor of size <span class="hljs-number">1</span>x1x4x4]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># specify a different output size than input size</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>unpool(output, indices, output_size=torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>]))
    Variable containing:
    (<span class="hljs-number">0</span> ,<span class="hljs-number">0</span> ,.,.) =
       <span class="hljs-number">0</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>
       <span class="hljs-number">6</span>   <span class="hljs-number">0</span>   <span class="hljs-number">8</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>
       <span class="hljs-number">0</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>  <span class="hljs-number">14</span>   <span class="hljs-number">0</span>
      <span class="hljs-number">16</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>
       <span class="hljs-number">0</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>
    [torch.FloatTensor of size <span class="hljs-number">1</span>x1x5x5]
</code></pre>
<h4 id="class-torchnnmaxunpool3dkernelsize-stridenone-padding0">class torch.nn.MaxUnpool3d(kernel_size, stride=None, padding=0)</h4>
<p><code>Maxpool3d</code>&#x7684;&#x9006;&#x8FC7;&#x7A0B;&#xFF0C;&#x4E0D;&#x8FC7;&#x5E76;&#x4E0D;&#x662F;&#x5B8C;&#x5168;&#x7684;&#x9006;&#x8FC7;&#x7A0B;&#xFF0C;&#x56E0;&#x4E3A;&#x5728;<code>maxpool3d</code>&#x7684;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x4E00;&#x4E9B;&#x6700;&#x5927;&#x503C;&#x7684;&#x5DF2;&#x7ECF;&#x4E22;&#x5931;&#x3002;
<code>MaxUnpool3d</code>&#x7684;&#x8F93;&#x5165;&#x5C31;&#x662F;<code>MaxPool3d</code>&#x7684;&#x8F93;&#x51FA;&#xFF0C;&#x5305;&#x62EC;&#x6700;&#x5927;&#x503C;&#x7684;&#x7D22;&#x5F15;&#xFF0C;&#x5E76;&#x8BA1;&#x7B97;&#x6240;&#x6709;<code>maxpool3d</code>&#x8FC7;&#x7A0B;&#x4E2D;&#x975E;&#x6700;&#x5927;&#x503C;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;&#x96F6;&#x7684;&#x90E8;&#x5206;&#x7684;&#x53CD;&#x5411;&#x3002;</p>
<p><strong>&#x6CE8;&#x610F;&#xFF1A;</strong><br><code>MaxPool3d</code>&#x53EF;&#x4EE5;&#x5C06;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x6620;&#x5C04;&#x5230;&#x76F8;&#x540C;&#x7684;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x53CD;&#x6F14;&#x8FC7;&#x7A0B;&#x53EF;&#x80FD;&#x4F1A;&#x53D8;&#x5F97;&#x6A21;&#x68F1;&#x4E24;&#x53EF;&#x3002;&#x4E3A;&#x4E86;&#x9002;&#x5E94;&#x8FD9;&#x4E00;&#x70B9;&#xFF0C;&#x53EF;&#x4EE5;&#x5728;&#x8C03;&#x7528;&#x4E2D;&#x5C06;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#xFF08;<code>output_size</code>&#xFF09;&#x4F5C;&#x4E3A;&#x989D;&#x5916;&#x7684;&#x53C2;&#x6570;&#x4F20;&#x5165;&#x3002;&#x5177;&#x4F53;&#x7528;&#x6CD5;&#xFF0C;&#x8BF7;&#x53C2;&#x9605;&#x4E0B;&#x9762;&#x7684;&#x8F93;&#x5165;&#x548C;&#x793A;&#x4F8B;</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - Maxpooling&#x7A97;&#x53E3;&#x5927;&#x5C0F;</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - max pooling&#x7684;&#x7A97;&#x53E3;&#x79FB;&#x52A8;&#x7684;&#x6B65;&#x957F;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#x662F;<code>kernel_size</code></li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x8FB9;&#x8865;&#x5145;0&#x7684;&#x5C42;&#x6570;</li>
</ul>
<p><strong>&#x8F93;&#x5165;&#xFF1A;</strong><br><code>input</code>:&#x9700;&#x8981;&#x8F6C;&#x6362;&#x7684;<code>tensor</code><br><code>indices</code>&#xFF1A;<code>Maxpool1d</code>&#x7684;&#x7D22;&#x5F15;&#x5E8F;&#x6570;<br><code>output_size</code>:&#x4E00;&#x4E2A;&#x6307;&#x5B9A;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x7684;<code>torch.Size</code></p>
<p><strong>&#x5927;&#x5C0F;:</strong><br><code>input</code>: (N,C,D_in,H_in,W_in)<br><code>outpu</code>t:(N,C,D_out,H_out,W_out)<br><script type="math/tex; mode=display">
\begin{aligned}
D_{out}=(D_{in}-1)*stride[0]-2*padding[0]+kernel\_size[0]\\  
H_{out}=(H_{in}-1)*stride[1]-2*padding[0]+kernel\_size[1]\\   W_{out}=(W_{in}-1)*stride[2]-2*padding[2]+kernel\_size[2]  
\end{aligned}
</script>     </p>
<p>&#x4E5F;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;<code>output_size</code>&#x6307;&#x5B9A;&#x8F93;&#x51FA;&#x7684;&#x5927;&#x5C0F;</p>
<p><strong>Example&#xFF1A;</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of square window of size=3, stride=2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pool = nn.MaxPool3d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, return_indices=<span class="hljs-keyword">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>unpool = nn.MaxUnpool3d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output, indices = pool(Variable(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">51</span>, <span class="hljs-number">33</span>, <span class="hljs-number">15</span>)))
<span class="hljs-meta">&gt;&gt;&gt; </span>unpooled_output = unpool(output, indices)
<span class="hljs-meta">&gt;&gt;&gt; </span>unpooled_output.size()
torch.Size([<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">51</span>, <span class="hljs-number">33</span>, <span class="hljs-number">15</span>])
</code></pre>
<h3 id="class-torchnnavgpool1dkernelsize-stridenone-padding0-ceilmodefalse-countincludepadtrue">class torch.nn.AvgPool1d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)</h3>
<p>&#x5BF9;&#x4FE1;&#x53F7;&#x7684;&#x8F93;&#x5165;&#x901A;&#x9053;&#xFF0C;&#x63D0;&#x4F9B;1&#x7EF4;&#x5E73;&#x5747;&#x6C60;&#x5316;&#xFF08;average pooling &#xFF09;
&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5927;&#x5C0F;(N,C,L)&#xFF0C;&#x8F93;&#x51FA;&#x5927;&#x5C0F;(N,C,L_out)&#x548C;&#x6C60;&#x5316;&#x7A97;&#x53E3;&#x5927;&#x5C0F;k&#x7684;&#x5173;&#x7CFB;&#x662F;&#xFF1A;<br><script type="math/tex; ">out(N_i,C_j,l)=1/k*\sum^{k}_{m=0}input(N_{i},C_{j},stride*l+m)</script><br>&#x5982;&#x679C;<code>padding</code>&#x4E0D;&#x662F;0&#xFF0C;&#x4F1A;&#x5728;&#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x8FB9;&#x6DFB;&#x52A0;&#x76F8;&#x5E94;&#x6570;&#x76EE;0</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - &#x6C60;&#x5316;&#x7A97;&#x53E3;&#x5927;&#x5C0F;</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - max pooling&#x7684;&#x7A97;&#x53E3;&#x79FB;&#x52A8;&#x7684;&#x6B65;&#x957F;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#x662F;<code>kernel_size</code></li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x8FB9;&#x8865;&#x5145;0&#x7684;&#x5C42;&#x6570;</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) &#x2013; &#x4E00;&#x4E2A;&#x63A7;&#x5236;&#x7A97;&#x53E3;&#x4E2D;&#x5143;&#x7D20;&#x6B65;&#x5E45;&#x7684;&#x53C2;&#x6570;</li>
<li>return_indices - &#x5982;&#x679C;&#x7B49;&#x4E8E;<code>True</code>&#xFF0C;&#x4F1A;&#x8FD4;&#x56DE;&#x8F93;&#x51FA;&#x6700;&#x5927;&#x503C;&#x7684;&#x5E8F;&#x53F7;&#xFF0C;&#x5BF9;&#x4E8E;&#x4E0A;&#x91C7;&#x6837;&#x64CD;&#x4F5C;&#x4F1A;&#x6709;&#x5E2E;&#x52A9;</li>
<li>ceil_mode - &#x5982;&#x679C;&#x7B49;&#x4E8E;<code>True</code>&#xFF0C;&#x8BA1;&#x7B97;&#x8F93;&#x51FA;&#x4FE1;&#x53F7;&#x5927;&#x5C0F;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4F1A;&#x4F7F;&#x7528;&#x5411;&#x4E0A;&#x53D6;&#x6574;&#xFF0C;&#x4EE3;&#x66FF;&#x9ED8;&#x8BA4;&#x7684;&#x5411;&#x4E0B;&#x53D6;&#x6574;&#x7684;&#x64CD;&#x4F5C;</li>
</ul>
<p><strong>&#x5927;&#x5C0F;&#xFF1A;</strong><br><code>input</code>:(N,C,L_in)<br><code>output</code>:(N,C,L_out)<br><script type="math/tex; ">L_{out}=floor((L_{in}+2*padding-kernel\_size)/stride+1)</script>      </p>
<p><strong>Example:</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool with window of size=3, stride=2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AvgPool1d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>m(Variable(torch.Tensor([[[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>]]])))
Variable containing:
    (<span class="hljs-number">0</span> ,.,.) =
    <span class="hljs-number">2</span>  <span class="hljs-number">4</span>  <span class="hljs-number">6</span>
    [torch.FloatTensor of size <span class="hljs-number">1</span>x1x3]
</code></pre>
<h3 id="class-torchnnavgpool2dkernelsize-stridenone-padding0-ceilmodefalse-countincludepadtrue">class torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)</h3>
<p>&#x5BF9;&#x4FE1;&#x53F7;&#x7684;&#x8F93;&#x5165;&#x901A;&#x9053;&#xFF0C;&#x63D0;&#x4F9B;2&#x7EF4;&#x7684;&#x5E73;&#x5747;&#x6C60;&#x5316;&#xFF08;average pooling &#xFF09;<br>&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5927;&#x5C0F;(N,C,H,W)&#xFF0C;&#x8F93;&#x51FA;&#x5927;&#x5C0F;(N,C,H_out,W_out)&#x548C;&#x6C60;&#x5316;&#x7A97;&#x53E3;&#x5927;&#x5C0F;(kH,kW)&#x7684;&#x5173;&#x7CFB;&#x662F;&#xFF1A;<br><script type="math/tex; mode=display">
out(N_i,C_j,h,w)=1/(kH*kW)*\sum^{kH-1}_{m=0}\sum^{kW-1}_{n=0}input(N_{i},C_{j},stride[0]*h+m,stride[1]*w+n)</script>       </p>
<p>&#x5982;&#x679C;<code>padding</code>&#x4E0D;&#x662F;0&#xFF0C;&#x4F1A;&#x5728;&#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x8FB9;&#x6DFB;&#x52A0;&#x76F8;&#x5E94;&#x6570;&#x76EE;0</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - &#x6C60;&#x5316;&#x7A97;&#x53E3;&#x5927;&#x5C0F;</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - max pooling&#x7684;&#x7A97;&#x53E3;&#x79FB;&#x52A8;&#x7684;&#x6B65;&#x957F;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#x662F;<code>kernel_size</code></li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - &#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x8FB9;&#x8865;&#x5145;0&#x7684;&#x5C42;&#x6570;</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) &#x2013; &#x4E00;&#x4E2A;&#x63A7;&#x5236;&#x7A97;&#x53E3;&#x4E2D;&#x5143;&#x7D20;&#x6B65;&#x5E45;&#x7684;&#x53C2;&#x6570;</li>
<li>ceil_mode - &#x5982;&#x679C;&#x7B49;&#x4E8E;<code>True</code>&#xFF0C;&#x8BA1;&#x7B97;&#x8F93;&#x51FA;&#x4FE1;&#x53F7;&#x5927;&#x5C0F;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4F1A;&#x4F7F;&#x7528;&#x5411;&#x4E0A;&#x53D6;&#x6574;&#xFF0C;&#x4EE3;&#x66FF;&#x9ED8;&#x8BA4;&#x7684;&#x5411;&#x4E0B;&#x53D6;&#x6574;&#x7684;&#x64CD;&#x4F5C;</li>
<li>count_include_pad - &#x5982;&#x679C;&#x7B49;&#x4E8E;<code>True</code>&#xFF0C;&#x8BA1;&#x7B97;&#x5E73;&#x5747;&#x6C60;&#x5316;&#x65F6;&#xFF0C;&#x5C06;&#x5305;&#x62EC;<code>padding</code>&#x586B;&#x5145;&#x7684;0</li>
</ul>
<p><strong>shape&#xFF1A;</strong><br><code>input</code>: (N,C,H_in,W_in)<br><code>output</code>: (N,C,H_out,W_out)<br><script type="math/tex; ">\begin{aligned}
H_{out}=floor((H_{in}+2*padding[0]-kernel\_size[0])/stride[0]+1)\\  
W_{out}=floor((W_{in}+2*padding[1]-kernel\_size[1])/stride[1]+1)
\end{aligned}
</script>    </p>
<p><strong>Example:</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of square window of size=3, stride=2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AvgPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of non-square window</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AvgPool2d((<span class="hljs-number">3</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>, <span class="hljs-number">32</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="class-torchnnavgpool3dkernelsize-stridenone">class torch.nn.AvgPool3d(kernel_size, stride=None)</h3>
<p>&#x5BF9;&#x4FE1;&#x53F7;&#x7684;&#x8F93;&#x5165;&#x901A;&#x9053;&#xFF0C;&#x63D0;&#x4F9B;3&#x7EF4;&#x7684;&#x5E73;&#x5747;&#x6C60;&#x5316;&#xFF08;<code>average pooling</code>&#xFF09;
&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x7684;&#x5927;&#x5C0F;(N,C,D,H,W)&#xFF0C;&#x8F93;&#x51FA;&#x5927;&#x5C0F;(N,C,D_out,H_out,W_out)&#x548C;&#x6C60;&#x5316;&#x7A97;&#x53E3;&#x5927;&#x5C0F;(kD,kH,kW)&#x7684;&#x5173;&#x7CFB;&#x662F;&#xFF1A;    </p>
<p><script type="math/tex; mode=display">
\begin{aligned}
out(N_i,C_j,d,h,w)=1/(kD*kH*kW)*\sum^{kD-1}_{k=0}\sum^{kH-1}_{m=0}\sum^{kW-1}_{n=0}input(N_{i},C_{j},stride[0]*d+k,stride[1]*h+m,stride[2]*w+n)
\end{aligned}
</script>
&#x5982;&#x679C;<code>padding</code>&#x4E0D;&#x662F;0&#xFF0C;&#x4F1A;&#x5728;&#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x8FB9;&#x6DFB;&#x52A0;&#x76F8;&#x5E94;&#x6570;&#x76EE;0</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - &#x6C60;&#x5316;&#x7A97;&#x53E3;&#x5927;&#x5C0F;</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - max <code>pooling</code>&#x7684;&#x7A97;&#x53E3;&#x79FB;&#x52A8;&#x7684;&#x6B65;&#x957F;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#x662F;<code>kernel_size</code></li>
</ul>
<p><strong>shape&#xFF1A;</strong><br>&#x8F93;&#x5165;&#x5927;&#x5C0F;:(N,C,D_in,H_in,W_in)<br>&#x8F93;&#x51FA;&#x5927;&#x5C0F;:(N,C,D_out,H_out,W_out)
<script type="math/tex; ">\begin{aligned}
D_{out}=floor((D_{in}+2*padding[0]-kernel\_size[0])/stride[0]+1)\\  
H_{out}=floor((H_{in}+2*padding[1]-kernel\_size[1])/stride[1]+1)\\  
W_{out}=floor((W_{in}+2*padding[2]-kernel\_size[2])/stride[2]+1)  
\end{aligned}
</script>      </p>
<p><strong>Example:</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of square window of size=3, stride=2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AvgPool3d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of non-square window</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AvgPool3d((<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>,<span class="hljs-number">44</span>, <span class="hljs-number">31</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="class-torchnnfractionalmaxpool2dkernelsize-outputsizenone-outputrationone-returnindicesfalse-randomsamplesnone">class torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)</h3>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x4FE1;&#x53F7;&#xFF0C;&#x63D0;&#x4F9B;2&#x7EF4;&#x7684;&#x5206;&#x6570;&#x6700;&#x5927;&#x5316;&#x6C60;&#x5316;&#x64CD;&#x4F5C;
&#x5206;&#x6570;&#x6700;&#x5927;&#x5316;&#x6C60;&#x5316;&#x7684;&#x7EC6;&#x8282;&#x8BF7;&#x9605;&#x8BFB;<a href="https://arxiv.org/abs/1412.6071" target="_blank">&#x8BBA;&#x6587;</a>
&#x7531;&#x76EE;&#x6807;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x786E;&#x5B9A;&#x7684;&#x968F;&#x673A;&#x6B65;&#x957F;,&#x5728;$kH*kW$&#x533A;&#x57DF;&#x8FDB;&#x884C;&#x6700;&#x5927;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x3002;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#x548C;&#x8F93;&#x5165;&#x7279;&#x5F81;&#x7684;&#x6570;&#x91CF;&#x76F8;&#x540C;&#x3002;</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - &#x6700;&#x5927;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x65F6;&#x7684;&#x7A97;&#x53E3;&#x5927;&#x5C0F;&#x3002;&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;&#x6570;&#x5B57;&#xFF08;&#x8868;&#x793A;<code>K*K</code>&#x7684;&#x7A97;&#x53E3;&#xFF09;&#xFF0C;&#x4E5F;&#x53EF;&#x4EE5;&#x662F;&#x4E00;&#x4E2A;&#x5143;&#x7EC4;&#xFF08;<code>kh*kw</code>&#xFF09;</li>
<li>output_size - &#x8F93;&#x51FA;&#x56FE;&#x50CF;&#x7684;&#x5C3A;&#x5BF8;&#x3002;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#x4E00;&#x4E2A;<code>tuple</code>&#x6307;&#x5B9A;(oH,oW)&#xFF0C;&#x4E5F;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#x4E00;&#x4E2A;&#x6570;&#x5B57;oH&#x6307;&#x5B9A;&#x4E00;&#x4E2A;oH*oH&#x7684;&#x8F93;&#x51FA;&#x3002;</li>
<li>output_ratio &#x2013; &#x5C06;&#x8F93;&#x5165;&#x56FE;&#x50CF;&#x7684;&#x5927;&#x5C0F;&#x7684;&#x767E;&#x5206;&#x6BD4;&#x6307;&#x5B9A;&#x4E3A;&#x8F93;&#x51FA;&#x56FE;&#x7247;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x4F7F;&#x7528;&#x4E00;&#x4E2A;&#x8303;&#x56F4;&#x5728;(0,1)&#x4E4B;&#x95F4;&#x7684;&#x6570;&#x5B57;&#x6307;&#x5B9A;   </li>
<li>return_indices - &#x9ED8;&#x8BA4;&#x503C;<code>False</code>&#xFF0C;&#x5982;&#x679C;&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>&#xFF0C;&#x4F1A;&#x8FD4;&#x56DE;&#x8F93;&#x51FA;&#x7684;&#x7D22;&#x5F15;&#xFF0C;&#x7D22;&#x5F15;&#x5BF9;  <code>nn.MaxUnpool2d</code>&#x6709;&#x7528;&#x3002;</li>
</ul>
<p><strong>Example&#xFF1A;</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of square window of size=3, and target output size 13x12</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.FractionalMaxPool2d(<span class="hljs-number">3</span>, output_size=(<span class="hljs-number">13</span>, <span class="hljs-number">12</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of square window and target output size being half of input image size</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.FractionalMaxPool2d(<span class="hljs-number">3</span>, output_ratio=(<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>, <span class="hljs-number">32</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="class-torchnnlppool2dnormtype-kernelsize-stridenone-ceilmodefalse">class torch.nn.LPPool2d(norm_type, kernel_size, stride=None, ceil_mode=False)</h3>
<p>&#x5BF9;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#x63D0;&#x4F9B;2&#x7EF4;&#x7684;&#x5E42;&#x5E73;&#x5747;&#x6C60;&#x5316;&#x64CD;&#x4F5C;&#x3002;
&#x8F93;&#x51FA;&#x7684;&#x8BA1;&#x7B97;&#x65B9;&#x5F0F;&#xFF1A;<br><script type="math/tex; ">f(x)=pow(sum(X,p),1/p)</script>  </p>
<ul>
<li>&#x5F53;p&#x4E3A;&#x65E0;&#x7A77;&#x5927;&#x7684;&#x65F6;&#x5019;&#x65F6;&#xFF0C;&#x7B49;&#x4EF7;&#x4E8E;&#x6700;&#x5927;&#x6C60;&#x5316;&#x64CD;&#x4F5C;</li>
<li>&#x5F53;<code>p=1</code>&#x65F6;&#xFF0C;&#x7B49;&#x4EF7;&#x4E8E;&#x5E73;&#x5747;&#x6C60;&#x5316;&#x64CD;&#x4F5C;</li>
</ul>
<p>&#x53C2;&#x6570;<code>kernel_size</code>, <code>stride</code>&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#xFF1A;</p>
<ul>
<li><code>int</code>&#xFF0C;&#x6C60;&#x5316;&#x7A97;&#x53E3;&#x7684;&#x5BBD;&#x548C;&#x9AD8;&#x76F8;&#x7B49;</li>
<li><code>tuple</code>&#x6570;&#x7EC4;&#xFF08;&#x4E24;&#x4E2A;&#x6570;&#x5B57;&#x7684;&#xFF09;&#xFF0C;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x662F;&#x6C60;&#x5316;&#x7A97;&#x53E3;&#x7684;&#x9AD8;&#xFF0C;&#x53E6;&#x4E00;&#x4E2A;&#x662F;&#x5BBD;</li>
</ul>
<p><strong>&#x53C2;&#x6570;</strong></p>
<ul>
<li>kernel_size: &#x6C60;&#x5316;&#x7A97;&#x53E3;&#x7684;&#x5927;&#x5C0F;</li>
<li>stride&#xFF1A;&#x6C60;&#x5316;&#x7A97;&#x53E3;&#x79FB;&#x52A8;&#x7684;&#x6B65;&#x957F;&#x3002;<code>kernel_size</code>&#x662F;&#x9ED8;&#x8BA4;&#x503C;</li>
<li>ceil_mode: <code>ceil_mode=True</code>&#x65F6;&#xFF0C;&#x5C06;&#x4F7F;&#x7528;&#x5411;&#x4E0B;&#x53D6;&#x6574;&#x4EE3;&#x66FF;&#x5411;&#x4E0A;&#x53D6;&#x6574;</li>
</ul>
<p><strong>shape</strong></p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;(N,C,H_in,W_in)  </li>
<li>&#x8F93;&#x51FA;&#xFF1A;(N,C,H_out,W_out)<br><script type="math/tex; ">\begin{aligned}
H_{out} = floor((H_{in}+2*padding[0]-dilation[0]*(kernel\_size[0]-1)-1)/stride[0]+1)\\
W_{out} = floor((W_{in}+2*padding[1]-dilation[1]*(kernel\_size[1]-1)-1)/stride[1]+1)
\end{aligned}
</script></li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># power-2 pool of square window of size=3, stride=2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.LPPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># pool of non-square window of power 1.2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.LPPool2d(<span class="hljs-number">1.2</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">50</span>, <span class="hljs-number">32</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="class-torchnnadaptivemaxpool1doutputsize-returnindicesfalse">class torch.nn.AdaptiveMaxPool1d(output_size, return_indices=False)</h3>
<p>&#x5BF9;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#xFF0C;&#x63D0;&#x4F9B;1&#x7EF4;&#x7684;&#x81EA;&#x9002;&#x5E94;&#x6700;&#x5927;&#x6C60;&#x5316;&#x64CD;&#x4F5C;
&#x5BF9;&#x4E8E;&#x4EFB;&#x4F55;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x7684;&#x8F93;&#x5165;&#xFF0C;&#x53EF;&#x4EE5;&#x5C06;&#x8F93;&#x51FA;&#x5C3A;&#x5BF8;&#x6307;&#x5B9A;&#x4E3A;H&#xFF0C;&#x4F46;&#x662F;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#x7684;&#x6570;&#x76EE;&#x4E0D;&#x4F1A;&#x53D8;&#x5316;&#x3002;</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li>output_size: &#x8F93;&#x51FA;&#x4FE1;&#x53F7;&#x7684;&#x5C3A;&#x5BF8;</li>
<li>return_indices: &#x5982;&#x679C;&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>&#xFF0C;&#x4F1A;&#x8FD4;&#x56DE;&#x8F93;&#x51FA;&#x7684;&#x7D22;&#x5F15;&#x3002;&#x5BF9;  <code>nn.MaxUnpool1d</code>&#x6709;&#x7528;&#xFF0C;&#x9ED8;&#x8BA4;&#x503C;&#x662F;<code>False</code></li>
</ul>
<p><strong>Example&#xFF1A;</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 5</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveMaxPool1d(<span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="class-torchnnadaptivemaxpool2doutputsize-returnindicesfalse">class torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False)</h3>
<p>&#x5BF9;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#xFF0C;&#x63D0;&#x4F9B;2&#x7EF4;&#x7684;&#x81EA;&#x9002;&#x5E94;&#x6700;&#x5927;&#x6C60;&#x5316;&#x64CD;&#x4F5C;
&#x5BF9;&#x4E8E;&#x4EFB;&#x4F55;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x7684;&#x8F93;&#x5165;&#xFF0C;&#x53EF;&#x4EE5;&#x5C06;&#x8F93;&#x51FA;&#x5C3A;&#x5BF8;&#x6307;&#x5B9A;&#x4E3A;H*W&#xFF0C;&#x4F46;&#x662F;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#x7684;&#x6570;&#x76EE;&#x4E0D;&#x4F1A;&#x53D8;&#x5316;&#x3002;</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li>output_size: &#x8F93;&#x51FA;&#x4FE1;&#x53F7;&#x7684;&#x5C3A;&#x5BF8;,&#x53EF;&#x4EE5;&#x7528;&#xFF08;H,W&#xFF09;&#x8868;&#x793A;<code>H*W</code>&#x7684;&#x8F93;&#x51FA;&#xFF0C;&#x4E5F;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#x6570;&#x5B57;<code>H</code>&#x8868;&#x793A;<code>H*H</code>&#x5927;&#x5C0F;&#x7684;&#x8F93;&#x51FA;</li>
<li>return_indices: &#x5982;&#x679C;&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>&#xFF0C;&#x4F1A;&#x8FD4;&#x56DE;&#x8F93;&#x51FA;&#x7684;&#x7D22;&#x5F15;&#x3002;&#x5BF9;  <code>nn.MaxUnpool2d</code>&#x6709;&#x7528;&#xFF0C;&#x9ED8;&#x8BA4;&#x503C;&#x662F;<code>False</code></li>
</ul>
<p><strong>Example&#xFF1A;</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 5x7</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveMaxPool2d((<span class="hljs-number">5</span>,<span class="hljs-number">7</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 7x7 (square)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveMaxPool2d(<span class="hljs-number">7</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="class-torchnnadaptiveavgpool1doutputsize">class torch.nn.AdaptiveAvgPool1d(output_size)</h3>
<p>&#x5BF9;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#xFF0C;&#x63D0;&#x4F9B;1&#x7EF4;&#x7684;&#x81EA;&#x9002;&#x5E94;&#x5E73;&#x5747;&#x6C60;&#x5316;&#x64CD;&#x4F5C;
&#x5BF9;&#x4E8E;&#x4EFB;&#x4F55;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x7684;&#x8F93;&#x5165;&#xFF0C;&#x53EF;&#x4EE5;&#x5C06;&#x8F93;&#x51FA;&#x5C3A;&#x5BF8;&#x6307;&#x5B9A;&#x4E3A;H*W&#xFF0C;&#x4F46;&#x662F;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#x7684;&#x6570;&#x76EE;&#x4E0D;&#x4F1A;&#x53D8;&#x5316;&#x3002;</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li>output_size: &#x8F93;&#x51FA;&#x4FE1;&#x53F7;&#x7684;&#x5C3A;&#x5BF8;  </li>
</ul>
<p><strong>Example&#xFF1A;</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 5</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveAvgPool1d(<span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h3 id="class-torchnnadaptiveavgpool2doutputsize">class torch.nn.AdaptiveAvgPool2d(output_size)</h3>
<p>&#x5BF9;&#x8F93;&#x5165;&#x4FE1;&#x53F7;&#xFF0C;&#x63D0;&#x4F9B;2&#x7EF4;&#x7684;&#x81EA;&#x9002;&#x5E94;&#x5E73;&#x5747;&#x6C60;&#x5316;&#x64CD;&#x4F5C;
&#x5BF9;&#x4E8E;&#x4EFB;&#x4F55;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#x7684;&#x8F93;&#x5165;&#xFF0C;&#x53EF;&#x4EE5;&#x5C06;&#x8F93;&#x51FA;&#x5C3A;&#x5BF8;&#x6307;&#x5B9A;&#x4E3A;<code>H*W</code>&#xFF0C;&#x4F46;&#x662F;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#x7684;&#x6570;&#x76EE;&#x4E0D;&#x4F1A;&#x53D8;&#x5316;&#x3002;</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li>output_size: &#x8F93;&#x51FA;&#x4FE1;&#x53F7;&#x7684;&#x5C3A;&#x5BF8;,&#x53EF;&#x4EE5;&#x7528;(H,W)&#x8868;&#x793A;<code>H*W</code>&#x7684;&#x8F93;&#x51FA;&#xFF0C;&#x4E5F;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#x803D;&#x6401;&#x6570;&#x5B57;H&#x8868;&#x793A;H*H&#x5927;&#x5C0F;&#x7684;&#x8F93;&#x51FA;</li>
</ul>
<p><strong>Example&#xFF1A;</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 5x7</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveAvgPool2d((<span class="hljs-number">5</span>,<span class="hljs-number">7</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target output size of 7x7 (square)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.AdaptiveAvgPool2d(<span class="hljs-number">7</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">10</span>, <span class="hljs-number">9</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h2 id="non-linear-activations-source">Non-Linear Activations <a href="https://pytorch.org/docs/nn.html#non-linear-activations" target="_blank"><font size="2">[source]</font></a></h2>
<blockquote>
<p>class torch.nn.ReLU(inplace=False) <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#ReLU" target="_blank"><font size="2">[source]</font></a></p>
</blockquote>
<p>&#x5BF9;&#x8F93;&#x5165;&#x8FD0;&#x7528;&#x4FEE;&#x6B63;&#x7EBF;&#x6027;&#x5355;&#x5143;&#x51FD;&#x6570;${ReLU}(x)= max(0, x)$&#xFF0C;</p>
<p>&#x53C2;&#x6570;&#xFF1A; inplace-&#x9009;&#x62E9;&#x662F;&#x5426;&#x8FDB;&#x884C;&#x8986;&#x76D6;&#x8FD0;&#x7B97;</p>
<p>shape&#xFF1A;</p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;$(N, <em>)$&#xFF0C;</em>&#x4EE3;&#x8868;&#x4EFB;&#x610F;&#x6570;&#x76EE;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;&#xFF1A;$(N, *)$&#xFF0C;&#x4E0E;&#x8F93;&#x5165;&#x62E5;&#x6709;&#x540C;&#x6837;&#x7684;shape&#x5C5E;&#x6027;</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ReLU()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>print(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.ReLU6(inplace=False) <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#ReLU6" target="_blank"><font size="2">[source]</font></a></p>
</blockquote>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x8FD0;&#x7528;&#x51FD;&#x6570;${ReLU6}(x) = min(max(0,x), 6)$&#xFF0C;</p>
<p>&#x53C2;&#x6570;&#xFF1A; inplace-&#x9009;&#x62E9;&#x662F;&#x5426;&#x8FDB;&#x884C;&#x8986;&#x76D6;&#x8FD0;&#x7B97;</p>
<p>shape&#xFF1A;</p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;$(N, <em>)$&#xFF0C;</em>&#x4EE3;&#x8868;&#x4EFB;&#x610F;&#x6570;&#x76EE;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;&#xFF1A;$(N, *)$&#xFF0C;&#x4E0E;&#x8F93;&#x5165;&#x62E5;&#x6709;&#x540C;&#x6837;&#x7684;shape&#x5C5E;&#x6027;</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ReLU6()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>print(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.ELU(alpha=1.0,   inplace=False) <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#ELU" target="_blank"><font size="2">[source]</font></a></p>
</blockquote>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x8FD0;&#x7528;&#x51FD;&#x6570;$f(x) = max(0,x) + min(0, alpha * (e^x - 1))$&#xFF0C;</p>
<p>shape&#xFF1A;</p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;$(N, *)$&#xFF0C;&#x661F;&#x53F7;&#x4EE3;&#x8868;&#x4EFB;&#x610F;&#x6570;&#x76EE;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;&#xFF1A;$(N, *)$&#x4E0E;&#x8F93;&#x5165;&#x62E5;&#x6709;&#x540C;&#x6837;&#x7684;shape&#x5C5E;&#x6027;</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.ELU()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>print(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.PReLU(num_parameters=1, init=0.25)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#PReLU" target="_blank"><font size="2">[source]</font></a></p>
</blockquote>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x8FD0;&#x7528;&#x51FD;&#x6570;$PReLU(x) = max(0,x) + a * min(0,x)$&#xFF0C;<code>a</code>&#x662F;&#x4E00;&#x4E2A;&#x53EF;&#x5B66;&#x4E60;&#x53C2;&#x6570;&#x3002;&#x5F53;&#x6CA1;&#x6709;&#x58F0;&#x660E;&#x65F6;&#xFF0C;<code>nn.PReLU()</code>&#x5728;&#x6240;&#x6709;&#x7684;&#x8F93;&#x5165;&#x4E2D;&#x53EA;&#x6709;&#x4E00;&#x4E2A;&#x53C2;&#x6570;<code>a</code>&#xFF1B;&#x5982;&#x679C;&#x662F;<code>nn.PReLU(nChannels)</code>&#xFF0C;<code>a</code>&#x5C06;&#x5E94;&#x7528;&#x5230;&#x6BCF;&#x4E2A;&#x8F93;&#x5165;&#x3002;</p>
<p>&#x6CE8;&#x610F;&#xFF1A;&#x5F53;&#x4E3A;&#x4E86;&#x8868;&#x73B0;&#x66F4;&#x4F73;&#x7684;&#x6A21;&#x578B;&#x800C;&#x5B66;&#x4E60;&#x53C2;&#x6570;<code>a</code>&#x65F6;&#x4E0D;&#x8981;&#x4F7F;&#x7528;&#x6743;&#x91CD;&#x8870;&#x51CF;&#xFF08;weight decay&#xFF09;</p>
<p>&#x53C2;&#x6570;&#xFF1A;</p>
<ul>
<li>num_parameters&#xFF1A;&#x9700;&#x8981;&#x5B66;&#x4E60;&#x7684;<code>a</code>&#x7684;&#x4E2A;&#x6570;&#xFF0C;&#x9ED8;&#x8BA4;&#x7B49;&#x4E8E;1</li>
<li>init&#xFF1A;<code>a</code>&#x7684;&#x521D;&#x59CB;&#x503C;&#xFF0C;&#x9ED8;&#x8BA4;&#x7B49;&#x4E8E;0.25</li>
</ul>
<p>shape&#xFF1A;</p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;$(N, <em>)$&#xFF0C;</em>&#x4EE3;&#x8868;&#x4EFB;&#x610F;&#x6570;&#x76EE;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;&#xFF1A;$(N, *)$&#xFF0C;&#x4E0E;&#x8F93;&#x5165;&#x62E5;&#x6709;&#x540C;&#x6837;&#x7684;shape&#x5C5E;&#x6027;</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.PReLU()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>print(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.LeakyReLU(negative_slope=0.01, inplace=False) <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#LeakyReLU" target="_blank"><font size="2">[source]</font></a></p>
</blockquote>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x8FD0;&#x7528;$f(x) = max(0, x) + {negative_slope} * min(0, x)$</p>
<p>&#x53C2;&#x6570;&#xFF1A;</p>
<ul>
<li>negative_slope&#xFF1A;&#x63A7;&#x5236;&#x8D1F;&#x659C;&#x7387;&#x7684;&#x89D2;&#x5EA6;&#xFF0C;&#x9ED8;&#x8BA4;&#x7B49;&#x4E8E;0.01</li>
<li>inplace-&#x9009;&#x62E9;&#x662F;&#x5426;&#x8FDB;&#x884C;&#x8986;&#x76D6;&#x8FD0;&#x7B97;</li>
</ul>
<p>shape&#xFF1A;</p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;$(N, <em>)$&#xFF0C;</em>&#x4EE3;&#x8868;&#x4EFB;&#x610F;&#x6570;&#x76EE;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;&#xFF1A;$(N, *)$&#xFF0C;&#x4E0E;&#x8F93;&#x5165;&#x62E5;&#x6709;&#x540C;&#x6837;&#x7684;shape&#x5C5E;&#x6027;</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.LeakyReLU(<span class="hljs-number">0.1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>print(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.Threshold(threshold, value, inplace=False) <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Threshold" target="_blank"><font size="2">[source]</font></a></p>
</blockquote>
<p>Threshold&#x5B9A;&#x4E49;&#xFF1A;</p>
<p><script type="math/tex; mode=display">
y =  x ,if\ x >= threshold\\
y = value,if\ x <  threshold
</script></p>
<p>&#x53C2;&#x6570;&#xFF1A;</p>
<ul>
<li>threshold&#xFF1A;&#x9608;&#x503C;</li>
<li>value&#xFF1A;&#x8F93;&#x5165;&#x503C;&#x5C0F;&#x4E8E;&#x9608;&#x503C;&#x5219;&#x4F1A;&#x88AB;value&#x4EE3;&#x66FF;</li>
<li>inplace&#xFF1A;&#x9009;&#x62E9;&#x662F;&#x5426;&#x8FDB;&#x884C;&#x8986;&#x76D6;&#x8FD0;&#x7B97;</li>
</ul>
<p>shape&#xFF1A;</p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;$(N, <em>)$&#xFF0C;</em>&#x4EE3;&#x8868;&#x4EFB;&#x610F;&#x6570;&#x76EE;&#x9644;&#x52A0;&#x7EF4;&#x5EA6;</li>
<li>&#x8F93;&#x51FA;&#xFF1A;$(N, *)$&#xFF0C;&#x4E0E;&#x8F93;&#x5165;&#x62E5;&#x6709;&#x540C;&#x6837;&#x7684;shape&#x5C5E;&#x6027;</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Threshold(<span class="hljs-number">0.1</span>, <span class="hljs-number">20</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = Variable(torch.randn(<span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>print(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.Hardtanh(min_value=-1, max_value=1, inplace=False) <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Hardtanh" target="_blank"><font size="2">[source]</font></a></p>
</blockquote>
<p>&#x5BF9;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#xFF0C;</p>
<p><script type="math/tex; mode=display">
f(x) = +1, if\ x  >  1;\\
f(x) = -1, if\ x  < -1;\\
f(x) =  x,  otherwise
</script></p>
<p>&#x7EBF;&#x6027;&#x533A;&#x57DF;&#x7684;&#x8303;&#x56F4;[-1,1]&#x53EF;&#x4EE5;&#x88AB;&#x8C03;&#x6574;</p>
<p>&#x53C2;&#x6570;&#xFF1A;</p>
<ul>
<li>min_value&#xFF1A;&#x7EBF;&#x6027;&#x533A;&#x57DF;&#x8303;&#x56F4;&#x6700;&#x5C0F;&#x503C;</li>
<li>max_value&#xFF1A;&#x7EBF;&#x6027;&#x533A;&#x57DF;&#x8303;&#x56F4;&#x6700;&#x5927;&#x503C;</li>
<li>inplace&#xFF1A;&#x9009;&#x62E9;&#x662F;&#x5426;&#x8FDB;&#x884C;&#x8986;&#x76D6;&#x8FD0;&#x7B97;</li>
</ul>
<p>shape&#xFF1A;</p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;(N, *)&#xFF0C;*&#x8868;&#x793A;&#x4EFB;&#x610F;&#x7EF4;&#x5EA6;&#x7EC4;&#x5408;</li>
<li>&#x8F93;&#x51FA;&#xFF1A;(N, *)&#xFF0C;&#x4E0E;&#x8F93;&#x5165;&#x6709;&#x76F8;&#x540C;&#x7684;shape&#x5C5E;&#x6027;</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Hardtanh()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>print(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.Sigmoid <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Sigmoid" target="_blank"><font size="2">[source]</font></a></p>
</blockquote>
<p>&#x5BF9;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x8FD0;&#x7528;Sigmoid&#x51FD;&#x6570;&#xFF0C;Sigmoid &#x5B9A;&#x4E49;&#x5982;&#x4E0B;&#xFF1A;</p>
<p><script type="math/tex; ">f(x) = 1 / ( 1 + e^{-x})</script></p>
<p>shape&#xFF1A;</p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;(N, *)&#xFF0C;*&#x8868;&#x793A;&#x4EFB;&#x610F;&#x7EF4;&#x5EA6;&#x7EC4;&#x5408;</li>
<li>&#x8F93;&#x51FA;&#xFF1A;(N, *)&#xFF0C;&#x4E0E;&#x8F93;&#x5165;&#x6709;&#x76F8;&#x540C;&#x7684;shape&#x5C5E;&#x6027;</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Sigmoid()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>print(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.Tanh <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Tanh" target="_blank"><font size="2">[source]</font></a></p>
</blockquote>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#xFF0C;</p>
<p><script type="math/tex; ">f(x) = \frac{e^{x} - e^{-x}} {e^{x} + e^{x}}</script></p>
<p>shape&#xFF1A;</p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;(N, *)&#xFF0C;*&#x8868;&#x793A;&#x4EFB;&#x610F;&#x7EF4;&#x5EA6;&#x7EC4;&#x5408;</li>
<li>&#x8F93;&#x51FA;&#xFF1A;(N, *)&#xFF0C;&#x4E0E;&#x8F93;&#x5165;&#x6709;&#x76F8;&#x540C;&#x7684;shape&#x5C5E;&#x6027;</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Tanh()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>print(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.LogSigmoid <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#LogSigmoid" target="_blank"><font size="2">[source]</font></a></p>
</blockquote>
<p>&#x5BF9;&#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#xFF0C;$LogSigmoid(x) = log( 1 / ( 1 + e^{-x}))$</p>
<p>shape&#xFF1A;</p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;(N, *)&#xFF0C;*&#x8868;&#x793A;&#x4EFB;&#x610F;&#x7EF4;&#x5EA6;&#x7EC4;&#x5408;</li>
<li>&#x8F93;&#x51FA;&#xFF1A;(N, *)&#xFF0C;&#x4E0E;&#x8F93;&#x5165;&#x6709;&#x76F8;&#x540C;&#x7684;shape&#x5C5E;&#x6027;</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.LogSigmoid()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>print(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.Softplus(beta=1, threshold=20)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Softplus" target="_blank"><font size="2">[source]</font></a></p>
</blockquote>
<p>&#x5BF9;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x8FD0;&#x7528;Softplus&#x51FD;&#x6570;&#xFF0C;Softplus &#x5B9A;&#x4E49;&#x5982;&#x4E0B;&#xFF1A;</p>
<p><script type="math/tex; ">f(x) = \frac{1}{beta} * log(1 + e^{(beta * x_i)})</script></p>
<p>Softplus&#x51FD;&#x6570;&#x662F;ReLU&#x51FD;&#x6570;&#x7684;&#x5E73;&#x6ED1;&#x903C;&#x8FD1;&#xFF0C;Softplus&#x51FD;&#x6570;&#x53EF;&#x4EE5;&#x4F7F;&#x5F97;&#x8F93;&#x51FA;&#x503C;&#x9650;&#x5B9A;&#x4E3A;&#x6B63;&#x6570;&#x3002;</p>
<p>&#x4E3A;&#x4E86;&#x4FDD;&#x8BC1;&#x6570;&#x503C;&#x7A33;&#x5B9A;&#x6027;&#xFF0C;&#x7EBF;&#x6027;&#x51FD;&#x6570;&#x7684;&#x8F6C;&#x6362;&#x53EF;&#x4EE5;&#x4F7F;&#x8F93;&#x51FA;&#x5927;&#x4E8E;&#x67D0;&#x4E2A;&#x503C;&#x3002;</p>
<p>&#x53C2;&#x6570;&#xFF1A;</p>
<ul>
<li>beta&#xFF1A;Softplus&#x51FD;&#x6570;&#x7684;beta&#x503C;</li>
<li>threshold&#xFF1A;&#x9608;&#x503C;</li>
</ul>
<p>shape&#xFF1A;</p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;(N, *)&#xFF0C;*&#x8868;&#x793A;&#x4EFB;&#x610F;&#x7EF4;&#x5EA6;&#x7EC4;&#x5408;</li>
<li>&#x8F93;&#x51FA;&#xFF1A;(N, *)&#xFF0C;&#x4E0E;&#x8F93;&#x5165;&#x6709;&#x76F8;&#x540C;&#x7684;shape&#x5C5E;&#x6027;</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Softplus()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>print(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.Softshrink(lambd=0.5)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Softshrink" target="_blank"><font size="2">[source]</font></a></p>
</blockquote>
<p>&#x5BF9;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x8FD0;&#x7528;Softshrink&#x51FD;&#x6570;&#xFF0C;Softshrink&#x51FD;&#x6570;&#x5B9A;&#x4E49;&#x5982;&#x4E0B;&#xFF1A;</p>
<p><script type="math/tex; mode=display">
f(x) = x-lambda, if\ x > lambda\\
f(x) = x+lambda, if\ x < -lambda\\
f(x) = 0, otherwise
</script></p>
<p>&#x53C2;&#x6570;&#xFF1A;</p>
<p>lambd&#xFF1A;Softshrink&#x51FD;&#x6570;&#x7684;lambda&#x503C;&#xFF0C;&#x9ED8;&#x8BA4;&#x4E3A;0.5</p>
<p>shape&#xFF1A;</p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;(N, *)&#xFF0C;*&#x8868;&#x793A;&#x4EFB;&#x610F;&#x7EF4;&#x5EA6;&#x7EC4;&#x5408;</li>
<li>&#x8F93;&#x51FA;&#xFF1A;(N, *)&#xFF0C;&#x4E0E;&#x8F93;&#x5165;&#x6709;&#x76F8;&#x540C;&#x7684;shape&#x5C5E;&#x6027;</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Softshrink()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>print(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.Softsign <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Softsign" target="_blank"><font size="2">[source]</font></a></p>
</blockquote>
<p>$f(x) = x / (1 + |x|)$</p>
<p>shape&#xFF1A;</p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;(N, *)&#xFF0C;*&#x8868;&#x793A;&#x4EFB;&#x610F;&#x7EF4;&#x5EA6;&#x7EC4;&#x5408;</li>
<li>&#x8F93;&#x51FA;&#xFF1A;(N, *)&#xFF0C;&#x4E0E;&#x8F93;&#x5165;&#x6709;&#x76F8;&#x540C;&#x7684;shape&#x5C5E;&#x6027;</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Softsign()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>print(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.Softshrink(lambd=0.5)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Softshrink" target="_blank"><font size="2">[source]</font></a></p>
</blockquote>
<p>&#x5BF9;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x8FD0;&#x7528;Tanhshrink&#x51FD;&#x6570;&#xFF0C;Tanhshrink&#x51FD;&#x6570;&#x5B9A;&#x4E49;&#x5982;&#x4E0B;&#xFF1A;</p>
<p><script type="math/tex; mode=display">
Tanhshrink(x) = x - Tanh(x)
</script></p>
<p>shape&#xFF1A;</p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;(N, *)&#xFF0C;*&#x8868;&#x793A;&#x4EFB;&#x610F;&#x7EF4;&#x5EA6;&#x7EC4;&#x5408;</li>
<li>&#x8F93;&#x51FA;&#xFF1A;(N, *)&#xFF0C;&#x4E0E;&#x8F93;&#x5165;&#x6709;&#x76F8;&#x540C;&#x7684;shape&#x5C5E;&#x6027;</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Tanhshrink()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">2</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>print(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.Softmin <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Softmin" target="_blank"><font size="2">[source]</font></a></p>
</blockquote>
<p>&#x5BF9;n&#x7EF4;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x8FD0;&#x7528;Softmin&#x51FD;&#x6570;&#xFF0C;&#x5C06;&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7F29;&#x653E;&#x5230;&#xFF08;0,1&#xFF09;&#x533A;&#x95F4;&#x4E14;&#x548C;&#x4E3A;1&#x3002;Softmin&#x51FD;&#x6570;&#x5B9A;&#x4E49;&#x5982;&#x4E0B;&#xFF1A;</p>
<p><script type="math/tex; ">f_i(x) = \frac{e^{(-x_i - shift)}} { \sum^j e^{(-x_j - shift)}},shift = max (x_i)</script></p>
<p>shape&#xFF1A;</p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;(N, L)</li>
<li>&#x8F93;&#x51FA;&#xFF1A;(N, L)</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Softmin()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>print(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(m(input))
</code></pre>
<hr>
<blockquote>
<p>class torch.nn.Softmax <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Softmax" target="_blank"><font size="2">[source]</font></a></p>
</blockquote>
<p>&#x5BF9;n&#x7EF4;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x8FD0;&#x7528;Softmax&#x51FD;&#x6570;&#xFF0C;&#x5C06;&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7F29;&#x653E;&#x5230;&#xFF08;0,1&#xFF09;&#x533A;&#x95F4;&#x4E14;&#x548C;&#x4E3A;1&#x3002;Softmax&#x51FD;&#x6570;&#x5B9A;&#x4E49;&#x5982;&#x4E0B;&#xFF1A;</p>
<p><script type="math/tex; ">f_i(x) = \frac{e^{(x_i - shift)}} { \sum^j e^{(x_j - shift)}},shift = max (x_i)</script></p>
<p>shape&#xFF1A;</p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;(N, L)</li>
<li>&#x8F93;&#x51FA;&#xFF1A;(N, L)</li>
</ul>
<p>&#x8FD4;&#x56DE;&#x7ED3;&#x679C;&#x662F;&#x4E00;&#x4E2A;&#x4E0E;&#x8F93;&#x5165;&#x7EF4;&#x5EA6;&#x76F8;&#x540C;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x53D6;&#x503C;&#x8303;&#x56F4;&#x5728;&#xFF08;0,1&#xFF09;&#x533A;&#x95F4;&#x3002;</p>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Softmax()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>print(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.LogSoftmax <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#LogSoftmax" target="_blank"><font size="2">[source]</font></a></p>
</blockquote>
<p>&#x5BF9;n&#x7EF4;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x8FD0;&#x7528;LogSoftmax&#x51FD;&#x6570;&#xFF0C;LogSoftmax&#x51FD;&#x6570;&#x5B9A;&#x4E49;&#x5982;&#x4E0B;&#xFF1A;</p>
<p><script type="math/tex; ">f_i(x) = log \frac{e^{(x_i)}} {a}, a = \sum^j e^{(x_j)}</script></p>
<p>shape&#xFF1A;</p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;(N, L)</li>
<li>&#x8F93;&#x51FA;&#xFF1A;(N, L)</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.LogSoftmax()
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>print(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(m(input))
</code></pre>
<h2 id="normalization-layers-source">Normalization layers <a href="https://pytorch.org/docs/nn.html#normalization-layers" target="_blank"><font size="2">[source]</font></a></h2>
<h3 id="class-torchnnbatchnorm1dnumfeatures-eps1e-05-momentum01-affinetrue-source">class torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True) <a href="https://pytorch.org/docs/nn.html#torch.nn.BatchNorm1d" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x5BF9;&#x5C0F;&#x6279;&#x91CF;(mini-batch)&#x7684;2d&#x6216;3d&#x8F93;&#x5165;&#x8FDB;&#x884C;&#x6279;&#x6807;&#x51C6;&#x5316;(Batch Normalization)&#x64CD;&#x4F5C;</p>
<p><script type="math/tex; "> y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + beta </script></p>
<p>&#x5728;&#x6BCF;&#x4E00;&#x4E2A;&#x5C0F;&#x6279;&#x91CF;&#xFF08;mini-batch&#xFF09;&#x6570;&#x636E;&#x4E2D;&#xFF0C;&#x8BA1;&#x7B97;&#x8F93;&#x5165;&#x5404;&#x4E2A;&#x7EF4;&#x5EA6;&#x7684;&#x5747;&#x503C;&#x548C;&#x6807;&#x51C6;&#x5DEE;&#x3002;gamma&#x4E0E;beta&#x662F;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x5927;&#x5C0F;&#x4E3A;C&#x7684;&#x53C2;&#x6570;&#x5411;&#x91CF;&#xFF08;C&#x4E3A;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#xFF09;</p>
<p>&#x5728;&#x8BAD;&#x7EC3;&#x65F6;&#xFF0C;&#x8BE5;&#x5C42;&#x8BA1;&#x7B97;&#x6BCF;&#x6B21;&#x8F93;&#x5165;&#x7684;&#x5747;&#x503C;&#x4E0E;&#x65B9;&#x5DEE;&#xFF0C;&#x5E76;&#x8FDB;&#x884C;&#x79FB;&#x52A8;&#x5E73;&#x5747;&#x3002;&#x79FB;&#x52A8;&#x5E73;&#x5747;&#x9ED8;&#x8BA4;&#x7684;&#x52A8;&#x91CF;&#x503C;&#x4E3A;0.1&#x3002;</p>
<p>&#x5728;&#x9A8C;&#x8BC1;&#x65F6;&#xFF0C;&#x8BAD;&#x7EC3;&#x6C42;&#x5F97;&#x7684;&#x5747;&#x503C;/&#x65B9;&#x5DEE;&#x5C06;&#x7528;&#x4E8E;&#x6807;&#x51C6;&#x5316;&#x9A8C;&#x8BC1;&#x6570;&#x636E;&#x3002;</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li><strong>num_features&#xFF1A;</strong> &#x6765;&#x81EA;&#x671F;&#x671B;&#x8F93;&#x5165;&#x7684;&#x7279;&#x5F81;&#x6570;&#xFF0C;&#x8BE5;&#x671F;&#x671B;&#x8F93;&#x5165;&#x7684;&#x5927;&#x5C0F;&#x4E3A;&apos;batch_size x num_features [x width]&apos;</li>
<li><strong>eps&#xFF1A;</strong> &#x4E3A;&#x4FDD;&#x8BC1;&#x6570;&#x503C;&#x7A33;&#x5B9A;&#x6027;&#xFF08;&#x5206;&#x6BCD;&#x4E0D;&#x80FD;&#x8D8B;&#x8FD1;&#x6216;&#x53D6;0&#xFF09;,&#x7ED9;&#x5206;&#x6BCD;&#x52A0;&#x4E0A;&#x7684;&#x503C;&#x3002;&#x9ED8;&#x8BA4;&#x4E3A;1e-5&#x3002;</li>
<li><strong>momentum&#xFF1A;</strong> &#x52A8;&#x6001;&#x5747;&#x503C;&#x548C;&#x52A8;&#x6001;&#x65B9;&#x5DEE;&#x6240;&#x4F7F;&#x7528;&#x7684;&#x52A8;&#x91CF;&#x3002;&#x9ED8;&#x8BA4;&#x4E3A;0.1&#x3002;</li>
<li><strong>affine&#xFF1A;</strong> &#x4E00;&#x4E2A;&#x5E03;&#x5C14;&#x503C;&#xFF0C;&#x5F53;&#x8BBE;&#x4E3A;true&#xFF0C;&#x7ED9;&#x8BE5;&#x5C42;&#x6DFB;&#x52A0;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x4EFF;&#x5C04;&#x53D8;&#x6362;&#x53C2;&#x6570;&#x3002;</li>
</ul>
<p><strong>Shape&#xFF1A;</strong></p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;&#xFF08;N, C&#xFF09;&#x6216;&#x8005;(N, C, L)</li>
<li>&#x8F93;&#x51FA;&#xFF1A;&#xFF08;N, C&#xFF09;&#x6216;&#x8005;&#xFF08;N&#xFF0C;C&#xFF0C;L&#xFF09;&#xFF08;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x76F8;&#x540C;&#xFF09;</li>
</ul>
<p><strong>&#x4F8B;&#x5B50;</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.BatchNorm1d(<span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Without Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.BatchNorm1d(<span class="hljs-number">100</span>, affine=<span class="hljs-keyword">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">100</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<hr>
<h3 id="class-torchnnbatchnorm2dnumfeatures-eps1e-05-momentum01-affinetruesource">class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/batchnorm.html#BatchNorm2d" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x5BF9;&#x5C0F;&#x6279;&#x91CF;(mini-batch)3d&#x6570;&#x636E;&#x7EC4;&#x6210;&#x7684;4d&#x8F93;&#x5165;&#x8FDB;&#x884C;&#x6279;&#x6807;&#x51C6;&#x5316;(Batch Normalization)&#x64CD;&#x4F5C;</p>
<p><script type="math/tex; "> y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + beta </script></p>
<p>&#x5728;&#x6BCF;&#x4E00;&#x4E2A;&#x5C0F;&#x6279;&#x91CF;&#xFF08;mini-batch&#xFF09;&#x6570;&#x636E;&#x4E2D;&#xFF0C;&#x8BA1;&#x7B97;&#x8F93;&#x5165;&#x5404;&#x4E2A;&#x7EF4;&#x5EA6;&#x7684;&#x5747;&#x503C;&#x548C;&#x6807;&#x51C6;&#x5DEE;&#x3002;gamma&#x4E0E;beta&#x662F;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x5927;&#x5C0F;&#x4E3A;C&#x7684;&#x53C2;&#x6570;&#x5411;&#x91CF;&#xFF08;C&#x4E3A;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#xFF09;</p>
<p>&#x5728;&#x8BAD;&#x7EC3;&#x65F6;&#xFF0C;&#x8BE5;&#x5C42;&#x8BA1;&#x7B97;&#x6BCF;&#x6B21;&#x8F93;&#x5165;&#x7684;&#x5747;&#x503C;&#x4E0E;&#x65B9;&#x5DEE;&#xFF0C;&#x5E76;&#x8FDB;&#x884C;&#x79FB;&#x52A8;&#x5E73;&#x5747;&#x3002;&#x79FB;&#x52A8;&#x5E73;&#x5747;&#x9ED8;&#x8BA4;&#x7684;&#x52A8;&#x91CF;&#x503C;&#x4E3A;0.1&#x3002;</p>
<p>&#x5728;&#x9A8C;&#x8BC1;&#x65F6;&#xFF0C;&#x8BAD;&#x7EC3;&#x6C42;&#x5F97;&#x7684;&#x5747;&#x503C;/&#x65B9;&#x5DEE;&#x5C06;&#x7528;&#x4E8E;&#x6807;&#x51C6;&#x5316;&#x9A8C;&#x8BC1;&#x6570;&#x636E;&#x3002;</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li><strong>num_features&#xFF1A;</strong> &#x6765;&#x81EA;&#x671F;&#x671B;&#x8F93;&#x5165;&#x7684;&#x7279;&#x5F81;&#x6570;&#xFF0C;&#x8BE5;&#x671F;&#x671B;&#x8F93;&#x5165;&#x7684;&#x5927;&#x5C0F;&#x4E3A;&apos;batch_size x num_features x height x width&apos;</li>
<li><strong>eps&#xFF1A;</strong> &#x4E3A;&#x4FDD;&#x8BC1;&#x6570;&#x503C;&#x7A33;&#x5B9A;&#x6027;&#xFF08;&#x5206;&#x6BCD;&#x4E0D;&#x80FD;&#x8D8B;&#x8FD1;&#x6216;&#x53D6;0&#xFF09;,&#x7ED9;&#x5206;&#x6BCD;&#x52A0;&#x4E0A;&#x7684;&#x503C;&#x3002;&#x9ED8;&#x8BA4;&#x4E3A;1e-5&#x3002;</li>
<li><strong>momentum&#xFF1A;</strong> &#x52A8;&#x6001;&#x5747;&#x503C;&#x548C;&#x52A8;&#x6001;&#x65B9;&#x5DEE;&#x6240;&#x4F7F;&#x7528;&#x7684;&#x52A8;&#x91CF;&#x3002;&#x9ED8;&#x8BA4;&#x4E3A;0.1&#x3002;</li>
<li><strong>affine&#xFF1A;</strong> &#x4E00;&#x4E2A;&#x5E03;&#x5C14;&#x503C;&#xFF0C;&#x5F53;&#x8BBE;&#x4E3A;true&#xFF0C;&#x7ED9;&#x8BE5;&#x5C42;&#x6DFB;&#x52A0;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x4EFF;&#x5C04;&#x53D8;&#x6362;&#x53C2;&#x6570;&#x3002;</li>
</ul>
<p><strong>Shape&#xFF1A;</strong></p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;&#xFF08;N, C&#xFF0C;H, W)</li>
<li>&#x8F93;&#x51FA;&#xFF1A;&#xFF08;N, C, H, W&#xFF09;&#xFF08;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x76F8;&#x540C;&#xFF09;</li>
</ul>
<p><strong>&#x4F8B;&#x5B50;</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.BatchNorm2d(<span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Without Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.BatchNorm2d(<span class="hljs-number">100</span>, affine=<span class="hljs-keyword">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">100</span>, <span class="hljs-number">35</span>, <span class="hljs-number">45</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<hr>
<h3 id="class-torchnnbatchnorm3dnumfeatures-eps1e-05-momentum01-affinetruesource">class torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True)<a href="https://pytorch.org/docs/nn.html#torch.nn.BatchNorm3d" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x5BF9;&#x5C0F;&#x6279;&#x91CF;(mini-batch)4d&#x6570;&#x636E;&#x7EC4;&#x6210;&#x7684;5d&#x8F93;&#x5165;&#x8FDB;&#x884C;&#x6279;&#x6807;&#x51C6;&#x5316;(Batch Normalization)&#x64CD;&#x4F5C;</p>
<p><script type="math/tex; "> y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + beta </script></p>
<p>&#x5728;&#x6BCF;&#x4E00;&#x4E2A;&#x5C0F;&#x6279;&#x91CF;&#xFF08;mini-batch&#xFF09;&#x6570;&#x636E;&#x4E2D;&#xFF0C;&#x8BA1;&#x7B97;&#x8F93;&#x5165;&#x5404;&#x4E2A;&#x7EF4;&#x5EA6;&#x7684;&#x5747;&#x503C;&#x548C;&#x6807;&#x51C6;&#x5DEE;&#x3002;gamma&#x4E0E;beta&#x662F;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x5927;&#x5C0F;&#x4E3A;C&#x7684;&#x53C2;&#x6570;&#x5411;&#x91CF;&#xFF08;C&#x4E3A;&#x8F93;&#x5165;&#x5927;&#x5C0F;&#xFF09;</p>
<p>&#x5728;&#x8BAD;&#x7EC3;&#x65F6;&#xFF0C;&#x8BE5;&#x5C42;&#x8BA1;&#x7B97;&#x6BCF;&#x6B21;&#x8F93;&#x5165;&#x7684;&#x5747;&#x503C;&#x4E0E;&#x65B9;&#x5DEE;&#xFF0C;&#x5E76;&#x8FDB;&#x884C;&#x79FB;&#x52A8;&#x5E73;&#x5747;&#x3002;&#x79FB;&#x52A8;&#x5E73;&#x5747;&#x9ED8;&#x8BA4;&#x7684;&#x52A8;&#x91CF;&#x503C;&#x4E3A;0.1&#x3002;</p>
<p>&#x5728;&#x9A8C;&#x8BC1;&#x65F6;&#xFF0C;&#x8BAD;&#x7EC3;&#x6C42;&#x5F97;&#x7684;&#x5747;&#x503C;/&#x65B9;&#x5DEE;&#x5C06;&#x7528;&#x4E8E;&#x6807;&#x51C6;&#x5316;&#x9A8C;&#x8BC1;&#x6570;&#x636E;&#x3002;</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li><strong>num_features&#xFF1A;</strong> &#x6765;&#x81EA;&#x671F;&#x671B;&#x8F93;&#x5165;&#x7684;&#x7279;&#x5F81;&#x6570;&#xFF0C;&#x8BE5;&#x671F;&#x671B;&#x8F93;&#x5165;&#x7684;&#x5927;&#x5C0F;&#x4E3A;&apos;batch_size x num_features depth x height x width&apos;</li>
<li><strong>eps&#xFF1A;</strong> &#x4E3A;&#x4FDD;&#x8BC1;&#x6570;&#x503C;&#x7A33;&#x5B9A;&#x6027;&#xFF08;&#x5206;&#x6BCD;&#x4E0D;&#x80FD;&#x8D8B;&#x8FD1;&#x6216;&#x53D6;0&#xFF09;,&#x7ED9;&#x5206;&#x6BCD;&#x52A0;&#x4E0A;&#x7684;&#x503C;&#x3002;&#x9ED8;&#x8BA4;&#x4E3A;1e-5&#x3002;</li>
<li><strong>momentum&#xFF1A;</strong> &#x52A8;&#x6001;&#x5747;&#x503C;&#x548C;&#x52A8;&#x6001;&#x65B9;&#x5DEE;&#x6240;&#x4F7F;&#x7528;&#x7684;&#x52A8;&#x91CF;&#x3002;&#x9ED8;&#x8BA4;&#x4E3A;0.1&#x3002;</li>
<li><strong>affine&#xFF1A;</strong> &#x4E00;&#x4E2A;&#x5E03;&#x5C14;&#x503C;&#xFF0C;&#x5F53;&#x8BBE;&#x4E3A;true&#xFF0C;&#x7ED9;&#x8BE5;&#x5C42;&#x6DFB;&#x52A0;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x4EFF;&#x5C04;&#x53D8;&#x6362;&#x53C2;&#x6570;&#x3002;</li>
</ul>
<p><strong>Shape&#xFF1A;</strong></p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;&#xFF08;N, C&#xFF0C;D, H, W)</li>
<li>&#x8F93;&#x51FA;&#xFF1A;&#xFF08;N, C, D, H, W&#xFF09;&#xFF08;&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x76F8;&#x540C;&#xFF09;</li>
</ul>
<p><strong>&#x4F8B;&#x5B50;</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.BatchNorm3d(<span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Without Learnable Parameters</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.BatchNorm3d(<span class="hljs-number">100</span>, affine=<span class="hljs-keyword">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">100</span>, <span class="hljs-number">35</span>, <span class="hljs-number">45</span>, <span class="hljs-number">10</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<hr>
<h2 id="recurrent-layers">Recurrent layers</h2>
<h3 id="class-torchnnrnn-args--kwargssource">class torch.nn.RNN(<em> args, *</em> kwargs)[source]</h3>
<p>&#x5C06;&#x4E00;&#x4E2A;&#x591A;&#x5C42;&#x7684; <code>Elman RNN</code>&#xFF0C;&#x6FC0;&#x6D3B;&#x51FD;&#x6570;&#x4E3A;<code>tanh</code>&#x6216;&#x8005;<code>ReLU</code>&#xFF0C;&#x7528;&#x4E8E;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x3002;</p>
<p>&#x5BF9;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x4E2D;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#xFF0C;<code>RNN</code>&#x6BCF;&#x5C42;&#x7684;&#x8BA1;&#x7B97;&#x516C;&#x5F0F;&#x4E3A;
<script type="math/tex; mode=display">
h_t=tanh(w_{ih}* x_t+b_{ih}+w_{hh}* h_{t-1}+b_{hh})
</script>
$h_t$&#x662F;&#x65F6;&#x523B;$t$&#x7684;&#x9690;&#x72B6;&#x6001;&#x3002; $x_t$&#x662F;&#x4E0A;&#x4E00;&#x5C42;&#x65F6;&#x523B;$t$&#x7684;&#x9690;&#x72B6;&#x6001;&#xFF0C;&#x6216;&#x8005;&#x662F;&#x7B2C;&#x4E00;&#x5C42;&#x5728;&#x65F6;&#x523B;$t$&#x7684;&#x8F93;&#x5165;&#x3002;&#x5982;&#x679C;<code>nonlinearity=&apos;relu&apos;</code>,&#x90A3;&#x4E48;&#x5C06;&#x4F7F;&#x7528;<code>relu</code>&#x4EE3;&#x66FF;<code>tanh</code>&#x4F5C;&#x4E3A;&#x6FC0;&#x6D3B;&#x51FD;&#x6570;&#x3002;</p>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;:</p>
<ul>
<li><p>input_size &#x2013; &#x8F93;&#x5165;<code>x</code>&#x7684;&#x7279;&#x5F81;&#x6570;&#x91CF;&#x3002;</p>
</li>
<li><p>hidden_size &#x2013; &#x9690;&#x5C42;&#x7684;&#x7279;&#x5F81;&#x6570;&#x91CF;&#x3002;</p>
</li>
<li><p>num_layers &#x2013; RNN&#x7684;&#x5C42;&#x6570;&#x3002;</p>
</li>
<li><p>nonlinearity &#x2013; &#x6307;&#x5B9A;&#x975E;&#x7EBF;&#x6027;&#x51FD;&#x6570;&#x4F7F;&#x7528;<code>tanh</code>&#x8FD8;&#x662F;<code>relu</code>&#x3002;&#x9ED8;&#x8BA4;&#x662F;<code>tanh</code>&#x3002;</p>
</li>
<li><p>bias &#x2013; &#x5982;&#x679C;&#x662F;<code>False</code>&#xFF0C;&#x90A3;&#x4E48;RNN&#x5C42;&#x5C31;&#x4E0D;&#x4F1A;&#x4F7F;&#x7528;&#x504F;&#x7F6E;&#x6743;&#x91CD; $b_ih$&#x548C;$b_hh$,&#x9ED8;&#x8BA4;&#x662F;<code>True</code></p>
</li>
<li><p>batch_first &#x2013; &#x5982;&#x679C;<code>True</code>&#x7684;&#x8BDD;&#xFF0C;&#x90A3;&#x4E48;&#x8F93;&#x5165;<code>Tensor</code>&#x7684;shape&#x5E94;&#x8BE5;&#x662F;[batch_size, time_step, feature],&#x8F93;&#x51FA;&#x4E5F;&#x662F;&#x8FD9;&#x6837;&#x3002;</p>
</li>
<li><p>dropout &#x2013; &#x5982;&#x679C;&#x503C;&#x975E;&#x96F6;&#xFF0C;&#x90A3;&#x4E48;&#x9664;&#x4E86;&#x6700;&#x540E;&#x4E00;&#x5C42;&#x5916;&#xFF0C;&#x5176;&#x5B83;&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x90FD;&#x4F1A;&#x5957;&#x4E0A;&#x4E00;&#x4E2A;<code>dropout</code>&#x5C42;&#x3002;</p>
</li>
<li><p>bidirectional &#x2013; &#x5982;&#x679C;<code>True</code>&#xFF0C;&#x5C06;&#x4F1A;&#x53D8;&#x6210;&#x4E00;&#x4E2A;&#x53CC;&#x5411;<code>RNN</code>&#xFF0C;&#x9ED8;&#x8BA4;&#x4E3A;<code>False</code>&#x3002;</p>
</li>
</ul>
<p><code>RNN</code>&#x7684;&#x8F93;&#x5165;&#xFF1A;
<strong>(input, h_0)</strong></p>
<ul>
<li><p>input (seq_len, batch, input_size): &#x4FDD;&#x5B58;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x7279;&#x5F81;&#x7684;<code>tensor</code>&#x3002;<code>input</code>&#x53EF;&#x4EE5;&#x662F;&#x88AB;&#x586B;&#x5145;&#x7684;&#x53D8;&#x957F;&#x7684;&#x5E8F;&#x5217;&#x3002;&#x7EC6;&#x8282;&#x8BF7;&#x770B;<code>torch.nn.utils.rnn.pack_padded_sequence()</code></p>
</li>
<li><p>h_0 (num_layers * num_directions, batch, hidden_size): &#x4FDD;&#x5B58;&#x7740;&#x521D;&#x59CB;&#x9690;&#x72B6;&#x6001;&#x7684;<code>tensor</code></p>
</li>
</ul>
<p><code>RNN</code>&#x7684;&#x8F93;&#x51FA;&#xFF1A;
<strong>(output, h_n)</strong></p>
<ul>
<li>output (seq_len, batch, hidden_size * num_directions): &#x4FDD;&#x5B58;&#x7740;<code>RNN</code>&#x6700;&#x540E;&#x4E00;&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x7279;&#x5F81;&#x3002;&#x5982;&#x679C;&#x8F93;&#x5165;&#x662F;&#x88AB;&#x586B;&#x5145;&#x8FC7;&#x7684;&#x5E8F;&#x5217;&#xFF0C;&#x90A3;&#x4E48;&#x8F93;&#x51FA;&#x4E5F;&#x662F;&#x88AB;&#x586B;&#x5145;&#x7684;&#x5E8F;&#x5217;&#x3002;</li>
<li>h_n (num_layers * num_directions, batch, hidden_size): &#x4FDD;&#x5B58;&#x7740;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x65F6;&#x523B;&#x9690;&#x72B6;&#x6001;&#x3002;</li>
</ul>
<p><code>RNN</code>&#x6A21;&#x578B;&#x53C2;&#x6570;:</p>
<ul>
<li><p>weight_ih_l[k] &#x2013; &#x7B2C;<code>k</code>&#x5C42;&#x7684; <code>input-hidden</code> &#x6743;&#x91CD;&#xFF0C; &#x53EF;&#x5B66;&#x4E60;&#xFF0C;&#x5F62;&#x72B6;&#x662F;<code>(input_size x hidden_size)</code>&#x3002;</p>
</li>
<li><p>weight_hh_l[k] &#x2013; &#x7B2C;<code>k</code>&#x5C42;&#x7684; <code>hidden-hidden</code> &#x6743;&#x91CD;&#xFF0C; &#x53EF;&#x5B66;&#x4E60;&#xFF0C;&#x5F62;&#x72B6;&#x662F;<code>(hidden_size x hidden_size)</code></p>
</li>
<li><p>bias_ih_l[k] &#x2013; &#x7B2C;<code>k</code>&#x5C42;&#x7684; <code>input-hidden</code> &#x504F;&#x7F6E;&#xFF0C; &#x53EF;&#x5B66;&#x4E60;&#xFF0C;&#x5F62;&#x72B6;&#x662F;<code>(hidden_size)</code></p>
</li>
<li><p>bias_hh_l[k] &#x2013; &#x7B2C;<code>k</code>&#x5C42;&#x7684; <code>hidden-hidden</code> &#x504F;&#x7F6E;&#xFF0C; &#x53EF;&#x5B66;&#x4E60;&#xFF0C;&#x5F62;&#x72B6;&#x662F;<code>(hidden_size)</code></p>
</li>
</ul>
<p>&#x793A;&#x4F8B;&#xFF1A;</p>
<pre><code class="lang-python">rnn = nn.RNN(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">2</span>)
input = Variable(torch.randn(<span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>))
h0 = Variable(torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">20</span>))
output, hn = rnn(input, h0)
</code></pre>
<h3 id="class-torchnnlstm-args--kwargssource">class torch.nn.LSTM(<em> args, *</em> kwargs)[source]</h3>
<p>&#x5C06;&#x4E00;&#x4E2A;&#x591A;&#x5C42;&#x7684; <code>(LSTM)</code> &#x5E94;&#x7528;&#x5230;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x3002;</p>
<p>&#x5BF9;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#xFF0C;<code>LSTM</code>&#x7684;&#x6BCF;&#x5C42;&#x90FD;&#x4F1A;&#x6267;&#x884C;&#x4EE5;&#x4E0B;&#x8BA1;&#x7B97;&#xFF1A;
<script type="math/tex; mode=display">
\begin{aligned}
i_t &= sigmoid(W_{ii}x_t+b_{ii}+W_{hi}h_{t-1}+b_{hi}) \\
f_t &= sigmoid(W_{if}x_t+b_{if}+W_{hf}h_{t-1}+b_{hf}) \\
o_t &= sigmoid(W_{io}x_t+b_{io}+W_{ho}h_{t-1}+b_{ho})\\
g_t &= tanh(W_{ig}x_t+b_{ig}+W_{hg}h_{t-1}+b_{hg})\\
c_t &= f_t*c_{t-1}+i_t*g_t\\
h_t &= o_t*tanh(c_t)
\end{aligned}
</script>
$h_t$&#x662F;&#x65F6;&#x523B;$t$&#x7684;&#x9690;&#x72B6;&#x6001;,$c_t$&#x662F;&#x65F6;&#x523B;$t$&#x7684;&#x7EC6;&#x80DE;&#x72B6;&#x6001;&#xFF0C;$x_t$&#x662F;&#x4E0A;&#x4E00;&#x5C42;&#x7684;&#x5728;&#x65F6;&#x523B;$t$&#x7684;&#x9690;&#x72B6;&#x6001;&#x6216;&#x8005;&#x662F;&#x7B2C;&#x4E00;&#x5C42;&#x5728;&#x65F6;&#x523B;$t$&#x7684;&#x8F93;&#x5165;&#x3002;$i_t, f_t, g_t, o_t$ &#x5206;&#x522B;&#x4EE3;&#x8868; &#x8F93;&#x5165;&#x95E8;&#xFF0C;&#x9057;&#x5FD8;&#x95E8;&#xFF0C;&#x7EC6;&#x80DE;&#x548C;&#x8F93;&#x51FA;&#x95E8;&#x3002;</p>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;:</p>
<ul>
<li><p>input_size &#x2013; &#x8F93;&#x5165;&#x7684;&#x7279;&#x5F81;&#x7EF4;&#x5EA6;</p>
</li>
<li><p>hidden_size &#x2013; &#x9690;&#x72B6;&#x6001;&#x7684;&#x7279;&#x5F81;&#x7EF4;&#x5EA6;</p>
</li>
<li><p>num_layers &#x2013; &#x5C42;&#x6570;&#xFF08;&#x548C;&#x65F6;&#x5E8F;&#x5C55;&#x5F00;&#x8981;&#x533A;&#x5206;&#x5F00;&#xFF09;</p>
</li>
<li><p>bias &#x2013; &#x5982;&#x679C;&#x4E3A;<code>False</code>&#xFF0C;&#x90A3;&#x4E48;<code>LSTM</code>&#x5C06;&#x4E0D;&#x4F1A;&#x4F7F;&#x7528;$b<em>{ih},b</em>{hh}$&#xFF0C;&#x9ED8;&#x8BA4;&#x4E3A;<code>True</code>&#x3002;</p>
</li>
<li><p>batch_first &#x2013; &#x5982;&#x679C;&#x4E3A;<code>True</code>&#xFF0C;&#x90A3;&#x4E48;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;<code>Tensor</code>&#x7684;&#x5F62;&#x72B6;&#x4E3A;<code>(batch, seq, feature)</code></p>
</li>
<li><p>dropout &#x2013; &#x5982;&#x679C;&#x975E;&#x96F6;&#x7684;&#x8BDD;&#xFF0C;&#x5C06;&#x4F1A;&#x5728;<code>RNN</code>&#x7684;&#x8F93;&#x51FA;&#x4E0A;&#x52A0;&#x4E2A;<code>dropout</code>&#xFF0C;&#x6700;&#x540E;&#x4E00;&#x5C42;&#x9664;&#x5916;&#x3002;</p>
</li>
<li><p>bidirectional &#x2013; &#x5982;&#x679C;&#x4E3A;<code>True</code>&#xFF0C;&#x5C06;&#x4F1A;&#x53D8;&#x6210;&#x4E00;&#x4E2A;&#x53CC;&#x5411;<code>RNN</code>&#xFF0C;&#x9ED8;&#x8BA4;&#x4E3A;<code>False</code>&#x3002;</p>
</li>
</ul>
<p><code>LSTM</code>&#x8F93;&#x5165;:
input, (h_0, c_0)</p>
<ul>
<li><p>input (seq_len, batch, input_size): &#x5305;&#x542B;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x7279;&#x5F81;&#x7684;<code>Tensor</code>&#x3002;&#x4E5F;&#x53EF;&#x4EE5;&#x662F;<code>packed variable</code> &#xFF0C;&#x8BE6;&#x89C1; <a href="#torch.nn.utils.rnn.pack_padded_sequence(input,%20lengths,%20batch_first=False[source]">pack_padded_sequence</a></p>
</li>
<li><p>h_0 (num_layers * num_directions, batch, hidden_size):&#x4FDD;&#x5B58;&#x7740;<code>batch</code>&#x4E2D;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x521D;&#x59CB;&#x5316;&#x9690;&#x72B6;&#x6001;&#x7684;<code>Tensor</code></p>
</li>
<li><p>c_0 (num_layers * num_directions, batch, hidden_size): &#x4FDD;&#x5B58;&#x7740;<code>batch</code>&#x4E2D;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x521D;&#x59CB;&#x5316;&#x7EC6;&#x80DE;&#x72B6;&#x6001;&#x7684;<code>Tensor</code></p>
</li>
</ul>
<p><code>LSTM</code>&#x8F93;&#x51FA;
output, (h_n, c_n)</p>
<ul>
<li><p>output (seq_len, batch, hidden_size * num_directions): &#x4FDD;&#x5B58;<code>RNN</code>&#x6700;&#x540E;&#x4E00;&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x7684;<code>Tensor</code>&#x3002; &#x5982;&#x679C;&#x8F93;&#x5165;&#x662F;<code>torch.nn.utils.rnn.PackedSequence</code>&#xFF0C;&#x90A3;&#x4E48;&#x8F93;&#x51FA;&#x4E5F;&#x662F;<code>torch.nn.utils.rnn.PackedSequence</code>&#x3002;</p>
</li>
<li><p>h_n (num_layers * num_directions, batch, hidden_size): <code>Tensor</code>&#xFF0C;&#x4FDD;&#x5B58;&#x7740;<code>RNN</code>&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x65F6;&#x95F4;&#x6B65;&#x7684;&#x9690;&#x72B6;&#x6001;&#x3002;</p>
</li>
<li><p>c_n (num_layers * num_directions, batch, hidden_size): <code>Tensor</code>&#xFF0C;&#x4FDD;&#x5B58;&#x7740;<code>RNN</code>&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x65F6;&#x95F4;&#x6B65;&#x7684;&#x7EC6;&#x80DE;&#x72B6;&#x6001;&#x3002;</p>
</li>
</ul>
<p><code>LSTM</code>&#x6A21;&#x578B;&#x53C2;&#x6570;:</p>
<ul>
<li><p>weight<em>ih_l[k] &#x2013; &#x7B2C;<code>k</code>&#x5C42;&#x53EF;&#x5B66;&#x4E60;&#x7684;<code>input-hidden</code>&#x6743;&#x91CD;($W</em>{ii}|W<em>{if}|W</em>{ig}|W_{io}$)&#xFF0C;&#x5F62;&#x72B6;&#x4E3A;<code>(input_size x 4*hidden_size)</code></p>
</li>
<li><p>weight<em>hh_l[k] &#x2013; &#x7B2C;<code>k</code>&#x5C42;&#x53EF;&#x5B66;&#x4E60;&#x7684;<code>hidden-hidden</code>&#x6743;&#x91CD;($W</em>{hi}|W<em>{hf}|W</em>{hg}|W_{ho}$)&#xFF0C;&#x5F62;&#x72B6;&#x4E3A;<code>(hidden_size x 4*hidden_size)</code>&#x3002;</p>
</li>
<li><p>bias<em>ih_l[k] &#x2013; &#x7B2C;<code>k</code>&#x5C42;&#x53EF;&#x5B66;&#x4E60;&#x7684;<code>input-hidden</code>&#x504F;&#x7F6E;($b</em>{ii}|b<em>{if}|b</em>{ig}|b_{io}$)&#xFF0C;&#x5F62;&#x72B6;&#x4E3A;<code>( 4*hidden_size)</code></p>
</li>
<li><p>bias<em>hh_l[k] &#x2013; &#x7B2C;<code>k</code>&#x5C42;&#x53EF;&#x5B66;&#x4E60;&#x7684;<code>hidden-hidden</code>&#x504F;&#x7F6E;($b</em>{hi}|b<em>{hf}|b</em>{hg}|b_{ho}$)&#xFF0C;&#x5F62;&#x72B6;&#x4E3A;<code>( 4*hidden_size)</code>&#x3002;
&#x793A;&#x4F8B;:</p>
<pre><code class="lang-python">lstm = nn.LSTM(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">2</span>)
input = Variable(torch.randn(<span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>))
h0 = Variable(torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">20</span>))
c0 = Variable(torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">20</span>))
output, hn = lstm(input, (h0, c0))
</code></pre>
</li>
</ul>
<h3 id="class-torchnngru-args--kwargssource">class torch.nn.GRU(<em> args, *</em> kwargs)[source]</h3>
<p>&#x5C06;&#x4E00;&#x4E2A;&#x591A;&#x5C42;&#x7684;<code>GRU</code>&#x7528;&#x4E8E;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x3002;</p>
<p>&#x5BF9;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#xFF0C;&#x6BCF;&#x5C42;&#x8FDB;&#x884C;&#x4E86;&#x4E00;&#x4E0B;&#x8BA1;&#x7B97;&#xFF1A;</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
r_t&=sigmoid(W_{ir}x_t+b_{ir}+W_{hr}h_{(t-1)}+b_{hr})\\
i_t&=sigmoid(W_{ii}x_t+b_{ii}+W_{hi}h_{(t-1)}+b_{hi})\\
n_t&=tanh(W_{in}x_t+b_{in}+rt*(W_{hn}h_{(t-1)}+b_{hn}))\\
h_t&=(1-i_t)* nt+i_t*h(t-1)
\end{aligned}
</script>
$h_t$&#x662F;&#x662F;&#x65F6;&#x95F4;$t$&#x7684;&#x4E0A;&#x7684;&#x9690;&#x72B6;&#x6001;&#xFF0C;$x_t$&#x662F;&#x524D;&#x4E00;&#x5C42;$t$&#x65F6;&#x523B;&#x7684;&#x9690;&#x72B6;&#x6001;&#x6216;&#x8005;&#x662F;&#x7B2C;&#x4E00;&#x5C42;&#x7684;$t$&#x65F6;&#x523B;&#x7684;&#x8F93;&#x5165;&#xFF0C;$r_t, i_t, n_t$&#x5206;&#x522B;&#x662F;&#x91CD;&#x7F6E;&#x95E8;&#xFF0C;&#x8F93;&#x5165;&#x95E8;&#x548C;&#x65B0;&#x95E8;&#x3002;</p>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;&#xFF1A;</p>
<ul>
<li>input_size &#x2013; &#x671F;&#x671B;&#x7684;&#x8F93;&#x5165;$x$&#x7684;&#x7279;&#x5F81;&#x503C;&#x7684;&#x7EF4;&#x5EA6;</li>
<li>hidden_size &#x2013; &#x9690;&#x72B6;&#x6001;&#x7684;&#x7EF4;&#x5EA6;</li>
<li>num_layers &#x2013; <code>RNN</code>&#x7684;&#x5C42;&#x6570;&#x3002;</li>
<li>bias &#x2013; &#x5982;&#x679C;&#x4E3A;<code>False</code>&#xFF0C;&#x90A3;&#x4E48;<code>RNN</code>&#x5C42;&#x5C06;&#x4E0D;&#x4F1A;&#x4F7F;&#x7528;<code>bias</code>&#xFF0C;&#x9ED8;&#x8BA4;&#x4E3A;<code>True</code>&#x3002;</li>
<li>batch_first &#x2013; &#x5982;&#x679C;&#x4E3A;<code>True</code>&#x7684;&#x8BDD;&#xFF0C;&#x90A3;&#x4E48;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x7684;<code>tensor</code>&#x7684;&#x5F62;&#x72B6;&#x662F;<code>(batch, seq, feature)</code>&#x3002;</li>
<li>dropout &#x2013;  &#x5982;&#x679C;&#x975E;&#x96F6;&#x7684;&#x8BDD;&#xFF0C;&#x5C06;&#x4F1A;&#x5728;<code>RNN</code>&#x7684;&#x8F93;&#x51FA;&#x4E0A;&#x52A0;&#x4E2A;<code>dropout</code>&#xFF0C;&#x6700;&#x540E;&#x4E00;&#x5C42;&#x9664;&#x5916;&#x3002;</li>
<li>bidirectional &#x2013; &#x5982;&#x679C;&#x4E3A;<code>True</code>&#xFF0C;&#x5C06;&#x4F1A;&#x53D8;&#x6210;&#x4E00;&#x4E2A;&#x53CC;&#x5411;<code>RNN</code>&#xFF0C;&#x9ED8;&#x8BA4;&#x4E3A;<code>False</code>&#x3002;</li>
</ul>
<p>&#x8F93;&#x5165;&#xFF1A;
input, h_0</p>
<ul>
<li><p>input (seq_len, batch, input_size):  &#x5305;&#x542B;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x7279;&#x5F81;&#x7684;<code>Tensor</code>&#x3002;&#x4E5F;&#x53EF;&#x4EE5;&#x662F;<code>packed variable</code> &#xFF0C;&#x8BE6;&#x89C1; <a href="#torch.nn.utils.rnn.pack_padded_sequence(input,%20lengths,%20batch_first=False[source]">pack_padded_sequence</a>&#x3002;</p>
</li>
<li><p>h_0 (num_layers * num_directions, batch, hidden_size):&#x4FDD;&#x5B58;&#x7740;<code>batch</code>&#x4E2D;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x521D;&#x59CB;&#x5316;&#x9690;&#x72B6;&#x6001;&#x7684;<code>Tensor</code></p>
</li>
</ul>
<p>&#x8F93;&#x51FA;&#xFF1A;
output, h_n</p>
<ul>
<li><p>output (seq_len, batch, hidden_size * num_directions): ten&#x4FDD;&#x5B58;<code>RNN</code>&#x6700;&#x540E;&#x4E00;&#x5C42;&#x7684;&#x8F93;&#x51FA;&#x7684;<code>Tensor</code>&#x3002; &#x5982;&#x679C;&#x8F93;&#x5165;&#x662F;<code>torch.nn.utils.rnn.PackedSequence</code>&#xFF0C;&#x90A3;&#x4E48;&#x8F93;&#x51FA;&#x4E5F;&#x662F;<code>torch.nn.utils.rnn.PackedSequence</code>&#x3002;</p>
</li>
<li><p>h_n (num_layers * num_directions, batch, hidden_size): <code>Tensor</code>&#xFF0C;&#x4FDD;&#x5B58;&#x7740;<code>RNN</code>&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x65F6;&#x95F4;&#x6B65;&#x7684;&#x9690;&#x72B6;&#x6001;&#x3002;</p>
</li>
</ul>
<p>&#x53D8;&#x91CF;&#xFF1A;</p>
<ul>
<li><p>weight<em>ih_l[k] &#x2013; &#x7B2C;<code>k</code>&#x5C42;&#x53EF;&#x5B66;&#x4E60;&#x7684;<code>input-hidden</code>&#x6743;&#x91CD;($W</em>{ir}|W<em>{ii}|W</em>{in}$)&#xFF0C;&#x5F62;&#x72B6;&#x4E3A;<code>(input_size x 3*hidden_size)</code></p>
</li>
<li><p>weight<em>hh_l[k] &#x2013; &#x7B2C;<code>k</code>&#x5C42;&#x53EF;&#x5B66;&#x4E60;&#x7684;<code>hidden-hidden</code>&#x6743;&#x91CD;($W</em>{hr}|W<em>{hi}|W</em>{hn}$)&#xFF0C;&#x5F62;&#x72B6;&#x4E3A;<code>(hidden_size x 3*hidden_size)</code>&#x3002;</p>
</li>
<li><p>bias<em>ih_l[k] &#x2013; &#x7B2C;<code>k</code>&#x5C42;&#x53EF;&#x5B66;&#x4E60;&#x7684;<code>input-hidden</code>&#x504F;&#x7F6E;($b</em>{ir}|b<em>{ii}|b</em>{in}$)&#xFF0C;&#x5F62;&#x72B6;&#x4E3A;<code>( 3*hidden_size)</code></p>
</li>
<li><p>bias<em>hh_l[k] &#x2013; &#x7B2C;<code>k</code>&#x5C42;&#x53EF;&#x5B66;&#x4E60;&#x7684;<code>hidden-hidden</code>&#x504F;&#x7F6E;($b</em>{hr}|b<em>{hi}|b</em>{hn}$)&#xFF0C;&#x5F62;&#x72B6;&#x4E3A;<code>( 3*hidden_size)</code>&#x3002;</p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"> rnn = nn.GRU(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">2</span>)
 input = Variable(torch.randn(<span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>))
 h0 = Variable(torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">20</span>))
 output, hn = rnn(input, h0)
</code></pre>
<h3 id="class-torchnnrnncellinputsize-hiddensize-biastrue-nonlinearitytanhsource">class torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity=&apos;tanh&apos;)[source]</h3>
<p>&#x4E00;&#x4E2A; <code>Elan RNN cell</code>&#xFF0C;&#x6FC0;&#x6D3B;&#x51FD;&#x6570;&#x662F;<code>tanh</code>&#x6216;<code>ReLU</code>&#xFF0C;&#x7528;&#x4E8E;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x3002;
&#x5C06;&#x4E00;&#x4E2A;&#x591A;&#x5C42;&#x7684; <code>Elman RNNCell</code>&#xFF0C;&#x6FC0;&#x6D3B;&#x51FD;&#x6570;&#x4E3A;<code>tanh</code>&#x6216;&#x8005;<code>ReLU</code>&#xFF0C;&#x7528;&#x4E8E;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x3002;
<script type="math/tex; mode=display">
h'=tanh(w_{ih}* x+b_{ih}+w_{hh}* h+b_{hh})
</script>
&#x5982;&#x679C;<code>nonlinearity=relu</code>&#xFF0C;&#x90A3;&#x4E48;&#x5C06;&#x4F1A;&#x4F7F;&#x7528;<code>ReLU</code>&#x6765;&#x4EE3;&#x66FF;<code>tanh</code>&#x3002;</p>
<p>&#x53C2;&#x6570;&#xFF1A;</p>
<ul>
<li><p>input_size &#x2013; &#x8F93;&#x5165;$x$&#xFF0C;&#x7279;&#x5F81;&#x7684;&#x7EF4;&#x5EA6;&#x3002;</p>
</li>
<li><p>hidden_size &#x2013; &#x9690;&#x72B6;&#x6001;&#x7279;&#x5F81;&#x7684;&#x7EF4;&#x5EA6;&#x3002;</p>
</li>
<li><p>bias &#x2013; &#x5982;&#x679C;&#x4E3A;<code>False</code>&#xFF0C;<code>RNN cell</code>&#x4E2D;&#x5C06;&#x4E0D;&#x4F1A;&#x52A0;&#x5165;<code>bias</code>&#xFF0C;&#x9ED8;&#x8BA4;&#x4E3A;<code>True</code>&#x3002;</p>
</li>
<li><p>nonlinearity &#x2013; &#x7528;&#x4E8E;&#x9009;&#x62E9;&#x975E;&#x7EBF;&#x6027;&#x6FC0;&#x6D3B;&#x51FD;&#x6570; [<code>tanh</code>|<code>relu</code>]. &#x9ED8;&#x8BA4;&#x503C;&#x4E3A;&#xFF1A; <code>tanh</code></p>
</li>
</ul>
<p>&#x8F93;&#x5165;&#xFF1A;
input, hidden</p>
<ul>
<li><p>input (batch, input_size): &#x5305;&#x542B;&#x8F93;&#x5165;&#x7279;&#x5F81;&#x7684;<code>tensor</code>&#x3002;</p>
</li>
<li><p>hidden (batch, hidden_size): &#x4FDD;&#x5B58;&#x7740;&#x521D;&#x59CB;&#x9690;&#x72B6;&#x6001;&#x503C;&#x7684;<code>tensor</code>&#x3002;</p>
</li>
</ul>
<p>&#x8F93;&#x51FA;&#xFF1A; h&#x2019;</p>
<ul>
<li>h&#x2019; (batch, hidden_size):&#x4E0B;&#x4E00;&#x4E2A;&#x65F6;&#x523B;&#x7684;&#x9690;&#x72B6;&#x6001;&#x3002;</li>
</ul>
<p>&#x53D8;&#x91CF;&#xFF1A;</p>
<ul>
<li><p>weight_ih &#x2013;  <code>input-hidden</code> &#x6743;&#x91CD;&#xFF0C; &#x53EF;&#x5B66;&#x4E60;&#xFF0C;&#x5F62;&#x72B6;&#x662F;<code>(input_size x hidden_size)</code>&#x3002;</p>
</li>
<li><p>weight_hh &#x2013;  <code>hidden-hidden</code> &#x6743;&#x91CD;&#xFF0C; &#x53EF;&#x5B66;&#x4E60;&#xFF0C;&#x5F62;&#x72B6;&#x662F;<code>(hidden_size x hidden_size)</code></p>
</li>
<li><p>bias_ih &#x2013;  <code>input-hidden</code> &#x504F;&#x7F6E;&#xFF0C; &#x53EF;&#x5B66;&#x4E60;&#xFF0C;&#x5F62;&#x72B6;&#x662F;<code>(hidden_size)</code></p>
</li>
<li><p>bias_hh &#x2013;  <code>hidden-hidden</code> &#x504F;&#x7F6E;&#xFF0C; &#x53EF;&#x5B66;&#x4E60;&#xFF0C;&#x5F62;&#x72B6;&#x662F;<code>(hidden_size)</code></p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python">rnn = nn.RNNCell(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>)
input = Variable(torch.randn(<span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>))
hx = Variable(torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">20</span>))
output = []
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">6</span>):
   hx = rnn(input[i], hx)
   output.append(hx)
</code></pre>
<h3 id="class-torchnnlstmcellinputsize-hiddensize-biastruesource">class torch.nn.LSTMCell(input_size, hidden_size, bias=True)[source]</h3>
<p><code>LSTM cell</code>&#x3002;
<script type="math/tex; mode=display">
\begin{aligned}
i &= sigmoid(W_{ii}x+b_{ii}+W_{hi}h+b_{hi}) \\
f &= sigmoid(W_{if}x+b_{if}+W_{hf}h+b_{hf}) \\
o &= sigmoid(W_{io}x+b_{io}+W_{ho}h+b_{ho})\\
g &= tanh(W_{ig}x+b_{ig}+W_{hg}h+b_{hg})\\
c' &= f_t*c_{t-1}+i_t*g_t\\
h' &= o_t*tanh(c')
\end{aligned}
</script></p>
<p>&#x53C2;&#x6570;&#xFF1A;</p>
<ul>
<li>input_size &#x2013; &#x8F93;&#x5165;&#x7684;&#x7279;&#x5F81;&#x7EF4;&#x5EA6;&#x3002;</li>
<li>hidden_size &#x2013; &#x9690;&#x72B6;&#x6001;&#x7684;&#x7EF4;&#x5EA6;&#x3002;</li>
<li>bias &#x2013; &#x5982;&#x679C;&#x4E3A;<code>False</code>&#xFF0C;&#x90A3;&#x4E48;&#x5C06;&#x4E0D;&#x4F1A;&#x4F7F;&#x7528;<code>bias</code>&#x3002;&#x9ED8;&#x8BA4;&#x4E3A;<code>True</code>&#x3002;</li>
</ul>
<p><code>LSTM</code>&#x8F93;&#x5165;:
input, (h_0, c_0)</p>
<ul>
<li><p>input (seq_len, batch, input_size): &#x5305;&#x542B;&#x8F93;&#x5165;&#x5E8F;&#x5217;&#x7279;&#x5F81;&#x7684;<code>Tensor</code>&#x3002;&#x4E5F;&#x53EF;&#x4EE5;&#x662F;<code>packed variable</code> &#xFF0C;&#x8BE6;&#x89C1; <a href="#torch.nn.utils.rnn.pack_padded_sequence(input,%20lengths,%20batch_first=False[source]">pack_padded_sequence</a></p>
</li>
<li><p>h_0 ( batch, hidden_size):&#x4FDD;&#x5B58;&#x7740;<code>batch</code>&#x4E2D;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x521D;&#x59CB;&#x5316;&#x9690;&#x72B6;&#x6001;&#x7684;<code>Tensor</code></p>
</li>
<li><p>c_0 (batch, hidden_size): &#x4FDD;&#x5B58;&#x7740;<code>batch</code>&#x4E2D;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x521D;&#x59CB;&#x5316;&#x7EC6;&#x80DE;&#x72B6;&#x6001;&#x7684;<code>Tensor</code></p>
</li>
</ul>
<p>&#x8F93;&#x51FA;&#xFF1A;
h_1, c_1</p>
<ul>
<li>h_1 (batch, hidden_size): &#x4E0B;&#x4E00;&#x4E2A;&#x65F6;&#x523B;&#x7684;&#x9690;&#x72B6;&#x6001;&#x3002;</li>
<li>c_1 (batch, hidden_size): &#x4E0B;&#x4E00;&#x4E2A;&#x65F6;&#x523B;&#x7684;&#x7EC6;&#x80DE;&#x72B6;&#x6001;&#x3002;</li>
</ul>
<p><code>LSTM</code>&#x6A21;&#x578B;&#x53C2;&#x6570;:</p>
<ul>
<li><p>weight<em>ih &#x2013; <code>input-hidden</code>&#x6743;&#x91CD;($W</em>{ii}|W<em>{if}|W</em>{ig}|W_{io}$)&#xFF0C;&#x5F62;&#x72B6;&#x4E3A;<code>(input_size x 4*hidden_size)</code></p>
</li>
<li><p>weight<em>hh &#x2013; <code>hidden-hidden</code>&#x6743;&#x91CD;($W</em>{hi}|W<em>{hf}|W</em>{hg}|W_{ho}$)&#xFF0C;&#x5F62;&#x72B6;&#x4E3A;<code>(hidden_size x 4*hidden_size)</code>&#x3002;</p>
</li>
<li><p>bias<em>ih &#x2013; <code>input-hidden</code>&#x504F;&#x7F6E;($b</em>{ii}|b<em>{if}|b</em>{ig}|b_{io}$)&#xFF0C;&#x5F62;&#x72B6;&#x4E3A;<code>( 4*hidden_size)</code></p>
</li>
<li><p>bias<em>hh &#x2013; <code>hidden-hidden</code>&#x504F;&#x7F6E;($b</em>{hi}|b<em>{hf}|b</em>{hg}|b_{ho}$)&#xFF0C;&#x5F62;&#x72B6;&#x4E3A;<code>( 4*hidden_size)</code>&#x3002;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code class="lang-python">rnn = nn.LSTMCell(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>)
input = Variable(torch.randn(<span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>))
hx = Variable(torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">20</span>))
cx = Variable(torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">20</span>))
output = []
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">6</span>):
   hx, cx = rnn(input[i], (hx, cx))
   output.append(hx)
</code></pre>
<h3 id="class-torchnngrucellinputsize-hiddensize-biastruesource">class torch.nn.GRUCell(input_size, hidden_size, bias=True)[source]</h3>
<p>&#x4E00;&#x4E2A;<code>GRU cell</code>&#x3002;
<script type="math/tex; mode=display">
\begin{aligned}
r&=sigmoid(W_{ir}x+b_{ir}+W_{hr}h+b_{hr})\\
i&=sigmoid(W_{ii}x+b_{ii}+W_{hi}h+b_{hi})\\
n&=tanh(W_{in}x+b_{in}+r*(W_{hn}h+b_{hn}))\\
h'&=(1-i)* n+i*h
\end{aligned}
</script></p>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;&#xFF1A;</p>
<ul>
<li>input_size &#x2013; &#x671F;&#x671B;&#x7684;&#x8F93;&#x5165;$x$&#x7684;&#x7279;&#x5F81;&#x503C;&#x7684;&#x7EF4;&#x5EA6;</li>
<li>hidden_size &#x2013; &#x9690;&#x72B6;&#x6001;&#x7684;&#x7EF4;&#x5EA6;</li>
<li>bias &#x2013; &#x5982;&#x679C;&#x4E3A;<code>False</code>&#xFF0C;&#x90A3;&#x4E48;<code>RNN</code>&#x5C42;&#x5C06;&#x4E0D;&#x4F1A;&#x4F7F;&#x7528;<code>bias</code>&#xFF0C;&#x9ED8;&#x8BA4;&#x4E3A;<code>True</code>&#x3002;</li>
</ul>
<p>&#x8F93;&#x5165;&#xFF1A;
input, h_0</p>
<ul>
<li><p>input (batch, input_size):  &#x5305;&#x542B;&#x8F93;&#x5165;&#x7279;&#x5F81;&#x7684;<code>Tensor</code></p>
</li>
<li><p>h_0 (batch, hidden_size):&#x4FDD;&#x5B58;&#x7740;<code>batch</code>&#x4E2D;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x521D;&#x59CB;&#x5316;&#x9690;&#x72B6;&#x6001;&#x7684;<code>Tensor</code></p>
</li>
</ul>
<p>&#x8F93;&#x51FA;&#xFF1A;
h_1</p>
<ul>
<li>h_1 (batch, hidden_size): <code>Tensor</code>&#xFF0C;&#x4FDD;&#x5B58;&#x7740;<code>RNN</code>&#x4E0B;&#x4E00;&#x4E2A;&#x65F6;&#x523B;&#x7684;&#x9690;&#x72B6;&#x6001;&#x3002;</li>
</ul>
<p>&#x53D8;&#x91CF;&#xFF1A;</p>
<ul>
<li><p>weight<em>ih &#x2013; <code>input-hidden</code>&#x6743;&#x91CD;($W</em>{ir}|W<em>{ii}|W</em>{in}$)&#xFF0C;&#x5F62;&#x72B6;&#x4E3A;<code>(input_size x 3*hidden_size)</code></p>
</li>
<li><p>weight<em>hh &#x2013; <code>hidden-hidden</code>&#x6743;&#x91CD;($W</em>{hr}|W<em>{hi}|W</em>{hn}$)&#xFF0C;&#x5F62;&#x72B6;&#x4E3A;<code>(hidden_size x 3*hidden_size)</code>&#x3002;</p>
</li>
<li><p>bias<em>ih &#x2013; <code>input-hidden</code>&#x504F;&#x7F6E;($b</em>{ir}|b<em>{ii}|b</em>{in}$)&#xFF0C;&#x5F62;&#x72B6;&#x4E3A;<code>( 3*hidden_size)</code></p>
</li>
<li><p>bias<em>hh &#x2013; <code>hidden-hidden</code>&#x504F;&#x7F6E;($b</em>{hr}|b<em>{hi}|b</em>{hn}$)&#xFF0C;&#x5F62;&#x72B6;&#x4E3A;<code>( 3*hidden_size)</code>&#x3002;</p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python">rnn = nn.GRUCell(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>)
input = Variable(torch.randn(<span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>))
hx = Variable(torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">20</span>))
output = []
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">6</span>):
   hx = rnn(input[i], hx)
   output.append(hx)
</code></pre>
<h2 id="linear-layers">Linear layers</h2>
<pre><code class="lang-python">class torch.nn.Linear(in_features, out_features, bias=True)
</code></pre>
<p>&#x5BF9;&#x8F93;&#x5165;&#x6570;&#x636E;&#x505A;&#x7EBF;&#x6027;&#x53D8;&#x6362;&#xFF1A;\(y = Ax + b\)</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li><strong>in_features</strong> - &#x6BCF;&#x4E2A;&#x8F93;&#x5165;&#x6837;&#x672C;&#x7684;&#x5927;&#x5C0F;</li>
<li><strong>out_features</strong> - &#x6BCF;&#x4E2A;&#x8F93;&#x51FA;&#x6837;&#x672C;&#x7684;&#x5927;&#x5C0F;</li>
<li><strong>bias</strong> - &#x82E5;&#x8BBE;&#x7F6E;&#x4E3A;False&#xFF0C;&#x8FD9;&#x5C42;&#x4E0D;&#x4F1A;&#x5B66;&#x4E60;&#x504F;&#x7F6E;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;True</li>
</ul>
<p><strong>&#x5F62;&#x72B6;&#xFF1A;</strong></p>
<ul>
<li><strong>&#x8F93;&#x5165;:</strong> \((N, in\_features)\)</li>
<li><strong>&#x8F93;&#x51FA;&#xFF1A;</strong> \((N, out\_features)\)</li>
</ul>
<p><strong>&#x53D8;&#x91CF;&#xFF1A;</strong></p>
<ul>
<li><strong>weight</strong> -&#x5F62;&#x72B6;&#x4E3A;(out_features x in_features)&#x7684;&#x6A21;&#x5757;&#x4E2D;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x6743;&#x503C;</li>
<li><strong>bias</strong> -&#x5F62;&#x72B6;&#x4E3A;(out_features)&#x7684;&#x6A21;&#x5757;&#x4E2D;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x504F;&#x7F6E;</li>
</ul>
<p><strong>&#x4F8B;&#x5B50;&#xFF1A;</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">128</span>, <span class="hljs-number">20</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(output.size())
</code></pre>
<h2 id="dropout-layers">Dropout layers</h2>
<pre><code class="lang-python">class torch.nn.Dropout(p=0.5, inplace=False)
</code></pre>
<p>&#x968F;&#x673A;&#x5C06;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E2D;&#x90E8;&#x5206;&#x5143;&#x7D20;&#x8BBE;&#x7F6E;&#x4E3A;0&#x3002;&#x5BF9;&#x4E8E;&#x6BCF;&#x6B21;&#x524D;&#x5411;&#x8C03;&#x7528;&#xFF0C;&#x88AB;&#x7F6E;0&#x7684;&#x5143;&#x7D20;&#x90FD;&#x662F;&#x968F;&#x673A;&#x7684;&#x3002;</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li><strong>p</strong> - &#x5C06;&#x5143;&#x7D20;&#x7F6E;0&#x7684;&#x6982;&#x7387;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0.5</li>
<li><strong>in-place</strong> - &#x82E5;&#x8BBE;&#x7F6E;&#x4E3A;True&#xFF0C;&#x4F1A;&#x5728;&#x539F;&#x5730;&#x6267;&#x884C;&#x64CD;&#x4F5C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;False</li>
</ul>
<p><strong>&#x5F62;&#x72B6;&#xFF1A;</strong></p>
<ul>
<li><strong>&#x8F93;&#x5165;&#xFF1A;</strong> &#x4EFB;&#x610F;&#x3002;&#x8F93;&#x5165;&#x53EF;&#x4EE5;&#x4E3A;&#x4EFB;&#x610F;&#x5F62;&#x72B6;&#x3002;</li>
<li><strong>&#x8F93;&#x51FA;&#xFF1A;</strong> &#x76F8;&#x540C;&#x3002;&#x8F93;&#x51FA;&#x548C;&#x8F93;&#x5165;&#x5F62;&#x72B6;&#x76F8;&#x540C;&#x3002;</li>
</ul>
<p><strong>&#x4F8B;&#x5B50;&#xFF1A;</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Dropout(p=<span class="hljs-number">0.2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<pre><code class="lang-python">class torch.nn.Dropout2d(p=0.5, inplace=False)
</code></pre>
<p>&#x968F;&#x673A;&#x5C06;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E2D;&#x6574;&#x4E2A;&#x901A;&#x9053;&#x8BBE;&#x7F6E;&#x4E3A;0&#x3002;&#x5BF9;&#x4E8E;&#x6BCF;&#x6B21;&#x524D;&#x5411;&#x8C03;&#x7528;&#xFF0C;&#x88AB;&#x7F6E;0&#x7684;&#x901A;&#x9053;&#x90FD;&#x662F;&#x968F;&#x673A;&#x7684;&#x3002;</p>
<p><em>&#x901A;&#x5E38;&#x8F93;&#x5165;&#x6765;&#x81EA;Conv2d&#x6A21;&#x5757;&#x3002;</em></p>
<p>&#x50CF;&#x5728;&#x8BBA;&#x6587;<a href="https://arxiv.org/abs/1411.4280" target="_blank">Efficient Object Localization Using Convolutional Networks</a>&#xFF0C;&#x5982;&#x679C;&#x7279;&#x5F81;&#x56FE;&#x4E2D;&#x76F8;&#x90BB;&#x50CF;&#x7D20;&#x662F;&#x5F3A;&#x76F8;&#x5173;&#x7684;&#xFF08;&#x5728;&#x524D;&#x51E0;&#x5C42;&#x5377;&#x79EF;&#x5C42;&#x5F88;&#x5E38;&#x89C1;&#xFF09;&#xFF0C;&#x90A3;&#x4E48;iid dropout&#x4E0D;&#x4F1A;&#x5F52;&#x4E00;&#x5316;&#x6FC0;&#x6D3B;&#xFF0C;&#x800C;&#x53EA;&#x4F1A;&#x964D;&#x4F4E;&#x5B66;&#x4E60;&#x7387;&#x3002;</p>
<p>&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x5F62;&#xFF0C;<code>nn.Dropout2d()</code>&#x53EF;&#x4EE5;&#x63D0;&#x9AD8;&#x7279;&#x5F81;&#x56FE;&#x4E4B;&#x95F4;&#x7684;&#x72EC;&#x7ACB;&#x7A0B;&#x5EA6;&#xFF0C;&#x6240;&#x4EE5;&#x5E94;&#x8BE5;&#x4F7F;&#x7528;&#x5B83;&#x3002;</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li><strong>p</strong>(<em><a href="">float</a>, optional</em>) - &#x5C06;&#x5143;&#x7D20;&#x7F6E;0&#x7684;&#x6982;&#x7387;&#x3002;</li>
<li><strong>in-place</strong>(<em><a href="">bool,</a> optional</em>) - &#x82E5;&#x8BBE;&#x7F6E;&#x4E3A;True&#xFF0C;&#x4F1A;&#x5728;&#x539F;&#x5730;&#x6267;&#x884C;&#x64CD;&#x4F5C;&#x3002;</li>
</ul>
<p><strong>&#x5F62;&#x72B6;&#xFF1A;</strong></p>
<ul>
<li><strong>&#x8F93;&#x5165;&#xFF1A;</strong>  \((N, C, H, W)\)</li>
<li><strong>&#x8F93;&#x51FA;&#xFF1A;</strong>  \((N, C, H, W)\)&#xFF08;&#x4E0E;&#x8F93;&#x5165;&#x5F62;&#x72B6;&#x76F8;&#x540C;&#xFF09;</li>
</ul>
<p><strong>&#x4F8B;&#x5B50;&#xFF1A;</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Dropout2d(p=<span class="hljs-number">0.2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<pre><code class="lang-python">class torch.nn.Dropout3d(p=0.5, inplace=False)
</code></pre>
<p>&#x968F;&#x673A;&#x5C06;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E2D;&#x6574;&#x4E2A;&#x901A;&#x9053;&#x8BBE;&#x7F6E;&#x4E3A;0&#x3002;&#x5BF9;&#x4E8E;&#x6BCF;&#x6B21;&#x524D;&#x5411;&#x8C03;&#x7528;&#xFF0C;&#x88AB;&#x7F6E;0&#x7684;&#x901A;&#x9053;&#x90FD;&#x662F;&#x968F;&#x673A;&#x7684;&#x3002;</p>
<p><em>&#x901A;&#x5E38;&#x8F93;&#x5165;&#x6765;&#x81EA;Conv3d&#x6A21;&#x5757;&#x3002;</em></p>
<p>&#x50CF;&#x5728;&#x8BBA;&#x6587;<a href="https://arxiv.org/abs/1411.4280" target="_blank">Efficient Object Localization Using Convolutional Networks</a>&#xFF0C;&#x5982;&#x679C;&#x7279;&#x5F81;&#x56FE;&#x4E2D;&#x76F8;&#x90BB;&#x50CF;&#x7D20;&#x662F;&#x5F3A;&#x76F8;&#x5173;&#x7684;&#xFF08;&#x5728;&#x524D;&#x51E0;&#x5C42;&#x5377;&#x79EF;&#x5C42;&#x5F88;&#x5E38;&#x89C1;&#xFF09;&#xFF0C;&#x90A3;&#x4E48;iid dropout&#x4E0D;&#x4F1A;&#x5F52;&#x4E00;&#x5316;&#x6FC0;&#x6D3B;&#xFF0C;&#x800C;&#x53EA;&#x4F1A;&#x964D;&#x4F4E;&#x5B66;&#x4E60;&#x7387;&#x3002;</p>
<p>&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x5F62;&#xFF0C;<code>nn.Dropout3d()</code>&#x53EF;&#x4EE5;&#x63D0;&#x9AD8;&#x7279;&#x5F81;&#x56FE;&#x4E4B;&#x95F4;&#x7684;&#x72EC;&#x7ACB;&#x7A0B;&#x5EA6;&#xFF0C;&#x6240;&#x4EE5;&#x5E94;&#x8BE5;&#x4F7F;&#x7528;&#x5B83;&#x3002;</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li><strong>p</strong>(<em><a href="">float</a>, optional</em>) - &#x5C06;&#x5143;&#x7D20;&#x7F6E;0&#x7684;&#x6982;&#x7387;&#x3002;</li>
<li><strong>in-place</strong>(<em><a href="">bool,</a> optional</em>) - &#x82E5;&#x8BBE;&#x7F6E;&#x4E3A;True&#xFF0C;&#x4F1A;&#x5728;&#x539F;&#x5730;&#x6267;&#x884C;&#x64CD;&#x4F5C;&#x3002;</li>
</ul>
<p><strong>&#x5F62;&#x72B6;&#xFF1A;</strong></p>
<ul>
<li><strong>&#x8F93;&#x5165;&#xFF1A;</strong>  \(N, C, D, H, W)\)</li>
<li><strong>&#x8F93;&#x51FA;&#xFF1A;</strong>  \((N, C, D, H, W)\)&#xFF08;&#x4E0E;&#x8F93;&#x5165;&#x5F62;&#x72B6;&#x76F8;&#x540C;&#xFF09;</li>
</ul>
<p><strong>&#x4F8B;&#x5B50;&#xFF1A;</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Dropout3d(p=<span class="hljs-number">0.2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">16</span>, <span class="hljs-number">4</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = m(input)
</code></pre>
<h2 id="sparse-layers">Sparse layers</h2>
<pre><code class="lang-python">class torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False)
</code></pre>
<p>&#x4E00;&#x4E2A;&#x4FDD;&#x5B58;&#x4E86;&#x56FA;&#x5B9A;&#x5B57;&#x5178;&#x548C;&#x5927;&#x5C0F;&#x7684;&#x7B80;&#x5355;&#x67E5;&#x627E;&#x8868;&#x3002;</p>
<p>&#x8FD9;&#x4E2A;&#x6A21;&#x5757;&#x5E38;&#x7528;&#x6765;&#x4FDD;&#x5B58;&#x8BCD;&#x5D4C;&#x5165;&#x548C;&#x7528;&#x4E0B;&#x6807;&#x68C0;&#x7D22;&#x5B83;&#x4EEC;&#x3002;&#x6A21;&#x5757;&#x7684;&#x8F93;&#x5165;&#x662F;&#x4E00;&#x4E2A;&#x4E0B;&#x6807;&#x7684;&#x5217;&#x8868;&#xFF0C;&#x8F93;&#x51FA;&#x662F;&#x5BF9;&#x5E94;&#x7684;&#x8BCD;&#x5D4C;&#x5165;&#x3002;</p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li><strong>num_embeddings</strong> (<em><a href="">int</a></em>) - &#x5D4C;&#x5165;&#x5B57;&#x5178;&#x7684;&#x5927;&#x5C0F;</li>
<li><strong>embedding_dim</strong> (<em><a href="">int</a></em>) - &#x6BCF;&#x4E2A;&#x5D4C;&#x5165;&#x5411;&#x91CF;&#x7684;&#x5927;&#x5C0F;</li>
<li><strong>padding_idx </strong> (<em><a href="">int</a>, optional</em>) - &#x5982;&#x679C;&#x63D0;&#x4F9B;&#x7684;&#x8BDD;&#xFF0C;&#x8F93;&#x51FA;&#x9047;&#x5230;&#x6B64;&#x4E0B;&#x6807;&#x65F6;&#x7528;&#x96F6;&#x586B;&#x5145;</li>
<li><strong>max_norm</strong> (<em><a href="">float</a>, optional</em>) - &#x5982;&#x679C;&#x63D0;&#x4F9B;&#x7684;&#x8BDD;&#xFF0C;&#x4F1A;&#x91CD;&#x65B0;&#x5F52;&#x4E00;&#x5316;&#x8BCD;&#x5D4C;&#x5165;&#xFF0C;&#x4F7F;&#x5B83;&#x4EEC;&#x7684;&#x8303;&#x6570;&#x5C0F;&#x4E8E;&#x63D0;&#x4F9B;&#x7684;&#x503C;</li>
<li><strong>norm_type</strong> (<em><a href="">float</a>, optional</em>) - &#x5BF9;&#x4E8E;max_norm&#x9009;&#x9879;&#x8BA1;&#x7B97;p&#x8303;&#x6570;&#x65F6;&#x7684;p</li>
<li><strong>scale_grad_by_freq</strong> (<em>boolean, optional</em>) - &#x5982;&#x679C;&#x63D0;&#x4F9B;&#x7684;&#x8BDD;&#xFF0C;&#x4F1A;&#x6839;&#x636E;&#x5B57;&#x5178;&#x4E2D;&#x5355;&#x8BCD;&#x9891;&#x7387;&#x7F29;&#x653E;&#x68AF;&#x5EA6;</li>
</ul>
<p><strong>&#x53D8;&#x91CF;&#xFF1A;</strong></p>
<ul>
<li><strong>weight (<em><a href="https://pytorch.org/docs/tensors.html#torch.Tensor" target="_blank">Tensor</a></em>) </strong> -&#x5F62;&#x72B6;&#x4E3A;(num_embeddings, embedding_dim)&#x7684;&#x6A21;&#x5757;&#x4E2D;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x6743;&#x503C;</li>
</ul>
<p><strong>&#x5F62;&#x72B6;&#xFF1A;</strong></p>
<ul>
<li><strong>&#x8F93;&#x5165;&#xFF1A;</strong>  LongTensor <em>(N, W)</em>, N = mini-batch, W = &#x6BCF;&#x4E2A;mini-batch&#x4E2D;&#x63D0;&#x53D6;&#x7684;&#x4E0B;&#x6807;&#x6570;</li>
<li><strong>&#x8F93;&#x51FA;&#xFF1A;</strong>  <em>(N, W, embedding_dim)</em></li>
</ul>
<p><strong>&#x4F8B;&#x5B50;&#xFF1A;</strong></p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># an Embedding module containing 10 tensors of size 3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>embedding = nn.Embedding(<span class="hljs-number">10</span>, <span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># a batch of 2 samples of 4 indices each</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input = Variable(torch.LongTensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">9</span>]]))
<span class="hljs-meta">&gt;&gt;&gt; </span>embedding(input)

Variable containing:
(<span class="hljs-number">0</span> ,.,.) =
 <span class="hljs-number">-1.0822</span>  <span class="hljs-number">1.2522</span>  <span class="hljs-number">0.2434</span>
  <span class="hljs-number">0.8393</span> <span class="hljs-number">-0.6062</span> <span class="hljs-number">-0.3348</span>
  <span class="hljs-number">0.6597</span>  <span class="hljs-number">0.0350</span>  <span class="hljs-number">0.0837</span>
  <span class="hljs-number">0.5521</span>  <span class="hljs-number">0.9447</span>  <span class="hljs-number">0.0498</span>

(<span class="hljs-number">1</span> ,.,.) =
  <span class="hljs-number">0.6597</span>  <span class="hljs-number">0.0350</span>  <span class="hljs-number">0.0837</span>
 <span class="hljs-number">-0.1527</span>  <span class="hljs-number">0.0877</span>  <span class="hljs-number">0.4260</span>
  <span class="hljs-number">0.8393</span> <span class="hljs-number">-0.6062</span> <span class="hljs-number">-0.3348</span>
 <span class="hljs-number">-0.8738</span> <span class="hljs-number">-0.9054</span>  <span class="hljs-number">0.4281</span>
[torch.FloatTensor of size <span class="hljs-number">2</span>x4x3]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># example with padding_idx</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>embedding = nn.Embedding(<span class="hljs-number">10</span>, <span class="hljs-number">3</span>, padding_idx=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = Variable(torch.LongTensor([[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>,<span class="hljs-number">5</span>]]))
<span class="hljs-meta">&gt;&gt;&gt; </span>embedding(input)

Variable containing:
(<span class="hljs-number">0</span> ,.,.) =
  <span class="hljs-number">0.0000</span>  <span class="hljs-number">0.0000</span>  <span class="hljs-number">0.0000</span>
  <span class="hljs-number">0.3452</span>  <span class="hljs-number">0.4937</span> <span class="hljs-number">-0.9361</span>
  <span class="hljs-number">0.0000</span>  <span class="hljs-number">0.0000</span>  <span class="hljs-number">0.0000</span>
  <span class="hljs-number">0.0706</span> <span class="hljs-number">-2.1962</span> <span class="hljs-number">-0.6276</span>
[torch.FloatTensor of size <span class="hljs-number">1</span>x4x3]
</code></pre>
<h2 id="distance-functions">Distance functions</h2>
<pre><code class="lang-python">class torch.nn.PairwiseDistance(p=2, eps=1e-06)
</code></pre>
<p>&#x6309;&#x6279;&#x8BA1;&#x7B97;&#x5411;&#x91CF;v1, v2&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#xFF1A;</p>
<p><script type="math/tex; ">\Vert x \Vert _p := \left( \sum\_{i=1}^n  \vert x_i \vert ^ p \right) ^ {1/p}</script></p>
<p><strong>&#x53C2;&#x6570;&#xFF1A;</strong></p>
<ul>
<li><strong>x</strong> (<em>Tensor</em>):  &#x5305;&#x542B;&#x4E24;&#x4E2A;&#x8F93;&#x5165;batch&#x7684;&#x5F20;&#x91CF;</li>
<li><strong>p</strong> (real): &#x8303;&#x6570;&#x6B21;&#x6570;&#xFF0C;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;2</li>
</ul>
<p><strong>&#x5F62;&#x72B6;&#xFF1A;</strong></p>
<ul>
<li><strong>&#x8F93;&#x5165;&#xFF1A;</strong>  \((N, D)\)&#xFF0C;&#x5176;&#x4E2D;D=&#x5411;&#x91CF;&#x7EF4;&#x6570;</li>
<li><strong>&#x8F93;&#x51FA;&#xFF1A;</strong>  \((N, 1)\)</li>
</ul>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>pdist = nn.PairwiseDistance(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input1 = autograd.Variable(torch.randn(<span class="hljs-number">100</span>, <span class="hljs-number">128</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>input2 = autograd.Variable(torch.randn(<span class="hljs-number">100</span>, <span class="hljs-number">128</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = pdist(input1, input2)
</code></pre>
<h2 id="loss-functions">Loss functions</h2>
<p>&#x57FA;&#x672C;&#x7528;&#x6CD5;&#xFF1A;</p>
<pre><code class="lang-python">criterion = LossCriterion() <span class="hljs-comment">#&#x6784;&#x9020;&#x51FD;&#x6570;&#x6709;&#x81EA;&#x5DF1;&#x7684;&#x53C2;&#x6570;</span>
loss = criterion(x, y) <span class="hljs-comment">#&#x8C03;&#x7528;&#x6807;&#x51C6;&#x65F6;&#x4E5F;&#x6709;&#x53C2;&#x6570;</span>
</code></pre>
<p>&#x8BA1;&#x7B97;&#x51FA;&#x6765;&#x7684;&#x7ED3;&#x679C;&#x5DF2;&#x7ECF;&#x5BF9;<code>mini-batch</code>&#x53D6;&#x4E86;&#x5E73;&#x5747;&#x3002;</p>
<h3 id="class-torchnnl1losssizeaveragetruesource">class torch.nn.L1Loss(size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#L1Loss" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x8861;&#x91CF;&#x8F93;&#x5165;<code>x</code>(<code>&#x6A21;&#x578B;&#x9884;&#x6D4B;&#x8F93;&#x51FA;</code>)&#x548C;&#x76EE;&#x6807;<code>y</code>&#x4E4B;&#x95F4;&#x5DEE;&#x7684;&#x7EDD;&#x5BF9;&#x503C;&#x7684;&#x5E73;&#x5747;&#x503C;&#x7684;&#x6807;&#x51C6;&#x3002;
<script type="math/tex; mode=display">
loss(x,y)=1/n\sum|x_i-y_i|
</script></p>
<ul>
<li><p><code>x</code> &#x548C; <code>y</code> &#x53EF;&#x4EE5;&#x662F;&#x4EFB;&#x610F;&#x5F62;&#x72B6;&#xFF0C;&#x6BCF;&#x4E2A;&#x5305;&#x542B;<code>n</code>&#x4E2A;&#x5143;&#x7D20;&#x3002;</p>
</li>
<li><p>&#x5BF9;<code>n</code>&#x4E2A;&#x5143;&#x7D20;&#x5BF9;&#x5E94;&#x7684;&#x5DEE;&#x503C;&#x7684;&#x7EDD;&#x5BF9;&#x503C;&#x6C42;&#x548C;&#xFF0C;&#x5F97;&#x51FA;&#x6765;&#x7684;&#x7ED3;&#x679C;&#x9664;&#x4EE5;<code>n</code>&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;&#x5728;&#x521B;&#x5EFA;<code>L1Loss</code>&#x5B9E;&#x4F8B;&#x7684;&#x65F6;&#x5019;&#x5728;&#x6784;&#x9020;&#x51FD;&#x6570;&#x4E2D;&#x4F20;&#x5165;<code>size_average=False</code>&#xFF0C;&#x90A3;&#x4E48;&#x6C42;&#x51FA;&#x6765;&#x7684;&#x7EDD;&#x5BF9;&#x503C;&#x7684;&#x548C;&#x5C06;&#x4E0D;&#x4F1A;&#x9664;&#x4EE5;<code>n</code></p>
</li>
</ul>
<h3 id="class-torchnnmselosssizeaveragetruesource">class torch.nn.MSELoss(size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#MSELoss" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x8861;&#x91CF;&#x8F93;&#x5165;<code>x</code>(<code>&#x6A21;&#x578B;&#x9884;&#x6D4B;&#x8F93;&#x51FA;</code>)&#x548C;&#x76EE;&#x6807;<code>y</code>&#x4E4B;&#x95F4;&#x5747;&#x65B9;&#x8BEF;&#x5DEE;&#x6807;&#x51C6;&#x3002;
<script type="math/tex; mode=display">
loss(x,y)=1/n\sum(x_i-y_i)^2
</script></p>
<ul>
<li><p><code>x</code> &#x548C; <code>y</code> &#x53EF;&#x4EE5;&#x662F;&#x4EFB;&#x610F;&#x5F62;&#x72B6;&#xFF0C;&#x6BCF;&#x4E2A;&#x5305;&#x542B;<code>n</code>&#x4E2A;&#x5143;&#x7D20;&#x3002;</p>
</li>
<li><p>&#x5BF9;<code>n</code>&#x4E2A;&#x5143;&#x7D20;&#x5BF9;&#x5E94;&#x7684;&#x5DEE;&#x503C;&#x7684;&#x7EDD;&#x5BF9;&#x503C;&#x6C42;&#x548C;&#xFF0C;&#x5F97;&#x51FA;&#x6765;&#x7684;&#x7ED3;&#x679C;&#x9664;&#x4EE5;<code>n</code>&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;&#x5728;&#x521B;&#x5EFA;<code>MSELoss</code>&#x5B9E;&#x4F8B;&#x7684;&#x65F6;&#x5019;&#x5728;&#x6784;&#x9020;&#x51FD;&#x6570;&#x4E2D;&#x4F20;&#x5165;<code>size_average=False</code>&#xFF0C;&#x90A3;&#x4E48;&#x6C42;&#x51FA;&#x6765;&#x7684;&#x5E73;&#x65B9;&#x548C;&#x5C06;&#x4E0D;&#x4F1A;&#x9664;&#x4EE5;<code>n</code></p>
</li>
</ul>
<h3 id="class-torchnncrossentropylossweightnone-sizeaveragetruesource">class torch.nn.CrossEntropyLoss(weight=None, size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#CrossEntropyLoss" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x6B64;&#x6807;&#x51C6;&#x5C06;<code>LogSoftMax</code>&#x548C;<code>NLLLoss</code>&#x96C6;&#x6210;&#x5230;&#x4E00;&#x4E2A;&#x7C7B;&#x4E2D;&#x3002;</p>
<p>&#x5F53;&#x8BAD;&#x7EC3;&#x4E00;&#x4E2A;&#x591A;&#x7C7B;&#x5206;&#x7C7B;&#x5668;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x8FD9;&#x4E2A;&#x65B9;&#x6CD5;&#x662F;&#x5341;&#x5206;&#x6709;&#x7528;&#x7684;&#x3002;</p>
<ul>
<li>weight(tensor): <code>1-D</code> tensor&#xFF0C;<code>n</code>&#x4E2A;&#x5143;&#x7D20;&#xFF0C;&#x5206;&#x522B;&#x4EE3;&#x8868;<code>n</code>&#x7C7B;&#x7684;&#x6743;&#x91CD;&#xFF0C;&#x5982;&#x679C;&#x4F60;&#x7684;&#x8BAD;&#x7EC3;&#x6837;&#x672C;&#x5F88;&#x4E0D;&#x5747;&#x8861;&#x7684;&#x8BDD;&#xFF0C;&#x662F;&#x975E;&#x5E38;&#x6709;&#x7528;&#x7684;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#x4E3A;None&#x3002;</li>
</ul>
<p>&#x8C03;&#x7528;&#x65F6;&#x53C2;&#x6570;&#xFF1A;</p>
<ul>
<li><p>input : &#x5305;&#x542B;&#x6BCF;&#x4E2A;&#x7C7B;&#x7684;&#x5F97;&#x5206;&#xFF0C;<code>2-D</code> tensor,<code>shape</code>&#x4E3A; <code>batch*n</code></p>
</li>
<li><p>target: &#x5927;&#x5C0F;&#x4E3A; <code>n</code> &#x7684; <code>1&#x2014;D</code> <code>tensor</code>&#xFF0C;&#x5305;&#x542B;&#x7C7B;&#x522B;&#x7684;&#x7D22;&#x5F15;(<code>0&#x5230; n-1</code>)&#x3002;</p>
</li>
</ul>
<p>Loss&#x53EF;&#x4EE5;&#x8868;&#x8FF0;&#x4E3A;&#x4EE5;&#x4E0B;&#x5F62;&#x5F0F;&#xFF1A;
<script type="math/tex; mode=display">
\begin{aligned}
loss(x, class) &= -\text{log}\frac{exp(x[class])}{\sum_j exp(x[j]))}\\
               &= -x[class] + log(\sum_j exp(x[j]))
\end{aligned}
</script>
&#x5F53;<code>weight</code>&#x53C2;&#x6570;&#x88AB;&#x6307;&#x5B9A;&#x7684;&#x65F6;&#x5019;&#xFF0C;<code>loss</code>&#x7684;&#x8BA1;&#x7B97;&#x516C;&#x5F0F;&#x53D8;&#x4E3A;&#xFF1A;
<script type="math/tex; mode=display">
loss(x, class) = weights[class] * (-x[class] + log(\sum_j exp(x[j])))
</script>
&#x8BA1;&#x7B97;&#x51FA;&#x7684;<code>loss</code>&#x5BF9;<code>mini-batch</code>&#x7684;&#x5927;&#x5C0F;&#x53D6;&#x4E86;&#x5E73;&#x5747;&#x3002;</p>
<p>&#x5F62;&#x72B6;(<code>shape</code>)&#xFF1A;</p>
<ul>
<li><p>Input: (N,C) <code>C</code> &#x662F;&#x7C7B;&#x522B;&#x7684;&#x6570;&#x91CF;</p>
</li>
<li><p>Target: (N) <code>N</code>&#x662F;<code>mini-batch</code>&#x7684;&#x5927;&#x5C0F;&#xFF0C;0 &lt;= targets[i] &lt;= C-1</p>
</li>
</ul>
<h3 id="class-torchnnnlllossweightnone-sizeaveragetruesource">class torch.nn.NLLLoss(weight=None, size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#NLLLoss" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x8D1F;&#x7684;<code>log likelihood loss</code>&#x635F;&#x5931;&#x3002;&#x7528;&#x4E8E;&#x8BAD;&#x7EC3;&#x4E00;&#x4E2A;<code>n</code>&#x7C7B;&#x5206;&#x7C7B;&#x5668;&#x3002;</p>
<p>&#x5982;&#x679C;&#x63D0;&#x4F9B;&#x7684;&#x8BDD;&#xFF0C;<code>weight</code>&#x53C2;&#x6570;&#x5E94;&#x8BE5;&#x662F;&#x4E00;&#x4E2A;<code>1-D</code>tensor&#xFF0C;&#x91CC;&#x9762;&#x7684;&#x503C;&#x5BF9;&#x5E94;&#x7C7B;&#x522B;&#x7684;&#x6743;&#x91CD;&#x3002;&#x5F53;&#x4F60;&#x7684;&#x8BAD;&#x7EC3;&#x96C6;&#x6837;&#x672C;&#x4E0D;&#x5747;&#x8861;&#x7684;&#x8BDD;&#xFF0C;&#x4F7F;&#x7528;&#x8FD9;&#x4E2A;&#x53C2;&#x6570;&#x662F;&#x975E;&#x5E38;&#x6709;&#x7528;&#x7684;&#x3002;</p>
<p>&#x8F93;&#x5165;&#x662F;&#x4E00;&#x4E2A;&#x5305;&#x542B;&#x7C7B;&#x522B;<code>log-probabilities</code>&#x7684;<code>2-D</code> tensor&#xFF0C;&#x5F62;&#x72B6;&#x662F;<code>&#xFF08;mini-batch&#xFF0C; n&#xFF09;</code></p>
<p>&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x5728;&#x6700;&#x540E;&#x4E00;&#x5C42;&#x52A0;<code>LogSoftmax</code>&#x6765;&#x83B7;&#x5F97;&#x7C7B;&#x522B;&#x7684;<code>log-probabilities</code>&#x3002;</p>
<p>&#x5982;&#x679C;&#x60A8;&#x4E0D;&#x60F3;&#x589E;&#x52A0;&#x4E00;&#x4E2A;&#x989D;&#x5916;&#x5C42;&#x7684;&#x8BDD;&#xFF0C;&#x60A8;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;<code>CrossEntropyLoss</code>&#x3002;</p>
<p>&#x6B64;<code>loss</code>&#x671F;&#x671B;&#x7684;<code>target</code>&#x662F;&#x7C7B;&#x522B;&#x7684;&#x7D22;&#x5F15; (0 to N-1, where N = number of classes)</p>
<p>&#x6B64;<code>loss</code>&#x53EF;&#x4EE5;&#x88AB;&#x8868;&#x793A;&#x5982;&#x4E0B;&#xFF1A;
<script type="math/tex; mode=display">
loss(x, class) = -x[class]
</script>
&#x5982;&#x679C;<code>weights</code>&#x53C2;&#x6570;&#x88AB;&#x6307;&#x5B9A;&#x7684;&#x8BDD;&#xFF0C;<code>loss</code>&#x53EF;&#x4EE5;&#x8868;&#x793A;&#x5982;&#x4E0B;&#xFF1A;
<script type="math/tex; mode=display">
loss(x, class) = -weights[class] * x[class]
</script>
&#x53C2;&#x6570;&#x8BF4;&#x660E;&#xFF1A;</p>
<ul>
<li><p>weight (Tensor, optional) &#x2013; &#x624B;&#x52A8;&#x6307;&#x5B9A;&#x6BCF;&#x4E2A;&#x7C7B;&#x522B;&#x7684;&#x6743;&#x91CD;&#x3002;&#x5982;&#x679C;&#x7ED9;&#x5B9A;&#x7684;&#x8BDD;&#xFF0C;&#x5FC5;&#x987B;&#x662F;&#x957F;&#x5EA6;&#x4E3A;<code>nclasses</code></p>
</li>
<li><p>size_average (bool, optional) &#x2013; &#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x4F1A;&#x8BA1;&#x7B97;<code>mini-batch``loss</code>&#x7684;&#x5E73;&#x5747;&#x503C;&#x3002;&#x7136;&#x800C;&#xFF0C;&#x5982;&#x679C;<code>size_average=False</code>&#x90A3;&#x4E48;&#x5C06;&#x4F1A;&#x628A;<code>mini-batch</code>&#x4E2D;&#x6240;&#x6709;&#x6837;&#x672C;&#x7684;<code>loss</code>&#x7D2F;&#x52A0;&#x8D77;&#x6765;&#x3002;</p>
</li>
</ul>
<p>&#x5F62;&#x72B6;:</p>
<ul>
<li><p>Input: (N,C) , <code>C</code>&#x662F;&#x7C7B;&#x522B;&#x7684;&#x4E2A;&#x6570;</p>
</li>
<li><p>Target: (N) &#xFF0C; <code>target</code>&#x4E2D;&#x6BCF;&#x4E2A;&#x503C;&#x7684;&#x5927;&#x5C0F;&#x6EE1;&#x8DB3; <code>0 &lt;= targets[i] &lt;= C-1</code></p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-python"> m = nn.LogSoftmax()
 loss = nn.NLLLoss()
 <span class="hljs-comment"># input is of size nBatch x nClasses = 3 x 5</span>
 input = autograd.Variable(torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>), requires_grad=<span class="hljs-keyword">True</span>)
 <span class="hljs-comment"># each element in target has to have 0 &lt;= value &lt; nclasses</span>
 target = autograd.Variable(torch.LongTensor([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>]))
 output = loss(m(input), target)
 output.backward()
</code></pre>
<h3 id="class-torchnnnllloss2dweightnone-sizeaveragetruesource">class torch.nn.NLLLoss2d(weight=None, size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#NLLLoss2d" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x5BF9;&#x4E8E;&#x56FE;&#x7247;&#x7684; <code>negative log likehood loss</code>&#x3002;&#x8BA1;&#x7B97;&#x6BCF;&#x4E2A;&#x50CF;&#x7D20;&#x7684; <code>NLL loss</code>&#x3002;</p>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;&#xFF1A;</p>
<ul>
<li><p>weight (Tensor, optional) &#x2013; &#x7528;&#x6765;&#x4F5C;&#x4E3A;&#x6BCF;&#x7C7B;&#x7684;&#x6743;&#x91CD;&#xFF0C;&#x5982;&#x679C;&#x63D0;&#x4F9B;&#x7684;&#x8BDD;&#xFF0C;&#x5FC5;&#x987B;&#x4E3A;<code>1-D</code>tensor&#xFF0C;&#x5927;&#x5C0F;&#x4E3A;<code>C</code>&#xFF1A;&#x7C7B;&#x522B;&#x7684;&#x4E2A;&#x6570;&#x3002;</p>
</li>
<li><p>size_average &#x2013; &#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x4F1A;&#x8BA1;&#x7B97; <code>mini-batch</code> loss&#x5747;&#x503C;&#x3002;&#x5982;&#x679C;&#x8BBE;&#x7F6E;&#x4E3A; <code>False</code> &#x7684;&#x8BDD;&#xFF0C;&#x5C06;&#x4F1A;&#x7D2F;&#x52A0;<code>mini-batch</code>&#x4E2D;&#x6240;&#x6709;&#x6837;&#x672C;&#x7684;<code>loss</code>&#x503C;&#x3002;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>True</code>&#x3002;</p>
</li>
</ul>
<p>&#x5F62;&#x72B6;&#xFF1A;</p>
<ul>
<li><p>Input: (N,C,H,W)  <code>C</code> &#x7C7B;&#x7684;&#x6570;&#x91CF;</p>
</li>
<li><p>Target: (N,H,W) where each value is 0 &lt;= targets[i] &lt;= C-1</p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"> m = nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">32</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)).float()
 loss = nn.NLLLoss2d()
 <span class="hljs-comment"># input is of size nBatch x nClasses x height x width</span>
 input = autograd.Variable(torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">16</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>))
 <span class="hljs-comment"># each element in target has to have 0 &lt;= value &lt; nclasses</span>
 target = autograd.Variable(torch.LongTensor(<span class="hljs-number">3</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>).random_(<span class="hljs-number">0</span>, <span class="hljs-number">4</span>))
 output = loss(m(input), target)
 output.backward()
</code></pre>
<h3 id="class-torchnnkldivlossweightnone-sizeaveragetruesource">class torch.nn.KLDivLoss(weight=None, size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#KLDivLoss" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x8BA1;&#x7B97; KL &#x6563;&#x5EA6;&#x635F;&#x5931;&#x3002;</p>
<p>KL&#x6563;&#x5EA6;&#x5E38;&#x7528;&#x6765;&#x63CF;&#x8FF0;&#x4E24;&#x4E2A;&#x5206;&#x5E03;&#x7684;&#x8DDD;&#x79BB;&#xFF0C;&#x5E76;&#x5728;&#x8F93;&#x51FA;&#x5206;&#x5E03;&#x7684;&#x7A7A;&#x95F4;&#x4E0A;&#x6267;&#x884C;&#x76F4;&#x63A5;&#x56DE;&#x5F52;&#x662F;&#x6709;&#x7528;&#x7684;&#x3002;</p>
<p>&#x4E0E;<code>NLLLoss</code>&#x4E00;&#x6837;&#xFF0C;&#x7ED9;&#x5B9A;&#x7684;&#x8F93;&#x5165;&#x5E94;&#x8BE5;&#x662F;<code>log-probabilities</code>&#x3002;&#x7136;&#x800C;&#x3002;&#x548C;<code>NLLLoss</code>&#x4E0D;&#x540C;&#x7684;&#x662F;&#xFF0C;<code>input</code>&#x4E0D;&#x9650;&#x4E8E;<code>2-D</code> tensor&#xFF0C;&#x56E0;&#x4E3A;&#x6B64;&#x6807;&#x51C6;&#x662F;&#x57FA;&#x4E8E;<code>element</code>&#x7684;&#x3002;</p>
<p><code>target</code> &#x5E94;&#x8BE5;&#x548C; <code>input</code>&#x7684;&#x5F62;&#x72B6;&#x76F8;&#x540C;&#x3002;</p>
<p>&#x6B64;loss&#x53EF;&#x4EE5;&#x8868;&#x793A;&#x4E3A;&#xFF1A;
<script type="math/tex; mode=display">
loss(x,target)=\frac{1}{n}\sum_i(target_i*(log(target_i)-x_i))
</script>
&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;loss&#x4F1A;&#x57FA;&#x4E8E;<code>element</code>&#x6C42;&#x5E73;&#x5747;&#x3002;&#x5982;&#x679C; <code>size_average=False</code> <code>loss</code> &#x4F1A;&#x88AB;&#x7D2F;&#x52A0;&#x8D77;&#x6765;&#x3002;</p>
<h3 id="class-torchnnbcelossweightnone-sizeaveragetruesource">class torch.nn.BCELoss(weight=None, size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#BCELoss" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x8BA1;&#x7B97; <code>target</code> &#x4E0E; <code>output</code> &#x4E4B;&#x95F4;&#x7684;&#x4E8C;&#x8FDB;&#x5236;&#x4EA4;&#x53C9;&#x71B5;&#x3002;
<script type="math/tex; mode=display">
loss(o,t)=-\frac{1}{n}\sum_i(t[i]* log(o[i])+(1-t[i])* log(1-o[i]))
</script>
&#x5982;&#x679C;<code>weight</code>&#x88AB;&#x6307;&#x5B9A; &#xFF1A;
<script type="math/tex; mode=display">
loss(o,t)=-\frac{1}{n}\sum_iweights[i]* (t[i]* log(o[i])+(1-t[i])* log(1-o[i]))
</script></p>
<p>&#x8FD9;&#x4E2A;&#x7528;&#x4E8E;&#x8BA1;&#x7B97; <code>auto-encoder</code> &#x7684; <code>reconstruction error</code>&#x3002;&#x6CE8;&#x610F; 0&lt;=target[i]&lt;=1&#x3002;</p>
<p>&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;loss&#x4F1A;&#x57FA;&#x4E8E;<code>element</code>&#x5E73;&#x5747;&#xFF0C;&#x5982;&#x679C;<code>size_average=False</code>&#x7684;&#x8BDD;&#xFF0C;<code>loss</code>&#x4F1A;&#x88AB;&#x7D2F;&#x52A0;&#x3002;</p>
<h3 id="class-torchnnmarginrankinglossmargin0-sizeaveragetruesource">class torch.nn.MarginRankingLoss(margin=0, size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#MarginRankingLoss" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x6807;&#x51C6;&#xFF0C;&#x7ED9;&#x5B9A;&#x8F93;&#x5165; $x1$,$x2$&#x4E24;&#x4E2A;1-D mini-batch Tensor&apos;s&#xFF0C;&#x548C;&#x4E00;&#x4E2A;$y$(1-D mini-batch tensor) ,$y$&#x91CC;&#x9762;&#x7684;&#x503C;&#x53EA;&#x80FD;&#x662F;-1&#x6216;1&#x3002;</p>
<p>&#x5982;&#x679C; <code>y=1</code>&#xFF0C;&#x4EE3;&#x8868;&#x7B2C;&#x4E00;&#x4E2A;&#x8F93;&#x5165;&#x7684;&#x503C;&#x5E94;&#x8BE5;&#x5927;&#x4E8E;&#x7B2C;&#x4E8C;&#x4E2A;&#x8F93;&#x5165;&#x7684;&#x503C;&#xFF0C;&#x5982;&#x679C;<code>y=-1</code>&#x7684;&#x8BDD;&#xFF0C;&#x5219;&#x76F8;&#x53CD;&#x3002;</p>
<p><code>mini-batch</code>&#x4E2D;&#x6BCF;&#x4E2A;&#x6837;&#x672C;&#x7684;loss&#x7684;&#x8BA1;&#x7B97;&#x516C;&#x5F0F;&#x5982;&#x4E0B;&#xFF1A;</p>
<p><script type="math/tex; ">loss(x, y) = max(0, -y * (x1 - x2) + margin)</script></p>
<p>&#x5982;&#x679C;<code>size_average=True</code>,&#x90A3;&#x4E48;&#x6C42;&#x51FA;&#x7684;<code>loss</code>&#x5C06;&#x4F1A;&#x5BF9;<code>mini-batch</code>&#x6C42;&#x5E73;&#x5747;&#xFF0C;&#x53CD;&#x4E4B;&#xFF0C;&#x6C42;&#x51FA;&#x7684;<code>loss</code>&#x4F1A;&#x7D2F;&#x52A0;&#x3002;&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;<code>size_average=True</code>&#x3002;</p>
<h3 id="class-torchnnhingeembeddinglosssizeaveragetruesource">class torch.nn.HingeEmbeddingLoss(size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#HingeEmbeddingLoss" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x7ED9;&#x5B9A;&#x4E00;&#x4E2A;&#x8F93;&#x5165; $x$(2-D mini-batch tensor)&#x548C;&#x5BF9;&#x5E94;&#x7684; &#x6807;&#x7B7E; $y$ (1-D tensor,1,-1)&#xFF0C;&#x6B64;&#x51FD;&#x6570;&#x7528;&#x6765;&#x8BA1;&#x7B97;&#x4E4B;&#x95F4;&#x7684;&#x635F;&#x5931;&#x503C;&#x3002;&#x8FD9;&#x4E2A;<code>loss</code>&#x901A;&#x5E38;&#x7528;&#x6765;&#x6D4B;&#x91CF;&#x4E24;&#x4E2A;&#x8F93;&#x5165;&#x662F;&#x5426;&#x76F8;&#x4F3C;&#xFF0C;&#x5373;&#xFF1A;&#x4F7F;&#x7528;L1 &#x6210;&#x5BF9;&#x8DDD;&#x79BB;&#x3002;&#x5178;&#x578B;&#x662F;&#x7528;&#x5728;&#x5B66;&#x4E60;&#x975E;&#x7EBF;&#x6027; <code>embedding</code>&#x6216;&#x8005;&#x534A;&#x76D1;&#x7763;&#x5B66;&#x4E60;&#x4E2D;&#xFF1A;</p>
<p><script type="math/tex; mode=display">
loss(x,y)=\frac{1}{n}\sum_i
\begin{cases}
x_i, &\text if~y_i==1 \\
max(0, margin-x_i), &if ~y_i==-1
\end{cases}
</script>
$x$&#x548C;$y$&#x53EF;&#x4EE5;&#x662F;&#x4EFB;&#x610F;&#x5F62;&#x72B6;&#xFF0C;&#x4E14;&#x90FD;&#x6709;<code>n</code>&#x7684;&#x5143;&#x7D20;&#xFF0C;<code>loss</code>&#x7684;&#x6C42;&#x548C;&#x64CD;&#x4F5C;&#x4F5C;&#x7528;&#x5728;&#x6240;&#x6709;&#x7684;&#x5143;&#x7D20;&#x4E0A;&#xFF0C;&#x7136;&#x540E;&#x9664;&#x4EE5;<code>n</code>&#x3002;&#x5982;&#x679C;&#x60A8;&#x4E0D;&#x60F3;&#x9664;&#x4EE5;<code>n</code>&#x7684;&#x8BDD;&#xFF0C;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x8BBE;&#x7F6E;<code>size_average=False</code>&#x3002;</p>
<p><code>margin</code>&#x7684;&#x9ED8;&#x8BA4;&#x503C;&#x4E3A;1,&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x6784;&#x9020;&#x51FD;&#x6570;&#x6765;&#x8BBE;&#x7F6E;&#x3002;</p>
<h3 id="class-torchnnmultilabelmarginlosssizeaveragetruesource">class torch.nn.MultiLabelMarginLoss(size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#MultiLabelMarginLoss" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x8BA1;&#x7B97;&#x591A;&#x6807;&#x7B7E;&#x5206;&#x7C7B;&#x7684; <code>hinge loss</code>(<code>margin-based loss</code>) &#xFF0C;&#x8BA1;&#x7B97;<code>loss</code>&#x65F6;&#x9700;&#x8981;&#x4E24;&#x4E2A;&#x8F93;&#x5165;&#xFF1A; input x(<code>2-D mini-batch Tensor</code>)&#xFF0C;&#x548C; output y(<code>2-D tensor</code>&#x8868;&#x793A;mini-batch&#x4E2D;&#x6837;&#x672C;&#x7C7B;&#x522B;&#x7684;&#x7D22;&#x5F15;)&#x3002;</p>
<p><script type="math/tex; mode=display">
loss(x, y) = \frac{1}{x.size(0)}\sum_{i=0,j=0}^{I,J}(max(0, 1 - (x[y[j]] - x[i])))
</script>
&#x5176;&#x4E2D; <code>I=x.size(0),J=y.size(0)</code>&#x3002;&#x5BF9;&#x4E8E;&#x6240;&#x6709;&#x7684; <code>i</code>&#x548C; <code>j</code>&#xFF0C;&#x6EE1;&#x8DB3; $y[j]\neq0, i \neq y[j]$</p>
<p><code>x</code> &#x548C; <code>y</code> &#x5FC5;&#x987B;&#x5177;&#x6709;&#x540C;&#x6837;&#x7684; <code>size</code>&#x3002;</p>
<p>&#x8FD9;&#x4E2A;&#x6807;&#x51C6;&#x4EC5;&#x8003;&#x8651;&#x4E86;&#x7B2C;&#x4E00;&#x4E2A;&#x975E;&#x96F6; <code>y[j] targets</code>
&#x6B64;&#x6807;&#x51C6;&#x5141;&#x8BB8;&#x4E86;&#xFF0C;&#x5BF9;&#x4E8E;&#x6BCF;&#x4E2A;&#x6837;&#x672C;&#x6765;&#x8BF4;&#xFF0C;&#x53EF;&#x4EE5;&#x6709;&#x591A;&#x4E2A;&#x7C7B;&#x522B;&#x3002;</p>
<h3 id="class-torchnnsmoothl1losssizeaveragetruesource">class torch.nn.SmoothL1Loss(size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#SmoothL1Loss" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x5E73;&#x6ED1;&#x7248;<code>L1 loss</code>&#x3002;</p>
<p>loss&#x7684;&#x516C;&#x5F0F;&#x5982;&#x4E0B;&#xFF1A;
<script type="math/tex; mode=display">
loss(x, y) = \frac{1}{n}\sum_i
\begin{cases}
0.5*(x_i-y_i)^2, & if~|x_i - y_i| < 1\\
|x_i - y_i| - 0.5,  & otherwise    
\end{cases}
</script>
&#x6B64;loss&#x5BF9;&#x4E8E;&#x5F02;&#x5E38;&#x70B9;&#x7684;&#x654F;&#x611F;&#x6027;&#x4E0D;&#x5982;<code>MSELoss</code>&#xFF0C;&#x800C;&#x4E14;&#xFF0C;&#x5728;&#x67D0;&#x4E9B;&#x60C5;&#x51B5;&#x4E0B;&#x9632;&#x6B62;&#x4E86;&#x68AF;&#x5EA6;&#x7206;&#x70B8;&#xFF0C;(&#x53C2;&#x7167; <code>Fast R-CNN</code>)&#x3002;&#x8FD9;&#x4E2A;<code>loss</code>&#x6709;&#x65F6;&#x4E5F;&#x88AB;&#x79F0;&#x4E3A; <code>Huber loss</code>&#x3002;</p>
<p>x &#x548C; y &#x53EF;&#x4EE5;&#x662F;&#x4EFB;&#x4F55;&#x5305;&#x542B;<code>n</code>&#x4E2A;&#x5143;&#x7D20;&#x7684;tensor&#x3002;&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6C42;&#x51FA;&#x6765;&#x7684;<code>loss</code>&#x4F1A;&#x9664;&#x4EE5;<code>n</code>&#xFF0C;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x8BBE;&#x7F6E;<code>size_average=True</code>&#x4F7F;loss&#x7D2F;&#x52A0;&#x3002;</p>
<h3 id="class-torchnnsoftmarginlosssizeaveragetruesource">class torch.nn.SoftMarginLoss(size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#SoftMarginLoss" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x6807;&#x51C6;&#xFF0C;&#x7528;&#x6765;&#x4F18;&#x5316;2&#x5206;&#x7C7B;&#x7684;<code>logistic loss</code>&#x3002;&#x8F93;&#x5165;&#x4E3A; <code>x</code>&#xFF08;&#x4E00;&#x4E2A; 2-D mini-batch Tensor&#xFF09;&#x548C; &#x76EE;&#x6807;<code>y</code>&#xFF08;&#x4E00;&#x4E2A;&#x5305;&#x542B;1&#x6216;-1&#x7684;Tensor&#xFF09;&#x3002;
<script type="math/tex; mode=display">
loss(x, y) = \frac{1}{x.nelement()}\sum_i (log(1 + exp(-y[i]* x[i])))
</script>
&#x5982;&#x679C;&#x6C42;&#x51FA;&#x7684;<code>loss</code>&#x4E0D;&#x60F3;&#x88AB;&#x5E73;&#x5747;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x8BBE;&#x7F6E;<code>size_average=False</code>&#x3002;</p>
<h3 id="class-torchnnmultilabelsoftmarginlossweightnone-sizeaveragetruesource">class torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#MultiLabelSoftMarginLoss" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x6807;&#x51C6;&#xFF0C;&#x57FA;&#x4E8E;&#x8F93;&#x5165;x&#x548C;&#x76EE;&#x6807;y&#x7684; <code>max-entropy</code>&#xFF0C;&#x4F18;&#x5316;&#x591A;&#x6807;&#x7B7E; <code>one-versus-all</code> &#x7684;&#x635F;&#x5931;&#x3002;<code>x</code>:2-D mini-batch Tensor;<code>y</code>:binary 2D Tensor&#x3002;&#x5BF9;&#x6BCF;&#x4E2A;mini-batch&#x4E2D;&#x7684;&#x6837;&#x672C;&#xFF0C;&#x5BF9;&#x5E94;&#x7684;loss&#x4E3A;&#xFF1A;
<script type="math/tex; mode=display">
loss(x, y) = - \frac{1}{x.nElement()}\sum_{i=0}^I y[i]\text{log}\frac{exp(x[i])}{(1 + exp(x[i])}
                      + (1-y[i])\text{log}\frac{1}{1+exp(x[i])}
</script>
&#x5176;&#x4E2D; <code>I=x.nElement()-1</code>, $y[i] \in {0,1}$&#xFF0C;<code>y</code> &#x548C; <code>x</code>&#x5FC5;&#x987B;&#x8981;&#x6709;&#x540C;&#x6837;<code>size</code>&#x3002;</p>
<h3 id="class-torchnncosineembeddinglossmargin0-sizeaveragetruesource">class torch.nn.CosineEmbeddingLoss(margin=0, size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#CosineEmbeddingLoss" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x7ED9;&#x5B9A; &#x8F93;&#x5165; <code>Tensors</code>&#xFF0C;<code>x1</code>, <code>x2</code> &#x548C;&#x4E00;&#x4E2A;&#x6807;&#x7B7E;Tensor <code>y</code>(&#x5143;&#x7D20;&#x7684;&#x503C;&#x4E3A;1&#x6216;-1)&#x3002;&#x6B64;&#x6807;&#x51C6;&#x4F7F;&#x7528;<code>cosine</code>&#x8DDD;&#x79BB;&#x6D4B;&#x91CF;&#x4E24;&#x4E2A;&#x8F93;&#x5165;&#x662F;&#x5426;&#x76F8;&#x4F3C;&#xFF0C;&#x4E00;&#x822C;&#x7528;&#x6765;&#x7528;&#x6765;&#x5B66;&#x4E60;&#x975E;&#x7EBF;&#x6027;<code>embedding</code>&#x6216;&#x8005;&#x534A;&#x76D1;&#x7763;&#x5B66;&#x4E60;&#x3002;</p>
<p><code>margin</code>&#x5E94;&#x8BE5;&#x662F;-1&#x5230;1&#x4E4B;&#x95F4;&#x7684;&#x503C;&#xFF0C;&#x5EFA;&#x8BAE;&#x4F7F;&#x7528;0&#x5230;0.5&#x3002;&#x5982;&#x679C;&#x6CA1;&#x6709;&#x4F20;&#x5165;<code>margin</code>&#x5B9E;&#x53C2;&#xFF0C;&#x9ED8;&#x8BA4;&#x503C;&#x4E3A;0&#x3002;</p>
<p>&#x6BCF;&#x4E2A;&#x6837;&#x672C;&#x7684;loss&#x662F;&#xFF1A;
<script type="math/tex; mode=display">
loss(x, y) =
\begin{cases}
1 - cos(x1, x2),              &if~y ==  1 \\
max(0, cos(x1, x2) - margin), &if~y == -1
\end{cases}
</script>
&#x5982;&#x679C;<code>size_average=True</code> &#x6C42;&#x51FA;&#x7684;loss&#x4F1A;&#x5BF9;batch&#x6C42;&#x5747;&#x503C;&#xFF0C;&#x5982;&#x679C;<code>size_average=False</code>&#x7684;&#x8BDD;&#xFF0C;&#x5219;&#x4F1A;&#x7D2F;&#x52A0;<code>loss</code>&#x3002;&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;<code>size_average=True</code>&#x3002;</p>
<h3 id="class-torchnnmultimarginlossp1-margin1-weightnone-sizeaveragetruesource">class torch.nn.MultiMarginLoss(p=1, margin=1, weight=None, size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#MultiMarginLoss" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x7528;&#x6765;&#x8BA1;&#x7B97;multi-class classification&#x7684;hinge loss&#xFF08;magin-based loss&#xFF09;&#x3002;&#x8F93;&#x5165;&#x662F; <code>x</code>(2D mini-batch Tensor), <code>y</code>(1D Tensor)&#x5305;&#x542B;&#x7C7B;&#x522B;&#x7684;&#x7D22;&#x5F15;&#xFF0C; <code>0 &lt;= y &lt;= x.size(1))</code>&#x3002;</p>
<p>&#x5BF9;&#x6BCF;&#x4E2A;mini-batch&#x6837;&#x672C;&#xFF1A;
<script type="math/tex; mode=display">
loss(x, y) = \frac{1}{x.size(0)}\sum_{i=0}^I(max(0, margin - x[y] + x[i])^p)
</script>
&#x5176;&#x4E2D; <code>I=x.size(0)</code> $i\neq y$&#x3002;
&#x53EF;&#x9009;&#x62E9;&#x7684;&#xFF0C;&#x5982;&#x679C;&#x60A8;&#x4E0D;&#x60F3;&#x6240;&#x6709;&#x7684;&#x7C7B;&#x62E5;&#x6709;&#x540C;&#x6837;&#x7684;&#x6743;&#x91CD;&#x7684;&#x8BDD;&#xFF0C;&#x60A8;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x5728;&#x6784;&#x9020;&#x51FD;&#x6570;&#x4E2D;&#x4F20;&#x5165;<code>weights</code>&#x53C2;&#x6570;&#x6765;&#x89E3;&#x51B3;&#x8FD9;&#x4E2A;&#x95EE;&#x9898;&#xFF0C;<code>weights</code>&#x662F;&#x4E00;&#x4E2A;1D&#x6743;&#x91CD;Tensor&#x3002;</p>
<p>&#x4F20;&#x5165;weights&#x540E;&#xFF0C;loss&#x51FD;&#x6570;&#x53D8;&#x4E3A;&#xFF1A;
<script type="math/tex; mode=display">
loss(x, y) = \frac{1}{x.size(0)}\sum_imax(0, w[y] * (margin - x[y] - x[i]))^p
</script>
&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6C42;&#x51FA;&#x7684;loss&#x4F1A;&#x5BF9;mini-batch&#x53D6;&#x5E73;&#x5747;&#xFF0C;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x8BBE;&#x7F6E;<code>size_average=False</code>&#x6765;&#x53D6;&#x6D88;&#x53D6;&#x5E73;&#x5747;&#x64CD;&#x4F5C;&#x3002;</p>
<h2 id="vision-layers">Vision layers</h2>
<h3 id="class-torchnnpixelshuffleupscalefactorsource">class torch.nn.PixelShuffle(upscale_factor)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/pixelshuffle.html#PixelShuffle" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x5C06;shape&#x4E3A;$[N, C<em>r^2, H, W]$&#x7684;<code>Tensor</code>&#x91CD;&#x65B0;&#x6392;&#x5217;&#x4E3A;shape&#x4E3A;$[N, C, H</em>r, W*r]$&#x7684;Tensor&#x3002;
&#x5F53;&#x4F7F;&#x7528;<code>stride=1/r</code> &#x7684;sub-pixel&#x5377;&#x79EF;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x8FD9;&#x4E2A;&#x65B9;&#x6CD5;&#x662F;&#x975E;&#x5E38;&#x6709;&#x7528;&#x7684;&#x3002;</p>
<p>&#x8BF7;&#x770B;paper<a href="https://arxiv.org/abs/1609.05158" target="_blank">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network by Shi et. al (2016)</a> &#x83B7;&#x53D6;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;&#x3002;</p>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;&#xFF1A;</p>
<ul>
<li>upscale_factor (int) &#x2013; &#x589E;&#x52A0;&#x7A7A;&#x95F4;&#x5206;&#x8FA8;&#x7387;&#x7684;&#x56E0;&#x5B50;</li>
</ul>
<p>Shape:</p>
<ul>
<li><p>Input: $[N,C*upscale_factor^2,H,W$]</p>
</li>
<li><p>Output: $[N,C,H<em>upscale_factor,W</em>upscale_factor]$</p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;:</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>ps = nn.PixelShuffle(<span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input = autograd.Variable(torch.Tensor(<span class="hljs-number">1</span>, <span class="hljs-number">9</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>output = ps(input)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(output.size())
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])
</code></pre>
<h3 id="class-torchnnupsamplingnearest2dsizenone-scalefactornonesource">class torch.nn.UpsamplingNearest2d(size=None, scale_factor=None)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/upsampling.html#UpsamplingNearest2d" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x5BF9;&#x4E8E;&#x591A;channel &#x8F93;&#x5165; &#x8FDB;&#x884C; <code>2-D</code> &#x6700;&#x8FD1;&#x90BB;&#x4E0A;&#x91C7;&#x6837;&#x3002;</p>
<p>&#x53EF;&#x4EE5;&#x901A;&#x8FC7;<code>size</code>&#x6216;&#x8005;<code>scale_factor</code>&#x6765;&#x6307;&#x5B9A;&#x4E0A;&#x91C7;&#x6837;&#x540E;&#x7684;&#x56FE;&#x7247;&#x5927;&#x5C0F;&#x3002;</p>
<p>&#x5F53;&#x7ED9;&#x5B9A;<code>size</code>&#x65F6;&#xFF0C;<code>size</code>&#x7684;&#x503C;&#x5C06;&#x4F1A;&#x662F;&#x8F93;&#x51FA;&#x56FE;&#x7247;&#x7684;&#x5927;&#x5C0F;&#x3002;</p>
<p>&#x53C2;&#x6570;&#xFF1A;</p>
<ul>
<li>size (tuple, optional) &#x2013; &#x4E00;&#x4E2A;&#x5305;&#x542B;&#x4E24;&#x4E2A;&#x6574;&#x6570;&#x7684;&#x5143;&#x7EC4; (H_out, W_out)&#x6307;&#x5B9A;&#x4E86;&#x8F93;&#x51FA;&#x7684;&#x957F;&#x5BBD;</li>
<li>scale_factor (int, optional) &#x2013; &#x957F;&#x548C;&#x5BBD;&#x7684;&#x4E00;&#x4E2A;&#x4E58;&#x5B50;</li>
</ul>
<p>&#x5F62;&#x72B6;&#xFF1A;</p>
<ul>
<li>Input: (N,C,H_in,W_in)</li>
<li>Output: (N,C,H_out,W_out)  Hout=floor(H_in&#x2217;scale_factor) Wout=floor(W_in&#x2217;scale_factor)</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>inp
Variable containing:
(<span class="hljs-number">0</span> ,<span class="hljs-number">0</span> ,.,.) =
  <span class="hljs-number">1</span>  <span class="hljs-number">2</span>
  <span class="hljs-number">3</span>  <span class="hljs-number">4</span>
[torch.FloatTensor of size <span class="hljs-number">1</span>x1x2x2]

<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.UpsamplingNearest2d(scale_factor=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>m(inp)
Variable containing:
(<span class="hljs-number">0</span> ,<span class="hljs-number">0</span> ,.,.) =
  <span class="hljs-number">1</span>  <span class="hljs-number">1</span>  <span class="hljs-number">2</span>  <span class="hljs-number">2</span>
  <span class="hljs-number">1</span>  <span class="hljs-number">1</span>  <span class="hljs-number">2</span>  <span class="hljs-number">2</span>
  <span class="hljs-number">3</span>  <span class="hljs-number">3</span>  <span class="hljs-number">4</span>  <span class="hljs-number">4</span>
  <span class="hljs-number">3</span>  <span class="hljs-number">3</span>  <span class="hljs-number">4</span>  <span class="hljs-number">4</span>
[torch.FloatTensor of size <span class="hljs-number">1</span>x1x4x4]
</code></pre>
<h3 id="class-torchnnupsamplingbilinear2dsizenone-scalefactornonesource">class torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/upsampling.html#UpsamplingBilinear2d" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x5BF9;&#x4E8E;&#x591A;channel &#x8F93;&#x5165; &#x8FDB;&#x884C; <code>2-D</code> <code>bilinear</code> &#x4E0A;&#x91C7;&#x6837;&#x3002;</p>
<p>&#x53EF;&#x4EE5;&#x901A;&#x8FC7;<code>size</code>&#x6216;&#x8005;<code>scale_factor</code>&#x6765;&#x6307;&#x5B9A;&#x4E0A;&#x91C7;&#x6837;&#x540E;&#x7684;&#x56FE;&#x7247;&#x5927;&#x5C0F;&#x3002;</p>
<p>&#x5F53;&#x7ED9;&#x5B9A;<code>size</code>&#x65F6;&#xFF0C;<code>size</code>&#x7684;&#x503C;&#x5C06;&#x4F1A;&#x662F;&#x8F93;&#x51FA;&#x56FE;&#x7247;&#x7684;&#x5927;&#x5C0F;&#x3002;</p>
<p>&#x53C2;&#x6570;&#xFF1A;</p>
<ul>
<li>size (tuple, optional) &#x2013; &#x4E00;&#x4E2A;&#x5305;&#x542B;&#x4E24;&#x4E2A;&#x6574;&#x6570;&#x7684;&#x5143;&#x7EC4; (H_out, W_out)&#x6307;&#x5B9A;&#x4E86;&#x8F93;&#x51FA;&#x7684;&#x957F;&#x5BBD;</li>
<li>scale_factor (int, optional) &#x2013; &#x957F;&#x548C;&#x5BBD;&#x7684;&#x4E00;&#x4E2A;&#x4E58;&#x5B50;</li>
</ul>
<p>&#x5F62;&#x72B6;&#xFF1A;</p>
<ul>
<li>Input: (N,C,H_in,W_in)</li>
<li>Output: (N,C,H_out,W_out)  Hout=floor(H_in&#x2217;scale_factor) Wout=floor(W_in&#x2217;scale_factor)</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-meta">&gt;&gt;&gt; </span>inp
Variable containing:
(<span class="hljs-number">0</span> ,<span class="hljs-number">0</span> ,.,.) =
  <span class="hljs-number">1</span>  <span class="hljs-number">2</span>
  <span class="hljs-number">3</span>  <span class="hljs-number">4</span>
[torch.FloatTensor of size <span class="hljs-number">1</span>x1x2x2]

<span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.UpsamplingBilinear2d(scale_factor=<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>m(inp)
Variable containing:
(<span class="hljs-number">0</span> ,<span class="hljs-number">0</span> ,.,.) =
  <span class="hljs-number">1.0000</span>  <span class="hljs-number">1.3333</span>  <span class="hljs-number">1.6667</span>  <span class="hljs-number">2.0000</span>
  <span class="hljs-number">1.6667</span>  <span class="hljs-number">2.0000</span>  <span class="hljs-number">2.3333</span>  <span class="hljs-number">2.6667</span>
  <span class="hljs-number">2.3333</span>  <span class="hljs-number">2.6667</span>  <span class="hljs-number">3.0000</span>  <span class="hljs-number">3.3333</span>
  <span class="hljs-number">3.0000</span>  <span class="hljs-number">3.3333</span>  <span class="hljs-number">3.6667</span>  <span class="hljs-number">4.0000</span>
[torch.FloatTensor of size <span class="hljs-number">1</span>x1x4x4]
</code></pre>
<h2 id="multi-gpu-layers">Multi-GPU layers</h2>
<h3 id="class-torchnndataparallelmodule-deviceidsnone-outputdevicenone-dim0source">class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)<a href="https://pytorch.org/docs/_modules/torch/nn/parallel/data_parallel.html#DataParallel" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x5728;&#x6A21;&#x5757;&#x7EA7;&#x522B;&#x4E0A;&#x5B9E;&#x73B0;&#x6570;&#x636E;&#x5E76;&#x884C;&#x3002;</p>
<p>&#x6B64;&#x5BB9;&#x5668;&#x901A;&#x8FC7;&#x5C06;<code>mini-batch</code>&#x5212;&#x5206;&#x5230;&#x4E0D;&#x540C;&#x7684;&#x8BBE;&#x5907;&#x4E0A;&#x6765;&#x5B9E;&#x73B0;&#x7ED9;&#x5B9A;<code>module</code>&#x7684;&#x5E76;&#x884C;&#x3002;&#x5728;<code>forward</code>&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;<code>module</code>&#x4F1A;&#x5728;&#x6BCF;&#x4E2A;&#x8BBE;&#x5907;&#x4E0A;&#x90FD;&#x590D;&#x5236;&#x4E00;&#x904D;&#xFF0C;&#x6BCF;&#x4E2A;&#x526F;&#x672C;&#x90FD;&#x4F1A;&#x5904;&#x7406;&#x90E8;&#x5206;&#x8F93;&#x5165;&#x3002;&#x5728;<code>backward</code>&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x526F;&#x672C;&#x4E0A;&#x7684;&#x68AF;&#x5EA6;&#x4F1A;&#x7D2F;&#x52A0;&#x5230;&#x539F;&#x59CB;<code>module</code>&#x4E0A;&#x3002;</p>
<p>batch&#x7684;&#x5927;&#x5C0F;&#x5E94;&#x8BE5;&#x5927;&#x4E8E;&#x6240;&#x4F7F;&#x7528;&#x7684;GPU&#x7684;&#x6570;&#x91CF;&#x3002;&#x8FD8;&#x5E94;&#x5F53;&#x662F;GPU&#x4E2A;&#x6570;&#x7684;&#x6574;&#x6570;&#x500D;&#xFF0C;&#x8FD9;&#x6837;&#x5212;&#x5206;&#x51FA;&#x6765;&#x7684;&#x6BCF;&#x4E00;&#x5757;&#x90FD;&#x4F1A;&#x6709;&#x76F8;&#x540C;&#x7684;&#x6837;&#x672C;&#x6570;&#x91CF;&#x3002;</p>
<p>&#x8BF7;&#x770B;: <a href="">Use nn.DataParallel instead of multiprocessing</a></p>
<p>&#x9664;&#x4E86;<code>Tensor</code>&#xFF0C;&#x4EFB;&#x4F55;&#x4F4D;&#x7F6E;&#x53C2;&#x6570;&#x548C;&#x5173;&#x952E;&#x5B57;&#x53C2;&#x6570;&#x90FD;&#x53EF;&#x4EE5;&#x4F20;&#x5230;DataParallel&#x4E2D;&#x3002;&#x6240;&#x6709;&#x7684;&#x53D8;&#x91CF;&#x4F1A;&#x901A;&#x8FC7;&#x6307;&#x5B9A;&#x7684;<code>dim</code>&#x6765;&#x5212;&#x5206;&#xFF08;&#x9ED8;&#x8BA4;&#x503C;&#x4E3A;0&#xFF09;&#x3002;&#x539F;&#x59CB;&#x7C7B;&#x578B;&#x5C06;&#x4F1A;&#x88AB;&#x5E7F;&#x64AD;&#xFF0C;&#x4F46;&#x662F;&#x6240;&#x6709;&#x7684;&#x5176;&#x5B83;&#x7C7B;&#x578B;&#x90FD;&#x4F1A;&#x88AB;&#x6D45;&#x590D;&#x5236;&#x3002;&#x6240;&#x4EE5;&#x5982;&#x679C;&#x5728;&#x6A21;&#x578B;&#x7684;<code>forward</code>&#x8FC7;&#x7A0B;&#x4E2D;&#x5199;&#x5165;&#x7684;&#x8BDD;&#xFF0C;&#x5C06;&#x4F1A;&#x88AB;&#x635F;&#x574F;&#x3002;</p>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;&#xFF1A;</p>
<ul>
<li>module &#x2013; &#x8981;&#x88AB;&#x5E76;&#x884C;&#x7684;module</li>
<li>device_ids &#x2013; CUDA&#x8BBE;&#x5907;&#xFF0C;&#x9ED8;&#x8BA4;&#x4E3A;&#x6240;&#x6709;&#x8BBE;&#x5907;&#x3002;</li>
<li>output_device &#x2013; &#x8F93;&#x51FA;&#x8BBE;&#x5907;&#xFF08;&#x9ED8;&#x8BA4;&#x4E3A;device_ids[0]&#xFF09;</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"> net = torch.nn.DataParallel(model, device_ids=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>])
 output = net(input_var)
</code></pre>
<h2 id="utilities">Utilities</h2>
<p>&#x5DE5;&#x5177;&#x51FD;&#x6570;</p>
<h3 id="torchnnutilsclipgradnormparameters-maxnorm-normtype2source">torch.nn.utils.clip_grad_norm(parameters, max_norm, norm_type=2)<a href="https://pytorch.org/docs/_modules/torch/nn/utils/clip_grad.html#clip_grad_norm" target="_blank"><font size="2">[source]</font></a></h3>
<p>Clips gradient norm of an iterable of parameters.</p>
<p>&#x6B63;&#x5219;&#x9805;&#x7684;&#x503C;&#x7531;&#x6240;&#x6709;&#x7684;&#x68AF;&#x5EA6;&#x8BA1;&#x7B97;&#x51FA;&#x6765;&#xFF0C;&#x5C31;&#x50CF;&#x4ED6;&#x4EEC;&#x8FDE;&#x6210;&#x4E00;&#x4E2A;&#x5411;&#x91CF;&#x4E00;&#x6837;&#x3002;&#x68AF;&#x5EA6;&#x88AB;<code>in-place operation</code>&#x4FEE;&#x6539;&#x3002;</p>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;:</p>
<ul>
<li>parameters (Iterable[Variable]) &#x2013; &#x53EF;&#x8FED;&#x4EE3;&#x7684;<code>Variables</code>&#xFF0C;&#x5B83;&#x4EEC;&#x7684;&#x68AF;&#x5EA6;&#x5373;&#x5C06;&#x88AB;&#x6807;&#x51C6;&#x5316;&#x3002;</li>
<li>max_norm (float or int) &#x2013; <code>clip</code>&#x540E;&#xFF0C;<code>gradients</code> p-norm &#x503C;</li>
<li>norm_type (float or int) &#x2013; &#x6807;&#x51C6;&#x5316;&#x7684;&#x7C7B;&#x578B;&#xFF0C;p-norm. &#x53EF;&#x4EE5;&#x662F;<code>inf</code> &#x4EE3;&#x8868; infinity norm.</li>
</ul>
<p><a href="https://rorasa.wordpress.com/2012/05/13/l0-norm-l1-norm-l2-norm-l-infinity-norm/" target="_blank">&#x5173;&#x4E8E;norm</a></p>
<p>&#x8FD4;&#x56DE;&#x503C;:</p>
<p>&#x6240;&#x6709;&#x53C2;&#x6570;&#x7684;p-norm&#x503C;&#x3002;</p>
<h3 id="torchnnutilsrnnpackedsequencecls-data-batchsizessource">torch.nn.utils.rnn.PackedSequence(_cls, data, batch_sizes)<a href="https://pytorch.org/docs/_modules/torch/nn/utils/rnn.html#PackedSequence" target="_blank"><font size="2">[source]</font></a></h3>
<p>Holds the data and list of batch_sizes of a packed sequence.</p>
<p>All RNN modules accept packed sequences as inputs.
&#x6240;&#x6709;&#x7684;<code>RNN</code>&#x6A21;&#x5757;&#x90FD;&#x63A5;&#x6536;&#x8FD9;&#x79CD;&#x88AB;&#x5305;&#x88F9;&#x540E;&#x7684;&#x5E8F;&#x5217;&#x4F5C;&#x4E3A;&#x5B83;&#x4EEC;&#x7684;&#x8F93;&#x5165;&#x3002;</p>
<p><code>NOTE&#xFF1A;</code>
&#x8FD9;&#x4E2A;&#x7C7B;&#x7684;&#x5B9E;&#x4F8B;&#x4E0D;&#x80FD;&#x624B;&#x52A8;&#x521B;&#x5EFA;&#x3002;&#x5B83;&#x4EEC;&#x53EA;&#x80FD;&#x88AB; <code>pack_padded_sequence()</code> &#x5B9E;&#x4F8B;&#x5316;&#x3002;</p>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;:</p>
<ul>
<li><p>data (Variable) &#x2013; &#x5305;&#x542B;&#x6253;&#x5305;&#x540E;&#x5E8F;&#x5217;&#x7684;<code>Variable</code>&#x3002;</p>
</li>
<li><p>batch_sizes (list[int]) &#x2013; &#x5305;&#x542B; <code>mini-batch</code> &#x4E2D;&#x6BCF;&#x4E2A;&#x5E8F;&#x5217;&#x957F;&#x5EA6;&#x7684;&#x5217;&#x8868;&#x3002;</p>
</li>
</ul>
<h3 id="torchnnutilsrnnpackpaddedsequenceinput-lengths-batchfirstfalsesource">torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False)<a href="https://pytorch.org/docs/_modules/torch/nn/utils/rnn.html#PackedSequence" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x8FD9;&#x91CC;&#x7684;<code>pack</code>&#xFF0C;&#x7406;&#x89E3;&#x6210;&#x538B;&#x7D27;&#x6BD4;&#x8F83;&#x597D;&#x3002;
&#x5C06;&#x4E00;&#x4E2A; &#x586B;&#x5145;&#x8FC7;&#x7684;&#x53D8;&#x957F;&#x5E8F;&#x5217; &#x538B;&#x7D27;&#x3002;&#xFF08;&#x586B;&#x5145;&#x65F6;&#x5019;&#xFF0C;&#x4F1A;&#x6709;&#x5197;&#x4F59;&#xFF0C;&#x6240;&#x4EE5;&#x538B;&#x7D27;&#x4E00;&#x4E0B;&#xFF09;</p>
<p>&#x8F93;&#x5165;&#x7684;&#x5F62;&#x72B6;&#x53EF;&#x4EE5;&#x662F;(T&#xD7;B&#xD7;<em> )&#x3002;<code>T</code>&#x662F;&#x6700;&#x957F;&#x5E8F;&#x5217;&#x957F;&#x5EA6;&#xFF0C;<code>B</code>&#x662F;<code>batch size</code>&#xFF0C;`</em><code>&#x4EE3;&#x8868;&#x4EFB;&#x610F;&#x7EF4;&#x5EA6;(&#x53EF;&#x4EE5;&#x662F;0)&#x3002;&#x5982;&#x679C;</code>batch_first=True<code>&#x7684;&#x8BDD;&#xFF0C;&#x90A3;&#x4E48;&#x76F8;&#x5E94;&#x7684;</code>input size<code>&#x5C31;&#x662F;</code>(B&#xD7;T&#xD7;*)`&#x3002;</p>
<p><code>Variable</code>&#x4E2D;&#x4FDD;&#x5B58;&#x7684;&#x5E8F;&#x5217;&#xFF0C;&#x5E94;&#x8BE5;&#x6309;&#x5E8F;&#x5217;&#x957F;&#x5EA6;&#x7684;&#x957F;&#x77ED;&#x6392;&#x5E8F;&#xFF0C;&#x957F;&#x7684;&#x5728;&#x524D;&#xFF0C;&#x77ED;&#x7684;&#x5728;&#x540E;&#x3002;&#x5373;<code>input[:,0]</code>&#x4EE3;&#x8868;&#x7684;&#x662F;&#x6700;&#x957F;&#x7684;&#x5E8F;&#x5217;&#xFF0C;<code>input[:, B-1]</code>&#x4FDD;&#x5B58;&#x7684;&#x662F;&#x6700;&#x77ED;&#x7684;&#x5E8F;&#x5217;&#x3002;</p>
<p><code>NOTE&#xFF1A;</code>
&#x53EA;&#x8981;&#x662F;&#x7EF4;&#x5EA6;&#x5927;&#x4E8E;&#x7B49;&#x4E8E;2&#x7684;<code>input</code>&#x90FD;&#x53EF;&#x4EE5;&#x4F5C;&#x4E3A;&#x8FD9;&#x4E2A;&#x51FD;&#x6570;&#x7684;&#x53C2;&#x6570;&#x3002;&#x4F60;&#x53EF;&#x4EE5;&#x7528;&#x5B83;&#x6765;&#x6253;&#x5305;<code>labels</code>&#xFF0C;&#x7136;&#x540E;&#x7528;<code>RNN</code>&#x7684;&#x8F93;&#x51FA;&#x548C;&#x6253;&#x5305;&#x540E;&#x7684;<code>labels</code>&#x6765;&#x8BA1;&#x7B97;<code>loss</code>&#x3002;&#x901A;&#x8FC7;<code>PackedSequence</code>&#x5BF9;&#x8C61;&#x7684;<code>.data</code>&#x5C5E;&#x6027;&#x53EF;&#x4EE5;&#x83B7;&#x53D6; <code>Variable</code>&#x3002;</p>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;:</p>
<ul>
<li><p>input (Variable) &#x2013; &#x53D8;&#x957F;&#x5E8F;&#x5217; &#x88AB;&#x586B;&#x5145;&#x540E;&#x7684; batch</p>
</li>
<li><p>lengths (list[int]) &#x2013; <code>Variable</code> &#x4E2D; &#x6BCF;&#x4E2A;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#x3002;</p>
</li>
<li><p>batch_first (bool, optional) &#x2013; &#x5982;&#x679C;&#x662F;<code>True</code>&#xFF0C;input&#x7684;&#x5F62;&#x72B6;&#x5E94;&#x8BE5;&#x662F;<code>B*T*size</code>&#x3002;</p>
</li>
</ul>
<p>&#x8FD4;&#x56DE;&#x503C;:</p>
<p>&#x4E00;&#x4E2A;<code>PackedSequence</code> &#x5BF9;&#x8C61;&#x3002;</p>
<h3 id="torchnnutilsrnnpadpackedsequencesequence-batchfirstfalsesource">torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False)<a href="https://pytorch.org/docs/_modules/torch/nn/utils/rnn.html#pack_padded_sequence" target="_blank"><font size="2">[source]</font></a></h3>
<p>&#x586B;&#x5145;<code>packed_sequence</code>&#x3002;</p>
<p>&#x4E0A;&#x9762;&#x63D0;&#x5230;&#x7684;&#x51FD;&#x6570;&#x7684;&#x529F;&#x80FD;&#x662F;&#x5C06;&#x4E00;&#x4E2A;&#x586B;&#x5145;&#x540E;&#x7684;&#x53D8;&#x957F;&#x5E8F;&#x5217;&#x538B;&#x7D27;&#x3002; &#x8FD9;&#x4E2A;&#x64CD;&#x4F5C;&#x548C;pack_padded_sequence()&#x662F;&#x76F8;&#x53CD;&#x7684;&#x3002;&#x628A;&#x538B;&#x7D27;&#x7684;&#x5E8F;&#x5217;&#x518D;&#x586B;&#x5145;&#x56DE;&#x6765;&#x3002;</p>
<p>&#x8FD4;&#x56DE;&#x7684;Varaible&#x7684;&#x503C;&#x7684;<code>size</code>&#x662F; <code>T&#xD7;B&#xD7;*</code>, <code>T</code> &#x662F;&#x6700;&#x957F;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#xFF0C;<code>B</code> &#x662F; batch_size,&#x5982;&#x679C; <code>batch_first=True</code>,&#x90A3;&#x4E48;&#x8FD4;&#x56DE;&#x503C;&#x662F;<code>B&#xD7;T&#xD7;*</code>&#x3002;</p>
<p>Batch&#x4E2D;&#x7684;&#x5143;&#x7D20;&#x5C06;&#x4F1A;&#x4EE5;&#x5B83;&#x4EEC;&#x957F;&#x5EA6;&#x7684;&#x9006;&#x5E8F;&#x6392;&#x5217;&#x3002;</p>
<p>&#x53C2;&#x6570;&#x8BF4;&#x660E;:</p>
<ul>
<li><p>sequence (PackedSequence) &#x2013; &#x5C06;&#x8981;&#x88AB;&#x586B;&#x5145;&#x7684; batch</p>
</li>
<li><p>batch_first (bool, optional) &#x2013; &#x5982;&#x679C;&#x4E3A;True&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x6570;&#x636E;&#x7684;&#x683C;&#x5F0F;&#x4E3A; <code>B&#xD7;T&#xD7;*</code>&#x3002;</p>
</li>
</ul>
<p>&#x8FD4;&#x56DE;&#x503C;:
&#x4E00;&#x4E2A;tuple&#xFF0C;&#x5305;&#x542B;&#x88AB;&#x586B;&#x5145;&#x540E;&#x7684;&#x5E8F;&#x5217;&#xFF0C;&#x548C;batch&#x4E2D;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#x5217;&#x8868;&#x3002;</p>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">from</span> torch.autograd <span class="hljs-keyword">import</span> Variable
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> utils <span class="hljs-keyword">as</span> nn_utils
batch_size = <span class="hljs-number">2</span>
max_length = <span class="hljs-number">3</span>
hidden_size = <span class="hljs-number">2</span>
n_layers =<span class="hljs-number">1</span>

tensor_in = torch.FloatTensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]).resize_(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>)
tensor_in = Variable( tensor_in ) <span class="hljs-comment">#[batch, seq, feature], [2, 3, 1]</span>
seq_lengths = [<span class="hljs-number">3</span>,<span class="hljs-number">1</span>] <span class="hljs-comment"># list of integers holding information about the batch size at each sequence step</span>

<span class="hljs-comment"># pack it</span>
pack = nn_utils.rnn.pack_padded_sequence(tensor_in, seq_lengths, batch_first=<span class="hljs-keyword">True</span>)

<span class="hljs-comment"># initialize</span>
rnn = nn.RNN(<span class="hljs-number">1</span>, hidden_size, n_layers, batch_first=<span class="hljs-keyword">True</span>)
h0 = Variable(torch.randn(n_layers, batch_size, hidden_size))

<span class="hljs-comment">#forward</span>
out, _ = rnn(pack, h0)

<span class="hljs-comment"># unpack</span>
unpacked = nn_utils.rnn.pad_packed_sequence(out)
print(unpacked)
</code></pre>
<p><a href="https://discuss.pytorch.org/t/how-can-i-compute-seq2seq-loss-using-mask/861" target="_blank">&#x5173;&#x4E8E;packed_sequence</a></p>
<p><hr></p>
<div align="center">
    <p><a href="http://www.apachecn.org" target="_blank"><font face="KaiTi" size="6" color="red">&#x6211;&#x4EEC;&#x4E00;&#x76F4;&#x5728;&#x52AA;&#x529B;</font></a></p>
    <p><a href="https://github.com/apachecn/pytorch-doc-zh/" target="_blank">apachecn/pytorch-doc-zh</a></p>
    <p><iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=watch&amp;count=true&amp;v=2" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=star&amp;count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=fork&amp;count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <a target="_blank" href="../shang.qq.com/wpa/qunwpa"><img border="0" src="http://data.apachecn.org/img/logo/ApacheCN-group.png" alt="ML | ApacheCN" title="ML | ApacheCN"></a></p>
</div>
 <div style="text-align:center;margin:0 0 10.5px;">
     <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
     <ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-3565452474788507" data-ad-slot="2543897000">
     </ins>
     <script>(adsbygoogle = window.adsbygoogle || []).push({});</script>

    <script>
      var _hmt = _hmt || [];
      (function() {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91";
        var s = document.getElementsByTagName("script")[0]; 
        s.parentNode.insertBefore(hm, s);
      })();
    </script>

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-102475051-10"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-102475051-10');
    </script>
</div>

<p><meta name="google-site-verification" content="pyo9N70ZWyh8JB43bIu633mhxesJ1IcwWCZlM3jUfFo"></p>
<p><iframe src="https://www.bilibili.com/read/cv2710377" style="display:none"></iframe>
<img src="http://t.cn/AiCoDHwb" hidden="hidden"></p>
<div>
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
    <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script>
    <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
    <div id="gitalk-container"></div>
    <script type="text/javascript">
        const gitalk = new Gitalk({
        clientID: '2e62dee5b9896e2eede6',
        clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53',
        repo: 'pytorch-doc-zh',
        owner: 'apachecn',
        admin: ['jiangzhonglian', 'wizardforcel'],
        id: md5(location.pathname),
        distractionFreeMode: false
        })
        gitalk.render('gitalk-container')
    </script>
</div>

<footer class="page-footer"><span class="copyright">Copyright &#xA9; ibooker.org.cn 2019 all right reserved&#xFF0C;&#x7531; ApacheCN &#x56E2;&#x961F;&#x63D0;&#x4F9B;&#x652F;&#x6301;</span><span class="footer-modification">&#x8BE5;&#x6587;&#x4EF6;&#x4FEE;&#x8BA2;&#x65F6;&#x95F4;&#xFF1A; 
2019-12-27 08:05:23
</span></footer>
<script>console.log("plugin-popup....");document.onclick = function(e){ e.target.tagName === "IMG" && window.open(e.target.src,e.target.src)}</script><style>img{cursor:pointer}</style>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="Storage.html" class="navigation navigation-prev " aria-label="Previous page: torch.Storage">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="functional.html" class="navigation navigation-next " aria-label="Next page: torch.nn.functional">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"torch.nn","level":"1.3.4","depth":2,"next":{"title":"torch.nn.functional","level":"1.3.5","depth":2,"path":"package_references/functional.md","ref":"package_references/functional.md","articles":[]},"previous":{"title":"torch.Storage","level":"1.3.3","depth":2,"path":"package_references/Storage.md","ref":"package_references/Storage.md","articles":[]},"dir":"ltr"},"config":{"plugins":["github","github-buttons","-sharing","insert-logo","sharing-plus","back-to-top-button","code","copy-code-button","mathjax","pageview-count","edit-link","emphasize","alerts","auto-scroll-table","popup","hide-element","page-toc-button","tbfed-pagefooter","sitemap","advanced-emoji","expandable-chapters","splitter","search-pro"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright &copy ibooker.org.cn 2019","modify_label":"该文件修订时间： ","modify_format":"YYYY-MM-DD HH:mm:ss"},"emphasize":{},"github":{"url":"https://github.com/apachecn/pytorch-doc-zh"},"splitter":{},"search-pro":{},"search":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"auto-scroll-table":{},"popup":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"code":{"copyButtons":true},"hide-element":{"elements":[".gitbook-link"]},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"sitemap":{"hostname":"http://pytorch.apachecn.org"},"page-toc-button":{"maxTocDepth":4,"minTocSize":4},"back-to-top-button":{},"pageview-count":{},"alerts":{},"github-buttons":{"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"copy-code-button":{},"advanced-emoji":{"embedEmojis":false},"sharing":{"qq":false,"all":["qq","douban","facebook","google","linkedin","twitter","weibo","whatsapp"],"douban":false,"facebook":false,"weibo":true,"whatsapp":false,"twitter":false,"line":false,"google":false,"qzone":true},"edit-link":{"label":"编辑本页","base":"https://github.com/apachecn/pytorch-doc-zh/blob/master/docs/0.2"},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"insert-logo":{"style":"background: none; max-height: 150px; min-height: 150px","url":"http://data.apachecn.org/img/logo.jpg"},"expandable-chapters":{}},"my_links":{"sidebar":{"Home":"https://www.baidu.com"}},"theme":"default","author":"ApacheCN","my_plugins":["donate","todo","-lunr","-search","expandable-chapters-small","chapter-fold","expandable-chapters","expandable-chapters-small","back-to-top-button","ga","baidu","sitemap","tbfed-pagefooter","advanced-emoji","sectionx","page-treeview","simple-page-toc","ancre-navigation","theme-apachecn@git+https://github.com/apachecn/theme-apachecn#HEAD","pagefooter-apachecn@git+https://github.com/apachecn/gitbook-plugin-pagefooter-apachecn#HEAD"],"my_pluginsConfig":{"page-treeview":{"copyright":"Copyright &#169; aleen42","minHeaderCount":"2","minHeaderDeep":"2"},"ignores":["node_modules"],"simple-page-toc":{"maxDepth":3,"skipFirstH1":true},"page-copyright":{"wisdom":"Designer, Frontend Developer & overall web enthusiast","noPowered":false,"copyright":"Copyright &#169; 你的名字","style":"normal","timeColor":"#666","utcOffset":"8","format":"YYYY-MM-dd hh:mm:ss","signature":"你的签名","copyrightColor":"#666","description":"modified at"},"donate":{"wechat":"微信收款的二维码URL","alipay":"支付宝收款的二维码URL","title":"","button":"赏","alipayText":"支付宝打赏","wechatText":"微信打赏"},"page-toc-button":{"maxTocDepth":2,"minTocSize":2},"github-buttons":{"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"},{"user":"apachecn","width":"160","type":"follow","count":true,"size":"small"}]},"ga":{"token":"UA-102475051-10"},"baidu":{"token":"75439e2cbd22bdd813226000e9dcc12f"},"pagefooter-apachecn":{"copyright":"Copyright &copy ibooker.org.cn 2019","modify_label":"该文件修订时间： ","modify_format":"YYYY-MM-DD HH:mm:ss"}},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"Pytorch 中文文档","language":"zh-hans","gitbook":"*","description":"Pytorch 中文文档: 教程和文档"},"file":{"path":"package_references/torch-nn.md","mtime":"2019-12-27T08:05:23.679Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2019-12-27T08:07:00.068Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-insert-logo/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-code/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-copy-code-button/toggle.js"></script>
        
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-edit-link/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-alerts/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-auto-scroll-table/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-hide-element/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-page-toc-button/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search-pro/jquery.mark.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search-pro/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

